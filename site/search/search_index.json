{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Taikun CloudWorks Documentation","text":"<p>Welcome to Taikun CloudWorks!</p> <p>Our documentation is divided into the following sections:</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>This section describes the first steps in Taikun to create a functional Kubernetes Cluster for our new users. Please start with the From Login to Production Cluster article to understand how to register in Taikun.</p>"},{"location":"#taikun-cloudworks-overview","title":"Taikun CloudWorks Overview","text":"<p>Managing Kubernetes clusters, simplifying deployment, monitoring, and scaling. It offers centralized tools for managing infrastructure across multiple cloud environments and automates operations to increase efficiency and reliability.</p>"},{"location":"#navigating-in-taikun","title":"Navigating in Taikun","text":"<p>The Navigating in Taikun section describes all navigation elements. Use it whenever you want to learn about the features in every tab of our platform. Check the Taikun User Interface article to start getting acquainted with the user interface.</p>"},{"location":"#managing-your-projects","title":"Managing your Projects","text":"<p>This section contains all the necessary information to manage your Taikun projects. To understand the concept of Projects in Taikun, start with the Overview of Taikun Projects.</p>"},{"location":"#monitoring-your-projects","title":"Monitoring your Projects","text":"<p>Taikun is a versatile tool with many monitoring instruments to keep your Kubernetes clusters under control. Here, you will find a description of all available monitoring tools. Review the article on Available Monitoring Tools to see what is available.</p>"},{"location":"#profile-management","title":"Profile Management","text":"<p>There are a few profiles associated with Kubernetes usage. To understand the concept better, check the Profile Management section. The Creating Profiles article will help you create the necessary Profiles directly in Taikun.</p>"},{"location":"#account-management","title":"Account Management","text":"<p>This section explains how to manage your Taikun account and its users. To see the options available for your profile, start with the My Profile management article.</p>"},{"location":"#advanced-configuration","title":"Advanced Configuration","text":"<p>Here, you can read all about Taikun\u2019s advanced instruments. If you want to know how to configure Taikun CLI or how our API can be set up, this section is right for you!</p>"},{"location":"#taikun-dictionary","title":"Taikun Dictionary","text":"<p>Check Our Lingo article to become friends with Taikun terminology.</p>"},{"location":"#taikun-open-cloud-platform","title":"Taikun Open Cloud Platform","text":"<p>Flexible solution for managing Kubernetes clusters across different cloud environments. It supports integration with open-source tools, provides advanced security features, and optimizes resource usage for efficient infrastructure management.</p>"},{"location":"Account_Management/Add_and_Delete_Users/","title":"Add/Delete Users","text":"<p>A Partner or Manager of your account can easily manage users within Taikun. Here\u2019s how new User profiles can be created and deleted.</p>"},{"location":"Account_Management/Add_and_Delete_Users/#add-users","title":"Add Users","text":"<p>To add a user to your account, look for the button \u201cAdd User\u201d in the upper-right corner of the Users section. You\u2019ll need to provide the new user\u2019s:</p> <ul> <li>User name</li> <li>Display name (optional)</li> <li>Email address</li> </ul> <p>Additionally, you can specify the user\u2019s:  </p> <ul> <li>Organization (available to Partner),  </li> <li>Role (available to Partner and Manager).  </li> </ul> <p>Hit the \u201cAdd\u201d button to create a new profile.</p> <p></p> <p>Add User</p>"},{"location":"Account_Management/Add_and_Delete_Users/#delete-users","title":"Delete Users","text":"<p>To delete a user from your account, locate the user in the Users section and look for the option \u201cDelete.\u201d You need to confirm your action before a profile gets deleted.</p> <p></p> <p>Delete User</p>"},{"location":"Account_Management/Billing_of_your_Taikun_account/","title":"Billing of your Taikun account","text":"<p>If you registered your account on the Taikun webpage (Taikun.cloud), you can quickly check your subscription status by following these steps:</p>"},{"location":"Account_Management/Billing_of_your_Taikun_account/#how-to-check-your-subscription","title":"How to Check Your Subscription","text":"<ol> <li> <p>First Step    Log in to your account on our webpage.</p> </li> <li> <p>Second Step    Click on the Subscription tab in the left-hand menu.</p> </li> <li> <p>Third Step    You will see your current subscription plan and the expiration date of your subscription.</p> </li> </ol> <p>The Subscription menu lets you switch between various subscriptions by clicking on the chosen option.</p> <p>!!! Note     If you purchased your plan through one of our partners, you will need to reach out to that partner to make any changes to your account</p>"},{"location":"Account_Management/Billing_of_your_Taikun_account/#monitor-used-tcus","title":"Monitor Used TCUs","text":"<p>To calculate your resource usage, you can access the Usage Reports section. There, you can see the total amount of TCUs spent in your account and calculate the pricing of the resources you use.</p>"},{"location":"Account_Management/Delete_Account/","title":"Delete Account","text":"<p>You can delete an account in Taikun by following these steps:</p>"},{"location":"Account_Management/Delete_Account/#steps-to-delete-your-account","title":"Steps to Delete Your Account","text":"<ol> <li> <p>First Step    Head to your Profile section in the top right corner.</p> </li> <li> <p>Second Step    Press the \u201cDelete account\u201d icon.</p> </li> <li> <p>Third Step    Click \u201cyes, delete.\u201d</p> </li> </ol> <p>!!! warning</p> <pre><code>This action can\u2019t be **undone**. Make sure you want to delete your account.\n</code></pre> <p></p> <p>Delete Account</p>"},{"location":"Account_Management/Email_notifications/","title":"Email Notifications","text":""},{"location":"Account_Management/Email_notifications/#available-email-notifications","title":"Available Email Notifications","text":"<p>Taikun offers comprehensive action email services informing users about important events. These include emails for successful cluster creation, enabling or disabling monitoring, welcoming users to Taikun, and notifications regarding cluster failures or purging. Stay updated and connected with Taikun\u2019s action emails.</p>"},{"location":"Account_Management/Email_notifications/#enable-email-notifications","title":"Enable Email Notifications","text":"<p>To activate email notifications in Taikun, users need to turn on emails from the My Profile section.</p>"},{"location":"Account_Management/Email_notifications/#types-of-email-notifications","title":"Types of Email Notifications","text":""},{"location":"Account_Management/Email_notifications/#welcome-to-taikun","title":"Welcome to Taikun","text":"<p>Service messages are automated emails from the Taikun service to gain access to your account.</p> <p></p>"},{"location":"Account_Management/Email_notifications/#forgotten-password","title":"Forgotten Password","text":"<p>Email provides users with instructions and a link to reset passwords, ensuring secure access to their Taikun account.</p> <p></p>"},{"location":"Account_Management/Email_notifications/#cluster-creation","title":"Cluster Creation","text":"<p>Get notified when your cluster is created and ready to use.</p> <p></p>"},{"location":"Account_Management/Email_notifications/#monitoring-configuration","title":"Monitoring Configuration","text":"<p>Stay informed about your cluster\u2019s performance with notifications when monitoring is enabled or when monitoring gets deactivated.</p> <p></p>"},{"location":"Account_Management/Email_notifications/#failed-to-create-infrastructure","title":"Failed to Create Infrastructure","text":"<p>Email informs users about the unsuccessful attempt to establish the required infrastructure for their Project or service on Taikun.</p> <p></p>"},{"location":"Account_Management/Email_notifications/#failed-to-purge","title":"Failed to Purge","text":"<p>Email notifies users about the unsuccessful deletion or purging of specific data or resources within their Taikun environment.</p> <p></p>"},{"location":"Account_Management/Email_notifications/#enabling-policy","title":"Enabling Policy","text":"<p>Notifications when a policy profile is enabled.</p> <p></p>"},{"location":"Account_Management/Email_notifications/#creation-of-vms","title":"Creation of VM(s)","text":"<p>Email notifies users about succesful creation or purge VM(s). </p> <p></p>"},{"location":"Account_Management/Email_notifications/#enable-ai","title":"Enable AI","text":"<p>Use the power of AI to manage your Kubernetes cluster more efficiently. With our self-hosted solution, the AI assistant will help with fixing any issues with your Kubernetes cluster on the go.</p> <p></p>"},{"location":"Account_Management/Email_notifications/#scaling-up-server","title":"Scaling Up Server","text":"<p>Notification sent when a server is scaled up.</p> <p></p>"},{"location":"Account_Management/Keycloak_SSO/","title":"Keycloak SSO","text":"<p>To configure Keycloak SSO, follow these steps:</p>"},{"location":"Account_Management/Keycloak_SSO/#steps-to-configure-keycloak-sso","title":"Steps to Configure Keycloak SSO","text":"<ol> <li> <p>First Step    Open the My Profile menu within Taikun and find the Keycloak Configurations section.</p> </li> <li> <p>Second Step    Fill in the required fields to link your Keycloak account.</p> </li> <li> <p>Third Step    Hit Save.</p> </li> </ol> <p></p> <p>Keycloak configurations</p> <p>!!! note</p> <pre><code>Only one type of login (either Keycloak or Taikun sign-in) is supported at a time.\n</code></pre>"},{"location":"Account_Management/My_Profile_management/","title":"My Profile Management","text":""},{"location":"Account_Management/My_Profile_management/#access-my-profile","title":"Access My Profile","text":"<p>To access the My Profile menu, click on your profile in the top-right corner of Taikun. Here you can:</p>"},{"location":"Account_Management/My_Profile_management/#profile-options","title":"Profile Options","text":"<ul> <li> <p>Profile Information   View information about your profile such as your user name, organization, email, and role.</p> </li> <li> <p>Change Settings   Change your profile password and email address.</p> </li> <li> <p>Enable or Disable Features   Enable or disable email notifications and demo mode.</p> </li> </ul>"},{"location":"Account_Management/My_Profile_management/#profile-views","title":"Profile Views","text":"<p>My Profile View</p> <p></p> <p>My Profile view</p> <p>Settings View</p> <p></p> <p>Settings view</p>"},{"location":"Account_Management/My_Profile_management/#partner-view","title":"Partner View","text":"<p>Partners can view information about their Partner Organization. Additionally, they can apply custom logos and backgrounds.</p>"},{"location":"Account_Management/My_Profile_management/#partner-options","title":"Partner Options","text":"<ul> <li> <p>Profile Information   To view your profile information, including your user name, organization, email, and role, click your user name in the top-right corner.</p> </li> <li> <p>Change Settings   To change your profile password and email address, go to the \u201cSecurity\u201d section in your account settings. Enter your current password and your new password or email address to make updates.</p> </li> <li> <p>Enable or Disable Features   To enable or disable email notifications and demo mode, visit the \u201cNotifications\u201d section in your account settings. Use the toggle switches to adjust the settings.</p> </li> <li> <p>Partner Access   If you have Partner access, you can view your partner organization information in the \u201cPartner Organization\u201d section of your account settings. Here, you can customize your organization\u2019s logo and background.</p> </li> </ul>"},{"location":"Account_Management/Organizations_in_Taikun/","title":"Organizations in Taikun","text":"<p>Organizations in Taikun allow users to be grouped and share information. This feature helps businesses or groups collaborate and share resources within the platform.</p> <ul> <li>Users are assigned to organizations, and each organization can include multiple users.</li> <li>Information and resources are visible only within the assigned organization, ensuring data separation and security.</li> <li>Partners can manage multiple teams or businesses efficiently by grouping users into organizations.</li> <li>Organizations are managed by Partners, with a Manager-owner responsible for daily operations, Credentials, and Projects.</li> </ul>"},{"location":"Account_Management/Organizations_in_Taikun/#create-an-organization","title":"Create an Organization","text":"<p>Follow these steps to create an organization:</p> <ol> <li>Navigate to the Organizations menu in Taikun.</li> <li>Click the Add Organization button in the top-right corner.</li> <li>Fill in the mandatory fields (Name, Full Name, Discount Rate) and press +Add.</li> </ol> <p></p> <p>Create an Organization</p> <p>!!! tip </p> <pre><code>Check the \u201cAllow subscription to be changed by managers\u201d box during the creation process to let customers change their Taikun subscription.\n</code></pre> <p>After creating an organization, you can invite new or existing users directly from the Users section.</p>"},{"location":"Account_Management/Organizations_in_Taikun/#delete-an-organization","title":"Delete an Organization","text":"<p>Follow these steps to delete an organization from your Taikun account:</p> <ol> <li>Open the Organizations section in the left-hand navigation panel.</li> <li>Find the organization you want to delete and click the Delete Organization button.</li> <li>Confirm your action.</li> </ol> <p>!!! warning </p> <pre><code>Only **empty** organizations can be deleted.\n</code></pre>"},{"location":"Account_Management/Password_Reset/","title":"Password Reset","text":"<p>To reset your Taikun password, follow these steps:</p>"},{"location":"Account_Management/Password_Reset/#reset-password-via-login-page","title":"Reset Password via Login Page","text":"<ol> <li>Navigate to the login page and click the Forgot your password? link.</li> <li>Enter your email address or username on the password reset page.</li> <li>Check your email inbox for instructions to reset your password.</li> </ol> <p>Password Reset</p> <p>The email from Taikun will include a link to a password reset page where you can create a new password.</p>"},{"location":"Account_Management/Password_Reset/#reset-password-via-profile-menu","title":"Reset Password via Profile Menu","text":"<p>If you're logged into Taikun, you can reset your password directly from the Profile menu:</p> <ol> <li>Open the Profile menu.</li> <li>Navigate to the password change section.</li> <li>Enter your current password and the new password, then save the changes.</li> </ol> <p></p> <p>Change password in settings</p>"},{"location":"Account_Management/Single_Logout_Mechanism/","title":"Single Logout (SLO) Mechanism","text":"<p>The Single Logout (SLO) service seamlessly integrates with the Keystone component of Taikun OCP. It allows administrators of Taikun CloudWorks to configure SLO settings directly within the Taikun CloudWorks User Interface.</p>"},{"location":"Account_Management/Single_Logout_Mechanism/#key-features","title":"Key Features","text":""},{"location":"Account_Management/Single_Logout_Mechanism/#single-logout-configuration","title":"Single Logout Configuration","text":"<p>Taikun CloudWorks offers a comprehensive Single Logout (SLO) service, empowering administrators to enhance security by terminating user sessions across multiple applications in a single action. This feature ensures a cohesive and secure user experience.</p>"},{"location":"Account_Management/Single_Logout_Mechanism/#keystone-integration","title":"Keystone Integration","text":"<p>Taikun seamlessly integrates with Keystone Identity Management. Keystone enhances security by providing a centralized authentication and authorization system, ensuring a secure foundation for your applications.</p>"},{"location":"Account_Management/Single_Logout_Mechanism/#admin-configurability","title":"Admin Configurability","text":"<p>Administrators have full control over Single Logout settings in the account settings of Taikun CloudWorks. Easily configure SLO parameters to align with your security policies and ensure a smooth and secure logout process for your users.</p>"},{"location":"Account_Management/Single_Logout_Mechanism/#redirect-url-configuration","title":"Redirect URL Configuration","text":"<p>Taikun CloudWorks allows administrators to specify a custom Redirect URL. When the Single Logout feature is initiated, users are redirected to the specified URL.</p>"},{"location":"Account_Management/Single_Logout_Mechanism/#getting-started","title":"Getting Started","text":""},{"location":"Account_Management/Single_Logout_Mechanism/#access-logout-settings","title":"Access Logout Settings","text":"<p>Navigate to the account settings within Taikun CloudWorks to access the Single Logout configuration options.</p>"},{"location":"Account_Management/Single_Logout_Mechanism/#keystone-integration_1","title":"Keystone Integration","text":"<p>Leverage the power of Keystone Identity Management for a centralized and secure authentication process. Taikun ensures seamless integration, enhancing the overall security posture of your applications.</p>"},{"location":"Account_Management/Single_Logout_Mechanism/#custom-redirect-url","title":"Custom Redirect URL","text":"<p>Admins can define a custom Redirect URL where users are directed upon initiating the Single Logout feature. Tailor the user experience to align with your application\u2019s specific requirements.</p>"},{"location":"Account_Management/User_Types/","title":"User Types","text":"<p>Understanding Taikun\u2019s User Types is important as it will determine the privileges and permissions that each user has within your structure. There are three main roles that we will be covering: User, Manager, and Partner.</p>"},{"location":"Account_Management/User_Types/#user","title":"User","text":"<p>Users are the most basic type of user on our platform. They have the least amount of privileges and are typically used for accessing and interacting with the platform on a basic level. Users are able to perform tasks such as viewing Projects, monitoring their health, and accessing them via Kubeconfig files.</p>"},{"location":"Account_Management/User_Types/#manager","title":"Manager","text":"<p>Managers have more privileges than users and are typically responsible for managing an Organization within Taikun. In addition to the privileges that users have, managers are also able to create and manage Credentials, assign Projects to team members, and install Applications to clusters.</p>"},{"location":"Account_Management/User_Types/#partner","title":"Partner","text":"<p>Partners have the most privileges of all three user roles. They typically have a strategic relationship with our company and are able to access advanced features and resources within the platform. Partners are able to create and manage multiple Projects and Organizations, as well as access Billing information and additional reporting tools.</p> <p>!!! tip     Taikun also has Owner User which can be assigned only to one member of an account. Owner Users have the full set of permissions similarly to Partner and are also granted additional access to management of payments.</p> <p>!!! note     Partners can now white-label Taikun from the partner info section in the user info drawer.</p>"},{"location":"Account_Management/User_Types/#permissions-by-user-type","title":"Permissions by User Type","text":""},{"location":"Account_Management/User_Types/#managing-projects","title":"Managing Projects","text":"User Manager Partner Create a cluster \u2714 \u2714 \u2714 Manage Kubernetes \u2714 \u2714 \u2714 Add Servers to Projects \u2714 \u2714 \u2714 Add VMs to Projects \u2714 \u2714 \u2714 Repair Projects - \u2714 \u2714 Create/delete Kubeconfig - \u2714 \u2714 Share Projects - \u2714 \u2714 Delete Projects - \u2714 \u2714 Bind Flavor to Projects - \u2714 \u2714 Bind Image to Projects - \u2714 \u2714 Install Applications - \u2714 \u2714 Lock/Unlock Projects - \u2714 \u2714 Assign Projects - \u2714 \u2714 Adding Cloud Credentials - \u2714 \u2714 Project Quotas - \u2714 \u2714 Enable/Disable Backup - \u2714 \u2714 Enforce Policies - \u2714 \u2714 Attach/Detach Alerting Profile - \u2714 \u2714 Access Project History - \u2714 \u2714 Configure Backup Policy - \u2714 \u2714"},{"location":"Account_Management/User_Types/#monitoring-projects","title":"Monitoring Projects","text":"User Manager Partner Access Project Monitoring \u2714 \u2714 \u2714 Enable/Disable Monitoring - \u2714 \u2714 Access Servers overview - \u2714 \u2714 Enable Slack Notifications - \u2714 \u2714 Use ticketing system - \u2714 \u2714"},{"location":"Account_Management/User_Types/#credentials-and-profile-management","title":"Credentials and Profile Management","text":"User Manager Partner Add/delete Cloud Credentials - \u2714 \u2714 Add/delete Backup Credentials - \u2714 \u2714 Add/delete Showback Credentials - \u2714 \u2714 Add/delete Kubernetes profiles - \u2714 \u2714 Add/delete Access Profiles - \u2714 \u2714 Add/delete Alerting Profiles - \u2714 \u2714 Add/delete Policy Profiles - \u2714 \u2714 Add/delete Standalone Profiles - \u2714 \u2714"},{"location":"Account_Management/User_Types/#security","title":"Security","text":"User Manager Partner Access Audit Log - \u2714 \u2714 Configure KeyCloak - \u2714 \u2714 Issue API tokens - \u2714 \u2714"},{"location":"Account_Management/User_Types/#account-management","title":"Account Management","text":"User Manager Partner Add/delete users - \u2714 \u2714 Manage Organizations - - \u2714 Access Billing Information - \u2714 \u2714"},{"location":"Advanced_Configuration/Exposing_Applications_with_an_Internet-Facing_Load_Balancer_for_Zadara_cloud_in_Kubernetes_via_Taikun/","title":"Exposing Applications with an Internet-Facing Load Balancer for Zadara Cloud in Kubernetes via Taikun","text":"<p>When deploying applications on Zadara Cloud through Taikun, it is important to configure your service to be accessible online, particularly for public-facing applications. For platforms like AWS, this is typically done with the following annotation:</p> <pre><code>annotations:\n  service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n</code></pre> <p>By configuring your Kubernetes service with the correct annotations and settings, Zadara will allocate a public-facing IP address to your service, making it accessible over the internet. It\u2019s important to consult Zadara Cloud and Taikun documentation to ensure you\u2019re using the correct setup for your load balancer and external IP assignment.</p> <p>Additionally, ensure your service is secured by setting up appropriate firewall rules, security groups, and network policies to control access to your application. This configuration ensures your application is safely exposed to the internet while maintaining the security standards for external traffic.</p>"},{"location":"Advanced_Configuration/LoadBalancer/","title":"Load Balancer","text":"<p>!!! warning</p> <pre><code>LoadBalancing Flavor should have at least **\\* 1GB RAM \\* 1 vCPU**\n</code></pre>"},{"location":"Advanced_Configuration/LoadBalancer/#prepare-the-configuration-file-prodyaml","title":"Prepare the configuration file prod.yaml","text":"<pre><code>prod.yaml:\n  tenantName: \"kubernetes\"\n  clusterName: \"&lt;cluster_name&gt;\"\n  subnetId: \"&lt;kubernetes_subnet_id&gt;\"\n  flavorId: \"&lt;flavorId_for_loadbalancing_instances&gt;\"\n  publicKey: \"&lt;public_key_matching_the_ssh_key_below&gt;\"\n  floatingNetworkId: \"&lt;floating_network_id_for_loadbalancing&gt;\"\n  identityEndpoint: \"&lt;openstack_identity_endpoint&gt;\"\n  userName:         \"&lt;openstack_username&gt;&gt;\"\n  password:         \"&lt;openstack_password&gt;\"\n  domainName: \"default\"\n  keepalivedRouterIdRange: \"1-255\"\n  ssh_key: |\n  SSH_PRIVATE_KEY\n</code></pre>"},{"location":"Advanced_Configuration/LoadBalancer/#add-itera-help-chart-repository","title":"Add Itera Help Chart Repository","text":"<pre><code>helm repo add itera https://repo.itera.io/repository/itera --kubeconfig=admin.conf\n\nWARNING: Kubernetes configuration file is group-readable. This is insecure. Location: admin.conf\nWARNING: Kubernetes configuration file is world-readable. This is insecure. Location: admin.conf\n\"itera\" has been added to your repositories\n</code></pre>"},{"location":"Advanced_Configuration/LoadBalancer/#install-taikun-lb-using-helm","title":"Install taikun-lb using helm","text":"<p>!!! note      taikun-lb is only available for OpenStack with Octavia disabled. You need to add the image https://repo.itera.io/repository/images/taikun-lb.qcow2 to OpenStack with the tag \u201ctaikun-lb\u201d.</p> <p>Command to add an image to Openstack:</p> <pre><code>openstack image create --disk-format qcow2 --container-format bare --private --file taikun-lb.qcow2 --tag taikun-lb --property hw_disk_bus=scsi --property hw_scsi_model=virtio-scsi taikun-lb\n</code></pre> <pre><code>helm upgrade --install itera-lb -f prod.yaml --namespace=kube-system itera/taikun-lb --kubeconfig=admin.conf\n...\nWARNING: Kubernetes configuration file is group-readable. This is insecure. Location: admin.conf\nWARNING: Kubernetes configuration file is world-readable. This is insecure. Location: admin.conf\nRelease \"itera-lb\" has been upgraded. Happy Helming!\nNAME: itera-lb\nLAST DEPLOYED: Thu Mar  4 11:28:31 2021\nNAMESPACE: default\nSTATUS: deployedInfrastructure removal\n\nREVISION: 4\nTEST SUITE: None\n</code></pre>"},{"location":"Advanced_Configuration/LoadBalancer/#check-taikun-lb-in-installed","title":"Check taikun-lb in installed","text":"<pre><code>export KUBECONFIG=admin.conf; kubectl get pod\n\nNAME                                   READY   STATUS             RESTARTS   AGE\nitera-lb-deployment-69c44bb45c-wtwgm   1/1     Running            0          38m\n</code></pre>"},{"location":"Advanced_Configuration/LoadBalancer/#install-a-test-application-in-this-case-wordpress-from-bitnami","title":"Install a test application (in this case WordPress from bitnami)","text":"<pre><code>helm install test-lb bitnami/wordpress --kubeconfig=admin.conf\n</code></pre>"},{"location":"Advanced_Configuration/LoadBalancer/#wait-for-the-service-to-get-floating-ip-assigned","title":"Wait for the service to get floating IP assigned","text":"<pre><code>kubectl get svc test-lb-wordpress\n\nNAME                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\ntest-lb-wordpress   LoadBalancer   10.233.55.232   10.3.228.10   80:30634/TCP,443:31760/TCP   2m27s\n</code></pre> <p>To enable proxy service for the service, add the following annotation to the service:</p> <pre><code>loadbalancer.taikun.cloud/proxy-protocol: \"true\"\n</code></pre> <p>This is the way to restrict which IP has access to the service(https://www.haproxy.com/blog/haproxy/proxy-protocol/) for example:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    loadbalancer.taikun.cloud/proxy-protocol: \"true\"\n    meta.helm.sh/release-name: test-lb\n    meta.helm.sh/release-namespace: default\n...\n</code></pre>"},{"location":"Advanced_Configuration/Slack_Configuration/","title":"Slack Configuration","text":"<p>If your team uses Slack, you can receive all the changes made in your Taikun Organization directly in the Slack channel of your choice.</p> <p></p> <p>Slack configuration</p>"},{"location":"Advanced_Configuration/Slack_Configuration/#add-slack-configuration","title":"Add Slack Configuration","text":"<p>First, you need to set your webhooks for Slack and create a Slack channel where you want to receive notifications.</p> <p></p> <p>Add Slack Webhook</p>"},{"location":"Advanced_Configuration/Slack_Configuration/#fill-in-the-following-parameters","title":"Fill in the following parameters","text":"<ul> <li> <p>Organization Choose an Organization to which you want to add Slack notifications.</p> </li> <li> <p>Name Select a name for your configuration.</p> </li> <li> <p>URL Insert the webhook URL from the Slack app; this Slack guide can be of help.</p> </li> <li> <p>Channel Select a Slack channel for receiving notifications from Taikun.</p> </li> <li> <p>Type - Alert Receive only alert-type notifications or General Receive all notifications.</p> </li> </ul> <p>Now you will receive notifications in the selected Slack channel from your Taikun Projects!</p> <p></p> <p>Slack Channel </p> <p>!!! note     Slack notifications will contain a link for direct access to your Taikun Project.</p>"},{"location":"Advanced_Configuration/Taikun_API/","title":"Taikun API","text":"<p>As a software developer, it is crucial to have access to all used instruments within the code you write. Taikun\u2019s versatility allows you to use the API to manage your Kubernetes clusters without hesitation.</p> <p>Our API documentation contains all information about the authentication mechanisms, your Cloud instances\u2019 control, Taikun users\u2019 management, and much more!</p>"},{"location":"Advanced_Configuration/Taikun_API/#api-tokens","title":"API Tokens","text":"<p>To configure an API token for your team, follow these steps:</p> <p>1. Step Head to the Configurations section of the left-hand navigation panel and select the User Tokens tab.</p> <p>2. Step Press \"Add Token\" in the top-right corner.</p> <p>3. Step Specify the following:</p> <ul> <li>Name of your API token</li> <li>Expiration date (optional)</li> <li>Choose available endpoints</li> </ul> <p>Upon successful creation, you will be given the Access Key and Secret Key that can be used in your API methods.</p> <p></p> <p>User Tokens</p> <p>!!! note     Managers and Partners of your account can issue new API tokens.</p>"},{"location":"Advanced_Configuration/Taikun_CLI/","title":"Taikun CLI","text":"<p>You can manage Taikun\u2019s resources directly from the command line.</p>"},{"location":"Advanced_Configuration/Taikun_CLI/#steps-to-install","title":"Steps to Install","text":"<ol> <li> <p>Step    To download the CLI, head to the latest release page.</p> </li> <li> <p>Step    Scroll down to the Assets section and select the binary for your architecture.</p> </li> <li> <p>Step    Use the following command to move the binary: <code>sudo cp taikun /usr/local/bin/</code></p> </li> </ol> <p>Signing in to Taikun The TAikun CLI reads environment variables to authenticate to Taikun.</p> <p>To authenticate with your Taikun account:</p> <ul> <li>set the following environment variables.</li> </ul> <pre><code>TAIKUN_EMAIL\nTAIKUN_PASSWORD\n</code></pre> <ul> <li>To authenticate with Keycloak, set the following environment variables:</li> </ul> <pre><code>TAIKUN_KEYCLOAK_EMAIL\nAIKUN_KEYCLOAK_PASSWORD\n</code></pre> <ul> <li>The default API host is api.taikun.cloud. To override it, set the following environment variable:</li> </ul> <pre><code>TAIKUN_API_HOST (default value is: api.taikun.cloud)\n</code></pre> <ul> <li>Run the following command to check whether you are properly authenticated:</li> </ul> <pre><code>taikun whoami\n</code></pre>"},{"location":"Advanced_Configuration/Taikun_CLI/#configure-autocompletion","title":"Configure Autocompletion","text":"<p>Autocompletion is available for the following shells:</p> <ul> <li>Bash</li> <li>Zsh</li> <li>Fish</li> <li>PowerShell</li> </ul> <p>The command <code>taikun completion</code> generates an autocompletion script for the specified shell. For instructions on how to use the generated script, see the help command of the corresponding shell.</p> <p>For example, <code>taikun completion bash -h</code> provides instructions on how to set up autocompletion for the Bash shell.</p>"},{"location":"Advanced_Configuration/Taikun_CLI/#command-overview","title":"Command overview","text":"<p>To overview all the commands available, see the generated command tree.</p>"},{"location":"Advanced_Configuration/Taikun_CLI/#cli-help","title":"CLI Help","text":"<p>To get information on how to use a command, type:</p> <p>Long command</p> <p><code>taikun [command] --help</code></p> <p>Short command</p> <p><code>taikun [command] -h</code></p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/","title":"Terraform Provider for Taikun Workshop","text":""},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#introduction","title":"Introduction","text":"<p>This workshop aims to introduce you to Terraform and the Terraform Provider for Taikun. The latter will allow you to use Terraform to manage resources in Taikun.</p> <p>With Terraform providers, you can streamline the provisioning of infrastructure components for Kubernetes clusters, enabling efficient automation for facility management. This simplifies deploying and maintaining Taikun environments, making scaling and managing facilities easier.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#annotations","title":"Annotations","text":"<ul> <li>Text in this form will be typed as is on the command line.</li> </ul> <pre><code>cd workshop/\nls\necho Hello!\n</code></pre> <ul> <li>This form of text shows screen output, usually the output of commands.</li> </ul> <pre><code>task_00/\ntask_01/\n...\nHello!\n</code></pre> <ul> <li>This format is for code in Terraform\u2019s configuration language, HashiCorp Configuration Language (HCL).</li> </ul> <pre><code>resource \"aws_instance\" \"example\" {\n  ami = \"abc123\"\n\n  network_interface {\n    ...\n  }\n}\n</code></pre> <ul> <li>You may wish to skip reading information blocks if you are already familiar with Terraform and its configuration syntax.</li> </ul>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#setup","title":"Setup","text":"<p>You will need to install Terraform and the workshop files to complete this workshop. There are two ways to do this. \u2013 Read Local setup if you wish to install Terraform and the workshop files locally. \u2013 Use the provided Docker image, which already contains everything you need.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#setup-using-docker","title":"Setup using Docker","text":""},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#local-setup","title":"Local setup","text":"<p>Requirements:</p> <ul> <li> <p>You must have Terraform version 0.14 or newer installed.</p> </li> <li> <p>You will need Git to clone the provider\u2019s repo.</p> </li> </ul> <p>Installing Terraform: This Hashicorp tutorial explains how to install Terraform on OS X, Windows, and Linux.</p> <p>Downloading the Workshop Files: Clone the workshop directory and switch to the workshop/ directory.</p> <pre><code>git clone https://github.com/itera-io/terraform-provider-taikun-workshop.git\ncd terraform-provider-taikun-workshop/workshop/\n</code></pre>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#setup-using-docker_1","title":"Setup using Docker","text":"<p>Requirements: You must have Docker and Git installed.</p> <p>Setup: Start by cloning the workshop repository.</p> <pre><code>git clone --recursive https://github.com/itera-io/terraform-provider-taikun-workshop.git\n</code></pre>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#docker-image-creation","title":"Docker image creation","text":"<p>You\u2019ll need to build the image, which can take several minutes.</p> <pre><code>DOCKER_BUILDKIT=1 docker build --rm --target bin -t tf-workshop .\n</code></pre>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#docker-container-creation","title":"Docker container creation","text":"<p>To create the Docker container, run one of the following commands from the root of terraform-provider-taken-workshop.</p> <ul> <li>On Windows, run the following command in the command prompt (not Powershell).</li> </ul> <pre><code>docker run -v %CD%/workshop:/root/workshop --name tf-workshop -it tf-workshop\n</code></pre> <ul> <li>On linux and macOS</li> </ul> <pre><code>docker run -v $(pwd)/workshop:/root/workshop --name tf-workshop -it tf-workshop\n</code></pre> <p>This will mount the workshop/directory in the Docker container and log you into the container as root. In other words, you will need to run Terraform commands from within the container shell. However, you can edit the files in the workshop/directory with the editor of your choice on your machine.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#restarting-the-docker-container","title":"Restarting the Docker container","text":"<p>You can exit the container at any time without losing your progress. If you have exited the Docker container, run the following command to restart it.</p> <pre><code>docker start -i tf-workshop\n</code></pre>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#text-editing-within-the-container","title":"Text editing within the container","text":"<p>If you wish to edit the files in the workshop/directory from within the container, the Docker image has installed the vim, micro, and nano packages. If you are unfamiliar with Nano and Vim keybindings, the Micro editor has traditional Common User Access keybindings (Ctrl-C for copy, Ctrl-Z for undo, etc.).</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#documentation","title":"Documentation","text":"<p>The provider documentation is available on the Terraform Registry.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#tasks","title":"Tasks","text":"<p>The end goal of this workshop is to have an operational Taikun project built solely with Terraform configuration files. By following a step-by-step process, you will discover how various Taikun resources are declared and managed using Terraform.</p> <p>All your work will be done in the workshop/directory. These are its initial contents.</p> <pre><code>./workshop/\n|-- main.tf\n|-- taikun_auth.auto.tfvars\n|-- variables.tf\n</code></pre> <p>main.tf contains the Provider configuration, namely its source address and what credentials to use. You will not need to edit this file.</p> <pre><code># main.tf\nterraform {\n  required_providers {\n    taikun = {\n      source = \"itera-io/taikun\"\n      version = \"1.0.0\"\n    }\n  }\n}\n\nprovider \"taikun\" {\n  email = var.taikun_email\n  password = var.taikun_password\n}\n</code></pre> <p>Terraform reads its configuration from all the files in the working directory with the extension .tf. Having the provider configuration in main. tf is simply a convention.</p> <p>During this workshop, each task should be coded in a separate config file. Your directory will be organized accordingly at the end of the workshop.</p> <pre><code>./workshop/\n|-- main.tf\n|-- taikun_auth.auto.tfvars\n|-- task0.tf\n|-- task1.tf\n|-- task2.tf\n|-- task3.auto.tfvars\n|-- task3.tf\n|-- task4.tf\n|-- task5.tf\n|-- task6.tf\n|-- task7.tf\n|-- variables.tf\n|-- users.auto.tfvars\n|-- users.tf\n</code></pre>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#authentication","title":"Authentication","text":"<p>To complete the following tasks, you must provide Taikun credentials to Terraform. You will need a Partner account as some tasks, such as creating an organization, require Partner privileges. Input variables will be explained later in the workshop. For now, edit taikun_auth.auto.tfvars and replace the values of taikun_email and taikun_password with your credentials.</p> <pre><code># taikun_auth.auto.tfvars\ntaikun_email = \"jane.doe@itera.io\"\ntaikun_password = \"PassWord123\"\n</code></pre> <p>To find out more about providing sensitive data to Terraform, see this Hashicorp tutorial.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#task-0-organization","title":"Task 0: Organization","text":"<p>!!! Info       For this task, please write your code in the file \u2018task0.tf\u2019 at the root of the workshop/directory.</p> <p>The objective of this first task is to create an organization. All resources created in the future will be part of this organization. As this is the first task, every process step is documented.</p> <p>Before you do anything, start by preparing your working directory for other commands. terraform init.</p> <p>!!! Tip     Terraform init only needs to be run once when starting a new project or after updating the list of providers to use.</p> <p>If all goes well, you should see the following message.</p> <pre><code>Initializing the backend...\n[...]\nTerraform has been successfully initialized!\n</code></pre> <p>Once Terraform has been initialized correctly, you can declare your organization\u2019s resources. Create \u2018task0.tf\u2019 and write the following configuration block to it.</p> <pre><code>resource \"taikun_organization\" \"myorg\" {\n  name = \"&lt;name&gt;\"\n  full_name = \"&lt;full-name&gt;\"\n  discount_rate = 120\n}\n</code></pre> <p>Be sure to replace and with names of your choosing. You can also choose another label instead of my org.</p> <p>!!! Tip     Notice the syntax of the configuration block, as you are creating a resource, it begins with the keyword resource, followed by its type between double quotes.</p> <p>The resource type is always lowercase and prefixed by the provider\u2019s name, thus \u201ctaikun_organization.\u201d</p> <p>Following the resource\u2019s type is a label, it must be unique for this type of resource and is used to refer to this specific resource, as you will find out later.</p> <p>Watch out, this label does not correspond to the resource\u2019s name in Taikun.</p> <p>Three arguments are then defined: name, full_name, and discount_rate. The argument\u2019s identifier is on the left side of the equals sign, and its value is on the right.</p> <p>See the documentation of Taikun\u2019s organization resource for a complete list of arguments, i.e., the resource\u2019s schema.</p> <p>Labels and argument names can contain letters, digits, underscores, and hyphens and may not start with a digit.</p> <p>Run the following command to reformat your configuration in the standard style.</p> <pre><code>terraform fmt\n</code></pre> <p>Now apply your changes.Tip: If you have already created resources, applying will refresh Terraform\u2019s state by requesting Taikun\u2019s API. If you wish to check the validity of your changes without refreshing the state, you can run terraform validate.</p> <pre><code>terraform apply\n</code></pre> <p>You should get a validation error.</p> <pre><code>Error: expected discount_rate to be in the range (0.000000 - 100.000000), got 120.000000\nNow fix the discount rate so it is in the range 0-100 and run apply once more. Terraform will display a list of\nresources to create. After checking the plan is correct, type yes to execute it.\n</code></pre> <p>!!! Tip     You should notice a file terraform.tfstate in your working directory, Terraform uses this file to keep track of the state, do not modify or delete it. You may now list the resources in Terraform\u2019s state.</p> <pre><code>terraform state list\n</code></pre> <p>You can also display their content.</p> <pre><code>terraform show\n</code></pre> <pre><code>taikun_organization.myorg:\nresource \"taikun_organization\" \"myorg\" {\n    created_at = \"2021-11-05T14:00:50Z\"\n    discount_rate = 42\n    full_name = \"Jane Doe's organization\"\n    id = \"6383\"\n    is_read_only = false\n    lock = false\n    managers_can_change_subscription = true\n    name = \"my-organization\"\n    partner_id = \"119\"\n    partner_name = \"TF-CI\"\n    projects = 0\n    servers = 0\n}\n</code></pre> <p>You may wish to check the organization was indeed created at app.taikun.cloud/organizations.Tip</p> <p>Try running terraform apply again, Terraform will refresh its state and, as long as nothing has changed, tell you that no changes are needed.</p> <p>You can also try deleting the organization through the web UI, and running terraform apply. Terraform will tell you that changes have occurred outside of Terraform and recreate the resource.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#task-1-kubernetes-profile","title":"Task 1: Kubernetes Profile","text":"<p>!!! Info     For this task, please write your code in the file task1.tf at the root of the workshop/directory.</p> <p>Now that you have created an organization, you will create a Kubernetes profile belonging to it. Check the kubernetes_profile resource\u2019s schema on the provider\u2019s documentation and declare the resource in task1.tf. Set organization_id to the organization\u2019s ID created in the previous task (see Task 0: Organization).</p> <p>Feel free to set some of kubernetes_profile\u2019s other optional attributes, such as schedule_on_master and load_balancing_solution. Once you have declared your resource, apply your changes and move on to the next task.</p> <p>!!! Tip     To refer to the ID of your organization, use the following syntax. resource.taikun_organization.myorg.id Make sure to replace myorg if you used another label for your organization.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#task-2-slack-configuration-alerting-profile","title":"Task 2: Slack Configuration &amp; Alerting Profile","text":"<p>!!! Info     For this task, please write your code in the file task2.tf at the root of the workshop/directory.</p> <p>You will now create an alerting profile using a Slack configuration.</p> <ol> <li> <p>Start by declaring a Slack configuration. Here is its documentation.Its hook URL should be https://hooks.myapp.example/ci. It must send alert-type notifications only to channel ci.</p> </li> <li> <p>You can now declare the alerting profile. Here is its documentation. The alerting profile should send notifications daily using the Slack configuration displayed above.</p> </li> </ol> <p>!!! Tip     As always, your resources should belong to the organization created in Task 0: Organization.</p> <p>Once you have declared these two new resources, apply your changes and move on to the next task.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#task-3-cloud-credentials","title":"Task 3: Cloud Credentials","text":"<p>!!! Info     For this task, please write your code in the file task3.tf at the root of the workshop/directory. You will also be editing task3.auto.tfvars.Important</p> <p>You will need OpenStack credentials to complete this task.</p> <p>Cloud credentials are needed to create a Taikun project. In a natural work environment, cloud credentials should not be stored under version control Terraform\u2019s input variables help solve this problem.</p> <p>Variables are declared in variables.tf by convention. This file declares the variables taikun_email and taikun_password used for authentication.</p> <pre><code># variables.tf\nvariable \"taikun_email\" {\n  description = \"Taikun email\"\n  type = string\n  sensitive = true\n}\n\nvariable \"taikun_password\" {\n  description = \"Taikun password\"\n  type = string\n  sensitive = true\n}\n</code></pre> <p>!!! Tip     Input variables are declared with a variable block. The label that follows the variable keyword is the name of the variable.</p> <p>The description argument is used to specify the variable\u2019s documentation. \u2022 type is the type of this argument\u2019s value. \u2022 If set to true, sensitive will hide this variable\u2019s value in Terraform output. It defaults to false.</p> <p>To know more about input variables and a complete list of arguments, see the Terraform documentation on variables.</p> <p>Variables are then defined in .tfvars files, as seen in the subsection Authentication.</p> <p>For simplicity, variables will be declared in the task/.tf files. Thus, in task3.tf*, insert the following variable declarations</p> <pre><code>variable \"openstack_url\" {\n  description = \"OpenStack url\"\n  type = string\n  sensitive = true\n}\nvariable \"openstack_user\" {\n  description = \"OpenStack user\"\n  type = string\n  sensitive = true\n}\nvariable \"openstack_password\" {\n  description = \"OpenStack password\"\n  type = string\n  sensitive = true\n}\nvariable \"openstack_domain\" {\n  description = \"OpenStack domain\"\n  type = string\n  sensitive = true\n}\nvariable \"openstack_region\" {\n  description = \"OpenStack region\"\n  type = string\n}\nvariable \"openstack_public_network\" {\n  description = \"OpenStack public network\"\n  type = string\n}\nvariable \"openstack_project\" {\n  description = \"OpenStack project name\"\n  type = string\n}\n</code></pre> <p>!!! Info     If copy-pasting this code block into task3.tf does not indent the code properly, save task3.tf and run terraform fmt.</p> <p>Now that the OpenStack variables have been declared define the variables in task3.auto.tfvars using your credentials.</p> <p>Terraform must be told through command line arguments which .tfvars files to read. However, if variable definition files have the extension .auto.tfvars, as is the case with taikun_auth.auto.tfvars, Terraform will automatically fetch the variables\u2019 values.</p> <p>!!! Tip     As a reminder, here is the syntax used in taikun_auth.auto.tfvars to define the variables taikun_email and taikun_password.</p> <pre><code>taikun_email = \"jane.doe@itera.io\"\ntaikun_password = \"PassWord123\"\n</code></pre> <p>In order to get a variable\u2019s value, use the syntax var.. For example, the following line sets the OpenStack domain in the cloud_credential_openstack resource.</p> <pre><code>domain = var.openstack_domain\n</code></pre> <p>Once you have declared your new resource, apply your changes and move on to the next task.</p> <p>!!! Info     As always, your resources should belong to the organization created in Task 0: Organization.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#task-4-users","title":"Task 4: Users","text":"<p>!!! Note     For this task, please write your code in the file task4.tf at the root of the workshop/directory. You will also be editing users.tf and users.auto.tfvars.</p> <p>You will now add users to the Taikun organization. As the organization could have many users, you will use variables and the keyword for_each to avoid declaring multiple taikun_user blocks.</p> <p>Add the following variable declaration to users.tf.</p> <pre><code>variable \"users\" {\n  type = map(object({\n    email = string\n    role = string\n   }))\n   description = \"List of project users\"\n   default = {}\n}\n</code></pre> <p>The user variable is of a complex type: a map of objects with two arguments, email, and role. The keys of the map are strings, they will be the usernames of the users. The default = {} argument definition tells Terraform that the default value of var.users is an empty map.</p> <p>Here is an example definition of the user variable.</p> <pre><code>users = {\n  \"alice\" = {\n    email = \"alice@gmail.com\"\n    role = \"Manager\"\n  },\n  \"bob\" = {\n    email = \"bob@gmail.com\"\n    role = \"User\"\n  },\n}\n</code></pre> <p>In this example, user accounts are defined for Alice and Bob. \u2022 Alice has a Manager account with the username alice and the email alice@gmail.com. \u2022 Bob has a User account with the username bob and the email bob@gmail.com.</p> <p>Now edit users.auto.tfvars and define three users. \u2022 -manager with Manager role and the email -manager@mail.example. \u2022 -user1 with User role and the email -user1@mail.example. \u2022 -user2 with User role and the email -user2@mail.example.</p> <p>Replace with a name of your choosing.</p> <p>You can now declare the user resource in task4.tf, the users must belong to the organization created in Task 0: Organization. Here is its documentation. By using the for_each keyword, only one resource block is needed.</p> <p>!!! Tip     Here is an example using the for_each keyword. Consider a Terraform provider to order pizzas. Sup- pose the variable pizza_orders has the following definition.</p> <pre><code>pizza_orders = {\n  \"alice\" = {\n    type = \"pepperoni\"\n    size = \"large\"\n   },\n   \"bob\" = {\n     type = \"amatriciana\"\n     size = \"medium\"\n   },\n}\n</code></pre> <p>Here is how it would be used with the for_each keyword in a pizza_order resource.</p> <pre><code>resource \"pizza_order\" \"orders\" {\n  for_each = var.pizza_orders\n  client = each.key\n  type = each.value.type\n  size = each.value.size\n}\n</code></pre> <p>You can also have a look at Terraform\u2019s for_each documentation.</p> <p>Once you have declared the user resource, apply your changes and move on to the next task.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#task-5-access-profile","title":"Task 5: Access Profile","text":"<p>!!! Note     For this task, please write your code in the file task5.tf at the root of the workshop/directory. You will also be editing users.tf and users.auto.tfvars.</p> <p>Before creating your first project, you will need an access profile.</p> <p>The access profile must contain SSH keys for all the users created in the previous task, Task 4: Users. Rather than declare the SSH keys in a separate variable, you will add them to the user\u2019s variable to declare them on per user basis.</p> <p>Modify users\u2019 declaration in users.tf to include a list of SSH users per user.</p> <pre><code>variable \"users\" {\n  type = map(object({\n    email = string\n    role = string\n    ssh_users = list(object({\n      name = string\n      public_key = string\n    }))\n   }))\n   description = \"List of project users\"\n   default = {}\n}\n</code></pre> <p>You can now edit users.auto.tfvars and define a list of SSH users for each user. Considering the previous example of Alice and Bob, here is the exact definition of users with added SSH users.</p> <pre><code>users = {\n  \"alice\" = {\n    email = \"alice@gmail.com\"\n    role = \"Manager\"\n    ssh_users = [\n      {\n        name = \"alice-work\"\n        public_key = \"ssh-ed25519 AAAATHEQUICKBROWNFOXJUMPEDOVERTHELAZYDOG example\"\n      },\n      {\n        name = \"alice-home\"\n        public_key = \"ssh-ed25519 AAAATHEQUICKBROWNFOXJUMPEDOVERTHELAZYDOG example\"\n      }\n     ]\n    },\n    \"bob\" = {\n      email = \"bob@gmail.com\"\n      role = \"User\"\n      ssh_users = [\n        {\n          name = \"bob-laptop\"\n          public_key = \"ssh-ed25519 AAAATHEQUICKBROWNFOXJUMPEDOVERTHELAZYDOG example\"\n        }\n       ]\n      },\n     }\n</code></pre> <p>Add as many SSH users as you wish to the users defined in the previous task. Of course, you will need to use valid SSH keys. If you do not want to create your own, here is a public key value you can use:</p> <pre><code>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIB/8P0zXmI/Il81+/fnvGrf0X/VyNTrOJ9nQCxBxjc5m taikun\n</code></pre> <p>!!! Info     Within one access profile, the names of SSH users must be unique.</p> <p>You can now declare the taikun_access_profile resource in task5.tf. You will use for_each slightly differently, as ssh_user is a nested resource within the access_profile resource. Here is the documentation of the access profile resource.</p> <p>The access profile\u2019s requirements are \u2022 It should use the DNS servers 8.8.8.8 and 1.1.1.1. \u2022 It should use the NTP server time.windows.com. \u2022 It should include all the SSH users defined in users.auto.tfvars.</p> <p>!!! Tip     While reading access profile\u2019s schema, you may notice dns_server and ntp_server are of the type Block List.</p> <p>Returning to the pizza order example, here is how block lists are used. Suppose the pizza_order resource argues extra_topping defined as such:</p> <pre><code> - extra_topping (Block List, Max: 5) List of extra pizza toppings. (see below for nested\n schema)\n\n Nested Schema for extra_topping\n Required:\n - name (String) Name of the extra topping\n - quantity (Number) Quantity\n</code></pre> <p>Alice could add mozzarella di bufala and basil to her amatriciana pizza order.</p> <pre><code>resource \"pizza_order\" \"orders\" {\n  client = \"alice\"\n  type = \"amatriciana\"\n  size = \"large\"\n\n  extra_topping {\n    name = \"basil leaves\"\n    quantity = 8\n  }\n  extra_topping {\n    name = \"bufala slices\"\n    quantity = 4\n  }\n&lt;a href=\"https://docs.taikun.cloud/guidelines/terraform-provider/#__codelineno-32-14\"&gt;&lt;/a&gt;}\n</code></pre> <p>Spoiler ahead!</p> <p>Solution: Add the following dynamic \u201cssh_user\u201d block to your access profile\u2019s configuration.</p> <pre><code>resource \"taikun_access_profile\" \"...\" {\n\n   # Rest of configuration: DNS servers, NTP servers, etc.\n   # ...\n\n   dynamic \"ssh_user\" {\n     for_each = flatten([for user in var.users : user.ssh_users])\n     content {\n       name = ssh_user.value.name\n       public_key = ssh_user.value.public_key\n     }\n    }\n}\n</code></pre> <p>Once you have fully declared the access profile resource, apply your changes and move on to the next task.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#task-6-project","title":"Task 6: Project","text":"<p>!!! Note     For this task, please write all your code in the file task6.tf at the root of the workshop/directory.</p> <p>Finally, you can declare a project resource. However, as flavors must be bound to the project, you must first fetch a list of suitable flavors.</p> <p>To do this, declare a flavors data source. Datasources, instead of resources, only fetch information from providers and do not create any resources. Add the following block to task6.tf.</p> <pre><code>data \"taikun_flavors\" \"small\" {\n# FIXME\n}\n</code></pre> <p>!!! Tip     As you are declaring a datasource and not a resource, the block begins with the keyword data instead of a resource. Once again, the type of datasource is in lowercase and must be prefixed by the name of the provider. Finally, the label \u201csmall\u201d is used to designate this datasource.</p> <p>Edit the datasource to search for flavors with four or fewer CPUs and no more than 8GB of RAM. Set its cloud credential ID to that of the cloud credential created in Task 3: Cloud Credentials.</p> <p>Then declare local value flavors to be the list of names of the flavors read by the datasource. See the Terraform documentation to know more about local values.</p> <pre><code>locals {\n  flavors = [for flavor in data.taikun_flavors.small.flavors : flavor.name]\n}\n</code></pre> <p>This will allow you to refer to the list of flavor names with local.flavors, which will be helpful when defining the project.</p> <p>You now have everything you need to create a project in Taikun. Here is its documentation.</p> <p>These are the requirements for the project resource: \u2022 As with all previous resources, it must belong to the organization created in Task 0: Organization. \u2022 It must use the Kubernetes profile defined in Task 1: Kubernetes Profile. \u2022 Its alerting profile must be defined in Task 2: Slack Configuration &amp; Alerting Profile. \u2022 It should use the cloud credentials defined in Task 3: Cloud Credentials. \u2022 Monitoring must be enabled. \u2022 It should have one bastion, one kubemaster, and one kubeworker.</p> <p>!!! Tip     To access the first element of a list, use the syntax list[index]. For example, to get the first flavor read by the flavors datasource, use local.flavors[0].</p> <p>After creating a project with Terraform, one may want to know its access IP. Once you have declared the project resource, add the following output block to task6.tf.</p> <pre><code>output \"project_access_ip\" {\n  value = resource.taikun_project.&lt;project-label&gt;.access_ip\n}\n</code></pre> <p>Make sure to replace it with the label you gave your project. This will display the project\u2019s access IP once it has been created.</p> <p>!!! Tip     Output values are a way for the user to output a specific value from Terraform\u2019s state.</p> <p>An output value block begins with the keyword output followed by a unique label. The user can decide which value to produce by setting the value argument.</p> <p>To know more about output values, see Terraform\u2019s documentation.</p> <p>You can now apply your changes; expect to wait about 30 minutes for your project to be in a Ready state.</p>"},{"location":"Advanced_Configuration/Terraform_Provider_for_Taikun_Workshop/#task-7-project-user-attachments","title":"Task 7: Project User Attachments","text":"<p>!!! Note     For this task, please write all your code in the file task7.tf at the root of the workshop/directory.</p> <p>Now that your project is in a Ready state, you can attach some users to it.</p> <p>Declare a project user attachment resource with the following for_each argument, replacing it with the label you gave the taikun_user resource.</p> <pre><code>for_each = {\n  for user in resource.taikun_user.&lt;label&gt; : user.id =&gt; user\n  if user.role == \"User\"\n}\n</code></pre> <p>With this block, you can only attach users with the User role to the project. To know more about this syntax, see the documentation for expressions.</p> <p>Finally, set the proper values for the user_id and **project_id arguments and apply your changes.</p>"},{"location":"Getting_Started/Accessing_Cluster_with_Kubeconfig/","title":"Accessing Cluster with Kubeconfig","text":"<p>A kubeconfig file is a configuration file used by the Kubernetes command-line tool <code>kubectl</code> to connect to a specific Kubernetes cluster. The file contains the cluster\u2019s server address and authentication credentials. It also allows you to switch between different clusters and contexts within a cluster, such as different namespaces or user accounts. With Taikun, you can quickly generate a kubeconfig file and use it on your machine to access the Kubernetes cluster you created in the previous step.</p>"},{"location":"Getting_Started/Accessing_Cluster_with_Kubeconfig/#generate-kubeconfig-for-your-project","title":"Generate Kubeconfig for your Project","text":"<p>Add Kubeconfig</p>"},{"location":"Getting_Started/Accessing_Cluster_with_Kubeconfig/#steps-to-create-a-kubeconfig-file","title":"Steps to create a Kubeconfig file","text":"<p>1. Open the page of your Project.</p> <p>2. Locate the Kubeconfigs link among the function buttons of your Cluster.</p> <p>3. Press \"Add Kubeconfig\" and define the necessary characteristics of your file.</p> <p>4. Choose a Name: Use at least three lowercase alphanumeric characters.</p> <p>5. Choose a Namespace: An isolated group located within the clusters.</p> <p>6. Choose a Role: You can choose between <code>admin</code>, <code>edit</code>, or <code>view</code>.</p> <p>7. Select a Predefined or a Custom Validity Period: Choose a period for which the kubeconfig will remain valid.</p> <p>8. Select a Kubeconfig Type:</p> <ul> <li>Personal Kubeconfig: Only the selected user is allowed.</li> <li>Application Kubeconfig: Choose whether only project managers or all users with access to the project are allowed.</li> </ul> <p>9. Click \"Add\".</p>"},{"location":"Getting_Started/Accessing_Cluster_with_Kubeconfig/#actions-for-the-kubeconfig-file","title":"Actions for the Kubeconfig file","text":"<ul> <li>Open Terminal: Opens a new terminal for managing your Kubernetes cluster.</li> <li>Download: Downloads a <code>.yaml</code> configuration file to access your Kubernetes cluster.</li> <li>Delete: Deletes the kubeconfig file associated with your Kubernetes cluster.</li> </ul> <p>Your kubeconfig file in Taikun is already authenticated. To interact with your cluster, use the <code>kubectl</code> command-line tool.</p>"},{"location":"Getting_Started/Accessing_Cluster_with_Kubeconfig/#accessing-kubernetes-cluster-with-bash","title":"Accessing Kubernetes Cluster with Bash","text":"<p>Kubernetes bash</p> <p>Use bash commands to interact with your Kubernetes cluster using the kubeconfig file.</p>"},{"location":"Getting_Started/Connect_your_Cloud/","title":"Connect your Cloud","text":""},{"location":"Getting_Started/Connect_your_Cloud/#supported-cloud-providers","title":"Supported Cloud Providers","text":"<p>Taikun currently supports the following cloud services: AWS, Azure, GCP, OpenStack, VMware Tanzu, Openshift, VMware vSphere, Zadara, Zededa, and Proxmox. To use Taikun for managing your Kubernetes clusters on these platforms, you need to add your cloud credentials. For more details, refer to the Supported Cloud Providers section.</p>"},{"location":"Getting_Started/Connect_your_Cloud/#add-cloud-credentials","title":"Add Cloud Credentials","text":"<p>Follow these steps to add your Cloud credentials in Taikun:</p> <ol> <li> <p>Log in to your Taikun account and navigate to the Cloud Credentials section in the left-hand navigation panel.</p> </li> <li> <p>Click on \"Add Cloud Credentials\" in the top-right corner.</p> </li> <li> <p>Select the Cloud Provider you want to add credentials for.</p> </li> <li> <p>Enter the required parameters for your Cloud provider and any additional information specific to the provider.</p> </li> <li> <p>Click \"Add Cloud Credentials\" to save the credentials.</p> </li> <li> <p>Verify your credentials: After saving, your new credentials should appear in the list on the page.</p> </li> </ol> <p>Once your Cloud credentials are added, you can use Taikun to establish and manage Kubernetes clusters on the selected cloud provider.</p>"},{"location":"Getting_Started/Creating_Kubernetes_cluster/","title":"Creating Kubernetes cluster","text":""},{"location":"Getting_Started/Creating_Kubernetes_cluster/#adding-nodes","title":"Adding Nodes","text":"<p>To create a functioning Kubernetes cluster in Taikun, you need to add cloud servers to your Project. A Kubernetes cluster requires the following types of nodes:  </p> <ul> <li>Master Node: Manages the cluster\u2019s state, such as scaling and updates.</li> <li>Worker Nodes: Run the actual applications.</li> <li>Bastion Node: Connects Taikun to your cluster.</li> </ul>"},{"location":"Getting_Started/Creating_Kubernetes_cluster/#steps-to-create-your-first-cluster-in-taikun","title":"Steps to Create Your First Cluster in Taikun","text":"<p>Follow these steps to set up your Kubernetes cluster:</p> <p>1. Open your Project    Open the newly-created Project from the list.</p> <p>2. Click \"Add Server\"    Locate and click the \"Add Server\" button on the right side of your screen.</p> <p>3. Select a Flavor    Choose a specific flavor to bind to your Project.</p> <p>4. Add Three Servers    Add the following servers to your Project:</p> <ul> <li>1 Bastion Node</li> <li>(At least) 1 Worker Node</li> <li>1 Master Node</li> </ul> <p>(Optional): You may need to repair your Project to save the current configuration.</p> <p>5. Deploy    Confirm the changes by pressing the \"Deploy\" button.</p> <p>Congratulations! You\u2019ve successfully created your first Kubernetes cluster with Taikun!</p>"},{"location":"Getting_Started/Creating_Kubernetes_cluster/#additional-configuration-options","title":"Additional Configuration Options","text":"<p>You can make further changes to your configuration:</p> <ul> <li>Directly from Project Details: Options like Backup, Monitoring, or Profiles.</li> <li>Specific Tabs: Update Access Profile or Kubernetes Profile.</li> </ul> <p>!!! Warning     Project Repair:     Whenever you make changes to your Project, you must perform a Repair to ensure the changes are correctly applied, and your cluster functions as expected.</p>"},{"location":"Getting_Started/From_Login_to_Production_Cluster/","title":"From Login to Production Cluster","text":""},{"location":"Getting_Started/From_Login_to_Production_Cluster/#what-about-taikun","title":"What About Taikun?","text":"<p>Welcome to Taikun! Our platform is designed to simplify the lives of DevOps engineers and IT professionals by offering powerful tools for managing Kubernetes clusters and overall infrastructure.</p>"},{"location":"Getting_Started/From_Login_to_Production_Cluster/#why-choose-taikun","title":"Why Choose Taikun?","text":"<p>Setting up a Kubernetes cluster manually can take days or even weeks. With Taikun, you can have a fully functional Kubernetes cluster ready in just 5 minutes. Our user-friendly interface and automation tools allow you to focus on developing and deploying applications, instead of spending time on the complexities of infrastructure setup.</p> <p>This introduction will walk you through the basic features of Taikun.</p>"},{"location":"Getting_Started/From_Login_to_Production_Cluster/#login-and-password","title":"Login and Password","text":"<p>If you\u2019re new to Taikun, follow these steps to get started:</p> <p>1. Account Invitation    After signing up, you\u2019ll receive an invitation to your account via email. Click the \u201cVisit\u201d button in the email to create your password.</p> <p>2. Login    Use your registered credentials to log in. The first page you\u2019ll see is your organization\u2019s dashboard.</p> <p></p> <p>Taikun Login</p> <p>3. Keycloak Support    If configured for your organization, you can log in using Keycloak.</p> <p>4. Reset Password    If you forget your password, reset it via the Password Reset page or the Forgot your password? link on the login page.</p>"},{"location":"Getting_Started/Project_Creation/","title":"Project Creation","text":"<p>A Project in Taikun serves as the central hub for creating and managing Kubernetes clusters. It allows you to organize your resources, separate environments (e.g., development, staging, production), and streamline cluster management. With a single Project, you can view and manage all associated clusters and resources, making it especially useful for teams managing multiple environments or clusters.</p>"},{"location":"Getting_Started/Project_Creation/#adding-a-new-project","title":"Adding a New Project","text":"<p>You can create a new Project after adding your Cloud credentials. During Project creation, you can specify advanced features for your Kubernetes cluster, such as access profiles or monitoring configurations.</p> <p>Who Can Create Projects? Projects can be created by Managers or Partners of your account. These users can also assign Projects to colleagues.</p> <p></p> <p>Add new project</p>"},{"location":"Getting_Started/Project_Creation/#steps-to-create-a-new-project","title":"Steps to Create a New Project","text":"<p>Follow these steps to create your first Project in Taikun:</p> <p>1. Ensure you have added a Cloud Connection to your Taikun account.</p> <p>2. Navigate to the Projects menu in Taikun.</p> <p>3. Click \u201cAdd Project\u201d in the top-right corner.</p> <p>4. Provide the following details in the pop-up:</p> <ul> <li>Project Name</li> <li>Cloud Provider</li> </ul> <p>(Optional) Default profiles for Access, Alerting, and Kubernetes are auto-populated for new accounts. If needed, create custom profiles beforehand to use them in your Project.</p> <p>5. Click \u201cCreate Project\u201d to finalize and save your new Project.</p>"},{"location":"Getting_Started/Project_Creation/#advanced-features-after-project-creation","title":"Advanced Features After Project Creation","text":"<p>Once your Project is created, you can enable additional Taikun features such as:</p> <ul> <li>Alerting</li> <li>Policy Profiles</li> <li>Monitoring</li> <li>Backup</li> <li>Expiration Date Management</li> </ul> <p>Each feature ensures your Project is configured to meet your organization's specific needs.</p>"},{"location":"Getting_Started/Taikun_Infra/","title":"Taikun Infra","text":"<p>Taikun Infra revolutionizes deploying clusters, introducing ease and speed that significantly enhance your experience. The platform\u2019s robust capabilities accelerate your utilization of its resources, providing immediate access to cloud services and essential backup credentials. Upon creating a new account, you\u2019ll receive a 15-day trial of Taikun Infra, an excellent opportunity to delve into the platform\u2019s features and unlock its full potential.</p> <p>During this trial period, users have access to a pre-defined allocation of resources, including:</p> <ul> <li>24GB of RAM</li> <li>50 CPU units</li> <li>1000GB of volume</li> </ul> <p>This resource package is thoughtfully designed to empower you to explore and harness the capabilities of Taikun Infra without constraint.</p> <p></p> <p>resource overview</p> <p>Openstack credentials and Backup credentials are available when a new account is created. They can be found on the Credentials tab on the left side of the menu. You can use them when creating a new Project.</p> <p></p> <p>Cloud Credentials</p> <p></p> <p>Backup Credentials</p> <p>After the subscription process is concluded, the Taikun Infra option will appear in the left sidebar menu. This feature provides easy access to the platform\u2019s functions and capabilities.</p> <p>The costs associated with Taikun Infra are dynamically determined based on your usage, ensuring a flexible and tailored pricing structure that aligns with your needs and resource utilization.</p>"},{"location":"Getting_Started/Taikun_Infra/#pricing-structure","title":"Pricing Structure","text":"<p>Ensuring that you only pay for the resources you consume:</p> <ul> <li>CPU: 0.0041\u20ac</li> <li>RAM: 0.0053\u20ac</li> <li>Volume: 0.00036\u20ac</li> <li>Floating IP: 0.0067\u20ac</li> <li>S3: 0.00018\u20ac</li> </ul> <p></p> <p>resource overview</p> <p></p> <p>menu link</p>"},{"location":"Managing_your_Projects/AI_assistant/","title":"AI Assistant","text":"<p>Use the power of AI to manage your Kubernetes cluster more efficiently. With our self-hosted solution, the AI assistant will help with fixing any issues with your Kubernetes cluster on the go.</p> <p>Whenever your cluster has any Alerts, the AI Assistant window will describe the exact nature of the misbehavior and provide valuable suggestions for getting your infrastructure back on track.</p> <p>!!! note     The Taikun connection utilizes a self-hosted version of AI Assistance. Your data won\u2019t be sent to OpenAI for processing when choosing these credentials.</p>"},{"location":"Managing_your_Projects/AI_assistant/#enable-ai-assistant","title":"Enable AI Assistant","text":"<p>To enable AI Assistant, follow these steps:</p> <ol> <li>Open any Project within your Taikun account.</li> <li>Click the Settings button located at the top of the page.</li> <li>In the side menu, scroll down to the AI Assistant section and toggle this option on.</li> <li>Specify the set of AI Credentials you\u2019d like to use in your Project (Taikun is the default connection that is self-hosted by the Taikun team).</li> <li>Click \"Enable AI Assistant\".</li> </ol> <p></p> <p>Enabling AI Assistant</p>"},{"location":"Managing_your_Projects/AI_assistant/#add-a-custom-openai-account","title":"Add a Custom OpenAI Account","text":"<p>Users of Taikun can also connect their own OpenAI account to receive AI assistance directly within the application. To connect your AI Assistant, follow these steps:</p> <p>1. As a Manager or Partner, switch to the AI Credentials tab in the left-hand menu.</p> <p>2. Select the \"Add AI Credentials\" button in the top right corner.</p> <p>3. Provide the following information:</p> <ul> <li>Name \u2013 Name for your AI Credentials.</li> <li>OpenAI API Key \u2013 Refer to this OpenAI help center article to locate the key.</li> <li>Organization \u2013 Specify the Organization you\u2019d like to add the Credentials to.</li> </ul> <p></p> <p>Add AI Credentials</p>"},{"location":"Managing_your_Projects/Backup/","title":"Backup","text":""},{"location":"Managing_your_Projects/Backup/#create-new-credentials","title":"Create New Credentials","text":"<p>Add Backup Credentials</p>"},{"location":"Managing_your_Projects/Backup/#s3-name","title":"S3 Name","text":"<p>The name for backup credentials (3-30 characters).</p> <p>Fill in the remaining S3 data from Amazon and add new backup credentials. See AWS endpoints.</p> <p>Invalid S3 credentials error can pop up if you fill in wrong/non-existent credentials.</p> <p>After you add the credentials, you can back up the project by enabling Backup and adding a Backup Policy.</p>"},{"location":"Managing_your_Projects/Backup/#backup-credentials-list","title":"Backup Credentials List","text":"<p>Use the search field to find the credentials needed.</p>"},{"location":"Managing_your_Projects/Backup/#every-credential-has-its","title":"Every Credential has its","text":"<ul> <li>ID \u2013 Unique identifier of the storage.</li> <li>Organization \u2013 Name of the organization associated with the storage.</li> <li>S3 Access Key ID \u2013 Access key for S3.</li> <li>S3 Endpoint \u2013 URL of the S3 endpoint.</li> <li>S3 Name \u2013 Name of the storage.</li> <li>Associated Projects \u2013 Projects linked to this storage.</li> <li>Actions \u2013 Possible actions, such as edit or delete.</li> </ul> <p>Backup Credentials </p> <ul> <li>ID and Organization are immutable.</li> <li>S3 Access Key ID, S3 Endpoint, and S3 Name are security credentials.</li> <li>Associated Projects lists all projects using these backup credentials.</li> <li>Created By informs you who made the last change.</li> <li>Expanding the table shows the previous modification details (Last Modified, Last Modified By).</li> </ul>"},{"location":"Managing_your_Projects/Backup/#actions","title":"Actions","text":"<p>Make Default: Choose credentials that will be pre-filled during project creation. A lighter color indicates selected credentials.</p> <p>Lock/Unlock Credential: Locking the credentials disables them for Backup. Unlock to enable them again.</p> <p>Update Credential: Update S3 Name, S3 Access Key, and S3 Secret Key.</p> <p>Delete: You can delete only empty and unlocked Backup Credentials.</p>"},{"location":"Managing_your_Projects/Backup/#enabledisable-backup-in-projects","title":"Enable/Disable Backup in Projects","text":"<p>You can enable backup during project creation.</p> <p></p> <p>Enable Backup</p> <p>Backup can also be enabled after the project is created.</p> <p>First, you need to enable backup and then choose credentials.</p> <p></p> <p>Enable backup in Settings</p>"},{"location":"Managing_your_Projects/Backup/#backup-policy","title":"Backup Policy","text":"<p>After enabling backup, you must set up a backup policy.</p> <p></p> <p>Add Backup Policy </p> <p>!!! Note     Once the policy is added, the cron job starts.</p> <p>!!! Info      To terminate the backup, delete the policy. If you no longer want to use backup, disable it.</p>"},{"location":"Managing_your_Projects/Create_a_Project/","title":"Create a Project","text":"<p>You can create a new Project straight after adding your Cloud credentials! During the project creation, you can specify various functions to be used.</p> <p></p> <p>Projects</p>"},{"location":"Managing_your_Projects/Create_a_Project/#steps-to-create-your-first-project","title":"Steps to create your first Project","text":"<p>1. Go to the Projects menu in Taikun. 2. Click \"Add Project\" in the top right corner. 3. Specify your Project\u2019s name and select a Cloud provider.</p> <ul> <li>(Optional) Access, Alerting, Kubernetes, and Policy Profile are initially populated with a default option. These profiles are created with every new organization. Custom profiles need to be created before being used in your Projects.</li> </ul> <p>!!! Warning      If you want to use your Access Profile in a project, you need to create it before the Project and select it from the dropdown during creation.</p> <p>Some of Taikun\u2019s features (such as alerting, policy profiles, monitoring, backup, and expiration date) can be enabled after your Project is created.</p>"},{"location":"Managing_your_Projects/Create_a_Project/#project-settings","title":"Project Settings","text":"<p>1. Project Name</p> <p>Fill in your project name (only alphanumeric lowercase characters and dashes are allowed, 3-30 characters; no underscores, e.g., my-project1).</p> <p>2. Cloud</p> <p>Choose where you want to store your Project. For more info, see Cloud Credentials.</p> <p>3. Specify Kubernetes Version (Optional)</p> <p>If you need a specific version of Kubernetes, select it from the list.</p> <p>4. Specify CIDR (Optional)</p> <p>Define a network or a sub-network.</p> <p>5. Access Profile</p> <p>Choose an access profile for the project. (For how to create a new profile, see Access Profiles.)</p> <p>6. Alerting Profile</p> <p>If you have created a profile in Alerting Profiles, you can select it.</p> <p>7. Kubernetes Profile</p> <p>First, create a new profile in Kubernetes Profiles, then select it.</p> <p>For OpenStack: If you choose a profile with enabled Taikun Load Balancer, you must fill in:</p> <ul> <li>Taikun Load Balancer Flavor</li> <li>Router ID Start Range</li> <li>Router ID End Range</li> </ul> <p>8. Add Policy Profile</p> <p>Select a Policy Profile for the project.</p> <p>9. Enable Monitoring</p> <p>Monitor cluster resources like memory, CPU, and storage.</p> <p>10. Enable Backup</p> <p>Choose backup credentials (can be created in Backup Credentials). (You can enable this later if needed.)</p> <p>11. Add Expiration Date</p> <p>Set project duration (default is infinite). ??? Note     After the expiration date, your project will NOT be deleted or locked.</p> <p>12. Enable Serverless</p> <p>Handle events and triggers for serverless functions. (Setting up DNS records is required.)</p> <p>13. Enable Spot Instances (AWS, Azure, Google)</p> <ul> <li>Allow Spot VMs \u2013 Enable spots for standalone VMs</li> <li>Allow Full Spot Kubernetes \u2013 Enable full spot Kubernetes</li> <li>Allow Spot Workers \u2013 Enable spot instances for workers</li> <li>No Spot for Kubernetes \u2013 Disable spot for Kubernetes</li> </ul> <p>14. Add Flavor</p> <p>Bind one or more flavors to the project. (You can modify them later in Flavor Info.)</p> <p>15. Enable Autoscaler</p> <p>Define auto-scaling for your Kubernetes cluster.</p> <ul> <li>Name \u2013 Set a name for the new server</li> <li>Minimum Workers Count \u2013 Set a minimum worker count</li> <li>Maximum Workers Count \u2013 Set a maximum worker count</li> <li>Disk Size \u2013 Define the disk size</li> <li>Flavor \u2013 Bind a flavor to the server</li> </ul>"},{"location":"Managing_your_Projects/Create_a_Server/","title":"Create a Server","text":"<p>To create a new server, click \"Add Server\" and fill in all the required fields.</p> <p>!!! Warning     If no flavor has been bound to your project, you will receive an error message. You can bind a flavor during project creation.</p> <pre><code>For a well-functioning cluster, **avoid using small flavors**.\n</code></pre> <p>A working Kubernetes cluster consists of at least:</p> <ul> <li>1 Bastion</li> <li>1 Kubemaster</li> <li>1 Kubeworker</li> </ul> <p>For high availability, you can create a Multimaster setup by adding more than one Kubemaster. The number of masters must always be odd.</p> <p>!!! Tip     Bastion: Recommended 2 vCPU + 2GB RAM Masters: Recommended 4 vCPU + 8GB RAM </p> <p></p> <p>Detailed Project</p>"},{"location":"Managing_your_Projects/Create_a_Server/#to-add-a-new-server-fill-in-the-following-information","title":"To add a new server, fill in the following information","text":"<p>1. Server Name</p> <ul> <li>Only alphanumeric characters and dashes are allowed (1-30 characters).</li> <li>You can use these shortcuts:</li> <li>b \u2013 Bastion</li> <li>m \u2013 Master</li> <li>w \u2013 Worker</li> </ul> <p>2. Disk Size</p> <ul> <li>Minimum required: 30GB.</li> </ul> <p>3. Role</p> <ul> <li>Choose a role for the server based on Kubernetes settings.</li> </ul> <p>4. Number of Servers</p> <ul> <li>Set the number of Kubeworkers or Kubemasters.</li> <li>For Multimaster: Use an odd number (minimum 3 masters) for high availability.</li> </ul> <p>5. Flavor</p> <ul> <li>Choose a server flavor from the list (e.g., n0.xlarge).</li> <li>You can bind flavors to the project in Flavor Info.</li> </ul> <p>6. Kubernetes Node Labels</p> <ul> <li>Assign labels to control where Pods are deployed.</li> <li>Use the \"Add Label\" button to add multiple labels.</li> <li>For more details, see the Kubernetes documentation.</li> </ul> <p>!!! Info       Larger flavors take longer to create.</p> <p>!!! Warning     Double-check the number of servers you need\u2014creation takes time (~7 min per server).     You cannot delete servers after creation.</p> <p>Once servers are created, click \"Deploy\".</p> <p></p> <p>Add Server</p>"},{"location":"Managing_your_Projects/Create_a_Server/#server-creation-stages","title":"Server Creation Stages","text":"<p>During deployment, servers pass through these stages: PENDING \u2192 UPDATING \u2192 READY</p> <p></p> <p>Detailed Project with Kubernetes cluster</p>"},{"location":"Managing_your_Projects/Create_a_Server/#failed-server-creation","title":"Failed Server Creation","text":"<p>If a server fails to create and the project is Failed or Pending: - Use the \"Repair\" button to restore it.</p>"},{"location":"Managing_your_Projects/Create_a_Server/#deleting-servers","title":"Deleting Servers","text":"<ul> <li>Managers/Partners can delete unnecessary servers.</li> <li>You can delete individual servers while keeping a functional cluster (1 Bastion, 1 Master, 1 Worker).</li> <li>You can also delete the entire cluster.</li> </ul> <p>Delete Kubernetes cluster</p> <p>!!! Info     You can remove some servers while maintaining a functional cluster.     If needed, delete the entire cluster.</p>"},{"location":"Managing_your_Projects/Enable_autoscaler/","title":"Enable Autoscaler","text":"<p>Autoscaling is a crucial feature in cloud computing and Kubernetes deployment. It helps organizations: - Optimize resource usage - Reduce operational costs - Ensure high availability of applications</p> <p>With Taikun\u2019s autoscaling, you can set rules to automatically scale workers based on conditions like CPU utilization. For example, when CPU usage exceeds a certain threshold, new workers will be added to the cluster.</p>"},{"location":"Managing_your_Projects/Enable_autoscaler/#enabling-autoscaler-during-project-creation","title":"Enabling Autoscaler During Project Creation","text":"<p>The first way to enable autoscaling is during the project creation process.</p> <p></p> <p>Enable Autoscaler</p>"},{"location":"Managing_your_Projects/Enable_autoscaler/#define-autoscaling-parameters","title":"Define Autoscaling Parameters","text":"<p>1. Name</p> <p>Specify a name for the autoscaler.</p> <p>2. Minimum Workers Count</p> <p>Define the minimum number of workers in the cluster.</p> <p>3. Maximum Workers Count</p> <p>Define the maximum number of workers in the cluster.</p> <p>4. Disk Size (GB)</p> <p>Set the disk size for the worker nodes.</p> <p>5. Flavor</p> <p>Choose a server flavor for the autoscaling instances.</p>"},{"location":"Managing_your_Projects/Enable_autoscaler/#enabling-autoscaler-in-an-existing-project","title":"Enabling Autoscaler in an Existing Project","text":"<p>You can also enable autoscaling after the project has been created.</p> <p></p> <p>Enable Autoscaler</p>"},{"location":"Managing_your_Projects/Flavor_Information/","title":"Flavor Information","text":"<p>!!! Note     To use a flavor for a server in a project, you must assign the flavor to the project first. This can be done during project creation or in the Flavor Info tab.</p> <p></p> <p>Flavor Infromation </p> <p></p> <p>Flavor-Project Bounds</p>"},{"location":"Managing_your_Projects/Flavor_Information/#binding-a-flavor-to-a-project","title":"Binding a Flavor to a Project","text":"<p>Choose the Organization, Cloud Type, and Credentials where your project is stored. After selecting a flavor, you can bind it to the project.</p> <p>If the flavor is already bound to the project, an error message will be displayed.</p>"},{"location":"Managing_your_Projects/Flavor_Information/#viewing-and-unbinding-flavors","title":"Viewing and Unbinding Flavors","text":"<p>To view all flavors used in projects, use the \"Flavor-Project Bounds\" button. Flavors for each cloud provider (Amazon, Azure, OpenStack) can be sorted by:</p> <ul> <li>Name</li> <li>Project Name</li> <li>CPU</li> <li>RAM</li> </ul> <p>To unbind a flavor from a specific project, select a Cloud, choose the project-flavor pair, and click the \"Unbind Flavor\" button.</p>"},{"location":"Managing_your_Projects/Images/","title":"Images","text":"<p>A Kubernetes Image is a container image that is deployed and managed within the cluster, which includes everything needed to run a piece of software, such as the code, runtime, libraries, environment variables, and configurations.</p> <p>To access your project images, follow these steps:</p> <ol> <li>At the Overview tab, select Projects.</li> <li>Choose your Project.</li> <li>Select VMs at your Project page.</li> </ol> <p></p> <p>VM Images</p> <p>!!! Note      Before creating a VM, you need to bind the image to the project first.</p>"},{"location":"Managing_your_Projects/Images/#bind-to-project","title":"Bind to Project","text":"<p>To bind an image to a project, follow these steps:</p> <ol> <li>Click Images at the VMs page.</li> <li>You can access Bound Images, My Images, and Public Images.</li> <li>After selecting an image, bind it to the project by clicking the Bind to Project button.</li> </ol> <p></p> <p>Bind to Projects</p> <p></p> <p>Public Image</p> <p>A success message is displayed when the image is successfully bound.</p> <p></p> <p>Successful Image binding</p> <p>If the image is already bound to the project, an error message will be displayed.</p> <p></p> <p>Image Already bound </p>"},{"location":"Managing_your_Projects/Images/#unbind-image","title":"Unbind Image","text":"<p>To unbind an image from a specific project, go to the My Images tab, select the image, and use the Unbind Image button.</p> <p></p> <p>Unbind Image</p>"},{"location":"Managing_your_Projects/Installing_Applications/","title":"Installing Applications","text":"<p>In Taikun, you can find more than 200 public and private repositories, which you can import with the Helm Repo URL.</p>"},{"location":"Managing_your_Projects/Installing_Applications/#public-repositories","title":"Public Repositories","text":"<p>Public repositories provide access to a wide range of pre-built applications and software packages. You can enable or disable public repositories based on your organization\u2019s needs.</p> <p></p> <p>Public Repositories</p>"},{"location":"Managing_your_Projects/Installing_Applications/#private-repositories","title":"Private Repositories","text":"<p>Taikun supports the addition of private repositories, allowing users to upload and manage their own custom applications and software packages. Additionally, Taikun offers support for OCI (Open Container Initiative) images, ensuring compatibility and flexibility in managing containerized applications.</p> <p></p> <p>Private Repositories</p>"},{"location":"Managing_your_Projects/Installing_Applications/#how-to-install-an-application-in-a-cluster","title":"How to Install an Application in a Cluster","text":""},{"location":"Managing_your_Projects/Installing_Applications/#enable-required-repositories-for-your-application","title":"Enable Required Repositories for Your Application","text":"<p>Enable Repository</p>"},{"location":"Managing_your_Projects/Installing_Applications/#create-a-catalog-for-adding-applications-and-binding-projects","title":"Create a Catalog for Adding Applications and Binding Projects","text":"<p>Create Catalog</p> <p>Catalogs are curated collections of applications and software packages available for deployment within Taikun. To create a catalog, navigate to the Catalogs section in the Taikun dashboard and follow the prompts to define the catalog name, description, and any relevant metadata.</p> <p></p> <p>Catalogs</p> <ul> <li>When creating a catalog in Taikun, provide a clear description of the catalog and the applications it contains.</li> <li>Only applications whose repositories have been enabled will be visible.</li> <li>To ensure a successful installation, it is recommended only to bind Projects that are healthy and in a ready state.</li> <li>Parameter configurations can easily be located by browsing or searching applications within Taikun.</li> <li>When installing an application, it is possible to specify a version or use the latest version automatically.</li> <li>If the parameters are set to be both editable when installing and after installation, it is mandatory to configure them before proceeding with the installation.</li> </ul>"},{"location":"Managing_your_Projects/Installing_Applications/#install-the-application-in-the-cluster","title":"Install the Application in the Cluster","text":"<p>Application in the Cluster</p> <ul> <li>In the Instance name, only alphanumeric characters are allowed.</li> <li>Multiple projects can be bound to any catalog. When installing, a list of bonded projects will be visible.</li> <li>A Custom Namespace can be input.</li> <li>If autosync is enabled, applications will automatically sync whenever any updates are made.</li> <li>Selected parameters can be added/removed while installing applications. Only parameters that are set as \"Editable when installing\" can have their values changed during installation.</li> <li>Extra values can be added to the application during installation.</li> </ul> <p></p> <p>Add Application</p>"},{"location":"Managing_your_Projects/Installing_Applications/#after-installing-the-application","title":"After Installing the Application","text":"<p>After installing the application, users can find the status of the application in: Catalogs \u27a1 Select User Catalog \u27a1 Select Installed Application \u27a1 Bound Projects</p> <p></p> <p>Added Application in catalog</p> <ul> <li>Inside the application, basic details, parameters, and default values can be found.</li> <li>Parameters can be edited, and applications can be synced again.</li> <li>Different versions of applications can be selected, but users can only select the latest version, not an older one.</li> <li>A catalog can only be deleted if it is empty and has no applications or projects attached. If a project is bound to it or an application has been added, it cannot be deleted.</li> </ul>"},{"location":"Managing_your_Projects/Installing_Applications/#find-all-installed-applications","title":"Find All Installed Applications","text":"<ul> <li>In the Applications section, you can find a list of all installed applications with information such as Namespace, Catalog Name, and Bound Projects.</li> <li>You can directly uninstall any of the installed applications from this section.</li> </ul>"},{"location":"Managing_your_Projects/Kubernetes_Upgrades_with_Taikun/","title":"Kubernetes Upgrades with Taikun","text":"<p>Upgrading your Kubernetes cluster with Taikun ensures access to the latest features and security patches, essential for maintaining a robust and secure environment. Taikun supports various cloud providers, including OpenStack, AWS, Azure, GCE, Tanzu, OpenShift, Zadara, and Zededa, allowing users to create bastion, worker, and master nodes for their Kubernetes clusters. When upgrading, it is crucial to have at least two worker nodes in addition to the bastion and master nodes. This configuration ensures that at least one healthy worker node remains available to maintain cluster functionality during the upgrade process. Taikun\u2019s Kubernetes upgrade process follows a step-by-step procedure, providing a smooth transition between versions. When a user initiates an upgrade, the system updates the cluster to the next available version in sequence, guaranteeing compatibility and stability at each stage. If the cluster is healthy and the upgrade is successful, the upgrade button becomes available again, allowing the user to continue upgrading incrementally. This systematic process ensures that clusters remain stable and reliable throughout the upgrade.</p>"},{"location":"Managing_your_Projects/Kubernetes_Upgrades_with_Taikun/#requirements","title":"Requirements","text":"<ul> <li>You have a Kubernetes cluster with at least two worker nodes in addition to the bastion and master nodes.</li> <li>Your cluster is running an older version of Kubernetes.</li> </ul>"},{"location":"Managing_your_Projects/Kubernetes_Upgrades_with_Taikun/#steps-to-upgrade-your-kubernetes-cluster-with-taikun","title":"Steps to Upgrade Your Kubernetes Cluster with Taikun","text":"<p>Step 1: Create a Kubernetes Cluster with Two Worker Nodes.</p> <p>Step 2: Ensure the Cluster is Healthy.</p> <p></p> <p>Healthy Cluster</p> <p>Step 3: Upgrade your Kubernetes cluster.</p> <p></p> <p>Upgrading Cluster</p> <p>Step 4: Post-Upgrade Verification</p> <ul> <li>After the upgrade, confirm that all nodes are running the latest version.</li> <li>Perform health checks to ensure the cluster is fully functional and stable.</li> </ul> <p></p> <p>Upgraded Healthy Cluster</p>"},{"location":"Managing_your_Projects/Overview_of_Taikun_Projects/","title":"Overview of Taikun Projects","text":"<p>On the Projects tab, you can preview all existing projects for your organization.</p> <p></p> <p>Overview</p>"},{"location":"Managing_your_Projects/Overview_of_Taikun_Projects/#project-properties","title":"Project Properties","text":"<p>Project ID, Name, Organization</p> <p>Changeless descriptions for each Project.</p> <p>Status</p> <p>The status shows your servers\u2019 current status and actions in the project in real-time. Below are all possible statuses with their description.</p> <ul> <li>Ready: All servers in the project are prepared without any issues.</li> <li>Deleting: One or more servers in your current project are being deleted.</li> <li>Failure: One or more servers failed for some reason during the action (for instance, booting or creating).</li> <li>Pending: One or more servers are pending, which means that, for example, they have not yet been created on the hosted platform.</li> <li>Updating: One or more servers in the project are being updated by Taikun during the creation process.</li> <li>Upgrading: One or more servers upgrade Kubernetes, cloud credentials, or others.</li> </ul> <p>Health</p> <p>This column describes the condition of the project cluster. Keep in mind that a good-working project should always be Healthy.</p> <ul> <li>Healthy: Cluster id without any further problems.</li> <li>None: Cluster is probably empty, and there is nothing to check.</li> <li>Unhealthy: Problems with connection to Kubernetes or Monitoring API.</li> <li>Unknown: Cannot connect Kubernetes API.</li> <li>Warning: Minor Issues.</li> </ul> <p>Creation Date</p> <p>The exact timestamp when the project was created.</p> <p>Kubernetes Version</p> <p>Shows the current Kubernetes version for each project.</p> <p>Cloud Type</p> <p>Shows which provider is hosting your project cluster</p> <ul> <li>OpenStack</li> <li>AWS</li> <li>Azure</li> <li>Google</li> <li>Proxmox</li> <li>Zadara</li> <li>Zededa</li> <li>Tanzu</li> <li>vSphere</li> <li>Red Hat Openshift</li> </ul> <p>K8's</p> <ul> <li>Kubernetes active</li> <li>Kubernetes not active</li> </ul> <p>Expiration Date</p> <p>This feature helps you manage your project \u2013 its shelf life. By default, the expiration date of your project is set to infinite. You can set it during the project creation or change it after the project creation with Extend Project Lifetime.</p> <p>!!! Warning     After the expiration date, your project is NOT affected, deleted, or locked. It will stay the same.</p> <p>Assigned Users</p> <p>Edit which users should have access to the project and confirm with the Update button.</p> <p>!!! Info     You can also assign the user to a project in Users.</p> <p>Show hidden columns</p> <p>Click the small arrow on the right side of the table to see more details.</p> <p></p> <p>Show Hidden columms</p> <p>Expand the table to see:</p> <ul> <li>Alerts count - number of firing alerts in a project</li> <li>Created by</li> <li>Last Modified</li> <li>Last Modified by</li> </ul> <p>Sorting</p> <p>Projects can be sorted by Project Name, Organization Name, Status, Creation Date, Kubernetes version, or Cloud Type. Also, the search bar can be used to find specific projects.</p> <p>Extend Project Lifetime</p> <p>This feature helps the user to manage their project\u2019s lifetime. By default, the expiration date of projects is set to infinite, or the user can set it during the creation of projects. Users can use this feature to select projects\u2019 expiration dates or extend the lifetime of projects.</p> <p>Reset Project Status</p> <p>This feature helps users manage their project status. By using this feature, users can reset their project status. In some cases, we need to change the status of the projects, so we do it with this feature. Below are all possible statuses that we use with this feature:</p> <ul> <li>Deleting</li> <li>Pending</li> <li>Pending Delete</li> <li>Pending Purge</li> <li>Pending Upgrade</li> <li>Purging</li> <li>Ready</li> <li>Updating</li> <li>Upgrading</li> </ul> <p></p> <p>Project Details</p>"},{"location":"Managing_your_Projects/Overview_of_Taikun_Projects/#lockunlock-or-delete-a-project","title":"Lock/Unlock or Delete a Project","text":"<ul> <li>Lock Project: Disable all buttons which can cause some changes in the project.</li> <li>Unlock Project: Enable action buttons.</li> <li>Delete: To delete a project. The project must be empty with the status Ready.</li> </ul>"},{"location":"Managing_your_Projects/Project_Details/","title":"Project Details","text":"<p>By clicking the selected Project, you are redirected to the Servers. Here you can see all servers for the project with their description.</p> <p></p> <p>Overview</p>"},{"location":"Managing_your_Projects/Project_Details/#project-info","title":"Project Info","text":"<p>Under the Servers title is a brief description of the project \u2013 such as Project, Organization, Project Status, Cloud Type, Kubernetes Version, Access Profile, Cloud Credentials, Kubernetes Profile, Alerting Profile, Policy Profile, Access IP Address (if you use this address to SSH connect, please do not use user ubuntu, it\u2019s in use by Taikun for managing the cluster) and Kubernetes Health. Some of these include links to e.g. cloud or profiles.</p> <p></p> <p>Project info</p> <p>You can also see here ETC = Estimated Time to Complete. It is the approximate time (in minutes) until the cluster will be completed.</p> <p></p> <p>ETC</p>"},{"location":"Managing_your_Projects/Project_Details/#servers","title":"Servers","text":"<p>Every server is described by ID, Server Name, IP Address, Flavor, CPU/RAM/Disk Size, Role, Status, K8s Health, Created, and Actions. If you expand the table, you can see Created By, Last Modified, and Last Modified By.</p> <p></p> <p>Details</p> <p>Server status can be:</p> <ul> <li>Deleting</li> <li>Pending</li> <li>Pending Delete</li> <li>Pending Purge</li> <li>Pending Upgrade</li> <li>Purging</li> <li>Ready</li> <li>Updating</li> <li>Upgrading</li> </ul>"},{"location":"Managing_your_Projects/Project_Details/#reboot-servers","title":"Reboot Servers","text":"<p>You can reboot servers directly from Taikun.</p> <p></p> <p>Actions</p>"},{"location":"Managing_your_Projects/Project_Details/#show-status","title":"Show Status","text":"<p>Shows current status from the cloud for 3 seconds.</p>"},{"location":"Managing_your_Projects/Project_Details/#reboot","title":"Reboot","text":"<p>You can choose HARD or SOFT reboot for each server.</p> <ul> <li>HARD \u2013 the power to the system is physically turned off and back again, causing an initial boot.</li> <li>SOFT \u2013 system restarts without the need to interrupt the power.</li> </ul> <p>!!! Info     Hard or soft reboot can be chosen only for OpenStack. For AWS and Azure, there is only a simple reboot available.</p>"},{"location":"Managing_your_Projects/Supported_Application_Runtime_Environments/","title":"Supported Application Runtime Environments","text":"<p>Integrating NVIDIA GPU acceleration and Wasm in Taikun CloudWorks offers a comprehensive platform that caters to diverse computing needs, providing versatility, performance optimization, seamless container integration, monitoring capabilities, and security measures for GPU-accelerated workloads.</p> <p>Runc\u00a0(pronounced \u201crun-see\u201d) is a lightweight, portable container runtime that is part of the Open Container Initiative (OCI). It is designed to be a universal container runtime that can run containers conforming to the OCI specification. Runc is often used as the default runtime for container engines like Docker. It provides the basic functionality needed to run containers, such as creating and managing container processes, setting up namespaces and control groups, and managing container filesystems.</p>"},{"location":"Managing_your_Projects/Supported_Application_Runtime_Environments/#1-nvidia-gpu-acceleration","title":"1. NVIDIA GPU Acceleration","text":"<p>High-Performance Computing:\u00a0Tap into the power of NVIDIA GPUs for high-performance computing tasks. CloudWorks seamlessly integrates NVIDIA GPU acceleration, enabling applications to harness parallel processing capabilities for enhanced performance and efficiency.</p> <p>Optimized Libraries and Frameworks:\u00a0Leverage NVIDIA\u2019s GPU-accelerated libraries and frameworks, such as CUDA and cuDNN, to unlock the full potential of GPU computing. CloudWorks supports the integration of these tools, providing developers with powerful resources for parallel processing.</p> <p>Scalable Container Deployment:\u00a0Effortlessly deploy GPU-accelerated applications in containers with NVIDIA GPU support. CloudWorks ensures compatibility with container orchestration tools, facilitating scalable and efficient deployment in containerized environments.</p>"},{"location":"Managing_your_Projects/Supported_Application_Runtime_Environments/#2-webassembly-wasm-integration","title":"2. WebAssembly (Wasm) Integration","text":"<p>Cross-Language Compatibility:\u00a0Taikun CloudWorks supports WebAssembly (Wasm), allowing the execution of applications written in languages like C, C++, and Rust. Achieve cross-language compatibility and deploy diverse applications within the CloudWorks environment.</p> <p>Sandboxed Execution Environment:\u00a0Wasm integration provides a secure and sandboxed execution environment for applications, ensuring isolation from the underlying system. This enhances security measures, making Taikun CloudWorks a reliable platform for diverse deployment scenarios.</p> <p>Web and Beyond:\u00a0Extend the reach of your applications beyond traditional web environments. With Wasm support, Taikun CloudWorks enables the deployment of versatile applications, providing a platform-independent solution for a wide range of use cases.</p>"},{"location":"Managing_your_Projects/VMs_Management/","title":"VMs Management","text":"<p>By clicking the VMs button on the Servers page or VMs View in Project page, you are redirected to the VM Servers. Here you can see all the virtual machines for your Project with their description.</p> <p></p> <p>VMs</p>"},{"location":"Managing_your_Projects/VMs_Management/#project-info","title":"Project Info","text":"<p>Under Servers, the title is a brief description of the project \u2013 such as Project Name (with locked/unlocked image), Project Status, Cloud Type, or Cloud Credentials.</p> <p></p> <p>Project info</p> <p>When there is some operation going on, you can also see here ETC = Estimated Time to Complete. It is the approximate time (in minutes) until the cluster will be completed.</p> <p></p> <p>ETC</p>"},{"location":"Managing_your_Projects/VMs_Management/#servers","title":"Servers","text":"<p>Every server is described by ID, Name, Flavor, IP Address, Public IP Address, Status, Profile, Image, and Created. If you expand the table, you can see the last modification made (Created By, Last Modified, Last Modified By).</p> <p></p> <p>Actions</p> <p>Server status can be:</p> <ul> <li>Deleting</li> <li>Failure</li> <li>Pending</li> <li>Pending Delete</li> <li>Pending Upgrade</li> <li>Ready</li> <li>Updating</li> <li>Upgrading</li> </ul>"},{"location":"Managing_your_Projects/VMs_Management/#actions","title":"Actions","text":"<p>Commit</p> <p>Sends the changes to the repository. Once the cluster is committed, you will see ETC in the project info.</p> <p>Repair</p> <p>When the servers are Failing, use the repair button.</p> <p>Add VM</p> <p>To create a new server, click the \"Add VM\" button and fill in all the fields. You, as a user, can\u2019t delete servers \u2013 think twice about which and how many servers you want to create.</p> <p>!!! Note     To create a Virtual Machine, Manager of your Taikun account needs to bind a Flavor and an Image to your Project along with creating a Standalone Profile.</p> <p></p> <p>Add VM</p> <p>!!! Note     Letters must be lowercase!</p> <p>Configuration Options</p> <ul> <li>Server Name: Only alphanumeric characters and dashes are allowed, 1-30 characters.</li> <li>Flavor: Choose from the list of offered flavors (e.g., n0.large).</li> <li>Image: Choose from the list of provided images (e.g., ubuntu-20.04).</li> <li>Volume Size: Minimal size is automatically filled in when you select an image, and you can only increase the volume size.</li> <li>Volume Type: (optional) Choose from the drop-down selection.</li> <li>Profile: Choose a Standalone profile.</li> <li>Count: How many VMs you want to create.</li> <li>Public IP: Check if you\u2019re going to enable public IP.</li> <li>Cloud-init: (optional) If set, it will override the SSH key from the standalone profile.</li> <li>Tags: Enter Key and Value.</li> <li>Disk: Enter Name, choose Size, and select Volume Type.</li> </ul> <p>!!!Warning     If the project is locked, you can\u2019t use Commit, Repair, or Add VM.</p>"},{"location":"Managing_your_Projects/VMs_Management/#vm-action-buttons","title":"VM Action Buttons","text":"<p>Action Button</p> <p>Download RDP File</p> <p>RDP file (Remote Desktop Protocol file) is a configuration file used to initiate a remote desktop connection to a virtual machine (VM) or a remote computer. It contains the remote computer\u2019s address, authentication credentials, display settings, and other connection parameters. By opening an RDP file, users can access and control a remote VM or computer as if they were physically present at its location.</p> <p>??? Info </p> <pre><code>To enable RDP file option:\n\nPort 3389 should be added in the Standalone profile since port 3389 is the default port used for Remote Desktop Protocol (RDP) connections.\n\nProject should be unlocked.\n</code></pre> <p>Show Console</p> <p>Show Console allows administrators or users to access a VM\u2019s desktop environment without needing a remote desktop connection or access through a network.</p> <p>Show Status</p> <p>The Show Status feature provides a valuable means of determining the operational state of a virtual machine (VM) by indicating whether it is currently active or inactive. This functionality serves as an informative tool for users and administrators to ascertain the real-time status of a VM within a virtualized environment.</p> <p>Shelve</p> <p>Shelving an instance refers to temporarily setting aside an instance within a Cloud computing environment while preserving its associated resources, which may include volumes or disks. By shelving an instance, users can suspend its operation, freeing up resources and reducing costs while retaining its exact state and configuration for future use.</p> <p>Unshelve</p> <p>Unshelve instance is the reverse operation of Shelve.</p> <p>Start</p> <p>The Start button within a virtual machine (VM) is a control mechanism that initiates powering on and starting a previously stopped or powered-off VM. Its primary function is to transition the VM from a halted or inactive state to an operational state, allowing it to resume normal functioning and perform its intended tasks.</p> <p>Stop</p> <p>The Stop button within a virtual machine (VM) is a control mechanism that facilitates the termination of all active processes and operations running within the VM. Its primary purpose is to halt the execution of tasks and bring the VM to a stopped state, preventing any further activity or access to its resources.</p>"},{"location":"Managing_your_Projects/VMs_Management/#reboot-servers","title":"Reboot Servers","text":"<p>Soft Reboot</p> <p>A soft reboot of a server within a virtual machine (VM) involves initiating a restart of the server\u2019s operating system while allowing the system\u2019s processes and tasks to complete their current operations before the restart takes place. This method ensures that ongoing operations are gracefully terminated, and resources are properly released before the server restarts.</p> <p>Hard Reboot</p> <p>Hard reboot refers to a process in which a server within a virtual machine environment undergoes a complete and immediate shutdown of all running processes, resulting in the termination of all active data and system operations. Following this abrupt termination, the server is restarted, initiating a fresh boot-up sequence.</p>"},{"location":"Managing_your_Projects/VMs_Management/#view-details","title":"View Details","text":"<p>The View details button provides users comprehensive access to essential information regarding virtual machines (VMs) within a virtualized environment. Users can retrieve detailed data such as creation details, disk specifications, profile name, associated security group, and tags linked to the VM by clicking this button.</p>"},{"location":"Managing_your_Projects/vCluster/","title":"vCluster","text":"<p>A virtual cluster is a complete Kubernetes cluster nested within a single physical host cluster. This design offers improved isolation and adaptability, making it ideal for multi-tenancy support. Virtual clusters enable multiple teams to operate independently on the same physical infrastructure, reducing conflicts, enhancing autonomy, and lowering costs. Each virtual cluster operates within a host cluster\u2019s namespace but behaves as an independent Kubernetes cluster with its own API server, control plane, and set of resources.</p> <p>Although a virtual cluster shares the host cluster\u2019s physical resources, such as CPU, memory, and storage, it engages with the host cluster for resource scheduling and networking while maintaining an abstraction layer that ensures operations within a virtual cluster do not directly impact the global state of the host cluster. Pods of the virtual cluster are scheduled directly by the parent cluster, which ensures no performance degradation.</p> <p>!!! Tip     A vCluster requires a healthy and ready cluster with Kubernetes v1.29.4 or the latest version of Kubernetes.</p>"},{"location":"Managing_your_Projects/vCluster/#creating-a-virtual-cluster","title":"Creating a Virtual Cluster","text":"<p>There are two ways to create a vCluster in Taikun:</p>"},{"location":"Managing_your_Projects/vCluster/#1-adding-a-vcluster-from-the-projects-page","title":"1. Adding a vCluster from the Project's Page","text":"<p>1. Access Project Page: In the Overview section, locate the desired project and navigate to its page.</p> <p>2. Select 'Add Virtual Cluster': Click on the dropdown menu next to the \u201cAdd Project\u201d button and choose the \u201cCreate Virtual Cluster\u201d option.</p> <p>3. Define Cluster Details:</p> <ul> <li>Name: Provide a unique and descriptive name for the virtual cluster.</li> <li>Organization: Assign the cluster to the appropriate organization.</li> <li>Parent Cluster: Specify the parent cluster from which the virtual cluster will inherit settings and resources.</li> </ul> <p>4. Configure Alerting (optional):</p> <ul> <li>Inherit Profile: Decide whether to use the alerting profile defined for the parent cluster, or</li> <li>Define New Profile: Create a custom alerting profile with specific settings.</li> </ul> <p>5. Set a Project Expiration (optional):</p> <ul> <li>Individual Expiration: If desired, set a specific expiration time for the virtual cluster, overriding the project\u2019s default expiration.</li> </ul> <p></p> <p>Add Virtual Cluster</p>"},{"location":"Managing_your_Projects/vCluster/#2-creating-a-vcluster-from-within-the-parent-cluster","title":"2. Creating a vCluster from Within the Parent Cluster","text":"<p>1. Access the Parent Cluster: Navigate to the cluster that will serve as the parent cluster for the vCluster.</p> <p>2. Open the vCluster Tab: Go to the vCluster tab next to the K8s View, VMs, and Installed Applications tabs.</p> <p>3. Add vCluster Details: Enter a unique and descriptive name for the virtual cluster.</p> <p>4. Configure Alerting and Project Expiration (Optional):</p> <ul> <li>Inherit Profile: Decide whether to use the alerting profile defined for the parent cluster, or</li> <li>Define New Profile: Create a custom alerting profile with specific settings.</li> </ul> <p>5. Set a Project Expiration (optional): Set up the alerting profile and project expiration as previously described.</p> <p></p> <p>Add Virtual Cluster</p>"},{"location":"Monitoring_your_Projects/Audit_Log/","title":"Audit Log","text":"<p>You get real-time notifications via Inbox but you can also preview all the changes made in the Audit Log. The Audit Log can be located under the Manager section of your account.</p> <p>See who (User Name) made a change (Category, Message), where (Project), and what time (When). Choose Start and End dates, Hours, and Minutes for filtering if needed.</p> <p></p> <p>Audit Log</p> <p>By clicking Project Name, you will be redirected to the Project, where the described change was made (if Project is still available).</p> <p>Choose Start and End dates, Hours, and Minutes for filtering if needed.</p> <p></p> <p>Logs</p>"},{"location":"Monitoring_your_Projects/Audit_Log/#events-also-can-be-filtered-by","title":"Events also can be filtered by","text":"<ol> <li>Organization: Select an organization, which affects which projects or data are available for filtering.</li> <li>Project: Filter based on a specific project within the selected organization.</li> <li>Filtered by: Additional filtering criteria.</li> <li>Availability: Filters results based on availability or deleted.</li> </ol> <p>You can download the report from the Audit log via the \"Download\" button in the .CSV format; filters remain applied in the download.</p>"},{"location":"Monitoring_your_Projects/Available_Monitoring_tools/","title":"Available Monitoring Tools","text":"<p>Taikun offers a variety of tables, dashboards, and logs that allow you to monitor the performance and health of your clusters easily. These features are readily available within the application, giving you quick and easy access to information. Whether you\u2019re looking for a high-level overview of your clusters or need to dig into the details, our monitoring features have you covered.</p> <p>This article covers available Taikun features that will help you observe the state of your Kubernetes clusters.</p>"},{"location":"Monitoring_your_Projects/Available_Monitoring_tools/#dashboard","title":"Dashboard","text":"<p>The first page you\u2019ll see after signing into Taikun is Dashboard. With its help, you can see an overview of the state of your existing Projects.</p> <p>Up here, you can find the 2 main elements for monitoring the state of your resources:</p>"},{"location":"Monitoring_your_Projects/Available_Monitoring_tools/#charts","title":"Charts","text":"<p>Projects, Server Statuses, Servers, Cloud Credentials, Nodes Overview, Pod overview, Project with Alerts, and Kubernetes Health.</p>"},{"location":"Monitoring_your_Projects/Available_Monitoring_tools/#tables","title":"Tables","text":"<p>Kubernetes, Project Resource Allocation, and Recent Events.</p> <p>Our interactive Charts include the following widgets:</p> <ul> <li>Projects: This widget displays the state of your Projects via four statuses (Succeeded/Failed/Pending/Updating).</li> <li>Server Statuses: To dive deeper into your Projects, the widget indicates the state of each particular server in your Projects.</li> <li>Servers: This widget is a graphical representation of the number of servers running on connected Clouds.</li> <li>Cloud Credentials: The number of connected Cloud services is shown here.</li> <li>Nodes Overview: Here you can find the state of every Node of your Projects.</li> <li>Pods Overview: Similarly to Nodes, this widget displays the amount of healthy/unhealthy Pods running on your servers.</li> <li>Projects with Alerts: This widget indicates the number of Projects having various alerts.</li> <li>Kubernetes Health: An indication of the health of your Kubernetes in active Projects.</li> </ul> <p></p> <p>Interactive Charts</p> <p>With Tables, you can access more detailed information about:</p> <ul> <li>Nodes: Overview of Nodes for each particular project showing their state of Disk, Memory, PIDs, and readiness.</li> <li>Pods: Detailed information on every Pod of a specified Project.</li> <li>Project Resources Allocation: Allocation of resources per Project.</li> <li>Recent Events: Log of all events related to existing Projects.</li> </ul> <p></p> <p>Tables</p> <p>!!! Tip     All widgets presented on the page are interactive \u2013 feel free to click on any section to see detailed information.</p>"},{"location":"Monitoring_your_Projects/Available_Monitoring_tools/#project-function-buttons","title":"Project Function Buttons","text":"<p>Taikun offers various services to monitor the condition of Kubernetes clusters apart from the main Dashboard menu. To access these services, you need to open a Project where you have established a Kubernetes cluster and click one of the function buttons.</p> <p>There are several function buttons available, including:</p> <ul> <li>History: This button allows you to view the history of your cluster, including any changes that have been made and the date and time they were made.</li> <li>K8s Infra: This button provides information about your Kubernetes cluster, including details on the condition of your Nodes and Pods, Storage classes, Network Policies.</li> <li>Events: This button allows you to view events that have occurred in your cluster, such as Health checks, Node registrations, and scheduling.</li> <li>Logs: This button allows you to view detailed Grafana logs for your cluster.</li> <li>Alerts: This button allows you to view any alerts that have been triggered in your cluster, such as if a service goes down or if there are any security issues.</li> <li>Metrics: This button allows you to view metrics for your cluster with the help of Prometheus, including CPU and memory usage.</li> </ul> <p>!!! Info     By clicking on these function buttons, you can get a deeper understanding of the condition of your Kubernetes cluster and take necessary actions to ensure it runs smoothly.</p> <p></p> <p>Kubernetes Information</p>"},{"location":"Monitoring_your_Projects/Chargeback/","title":"Chargeback","text":"<p>The Chargeback tab is an overview of prices for every billing rule you set up in your account. Change the Start date and End date to adjust the measured time period (for example from the beginning to the end of the month).</p> <p>In the table, you will find:</p> <p>Rule Name The name of the billing rule applied.</p> <p>Price The cost associated with the rule, either fixed or dynamic.</p> <p>Start Date When the billing rule starts to apply.</p> <p>End Date When the billing rule stops being active.</p> <p></p> <p>Chargeback</p>"},{"location":"Monitoring_your_Projects/Chargeback/#export-chargeback-data","title":"Export Chargeback data","text":"<p>The data displayed on the page are exported to a .CSV file \u2013 with the name of your organization and time period (if selected).</p>"},{"location":"Monitoring_your_Projects/Chargeback/#send-chargeback-data-to-e-mail","title":"Send Chargeback data to e-mail","text":"<p>Use the button to send selected data to your mail (provided in My Profile).</p> <p></p> <p>Send Chargeback</p>"},{"location":"Monitoring_your_Projects/Chargeback/#price-development","title":"Price Development","text":"<p>Graph for Price Development shows decrease and increase of prices for every rule. You can change the timeline \u2013 Daily, Monthly, and Yearly.</p> <p>!!! Note     Monthly and Yearly graphs are plotted if there is enough data.</p>"},{"location":"Monitoring_your_Projects/Configure_Alerting_Profile/","title":"Configure Alerting Profiles","text":"<p>Taikun is designed to inform you of any issues or events in your Kubernetes clusters. If something goes wrong with one of your Projects, our tool can send notifications to any email address, webhook, or available integrations. Alerting Profiles can be added to your account\u2019s new or existing Project.</p>"},{"location":"Monitoring_your_Projects/Configure_Alerting_Profile/#create-alerting-profile","title":"Create Alerting Profile","text":"<p>To configure alerts in your Projects follow these steps:</p> <p>First Step Navigate to the \u201cAlerting Profiles\u201d menu.</p> <p>Second Step Hit the \u201cAdd Alerting Profile\u201d button and</p> <ul> <li>Specify a Name of your configuration</li> <li>Choose any of the connected Slack profiles (optional)</li> <li>Select the periodicity of notification (HalfHour/Hourly/Daily/None)</li> </ul> <p>Additional menus allow you to configure any E-mail, Webhook, or supported integration to the Alerting Profile.</p> <p></p> <p>Add Alerting Profile</p> <p>!!! Info     Our list of available integrations includes OpsGenie, Pagerduty, Splunk, and Microsoft Teams.</p>"},{"location":"Monitoring_your_Projects/Configure_Alerting_Profile/#attach-alerting-profile-to-your-project","title":"Attach Alerting Profile to your Project","text":"<p>To finalize the setup, you should add the newly-created Alerting Profile to your Project. This can be done during Project creation or directly within an existing Project.</p>"},{"location":"Monitoring_your_Projects/Configure_Alerting_Profile/#add-profile-during-project-creation","title":"Add Profile during Project Creation","text":"<p>Whenever you create a new Project, you can specify an already created Alerting Profile in the setup menu:</p> <p></p> <p>Add alerting Profile in the setup menu</p>"},{"location":"Monitoring_your_Projects/Configure_Alerting_Profile/#add-profile-to-existing-project","title":"Add Profile to existing Project","text":"<p>Alternatively, you can attach your Alerting Profile within existing Projects:</p> <p></p> <p>Add Alerting Profile in Settings </p> <p>Prometheus: With that, Taikun will send all notifications from Prometheus to the tool of your choice. You can click the link to learn more about Prometheus.</p>"},{"location":"Monitoring_your_Projects/Enable_Monitoring_in_Projects/","title":"Enable Monitoring in Projects","text":"<p>Some of the features described in the Available Monitoring Tools are accessible only if a Project has Monitoring enabled. With this step, you connect your Project to other open-source projects, such as Grafana and Prometheus, to access advanced monitoring instruments. With their help, you will be able to quickly determine a reason for the cluster\u2019s failure or quickly define existing bottlenecks that can be re-configured to improve the speed of your product.</p> <p>Available Monitoring Tools: Learn more about Available Monitoring Tools.</p>"},{"location":"Monitoring_your_Projects/Enable_Monitoring_in_Projects/#enable-monitoring","title":"Enable Monitoring","text":"<p>There are 2 options for the activation of Monitoring tools:</p>"},{"location":"Monitoring_your_Projects/Enable_Monitoring_in_Projects/#enable-monitoring-during-the-project-creation","title":"Enable monitoring during the Project creation","text":"<p>Enable Monitoring </p>"},{"location":"Monitoring_your_Projects/Enable_Monitoring_in_Projects/#alternatively-it-can-be-activated-in-the-already-created-projects","title":"Alternatively, it can be activated in the already created Projects","text":"<p>Enable Monitoring in already created project</p> <p>The process might take up to 2 minutes. Upon successful activation, you will be able to work with the Logs, Alerts, and Metrics menus of your Project.</p>"},{"location":"Monitoring_your_Projects/Enable_Monitoring_in_Projects/#disable-monitoring","title":"Disable Monitoring","text":"<p>To turn the Monitoring off, hit the \u201cDisable Monitoring\u201d button and confirm your action in the next window.</p>"},{"location":"Monitoring_your_Projects/Events%2C_Logs%2C_and_Metrics_of_Projects/","title":"Events, Logs, and Metrics of Projects","text":"<p>To access descriptive details about the state of your Projects, open up any active Project and locate the Events, Logs, and Metrics buttons. With their help, you are able to locate any potential problems that might be happening within your clusters.</p>"},{"location":"Monitoring_your_Projects/Events%2C_Logs%2C_and_Metrics_of_Projects/#events","title":"Events","text":"<p>You are redirected to Events, where you can see all Kubernetes changes made in a Project. To preview details click each action use button. A green strip indicates a successful action, and a brown strip indicates a failed action.</p> <p>You can sort Events by:</p> <ul> <li>Search field</li> <li>Filling Start and End Date</li> <li>Tick Only failed to filter failed actions</li> </ul> <p></p> <p>Project events</p>"},{"location":"Monitoring_your_Projects/Events%2C_Logs%2C_and_Metrics_of_Projects/#logs","title":"Logs","text":"<p>Preview Kubernetes cluster logs to Grafana.</p> <p>Logs button is disabled if Monitoring is disabled. To view logs, you must first Enable Monitoring.</p> <p></p> <p>Logs</p> <p>!!! Info     Write your query and use Start date and End Date for sorting. You can also expand every message \u2013 red is an added action, without color is other log.</p> <p></p> <p>Detailed logs</p>"},{"location":"Monitoring_your_Projects/Events%2C_Logs%2C_and_Metrics_of_Projects/#alerts","title":"Alerts","text":"<p>The first thing you will see when you access the Alerts menu is Firing Alerts. This section is refreshed every 5 minutes, but you can also use the refresh button to see the most updated data.</p> <p></p> <p>Alerts</p> <p>To see all alerts, use the upper right Show All Alerts button. As seen above, firing alerts are marked with red color.</p> <p>You can silence alerts and sort all the alerts by firing, silenced, all, or resolved.</p> <p>Alerts are accessible only if Monitoring is enabled and the project is not empty.</p> <p>For each alert, you can see details and use a link that will redirect you to Metrics with the query already filled.</p> <p>The index number at Alerts shows the number of firing alerts. When the firing alerts are resolved, the number disappears.</p> <p>Firing alerts also work from the real-time notifications bell in header.</p> <p></p> <p>Notifications</p> <p>The notification contains a brief message on a specific Project and an exact time of the change of an Alert. After clicking Show Project, you can access the project in which the alert is.</p>"},{"location":"Monitoring_your_Projects/Events%2C_Logs%2C_and_Metrics_of_Projects/#metrics","title":"Metrics","text":"<p>Write a query, search Prometheus Metrics and preview the value needed.</p> <p>Modify Step or Date.</p> <p>Switch between Console and Graph for better results.</p> <p></p> <p>Metrics</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/","title":"Kubernetes dashboard","text":"<p>At your Project page, select \"Dashboard\".</p> <p>Users can compare metrics across different periods and namespaces without the need for extensive command-line knowledge by using Prometheus queries (PromQL). It helps monitor resource utilization, enabling effective capacity planning and allocation.</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#taikun-tab","title":"Taikun Tab","text":"<p>On Cluster Dashboard you can find:</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#node-overview","title":"Node Overview","text":"<p>Display information about cluster nodes, such as CPU and memory utilization, pod count, and other relevant metrics.</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#pod-overview","title":"Pod Overview","text":"<p>Show the status of pods within the project, including their current state, restart counts, and resource usage.</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#overcommitment","title":"Overcommitment","text":"<p>CPU and Memory overcommit.</p> <p>You can define a specific time range.</p> <p></p> <p>Time Range</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#general","title":"General","text":""},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#number-of-pods-per-namespace","title":"Number of pods per namespace","text":"<p>This query counts the total number of pods running in each namespace of the cluster.</p> <p></p> <p>Pod per namespaces</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#number-of-ready-nodes","title":"Number of ready nodes","text":"<p>Ready nodes</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#pods","title":"Pods","text":"<p>Show the status of pods within the project, including their current state, restart counts, and resource usage.</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#pod-restarts-by-namespaces","title":"Pod restarts by namespaces","text":"<p>This metric indicates the cumulative count of restarts for each pod and container in your Kubernetes cluster.</p> <p></p> <p>Pod per namespaces</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#top-10-containers-by-cpu-usage","title":"Top 10 containers by CPU usage","text":"<p>Running this query will provide you with a list of the top 10 containers based on their recent CPU usage rates.</p> <p></p> <p>by CPU usage</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#top-10-containers-by-memory-usage","title":"Top 10 containers by memory usage","text":"<p>Running this query will provide you with a list of the top 10 containers based on their recent memory usage rates.</p> <p></p> <p>by memory usage</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#pods-not-ready","title":"Pods not ready","text":"<p>List the number of total pods not ready in each namespace.</p> <p></p> <p>not ready</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#total-pods","title":"Total pods","text":"<p>List the number of total pods available in each namespace.</p> <p></p> <p>total pods</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#nodes","title":"Nodes","text":"<p>Display information about cluster nodes, such as CPU and memory utilization, pod count, and other relevant metrics.</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#memory-load","title":"Memory Load","text":"<p>This metric indicates the cumulative count of restarts for each pod and container in your Kubernetes cluster.</p> <p></p> <p>memory load</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#node-load-1","title":"Node Load 1","text":"<p>The average number of processes in the run queue or waiting for CPU time over the last 1 minute.</p> <p></p> <p>Load by last minute</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#node-load-5","title":"Node Load 5","text":"<p>The average number of processes in the run queue or waiting for CPU time over the last 5 minutes.</p> <p></p> <p>Load by last 5 minutes</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#node-load-15","title":"Node Load 15","text":"<p>The average number of processes in the run queue or waiting for CPU time over the last 15 minutes.</p> <p></p> <p>Load by last 15 minutes</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#overcommitment_1","title":"Overcommitment","text":"<p>CPU and Memory overcommit.</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#cpu-overcommit","title":"CPU overcommit","text":"<p>Compares the requested CPU resources (limits) of pods to the total available CPU capacity of the cluster.</p> <p></p> <p>overcommit</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#memory-overcommit","title":"Memory overcommit","text":"<p>Compares the requested memory of pods to the total available memory capacity of the cluster.</p> <p></p> <p>memory overcommit</p>"},{"location":"Monitoring_your_Projects/Kubernetes_Dashboard/#organization-tab","title":"Organization Tab","text":"<p>The organization tab shows the list of Queries added by the user.</p> <p>To add a Query, follow these steps:</p> <ol> <li>Click \"Add Query\"</li> <li>Name: The name of the query, which briefly describes its purpose or what it intends to retrieve.</li> <li>Expression: The actual PromQL expression that defines how to retrieve and process the metrics. This expression specifies the mathematical and logical operations to be applied to the metrics.</li> <li>Description: A brief explanation of what the query does or what kind of insight it provides. This description helps other users understand the query\u2019s purpose and usage.</li> <li>Category name: Categories can be related to the type of metric, the aspect of monitoring, or any other relevant grouping.</li> <li>Click \"Add\"</li> </ol>"},{"location":"Monitoring_your_Projects/Live_Servers/","title":"Live Servers","text":"<p>Managers of Taikun can access the Servers menu from the left-hand navigation panel to see the list of all running servers in the created Project to monitor their state.</p> <p>Every server in this menu is described by ID, Server Name, IP Address, Flavor, CPU/RAM/Disk Size, Role, Status, K8s Health, Created, and Actions. If you expand the table, you can see Created By, Last Modified, and Last Modified By.</p> <p></p> <p>Live Servers</p>"},{"location":"Monitoring_your_Projects/Live_Servers/#server-status-can-be-one-of-the-following","title":"Server status can be one of the following","text":"<ul> <li>Deleting</li> <li>Failure</li> <li>Pending</li> <li>Pending Delete</li> <li>Pending Upgrade</li> <li>Ready</li> <li>Updating</li> <li>Upgrading</li> </ul> <p>The table can be filtered by the specific configuration of your hardware (for instance, the number of CPUs of RAM) as well as sorted by Project, Organization, Role, Status, Kubernetes Health, and Creation time.</p>"},{"location":"Monitoring_your_Projects/Project_Quotas/","title":"Project Quotas","text":"<p>With Project Quotas, you can limit the amount of CPU, RAM, and storage each Project can use. This can help you save money by avoiding unnecessary resource consumption, and it can also help you ensure that your Projects run at optimal performance.</p> <p>To access Project Quotas, navigate to the main menu and select the \u201cProject Quotas\u201d option. From there, you can select the project that you want to set a resource limit and specify the maximum amount of CPU, RAM, and storage that your Project is allowed to use.</p> <p></p> <p>Project Quotas</p> <p>Every Project\u2019s limits can be changed to your custom values. You can edit the CPU/RAM/Disk Size boundaries by clicking the Pencil button.</p> <p></p> <p>Edit Resources</p>"},{"location":"Monitoring_your_Projects/Showback_rules/","title":"Showback rules","text":"<p>Showback rules in Taikun are used to track and report the usage of cloud resources by individual users or departments within an organization. The primary purpose of the Showback Rules is to provide visibility into the costs associated with using Cloud resources and promote accountability for resource usage.</p> <p></p> <p>Showback Rules</p> <p>Every Showback Rule is described by ID, Name, Metric Name, Organization, Labels (if any), Kind, Type, Global alert limit, Project alert limit, Price, Showback credentials (if any), date and time Created, and Actions.</p>"},{"location":"Monitoring_your_Projects/Showback_rules/#create-showback-rule","title":"Create Showback Rule","text":"<p>Create a new Showback Rule for your Organization by pressing the \u201c+ Add rule\u201d button.</p> <p></p> <p>Add Showback Rule</p> <p>Specify the following parameters:</p>"},{"location":"Monitoring_your_Projects/Showback_rules/#name","title":"Name","text":"<p>Choose a name for your rule.</p>"},{"location":"Monitoring_your_Projects/Showback_rules/#metric-name","title":"Metric Name","text":"<p>Use any valid Prometheus metrics.</p>"},{"location":"Monitoring_your_Projects/Showback_rules/#kind","title":"Kind","text":"<p>Choose kind for the rule:</p> <ul> <li>General \u2013 data source is Taikun</li> <li>External \u2013 data source is external \u2013 you need to make sure correct Showback Credentials are used</li> </ul>"},{"location":"Monitoring_your_Projects/Showback_rules/#type","title":"Type","text":"<p>Choose from:</p> <ul> <li>Count (calculate package as a unit \u2013 e.g., flavors)</li> <li>Sum (calculate per quantity \u2013 e.g., GBs)</li> </ul>"},{"location":"Monitoring_your_Projects/Showback_rules/#price","title":"Price","text":"<p>Billing in CZK per selected unit.</p>"},{"location":"Monitoring_your_Projects/Showback_rules/#project-alert-limit","title":"Project Alert Limit","text":"<p>Set a limit of alerts for one Project.</p>"},{"location":"Monitoring_your_Projects/Showback_rules/#global-alert-limit","title":"Global Alert Limit","text":"<p>Set limit of alerts for all Projects.</p>"},{"location":"Monitoring_your_Projects/Showback_rules/#showback-credentials","title":"Showback Credentials","text":"<p>Specify the to-be-used Showback Credentials.</p>"},{"location":"Monitoring_your_Projects/Showback_rules/#labels","title":"Labels","text":"<p>The label indicates the variable name (Label) and value of the label (Value).</p> <p>!!! Note     Once you add the rule, the cron job starts the calculation. The price for Showback Summary starts at the beginning of every hour and it can\u2019t be paused. Rule can be stopped by deletion.</p>"},{"location":"Monitoring_your_Projects/Showback_rules/#available-actions","title":"Available Actions","text":""},{"location":"Monitoring_your_Projects/Showback_rules/#edit-showback-rule","title":"Edit Showback Rule","text":"<p>Change the parameters of your rule.</p>"},{"location":"Monitoring_your_Projects/Showback_rules/#copy-showback-rule","title":"Copy Showback Rule","text":"<p>Create a copy of your rule (don\u2019t forget to change the rule\u2019s name).</p>"},{"location":"Monitoring_your_Projects/Showback_rules/#delete-rule","title":"Delete Rule","text":"<p>Delete a rule if it is not used anymore.</p>"},{"location":"Monitoring_your_Projects/Taikun_Notifications/","title":"Taikun Notifications","text":"<p>Taikun offers a range of alerting instruments to help you stay informed about the status of your clusters.</p>"},{"location":"Monitoring_your_Projects/Taikun_Notifications/#inbox","title":"Inbox","text":"<p>Whenever any User of your Organization makes a change (e.g. creating a Project, adding a Backup, deleting a cluster), you will be notified in Taikun\u2019s Notifications menu, accessible from every page.</p> <p></p> <p>Notifications</p> <p>By default, the Notifications menu shows the latest 15 events. Press the \u201cAll Notifications\u201d button to access the Audit Log menu showing all changes made within your account.</p>"},{"location":"Monitoring_your_Projects/Taikun_Notifications/#audit-log","title":"Audit Log","text":"<p>The Audit Log is a feature that allows you to view a complete history of events in your account. It provides a detailed record of actions taken by users, such as creating servers and Projects, adding new users, and activating Monitoring tools.</p> <p>One of the great features of the Audit Log is that you can filter the events by date, project, user, action type, and Project availability. This allows you to narrow down the events to a specific period, project, user, or action, making it easier to find the information you\u2019re looking for.</p> <p></p> <p>Audit Log</p> <p>If you need to track changes made to your account or audit the activity happening in your account, the Audit Log is an invaluable tool. Head to the Audit Log section from your Inbox and use the filters to view the most relevant events.</p>"},{"location":"Monitoring_your_Projects/Taikun_Notifications/#email-notifications","title":"Email notifications","text":"<p>Taikun also has the ability to send notifications to your email address in order to help you monitor the state of your Kubernetes clusters. By setting up these notifications, you can stay informed about the status of your clusters and take action if any issues arise.</p>"},{"location":"Monitoring_your_Projects/Taikun_Requests/","title":"Taikun Requests","text":"<p>In this tab, you can see all requests made within Taikun by users of your account. In the table, you can see Organization Name, User Name, User role, Request Type, End Point, Ip, Status, Created At, and Message.</p> <p></p> <p>Taikun Requests </p>"},{"location":"Monitoring_your_Projects/Taikun_Requests/#request-types","title":"Request Types","text":"<ul> <li>failed</li> <li>post</li> <li>put</li> <li>delete</li> <li>get</li> </ul>"},{"location":"Monitoring_your_Projects/Taikun_Requests/#status-codes","title":"Status Codes","text":"<ul> <li>200 \u2013 ok</li> <li>400 \u2013 bad request</li> <li>403 \u2013 forbidden</li> <li>404 \u2013 not found</li> <li>500 \u2013 server error</li> </ul> <p>Messages can be filtered by Organization, Request Type, and/or by date.</p> <p></p> <p>Taikun Requests Filter</p>"},{"location":"Monitoring_your_Projects/Taikun_Requests/#export-requests","title":"Export Requests","text":"<p>The data displayed on the page can be exported to a .csv file, and used filters are already applied.</p>"},{"location":"Monitoring_your_Projects/Taikun_Requests/#send-requests-to-e-mail","title":"Send Requests to E-mail","text":"<p>Use the button to send selected data to your email provided in My Profile.</p>"},{"location":"Monitoring_your_Projects/Ticketing_in_Taikun/","title":"Ticketing in Taikun","text":"<p>Whenever you face any obstacles in Taikun, you have a direct channel to communicate with our team within the web application. Ticketing is Taikun\u2019s Board view system for any questions and concerns. The page is divided into 4 sections.</p>"},{"location":"Monitoring_your_Projects/Ticketing_in_Taikun/#1-new","title":"1. New","text":"<ul> <li>create a new ticket</li> <li>update the ticket \u2013 Name and Description (only if you are an owner/creator of the ticket)</li> <li>delete the ticket (only if you are an owner/creator of the ticket)</li> <li>transfer the ticket to another user (Manager or Partner role only)</li> </ul>"},{"location":"Monitoring_your_Projects/Ticketing_in_Taikun/#2-open","title":"2. Open","text":"<ul> <li>update the ticket \u2013 Name and Description (only if you are owner of the ticket)</li> <li>transfer the ticket to another user (Manager or Partner role only)</li> <li>add a comment</li> <li>delete the comment \u2013 only the last one added by your user</li> <li>transfer the ticket to Closed (only if you are an owner of the ticket)</li> </ul>"},{"location":"Monitoring_your_Projects/Ticketing_in_Taikun/#3-closed","title":"3. Closed","text":"<ul> <li>transfer the ticket to another user (Manager or Partner role only)</li> </ul>"},{"location":"Monitoring_your_Projects/Ticketing_in_Taikun/#4-archived","title":"4. Archived","text":"<ul> <li>transfer the ticket to another user (Manager or Partner role only)</li> </ul> <p>Ticketing </p>"},{"location":"Monitoring_your_Projects/Ticketing_in_Taikun/#how-to-raise-a-ticket-with-the-taikun-team","title":"How to raise a ticket with the Taikun team","text":"<p>Use the plus button to create a new ticket. Fill in the Name and Description for a ticket. Please describe the problem as thoroughly as possible.</p>"},{"location":"Monitoring_your_Projects/Ticketing_in_Taikun/#update-the-ticket","title":"Update the ticket","text":"<p>Just click on the ticket and update the fields you want to change.</p>"},{"location":"Monitoring_your_Projects/Ticketing_in_Taikun/#delete-the-ticket","title":"Delete the ticket","text":"<p>The ticket can be only deleted by the ticket owner and only with the status New.</p>"},{"location":"Monitoring_your_Projects/Ticketing_in_Taikun/#let-our-team-take-care-of-the-ticket","title":"Let our team take care of the ticket","text":"<p>If you have nothing more to add to the ticket, leave it in the New section and it will be soon taken care of by our support.</p> <p></p> <p>Create a New Ticket</p>"},{"location":"Monitoring_your_Projects/Ticketing_in_Taikun/#transfer-ticket","title":"Transfer Ticket","text":"<p>Transfer the ticket to another user e.g. if you think he\u2019s more reliable to take care of the ticket. The ticket can be transferred to another user (Manager or Partner) from your Organization in the sections:</p> <ul> <li>New</li> <li>Open</li> <li>Closed</li> </ul>"},{"location":"Monitoring_your_Projects/Ticketing_in_Taikun/#how-to-deal-with-open-status","title":"How to deal with open status","text":"<p>Once a ticket is in the Open section, it is already taken care of or is waiting for the problem to be solved. You can add a comment to the ticket and you can do so only in Open status.</p> <p>Once the problem is solved you can transfer it to Closed.</p> <p>After the issue is solved, a member of our team will transfer the ticket to Archived.</p> <p></p> <p>Edit Ticket </p>"},{"location":"Monitoring_your_Projects/Usage_Reports/","title":"Usage Reports","text":"<p>You can use the Usage Reports tab to monitor the usage of Cloud resources. In every Project, you can see the number of used TCUs (Taikun Compute Units) for a selected period limited by Start Date and End Date.</p> <p>Sort data by Project or TCU and filter them by choosing available or deleted projects.</p>"},{"location":"Monitoring_your_Projects/Usage_Reports/#tcu","title":"TCU","text":"<ul> <li>Indicates the amount of resources spent by Taikun to manage your resources</li> <li>Total CPUs and GB of RAM used in a Project (TCU = 1 vCPU or TCPU = 1GB of RAM)</li> </ul> <p>Usage Reports</p> <p>Clicking the Project will redirect you to the Project\u2019s info page (works only for available Projects).</p>"},{"location":"Monitoring_your_Projects/Usage_Reports/#export-data","title":"Export data","text":"<p>The data displayed on the page can be exported to a .CSV file \u2013 with your Organization\u2019s name and time period (if selected).</p>"},{"location":"Monitoring_your_Projects/Usage_Reports/#send-reports-to-e-mail","title":"Send Reports to E-mail","text":"<p>Use the button to send selected data to your mail (provided in My Profile).</p>"},{"location":"Monitoring_your_Projects/Usage_Reports/#price-development","title":"Price Development","text":"<p>Graph for Price Development shows decrease and increase of prices for every project. You can change the timeline \u2013 Daily, Monthly, and Yearly.</p> <p></p> <p>Price Development </p> <p>!!! Note      Monthly and Yearly graphs are only available if sufficient data is available to plot.</p>"},{"location":"Navigating_in_Taikun/Applications/","title":"Applications","text":"<p>Taikun allows you to install applications to Kubernetes clusters directly from the web interface.</p> <p>Access the\u00a0Applications\u00a0section in the left-hand navigation panel to find the following tabs.</p>"},{"location":"Navigating_in_Taikun/Applications/#repositories","title":"Repositories","text":"<p>Access available Repositories to bind them to your Taikun account. Later, applications from these Repositories can be installed in your Kubernetes clusters.</p> <p></p> <p>Repositories</p> <p>!!! Warning      Recommended repositories contain the applications that are fully compatible with Taikun Projects and are verified by the Taikun security team.</p> <p>When managing critical applications or production environments, opting for trusted sources like Bitnami and Jenkinsci offers numerous benefits. Their well-tested software ensures stability and reliability, instilling confidence in performance. Enhanced security measures and regular updates safeguard against potential vulnerabilities, minimizing the risk of security breaches. Improved compatibility and strong community support ensure smooth integration with various Kubernetes environments.</p>"},{"location":"Navigating_in_Taikun/Applications/#catalogs","title":"Catalogs","text":"<p>Group the applications you would like to install in your clusters into Catalogs. With the help of this feature, you can predefine a selected list of apps that would be later installable in your Projects.</p> <p></p> <p>Catalogs</p>"},{"location":"Navigating_in_Taikun/Applications/#applications_1","title":"Applications","text":"<p>Access the Applications tab to locate the list of apps bound to your Projects through Catalogs. On this page, you can install and configure any app according to your needs and find an overview of each program.</p> <p></p> <p>Applications</p>"},{"location":"Navigating_in_Taikun/Applications/#explore","title":"Explore","text":"<p>Explore the list of all applications from the Repositories you bound. The page shows all lists of available apps both from public and private Repositories added earlier.</p> <p></p> <p>Explore</p>"},{"location":"Navigating_in_Taikun/Billing/","title":"Billing","text":"<p>To view, manage and monitor the finances of your account, open up the Billing section in the left-hand navigation panel. Here you will find all information related to the payment of your account and the sections of Chargeback, Usage Reports, Subscription, and Payment.</p>"},{"location":"Navigating_in_Taikun/Billing/#chargeback","title":"Chargeback","text":"<p>The Chargeback tab is an overview of prices for every billing rule you set up in your account. The Billing Rules calculate your usage from the moment they have been assigned to an Organization and indicate used TCUs for a defined period. Changing the Start and End dates allows you to adjust the measured period to monitor the exact spending.</p> <p></p> <p>Chargeback</p> <p>The table can be sorted by Rule Name or Price for a better overview.</p> <p>!!! Tip      Partners are able to monitor Chargeback for different Organizations.</p>"},{"location":"Navigating_in_Taikun/Billing/#export-chargeback-data","title":"Export Chargeback data","text":"<p>The data displayed on this page can be exported to a CSV file \u2013 with your organization\u2019s name and specified time period. Use the Send Chargeback data to Email button to send this information to your mailbox.</p>"},{"location":"Navigating_in_Taikun/Billing/#price-development","title":"Price Development","text":"<p>Graph for Price Development shows decrease and increase of prices for every rule. You can change the timeline \u2013 Daily, Monthly, and Yearly.</p> <p></p> <p>Proce Development</p> <p>!!! Note      Monthly and yearly graphs are plotted if there is enough data.</p>"},{"location":"Navigating_in_Taikun/Billing/#usage-reports","title":"Usage Reports","text":"<p>Usage Reports tab shows the usage of resources per every Project. With its help, you can see the amount of used TCUs for a defined time period. Data can be sorted by Project or TCU columns. You can also specify the availability of the Projects you are searching for (All/Available/Deleted)</p> <p></p> <p>Usage Reports</p> <p>!!! Note     TCU stands for Taikun Compute Unit. It indicates the total amount of CPUs and GBs of RAM under Taikun\u2019s management.</p>"},{"location":"Navigating_in_Taikun/Billing/#export-reports","title":"Export Reports","text":"<p>The data displayed on this page can be exported to a CSV file with your organization\u2019s name and time period (if selected) in its title. Use the Send Reports to E-mail to send selected data to your mailbox.</p>"},{"location":"Navigating_in_Taikun/Billing/#subscription","title":"Subscription","text":"<p>The Subscription section allows you to change your Taikun plan on the go. Pick any desired option from the list, and your account automatically switches to the chosen subscription.</p> <p></p> <p>Subscription</p>"},{"location":"Navigating_in_Taikun/Billing/#payments","title":"Payments","text":"<p>Use the Payments tab to locate the invoices for Taikun usage. You can sort the table by any available parameter, filter the list of invoices by Date Time, and change the attached credit card here.</p> <p></p> <p>Payments</p>"},{"location":"Navigating_in_Taikun/Configurations/","title":"Configurations","text":"<p>The Configurations tab in the navigation panel allows you to set up additional advanced account configurations. Up there, you will find two sections: Slack and User Tokens.</p>"},{"location":"Navigating_in_Taikun/Configurations/#slack","title":"Slack","text":"<p>With the help of this tool, you can easily configure a Slack integration that would send Taikun alerts to your Slack environment. Notifications allow you to efficiently monitor the condition of your Kubernetes cluster based on the metrics dispatched by Prometheus Alert Manager.</p> <p></p> <p>Slack configuration</p>"},{"location":"Navigating_in_Taikun/Configurations/#user-tokens","title":"User Tokens","text":"<p>This section is a central management place for the issue of your team\u2019s API tokens. Our versatile solution allows you to create a token with a variety of available permissions to make sure your colleagues can enjoy Taikun in full power both within our web app and a code.</p> <p></p> <p>User tokens</p> <p></p> <p>Add User tokens </p> <p>Detailed instructions on configuring a token can be found here.</p>"},{"location":"Navigating_in_Taikun/Configurations/#api-documentation","title":"API documentation","text":"<p>Our API documentation is available here.</p>"},{"location":"Navigating_in_Taikun/Credentials/","title":"Credentials","text":"<p>Credentials tab is a central management panel for all tools you can connect with Taikun. Whenever you want to connect your Cloud provider, Backup Storage, or Showback tool, the Credentials section is right for you!</p>"},{"location":"Navigating_in_Taikun/Credentials/#supported-cloud-credentials","title":"Supported Cloud credentials","text":"<p>The Cloud Credentials tab is where you can manage connected Cloud providers. This includes adding, modifying, or deleting connections of your Cloud services that include:</p> <p>!!! Warning     Only a Partner can add Proxmox and Tanzu.</p> <ul> <li>Amazon Web Services (AWS)</li> <li>Microsoft Azure</li> <li>Google Cloud Platform (GCP)</li> <li>OpenStack</li> <li>Tanzu</li> <li>Proxmox</li> <li>Openshift</li> <li>VMware vSphere</li> <li>Zededa</li> <li>Zadara</li> </ul> <p></p> <p>Cloud Credentials</p> <p>Each Cloud provider has different parameters that will be reflected in the created Table. Tables can be sorted differently depending on the available data, and you can also expand columns to see a detailed overview.</p>"},{"location":"Navigating_in_Taikun/Credentials/#backup-credentials","title":"Backup credentials","text":"<p>Taikun\u2019s Backup system will allow you to back up your Project data to any S3-compatible storage offered on the market. The Backup Credentials tab is where you can connect your Backup storage to ensure that the data stays safe.</p> <p>You can learn more about our Backup system here.</p> <p>Up here, you will find a tabs with parameters of connected storage that involve:</p> <ul> <li>ID: Unique identifier of the storage</li> <li>Organization: Name of the organization associated with the storage</li> <li>S3 Access Key ID: Access key for S3</li> <li>S3 Endpoint: URL of the S3 endpoint</li> <li>S3 Name: Name of the storage</li> <li>Associated Projects: Projects linked to this storage</li> <li>Actions: Possible actions, such as edit or delete</li> </ul> <p></p> <p>Backup Credentials</p>"},{"location":"Navigating_in_Taikun/Credentials/#billing-credentials","title":"Billing credentials","text":"<p>These credentials are required to ensure timely payments for cloud resources like computing power, storage, and networking. Cloud providers also use them to generate invoices and manage subscription plans.</p> <p>To add billing credentials, users need to provide the following details in the \u201cAdd Credentials\u201d dialog:</p> <ul> <li>Organization: Select your organization</li> <li>Name: Specify a name for your credentials (e.g., \u201cbilling\u201d)</li> <li>Username: Enter the billing service username (e.g., \u201cuser\u201d)</li> <li>Password: Enter the service password</li> <li>URL: Provide the service URL (e.g., https://prometheus.####.###).</li> </ul> <p>This information is required to configure and authenticate billing for cloud services.</p> <p></p> <p>Billing Credentials</p> <p>!!! Note     Only Partners have access to the Billing credentials section.</p>"},{"location":"Navigating_in_Taikun/Credentials/#showback-credentials","title":"Showback credentials","text":"<p>If you want to use an external source for your Showback rules, add Showback Credentials to this menu. The table will display all added credentials along with additional parameters.</p> <p></p> <p>Showback Credentials</p> <p>!!! Note     Partners can filter any available Credentials menu by an Organization.</p>"},{"location":"Navigating_in_Taikun/Credentials/#ai-credentials","title":"AI credentials","text":"<p>These credentials are used to verify the identity of users and give them the appropriate permissions to authenticate and interact with AI resources through API Key (Application Programming Interfaces).</p> <p> /// captions AI credentials ///</p>"},{"location":"Navigating_in_Taikun/Manager/","title":"Manager","text":"<p>Manager panel is a section available for Managers and Partners of your Taikun account. You will find buttons that let you efficiently configure your Taikun account properties, users, and associated costs.</p>"},{"location":"Navigating_in_Taikun/Manager/#flavor-info","title":"Flavor Info","text":"<p>This menu organizes the flavors (sizes) of servers available in your Taikun Projects. After setting the Cloud Type and Credential filters on top of the page, you will see your provider\u2019s list of flavors.</p> <p></p> <p>Flavor Information </p>"},{"location":"Navigating_in_Taikun/Manager/#audit-log","title":"Audit Log","text":"<p>Security is one of the most important aspects of working with any web application. Audit Log provides a comprehensive review of every action users take within Projects. The table can be filtered by multiple parameters, such as Date Time, Project, Category, and Availability.</p> <p></p> <p>Audit Log</p>"},{"location":"Navigating_in_Taikun/Manager/#users","title":"Users","text":"<p>Administer users of your account with ease within the Users section. Up here, you can see the complete list of your account members. You have the ability to sort the table by any available parameter. On top of that, it is a central management panel for adding new colleagues to your account and changing the properties of existing profiles. Colleagues with the User access role can also be assigned a Project here.</p> <p></p> <p>Users</p>"},{"location":"Navigating_in_Taikun/Manager/#project-quotas","title":"Project Quotas","text":"<p>Coordinate quotas (limits) for resources of your Projects. If specified, your Projects will have a limit on the number of vCPUs, RAM, and Volume used. Taikun sets its default quotas in the beginning \u2013 it can be configured at any point manually.</p> <p></p> <p>Project Quotas</p>"},{"location":"Navigating_in_Taikun/Manager/#servers","title":"Servers","text":"<p>View and manage the Servers being used in your Projects. The all-inclusive table provides a quick overview of the instances created with Taikun\u2019s help. Regardless of the number of Projects created in your instance, you can always access this menu to understand the condition of servers within one list.</p> <p>The table can be filtered by a Cloud Provider and by servers\u2019 resources (CPU, RAM, Disk Size)</p> <p></p> <p>Servers</p>"},{"location":"Navigating_in_Taikun/Manager/#taikun-requests","title":"Taikun Requests","text":"<p>In the Taikun Requests section, you can find logs describing each user\u2019s actions within Taikun. This allows users to easily track and monitor the usage of Taikun and use this information for monitoring the activity of your colleagues.</p> <p></p> <p>Taikun Requests</p>"},{"location":"Navigating_in_Taikun/Manager/#showback-rules","title":"Showback Rules","text":"<p>Coordinate rules for calculating the costs of using various Cloud resources within Taikun. By adding a new Showback Rule, you can specify a particular metric that will be used in the calculation of your Showback Summary.</p> <p></p> <p>Showback Rules</p>"},{"location":"Navigating_in_Taikun/Manager/#showback-summary","title":"Showback Summary","text":"<p>This menu provides a summary of all costs associated with the usage of Cloud providers. Values shown in there are purely for information purposes only. This is particularly helpful if you would like to calculate your costs based on specific metrics set in Showback Rules.</p> <p></p> <p>Showback Summary</p>"},{"location":"Navigating_in_Taikun/Manager/#ticketing","title":"Ticketing","text":"<p>View and manage tickets submitted to the Taikun team. There is no need to open any 3rd-party page or write an email to get in touch with us, as all communication is handled directly within Taikun.</p> <p></p> <p>Ticketing</p> <p>Feel free to drop any questions or concerns you might encounter \u2013 we will be happy to assist you!</p>"},{"location":"Navigating_in_Taikun/Overview/","title":"Overview","text":""},{"location":"Navigating_in_Taikun/Overview/#outline","title":"Outline","text":"<p>User panel is a section available to every User type of Taikun. Up here, you and your colleagues can access an overview of all your created Projects, create new Projects, and make your additions to the already existing ones. It is the main menu of Taikun, where all team members can locate all the necessary information about the state of your Clusters.</p> <p>The User section consists of 2 sub-sections: Dashboard and Projects</p>"},{"location":"Navigating_in_Taikun/Overview/#dashboard","title":"Dashboard","text":"<p>Dashboard menu contains eight interactive widgets and four tables that provide you with an overview of the state of all your Projects.</p> <p>The following widgets are shown:</p>"},{"location":"Navigating_in_Taikun/Overview/#projects","title":"Projects","text":"<p>This widget displays the state of your Projects via four statuses (Succeeded/Failed/Pending/Updating)</p>"},{"location":"Navigating_in_Taikun/Overview/#server-statuses","title":"Server Statuses","text":"<p>To dive deeper into your Projects, the widget indicates the state of each particular server in your Projects (using the same Succeeded/Failed/Pending/Updating statuses)</p>"},{"location":"Navigating_in_Taikun/Overview/#servers","title":"Servers","text":"<p>This widget is a graphical representation of the number of servers running on connected Clouds</p>"},{"location":"Navigating_in_Taikun/Overview/#cloud-credentials","title":"Cloud Credentials","text":"<p>The number of connected Cloud services is shown here</p>"},{"location":"Navigating_in_Taikun/Overview/#nodes-overview","title":"Nodes Overview","text":"<p>Here you can find the state of every Node of your Projects</p>"},{"location":"Navigating_in_Taikun/Overview/#pods-overview","title":"Pods Overview","text":"<p>Similarly to Nodes, this widget displays the amount of healthy/unhealthy Pods running on your servers</p>"},{"location":"Navigating_in_Taikun/Overview/#projects-with-alerts","title":"Projects with Alerts","text":"<p>This widget indicates the number of Projects having various alerts</p>"},{"location":"Navigating_in_Taikun/Overview/#kubernetes-health","title":"Kubernetes Health","text":"<p>An indication of the health of your Kubernetes in active Projects is shown here</p> <p></p> <p>Dashboard </p>"},{"location":"Navigating_in_Taikun/Overview/#our-tables-include-information-about","title":"Our Tables include information about","text":""},{"location":"Navigating_in_Taikun/Overview/#nodes","title":"Nodes","text":"<p>Overview of Nodes for each particular project showing their state of Disk, Memory, PIDs, and readiness</p>"},{"location":"Navigating_in_Taikun/Overview/#pods","title":"Pods","text":"<p>Detailed information on every Pod of a specified Project</p>"},{"location":"Navigating_in_Taikun/Overview/#project-resources-allocation","title":"Project Resources Allocation","text":"<p>Allocation of resources per Project</p>"},{"location":"Navigating_in_Taikun/Overview/#recent-events","title":"Recent Events","text":"<p>Log of all events related to existing Projects</p> <p></p> <p>Dashboard Details</p> <p>!!! Tip     All widgets presented on the page are interactive \u2013 feel free to click on any section to see detailed information.</p>"},{"location":"Navigating_in_Taikun/Overview/#projects_1","title":"Projects","text":"<p>The projects section displays all of your existing Projects in a table format. This overview allows you to easily access any selected Project for making changes, see its current state and additional information, and change preferred Project Properties.</p> <p>The complete list of properties includes:</p>"},{"location":"Navigating_in_Taikun/Overview/#general","title":"General","text":"<ul> <li>Project ID: Unique identifier for the Project</li> <li>Project Name: The name of the Project</li> <li>Organization: The Organization the Project belongs to</li> <li>Status: Current state of the Project (e.g., Ready, Failure, Updating, etc.)</li> <li>Health: Indicates the health status of the Project</li> <li>Creation Date: Date when the Project was created</li> </ul>"},{"location":"Navigating_in_Taikun/Overview/#kubernetes","title":"Kubernetes","text":"<ul> <li>K8s Version: The applied version of Kubernetes</li> <li>K8s: Quick link to the Project\u2019s Kubernetes overview</li> </ul>"},{"location":"Navigating_in_Taikun/Overview/#cloud","title":"Cloud","text":"<ul> <li>Cloud Type: The connected Cloud service (e.g., AWS, Azure, etc.)</li> <li>Expiration Date: Expiration date of the Project (if set)</li> </ul>"},{"location":"Navigating_in_Taikun/Overview/#access-actions","title":"Access &amp; Actions","text":"<ul> <li>Assigned Users: Users or teammates having access to the Project</li> <li>Actions: Action buttons to manage the Project</li> <li>View: Links to quickly access the Kubernetes (K8s) or Virtual Machines (VMs) overview</li> </ul> <p>Project Page</p> <p>Every Project can be opened by clicking on its name. Up there, you will be presented with an overview of a Project showing its servers, detailed state, and additional properties.</p> <p></p> <p>Project Details page</p> <p>You can also expand the servers\u2019 overview by clicking on the Show Hidden Columns button on the right-hand side of the screen. This will show additional fields Created By, Last Modified, and Last Modified by.</p> <p></p> <p>Project Actions</p>"},{"location":"Navigating_in_Taikun/Partner/","title":"Partner","text":"<p>The Partner section is designed for our strategic partners. It provides access to several additional features related to managing organizations. These features help partners efficiently manage their affiliated organizations and streamline their operations.</p>"},{"location":"Navigating_in_Taikun/Partner/#organizations","title":"Organizations","text":"<p>Organize your account structure with organizations, which can be linked to specific users, projects, and credentials. Organizations also help you get a better overview of each supported site, which can be selected via the dropdown menu.</p> <p></p> <p>Organizations</p>"},{"location":"Navigating_in_Taikun/Partner/#billing-rules","title":"Billing Rules","text":"<p>With Billing Rules, you can create new rules to calculate customers' cloud usage based on specific metrics and assign them to any available organization. The final result can be viewed in the Chargeback menu.</p> <p>Rules can be sorted by:</p> <ul> <li>Name</li> <li>Metric Name</li> <li>Creation Date</li> <li>Rule Type</li> </ul> <p></p> <p>Billing Rules</p> <p>!!! Note     Once the organization is linked, a cron job starts calculating usage. The billing period begins at the start of every hour.</p>"},{"location":"Navigating_in_Taikun/Profiles/","title":"Profiles","text":"<p>Profiles provide an overview of each available profile within Taikun. They allow you to configure templates for accessing your clusters, set up notifications for alerts, and more. If you want to use a profile in your projects, you need to add it and select the appropriate option during project creation.</p> <p>!!! Note     Kubernetes, Access, and Standalone Profiles can only be added during project creation. Alerting and Policy Profiles can be added to existing projects.</p>"},{"location":"Navigating_in_Taikun/Profiles/#kubernetes-profiles","title":"Kubernetes Profiles","text":"<p>Set parameters for your Kubernetes profile and select it from the dropdown menu during project creation. Each profile includes the following details: - ID - Name - Organization Name - CNI (Container Network Interface) - Octavia - Proxy on Bastion - Projects - Actions</p> <p></p> <p>Kubernetes Profiles</p>"},{"location":"Navigating_in_Taikun/Profiles/#access-profiles","title":"Access Profiles","text":"<p>To enable SSH access to your clusters, you must create an access profile. This profile defines the parameters required for remote access and can be selected during project creation.</p> <p></p> <p>Access Profiles</p>"},{"location":"Navigating_in_Taikun/Profiles/#alerting-profiles","title":"Alerting Profiles","text":"<p>Taikun allows you to monitor the status of Kubernetes clusters using various tools. Alerting profiles let you configure email, webhook, or other integrations to receive Kubernetes notifications.</p> <p></p> <p>Alerting Profiles</p>"},{"location":"Navigating_in_Taikun/Profiles/#policy-profiles","title":"Policy Profiles","text":"<p>Policy profiles define various security configurations that can be applied to Kubernetes clusters. Set the necessary parameters and choose a policy profile from the dropdown menu during project creation.</p> <p></p> <p>Policy Profiles</p>"},{"location":"Navigating_in_Taikun/Profiles/#standalone-profiles","title":"Standalone Profiles","text":"<p>Standalone profiles allow you to configure access settings for individual virtual machines (VMs) that are added to Kubernetes clusters within Taikun. Set the necessary parameters and select a standalone profile during project creation.</p> <p></p> <p>Standalone Profile</p>"},{"location":"Navigating_in_Taikun/Taikun_User_Interface/","title":"Taikun User Interface","text":"<p>This article provides an overview of the essential elements of Taikun\u2019s user interface, explaining each section for efficient navigation and usage.</p>"},{"location":"Navigating_in_Taikun/Taikun_User_Interface/#home-page","title":"Home Page","text":"<p>When you sign into Taikun, the home page is the first screen you will see. At the top of the interface, there is a panel with four buttons:</p> <ul> <li>A link to the documentation</li> <li>Search</li> <li>Notifications</li> <li>A button to access the \"My Profile\" page</li> </ul> <p>These buttons provide quick and easy access to important information and features.</p> <p></p> <p>Dashboard</p> <p>!!! Tip     The \"Docs\" button is interactive \u2013 it redirects you to the exact documentation article that explains the page you are viewing.</p>"},{"location":"Navigating_in_Taikun/Taikun_User_Interface/#navigation-panel","title":"Navigation Panel","text":"<p>The left-hand navigation panel contains all the main features of Taikun. Below are the sections available:</p>"},{"location":"Navigating_in_Taikun/Taikun_User_Interface/#overview","title":"Overview","text":"<p>This section provides everything needed to manage and monitor projects. You can create new projects, view and edit existing ones, and track their performance.  </p>"},{"location":"Navigating_in_Taikun/Taikun_User_Interface/#applications","title":"Applications","text":"<p>Here, you will find a list of applications that can be installed in your clusters to automate tasks or enhance functionality.  </p>"},{"location":"Navigating_in_Taikun/Taikun_User_Interface/#manager","title":"Manager","text":"<p>This section focuses on account management. It allows you to invite new users, view active servers, and manage flavor and image configurations.  </p>"},{"location":"Navigating_in_Taikun/Taikun_User_Interface/#billing","title":"Billing","text":"<p>Find all necessary information to manage payments with Taikun and connected cloud providers. This section includes billing history, payment method updates, and more.  </p>"},{"location":"Navigating_in_Taikun/Taikun_User_Interface/#credentials","title":"Credentials","text":"<p>This section is used to add credentials for projects, including cloud credentials, backup storage, and showback credentials.  </p>"},{"location":"Navigating_in_Taikun/Taikun_User_Interface/#profiles","title":"Profiles","text":"<p>Edit various profiles associated with your account, such as Kubernetes, Access, or Policy profiles, at any time.  </p>"},{"location":"Navigating_in_Taikun/Taikun_User_Interface/#partner","title":"Partner","text":"<p>For users with the Partner role, this section includes management tools for organizations and billing rule configurations.  </p>"},{"location":"Navigating_in_Taikun/Taikun_User_Interface/#configurations","title":"Configurations","text":"<p>This section is for advanced technical configurations, such as API setup and Slack notifications.  </p> <p></p> <p>Navigation Panel </p> <p>!!! Note     The navigation panel layout may vary based on user type.</p>"},{"location":"Profile_Management/Access_Profiles/","title":"Access Profiles","text":"<p>!!! Note     When using SSH to connect to the servers, do not use the user \"ubuntu\"; it is reserved for Taikun\u2019s cluster management.</p> <p>Access Profiles allow you to securely connect to your Bastion server.</p> <p></p> <p>Access Profiles</p> <p>The table can be extended to display the last modification details, including \"Last Modified\" and \"Last Modified By.\"</p>"},{"location":"Profile_Management/Access_Profiles/#actions","title":"Actions","text":""},{"location":"Profile_Management/Access_Profiles/#edit-http-proxy","title":"Edit HTTP Proxy","text":"<p>Update the access profile.</p>"},{"location":"Profile_Management/Access_Profiles/#delete-access-profile","title":"Delete Access Profile","text":"<p>Delete an Access Profile, but the default profile cannot be removed.</p>"},{"location":"Profile_Management/Access_Profiles/#adding-an-access-profile","title":"Adding an Access Profile","text":"<p>Create a new Access Profile to access a specific project by clicking the \"Add Alerting Profile\" button.</p> <p></p> <p>Add Access Profiles</p> <p>Specify the following parameters:</p> <ul> <li>Name: Choose a name for the new profile (3-30 characters).</li> <li>HTTP Proxy: Set a proxy server to create a gateway between the cluster and the Internet, allowing access to external packages, Docker images, etc. This ensures security by preserving your own IP.</li> </ul>"},{"location":"Profile_Management/Access_Profiles/#additional-configuration-options","title":"Additional Configuration Options","text":""},{"location":"Profile_Management/Access_Profiles/#ssh-users","title":"SSH Users","text":"<p>Allow a user to access the Kubernetes API by adding a Public Key (supported key types: RSA, ECDSA, Ed25519).</p>"},{"location":"Profile_Management/Access_Profiles/#dns","title":"DNS","text":"<p>Resolves alphabetic names to IP addresses. You can specify a DNS for your server.</p>"},{"location":"Profile_Management/Access_Profiles/#ntp-server","title":"NTP Server","text":"<p>Synchronizes local time clocks with a selected time server, ensuring all clusters operate in the same time zone.</p>"},{"location":"Profile_Management/Access_Profiles/#allowed-hosts","title":"Allowed Hosts","text":"<p>Define an IP address or range to restrict access to your Kubernetes environment.</p> <p>!!! Note     If your Public Key contains special characters (such as <code>:</code> or <code>+</code>), Taikun will fail to create the servers in Proxmox.</p> <p>!!! Note     DNS settings will be ignored if you choose to import network configurations when establishing Cloud Credentials.</p> <p>If you need to update any of these parameters, you can use the Show button to modify the necessary fields.</p>"},{"location":"Profile_Management/Alerting_Profiles/","title":"Alerting Profile","text":"<p>Alerting Profiles allow you to receive notifications about the state of your Project using your preferred tool.</p>"},{"location":"Profile_Management/Alerting_Profiles/#adding-an-alerting-profile","title":"Adding an Alerting Profile","text":"<p>To create a new Alerting Profile, click the \"Add Alerting Profile\" button.</p> <p></p> <p>Add Alerting Profile</p>"},{"location":"Profile_Management/Alerting_Profiles/#fill-in-the-following-fields","title":"Fill in the following fields","text":""},{"location":"Profile_Management/Alerting_Profiles/#organization","title":"Organization","text":"<p>Choose from the available Organizations in the drop-down list.</p>"},{"location":"Profile_Management/Alerting_Profiles/#name","title":"Name","text":"<p>Enter a name for your Alerting Profile.</p>"},{"location":"Profile_Management/Alerting_Profiles/#slack-configuration","title":"Slack Configuration","text":"<p>If you have a pre-configured Slack integration, you can apply it to this profile.</p>"},{"location":"Profile_Management/Alerting_Profiles/#reminder","title":"Reminder","text":"<p>Set a reminder interval: None, Half Hour, Hourly, or Daily.</p>"},{"location":"Profile_Management/Alerting_Profiles/#additional-notification-methods","title":"Additional Notification Methods","text":"<ul> <li>E-mails \u2013 Receive notifications via email.  </li> <li>Webhooks \u2013 Use webhooks for application alerts.  </li> <li>Integrations \u2013 Set notifications for supported applications (Opsgenie, PagerDuty, Splunk, Microsoft Teams).</li> </ul>"},{"location":"Profile_Management/Alerting_Profiles/#alerting-profile-details","title":"Alerting Profile Details","text":"<p>Alerting Profiles</p> <p>Each Alerting Profile includes:</p> <ul> <li>ID  </li> <li>Name  </li> <li>Organization Name  </li> <li>Slack Configuration Name  </li> <li>Associated Projects  </li> <li>E-mails  </li> <li>Webhooks  </li> <li>Created By  </li> <li>Reminder Settings  </li> </ul> <p>You can modify the Profile by adding or updating E-mails and Webhooks. Additionally, multiple headers (Key, Value) can be added to each webhook. The table also provides details about the last modifications, including \"Last Modified\" and \"Last Modified By.\"</p>"},{"location":"Profile_Management/Alerting_Profiles/#available-actions","title":"Available Actions","text":""},{"location":"Profile_Management/Alerting_Profiles/#lockunlock","title":"Lock/Unlock","text":"<p>Make the Profile available during Project selection (only non-default Profiles can be locked).  </p>"},{"location":"Profile_Management/Alerting_Profiles/#edit","title":"Edit","text":"<p>Modify the Profile\u2019s name, Slack configuration, or Reminder settings.  </p>"},{"location":"Profile_Management/Alerting_Profiles/#delete","title":"Delete","text":"<p>Remove a Profile that is not in use (only Profiles without associated Projects can be deleted).  </p>"},{"location":"Profile_Management/Alerting_Profiles/#attaching-an-alerting-profile-to-a-project","title":"Attaching an Alerting Profile to a Project","text":"<p>There are two ways to attach an Alerting Profile to a Project:</p> <ol> <li>During Project Creation \u2013 Check the \"Add Alerting Profile\" box while setting up the Project.</li> <li>After Project Creation \u2013 Use the Actions drop-down menu in the Project, click Attach Alerting Profile, and select the desired Profile.</li> </ol>"},{"location":"Profile_Management/Alerting_Profiles/#detaching-an-alerting-profile","title":"Detaching an Alerting Profile","text":"<p>To detach an Alerting Profile from a Project, use the Actions drop-down menu, click Detach Alerting Profile, and select the Profile you want to remove.</p>"},{"location":"Profile_Management/Creating_Profiles/","title":"Creating a Profile","text":"<p>To use Profiles in your Projects, you must add them in Taikun and select them during Project creation.</p> <p>!!! Note     Kubernetes, Access, and Standalone Profiles must be added during Project creation. They cannot be added later.</p>"},{"location":"Profile_Management/Creating_Profiles/#kubernetes-profile","title":"Kubernetes Profile","text":"<p>Set parameters for your Kubernetes Profile and select it from the drop-down menu during Project creation.</p> <p></p> <p>Add Kubernetes Profile</p>"},{"location":"Profile_Management/Creating_Profiles/#access-profile","title":"Access Profile","text":"<p>Set parameters for your Access Profile and select it from the drop-down menu during Project creation.</p> <p></p> <p>Add Access Profile</p>"},{"location":"Profile_Management/Creating_Profiles/#alerting-profile","title":"Alerting Profile","text":"<p>!!! Note     The Alerting Profile can be attached after Project creation.</p> <p>Set parameters for your Alerting Profile and select it from the drop-down menu during Project creation.</p> <p></p> <p>Add Alerting Profile</p>"},{"location":"Profile_Management/Creating_Profiles/#policy-profile","title":"Policy Profile","text":"<p>!!! Note     The Policy Profile can be attached after Project creation.</p> <p>Set parameters for your Policy Profile and select it from the drop-down menu during Project creation.</p> <p></p> <p>Add Policy Profile</p>"},{"location":"Profile_Management/Creating_Profiles/#standalone-profile","title":"Standalone Profile","text":"<p>Set parameters for your Standalone Profile and select it from the drop-down menu during Project creation.</p> <p></p> <p>Add Standalone Profile</p>"},{"location":"Profile_Management/Kubernetes_Profiles/","title":"Kubernetes Profiles","text":"<p>Each profile is characterized by the following parameters:</p> <ul> <li>ID</li> <li>Name</li> <li>Organization Name</li> <li>CNI (Container Network Interface)</li> <li>Octavia</li> <li>Proxy on Bastion</li> <li>Projects</li> <li>Actions</li> </ul> <p></p> <p>Kubernetes Profiles</p> <p>Expand the table to see the last modification (Last Modified and Last Modified By).</p>"},{"location":"Profile_Management/Kubernetes_Profiles/#add-kubernetes-profile","title":"Add Kubernetes Profile","text":"<p>You can create a new profile where you can enable a few features that can be customized using the CNI plugin.</p> <p></p> <p>Add Kubernetes Profile</p>"},{"location":"Profile_Management/Kubernetes_Profiles/#profile-settings","title":"Profile Settings","text":"<ul> <li>Organization (optional): Choose an organization for your profile.</li> <li>Profile Name: Enter the name for your Kubernetes profile (3\u201330 characters).</li> <li>Octavia: Exposes the service externally using OpenStack load balancers.</li> <li>Enable Taikun Load Balancer: Manage your traffic, available only for OpenStack and when Octavia is disabled.</li> <li>Proxy on Bastion: Exposes the service on each node with a static port (NodePort).   It will be possible to contact this service using <code>NodeIP:NodePort</code>.</li> <li>Scheduling on Master: Schedule Pods on the control plane node to maximize resource usage, but it is not recommended.   If scheduling on Master is disabled, the Kubernetes worker node should have more than 2 CPUs.</li> <li>Enable Unique Cluster Name: If not enabled, the cluster name will be <code>cluster.local</code>.</li> <li>Enable NVIDIA GPU Operator: The NVIDIA GPU Operator simplifies the deployment and management of GPU resources within a Kubernetes cluster. It automates the installation of critical components such as NVIDIA drivers, the NVIDIA Container Toolkit, and device plugins.</li> <li>Enable WebAssembly (Wasm): WebAssembly (Wasm) enables high-performance execution of compiled code within Kubernetes.   It allows efficient and secure running of applications written in C, C++, and Rust within Kubernetes environments.</li> <li>Proxmox Storage (optional): In Taikun, \"Proxmox storage\" refers to storage managed by Taikun, not native Proxmox storage. It is integrated into your Kubernetes environment for efficient storage management. The following storage options are available:</li> <li>NFS (Network File System): Allows sharing file storage across multiple servers.</li> <li>OpenEBS: A solution for container-attached storage for Kubernetes.</li> <li>Longhorn: A lightweight, highly available distributed block storage system for Kubernetes.</li> </ul>"},{"location":"Profile_Management/Kubernetes_Profiles/#specify-container-network-interface-cni","title":"Specify Container Network Interface (CNI)","text":"<p>Choose the CNI plugin to be installed in your Kubernetes cluster. Currently, Calico CNI is the default option. Cilium CNI will be added in future releases.</p>"},{"location":"Profile_Management/Kubernetes_Profiles/#actions","title":"Actions","text":"<ul> <li>Unlock/Lock: Unlock or lock the profile to enable/disable it from the drop-down selection when creating a new project. The default profile cannot be locked.</li> <li>Delete: Delete the profile if it is no longer needed.   Only profiles that are not associated with any project can be deleted. The default profile cannot be deleted.</li> </ul>"},{"location":"Profile_Management/Policy_Profiles/","title":"Policy Profiles","text":"<p>Policy Profile uses OPA (Open Policy Agent) to centralize operations, security, and compliance.</p> <p>When accessing the page, you can see an overview of all created profiles with selected rules and associated Projects.</p> <p>Policy Profiles allow administrators to define specific configurations and security policies that are automatically enforced across their cloud infrastructure. When you set up Policy Profiles, these policies are immediately applied to all relevant resources without requiring manual intervention.</p> <p>For instance, if your policy profile includes a rule to block certain actions, such as enabling NodePort services on instances, this rule will be enforced automatically. As a result, any attempt to enable NodePort will be automatically blocked by the policy, and this action will be logged in the system events for auditing and compliance purposes. This ensures that your cloud environment adheres to the defined security and operational standards consistently.</p> <p>Below is an overview of the types of actions that can be blocked by Policy Profiles:</p> <ul> <li>Unauthorized Network Access: Restricts access from specific IP addresses or ranges, protecting resources from potential malicious attacks and unauthorized access attempts.</li> <li>Resource Quota Exceeding: Prevents the creation of resources that exceed predefined quotas, such as limits on virtual machines, storage, or memory, helping manage resource allocation efficiently and prevents overconsumption that could lead to additional costs.</li> <li>Unapproved Software Installations: Blocks the installation of software packages that are not pre-approved. Ensures that only vetted software is used, maintaining security and compliance.</li> <li>Configuration Changes: Prevents unauthorized modifications to system configurations and maintains system stability, preventing inadvertent or malicious changes that could impact operations.</li> <li>Access to Sensitive Data: Restricts access to databases or storage buckets containing sensitive information, protecting sensitive data from unauthorized access and potential breaches.</li> <li>Port and Protocol Restrictions: Blocks the use of specific network ports or protocols, enhancing network security by eliminating the use of insecure or unnecessary ports and protocols.</li> <li>Security Group Modifications: Prevents unauthorized changes to security groups, ensuring that security group rules remain consistent and secure, controlling the flow of traffic appropriately.</li> <li>Public IP Attachments: Blocks the attachment of public IP addresses to instances without proper authorization and reduces the risk of exposing instances to the public internet, enhancing overall security.</li> <li>Service Account Restriction: Limits the permissions and capabilities of service accounts, preventing misuse of service accounts and limiting the potential damage from compromised accounts.</li> <li>Auto-scaling Constraints: Enforces rules on auto-scaling groups to prevent excessive scaling, controlling costs and maintaining system stability by preventing over-scaling.</li> </ul> <p></p> <p>Policy Profiles</p>"},{"location":"Profile_Management/Policy_Profiles/#add-policy-profile","title":"Add Policy Profile","text":"<p>When adding a new Policy Profile, specify the following parameters:</p> <ul> <li>Name: Choose a name for the profile.</li> </ul> <p>Features: </p> <ul> <li>Forbid NodePort: By choosing to forbid NodePort, you\u2019re making sure that this method isn\u2019t used to expose Services. This can be helpful for keeping things secure or ensuring specific networking rules are followed in your Kubernetes setup.</li> <li>Forbid HTTP ingresses: Activating this feature will require all Ingress resources to be HTTPS only, meaning that incoming traffic must be secured using HTTPS protocol, and HTTP traffic will not be allowed. This enhances security by ensuring communications are encrypted.</li> <li>Require Probe: Readiness probes ensure that a pod is ready to serve traffic, and liveness probes check if a pod is running as expected and can restart it automatically. Enforcing this policy ensures that pods have crucial probes set up for better reliability and resilience.</li> <li>Unique Ingress: Ensures that no two rules within the same Ingress resource have the same host specified. This helps prevent conflicts and ensures clear routing of external traffic to the correct services.</li> <li>Force Pod Resource: Mandates that all Kubernetes Pods explicitly specify resource limits and requests for CPU and memory, ensuring predictable performance and resource allocation across the cluster.</li> </ul> <p>Add: </p> <ul> <li>Allowed Repositories: If a policy profile specifies a list of approved repositories, only container images from these repositories will be permitted.</li> <li>Forbid Specific Tags: Forbids specific tags, ensuring that container images with any of the listed tags are not allowed within the Kubernetes cluster.</li> <li>Ingress Whitelist: Only specific Ingress resources listed in the whitelist are allowed within the cluster. Any Ingress resources not listed will be restricted or denied.</li> </ul> <p></p> <p>Add new Policy Profile</p> <p></p> <p>Actions</p>"},{"location":"Profile_Management/Policy_Profiles/#add-profile-to-a-project","title":"Add Profile to a Project","text":"<p>You can add the Profile during Project creation by checking \u201cAdd Policy Profile\u201d from the drop-down selection.</p> <p></p> <p>Add Policy Profile</p> <p>Enforce Policies after the Project is created. You can disable it in the same way.</p> <p></p> <p>Add Policy Profile in settings</p> <p>!!! Warning     Please keep in mind that namespaces Monitoring, Velero, and Kube-system violate these policies.</p>"},{"location":"Profile_Management/Standalone_Profiles/","title":"Standalone Profile","text":"<p>Standalone Profiles set specific policies for creating Virtual Machines within your clusters. You can see all Standalone profiles created for your Organization in this tab. Each profile is described by its ID, Name, Organization, and associated Virtual Machines.</p> <p></p> <p>Standalone Profiles</p>"},{"location":"Profile_Management/Standalone_Profiles/#available-actions","title":"Available Actions","text":"<ul> <li>Unlock/lock: Profile cannot be used if locked.</li> <li>Update: Edit profile properties.</li> <li>Delete: Remove not used profile.</li> </ul>"},{"location":"Profile_Management/Standalone_Profiles/#add-standalone-profile","title":"Add Standalone Profile","text":""},{"location":"Profile_Management/Standalone_Profiles/#to-create-a-new-profile-for-accessing-your-virtual-machine","title":"To create a new profile for accessing your Virtual Machine","text":"<ol> <li>First Step: Head to the Standalone Profile section in the left-hand navigation panel.</li> <li>Second Step: Press \u201c+ Add Standalone Profile\u201d icon in the top right corner.</li> <li>Third Step: Specify the profile\u2019s properties:</li> </ol> <p>Profile name</p> <p>Public key \u2013 insert public SSH key to access your VM</p>"},{"location":"Profile_Management/Standalone_Profiles/#you-can-add-a-security-group-to-your-profile-if-you-want-to-allow-ssh-access","title":"You can add a Security Group to your Profile, if you want to allow SSH access","text":"<p>1. First Step: Click \"Add Group\".</p> <p>2. Second Step: Choose a Name: use at least 3 alphanumeric characters.</p> <p>3. Third Step: Choose a type of Protocol:</p> <p>ICMP: reports errors, generating messages to the source IP.</p> <p>TCP: Uses the three-way handshake. Error-free data is more important than speed.</p> <p>UDP: Focused on speed, processing info as fast as possible.</p> <p>4. Fourth Step: Choose a Port Minimum Range and a Port Maximum Range: which port you want to allow access. Default port for SSH is 22.</p> <p>5. Fifth Step: Choose a Remote IP Prefix: Fill in the IP of the machine you want to allow use.</p> <p>6. Sixth Step: Click \"Add\" in the bottom right.</p> <p></p> <p>Add Standalone Profile</p> <p>!!! Warning     Profile can be edited only during creation.</p>"},{"location":"Supported_Cloud_Providers/All_Cloud_Providers_Overview/","title":"All Cloud Providers Overview","text":"<p>Taikun CloudWorks provides robust multi-cloud management that allows you to deploy, manage, and scale Kubernetes clusters across multiple cloud environments. This overview highlights the cloud providers supported by Taikun CloudWorks and refers you to our detailed documentation on integrating each provider into your workflow.</p> <p>Here is a brief overview of the cloud providers you can connect to Taikun CloudWorks:</p>"},{"location":"Supported_Cloud_Providers/All_Cloud_Providers_Overview/#1-amazon-web-services-aws","title":"1. Amazon Web Services (AWS)","text":"<p>Amazon Web Services (AWS) is a comprehensive and widely used cloud computing platform offered by Amazon, one of the world\u2019s largest e-commerce and technology companies. AWS provides a range of cloud services that enable individuals, businesses, and organizations to access and utilize computing resources over the Internet.</p> <p>Easily manage your Kubernetes clusters and virtual machines on AWS, using its global reach and extensive cloud services.</p> <p>You can learn more about AWS HERE.</p>"},{"location":"Supported_Cloud_Providers/All_Cloud_Providers_Overview/#2-google-cloud-platform-gcp","title":"2. Google Cloud Platform (GCP)","text":"<p>Google Cloud Platform (GCP) is a suite of cloud computing services provided by Google. It offers a wide array of tools and services that enable individuals and businesses to build, deploy, and manage applications, data storage, and various computing resources in the cloud.</p> <p>Deploy and manage Kubernetes clusters and virtual machines on GCP, leveraging its powerful analytics and machine learning tools.</p> <p>You can learn more about GCP HERE.</p>"},{"location":"Supported_Cloud_Providers/All_Cloud_Providers_Overview/#3-microsoft-azure","title":"3. Microsoft Azure","text":"<p>Microsoft Azure, commonly referred to as Azure, is a comprehensive cloud computing platform provided by Microsoft. It offers a wide range of services and tools for building, deploying, and managing applications and services through Microsoft\u2019s global network of data centers.</p> <p>Integrate Azure with Taikun CloudWorks to manage Kubernetes clusters and virtual machines and scale your applications effortlessly.</p> <p>You can learn more about Microsoft Azure HERE.</p>"},{"location":"Supported_Cloud_Providers/All_Cloud_Providers_Overview/#4-openstack","title":"4. OpenStack","text":"<p>OpenStack is an open-source cloud computing platform that allows organizations to create and manage public and private clouds. It provides software tools for building and managing cloud infrastructure, offering flexibility and control over your environment.</p> <p>Use OpenStack for your private cloud needs, and manage your Kubernetes clusters, virtual machines, and resources with Taikun CloudWorks.</p> <p>You can learn more about OpenStack HERE.</p>"},{"location":"Supported_Cloud_Providers/All_Cloud_Providers_Overview/#5-proxmox","title":"5. Proxmox","text":"<p>Proxmox VE (Virtual Environment) is an open-source server virtualization management platform. It allows you to create and manage virtual machines (VMs) and containers on a cluster of physical servers. Proxmox VE is based on Debian Linux and uses KVM (Kernel-based Virtual Machine) as the hypervisor and LXC (Linux Containers) for lightweight virtualization.</p> <p>Deploy and manage Kubernetes clusters and virtual machines on Proxmox, an open-source platform for virtualization.</p> <p>You can learn more about Proxmox HERE.</p>"},{"location":"Supported_Cloud_Providers/All_Cloud_Providers_Overview/#6-red-hat-openshift","title":"6. Red Hat OpenShift","text":"<p>OpenShift is a containerization and Kubernetes platform developed by Red Hat, a leading open-source software company. It is designed to simplify containerized applications\u2019 deployment, management, and scaling. OpenShift provides a comprehensive set of tools and features for building, deploying, and running container-based applications in a cloud-native and DevOps-friendly manner.</p> <p>Integrate OpenShift to benefit from its enterprise-grade Kubernetes management features and deploy applications directly from repositories.</p> <p>You can learn more about Red Hat OpenShift HERE.</p>"},{"location":"Supported_Cloud_Providers/All_Cloud_Providers_Overview/#7-vmware-tanzu","title":"7. VMware Tanzu","text":"<p>Tanzu is a suite of products and services developed by VMware to facilitate the deployment, management, and modernization of applications across multi-cloud and hybrid-cloud environments. It aims to simplify and accelerate the process of building, running, and managing applications using modern cloud-native technologies.</p> <p>Manage Kubernetes clusters on VMware Tanzu, designed to modernize applications and infrastructure.</p> <p>You can learn more about VMware Tanzu HERE.</p>"},{"location":"Supported_Cloud_Providers/All_Cloud_Providers_Overview/#8-vmware-vsphere","title":"8. VMware vSphere","text":"<p>vSphere is a virtualization platform developed by VMware. It enables organizations to create and manage virtualized IT environments, including virtual machines (VMs), on a large scale. It also virtualizes features such as computing, storage, and networking resources.</p> <p>Use vSphere\u2019s virtualization capabilities to deploy and manage Kubernetes clusters and virtual machines efficiently.</p> <p>You can learn more about VMware vSphere HERE.</p>"},{"location":"Supported_Cloud_Providers/All_Cloud_Providers_Overview/#9-zadara","title":"9. Zadara","text":"<p>Zadara\u2019s storage infrastructure is built to be highly available and reliable. It offers a wide range of storage options, including block, file, and object storage, as well as specialized solutions for specific use cases such as high-performance computing (HPC) and big data analytics.</p> <p>Leverage Zadara\u2019s cloud services to run and manage Kubernetes clusters and virtual machines with Taikun CloudWorks.</p> <p>You can learn more about Zadara HERE.</p>"},{"location":"Supported_Cloud_Providers/All_Cloud_Providers_Overview/#10-zededa","title":"10. Zededa","text":"<p>Zededa is a cloud-based platform that simplifies applications\u2019 deployment, management, and security on edge devices. It provides a central control point for distributing and updating software across diverse hardware, ensuring seamless operations. ZEDEDA adopts a zero-trust security model to protect sensitive data and applications from cyber threats in edge environments while leaving data handling and processing to the user\u2019s specific needs.</p> <p>With Taikun CloudWorks, you can manage infrastructure and application deployment across edge devices using ZEDEDA\u2019s powerful platform.</p> <p>You can learn more about Zededa HERE.</p>"},{"location":"Supported_Cloud_Providers/Amazon_Web_Services/","title":"Amazon Web Services","text":"<p>Amazon Web Services (AWS) is a comprehensive and widely used cloud computing platform offered by Amazon, one of the world\u2019s largest e-commerce and technology companies. AWS provides a range of cloud services that enable individuals, businesses, and organizations to access and utilize computing resources over the Internet.</p> <p>Here you can learn more about Amazon Web Services.</p>"},{"location":"Supported_Cloud_Providers/Amazon_Web_Services/#where-to-find-the-amazon-web-services-aws-security-key","title":"Where to find the Amazon Web Services (AWS) security key","text":"<ol> <li>Log into your AWS account</li> <li>Click on your profile name in the top-right corner and access the Security credentials section.</li> </ol> <p>AWS</p> <p>3. Create a new Access key along with a Secret key in the Access key ID and secret access section.</p> <ul> <li>Alternatively, you can use your previous combination (as long as you have your Secret key).</li> </ul> <p>Here you can learn more about Amazon Web Services secret access key.</p>"},{"location":"Supported_Cloud_Providers/Amazon_Web_Services/#adding-aws-connection-to-taikun","title":"Adding AWS connection to Taikun","text":""},{"location":"Supported_Cloud_Providers/Amazon_Web_Services/#1-switch-to-cloud-credentials","title":"1. Switch to Cloud Credentials","text":"<p>Navigate to the Cloud Credentials section in Taikun.</p>"},{"location":"Supported_Cloud_Providers/Amazon_Web_Services/#2-add-cloud-credentials","title":"2. Add Cloud Credentials","text":"<p>Hit the Add Cloud Credentials button located in the top-right corner.</p>"},{"location":"Supported_Cloud_Providers/Amazon_Web_Services/#3-specify-parameters","title":"3. Specify Parameters","text":"<p>Fill in the required fields in the menu:</p> <ul> <li>Cloud Name: Enter a name for your Cloud Credentials (3-30 characters, e.g., <code>cloud-test</code>).</li> <li>Access Key ID and Secret Access Key: Provide your AWS credentials.</li> <li>Region: Choose a suitable region.</li> <li>Availability Zone: Select an availability zone for the region.</li> </ul>"},{"location":"Supported_Cloud_Providers/Amazon_Web_Services/#4-save-your-credentials","title":"4. Save Your Credentials","text":"<p>After filling in all the required parameters, save the credentials to finalize the process.</p> <p></p> <p>AWS Cloud Credentials</p> <p></p> <p>Add Cloud Crdentials</p>"},{"location":"Supported_Cloud_Providers/Google_Cloud_Platform/","title":"Google Cloud Platform","text":"<p>Google Cloud Platform (GCP) is a suite of cloud computing services provided by Google. It offers a wide array of tools and services that enable individuals and businesses to build, deploy, and manage applications, data storage, and various computing resources in the cloud.</p> <p>Learn more about Google Cloud Platform</p>"},{"location":"Supported_Cloud_Providers/Google_Cloud_Platform/#requirements-for-google-cloud-platform","title":"Requirements for Google Cloud Platform","text":"<p>To successfully establish a connection between your Taikun and GCP accounts:</p> <ul> <li>Ensure that you have created a Service account in GCP</li> <li>Add a new principal for a Billing account</li> </ul>"},{"location":"Supported_Cloud_Providers/Google_Cloud_Platform/#create-a-service-account-in-gcp","title":"Create a Service account in GCP","text":"<p>1. Go to console.cloud.google.com</p> <p>2. Select your project from the folder</p> <p></p> <p>Project Folder</p> <p>3. Select IAM &amp; Admin from the hamburger menu</p> <p></p> <p>IAM &amp; Admin</p> <p>4. Switch to the Service Accounts tab</p> <p></p> <p>Service Accounts</p> <p>5. Add a new Service Account and specify the necessary parameters in the Google section:</p> <ul> <li>As you add the service account name, it will automatically generate a service account ID, which you can copy for further steps</li> <li>In the Service account description, you need to describe what this service account will do (optional)</li> <li>Click Create and continue</li> <li>Region \u2013 choose a suitable region</li> <li>Grant this service account access to the project so that it has permission to complete specific actions on the resources in your project (optional)</li> <li>Grant access to users or groups that need to perform actions within this Service Account (optional)</li> <li>Add the ID in the selected project IAM\u2014add\u2014the principal ID that we created in the service account with the owner or editor role</li> </ul> <p>6. Click on KEYS and ADD KEY:</p> <ul> <li>You need to create a new key in a .json format for a new Service Account. If you have any existing key, you can add it, but the Principal ID should be the same in the .json file as the service ID</li> </ul> <p>7. Select the project's folder and click on IAM from the hamburger menu</p> <p></p> <p>GCP - IAM</p> <p>8. Click on add to roles for service account: in the new principal, you have to add the ID that you created in the service account</p> <p></p> <p>IAM - Roles</p> <p>!!! Note     You can find ID in the downloaded .json file.</p>"},{"location":"Supported_Cloud_Providers/Google_Cloud_Platform/#billing-account","title":"Billing Account","text":"<p>1. Select your project and select Billing from the hamburger menu</p> <p></p> <p>Billing Account</p> <p>2. Click on Manage button located next to the Billing account section</p> <p></p> <p>Manage Billing Account</p> <p>3. Hit the Add Principal button</p> <p></p> <p>Add Principal </p> <p>4. Add your Service account ID with the Billing Account User role</p> <p></p> <p>Add Principal to the service account</p>"},{"location":"Supported_Cloud_Providers/Google_Cloud_Platform/#project-configuration","title":"Project Configuration","text":"<p>Taikun added support for importing existing Google projects when creating a Google Cloud credential. The procedure is the following:</p> <p>1. Enable billing on the project 2. Enable the following APIs on the project:</p> <pre><code>cloudresourcemanager.googleapis.com\niam.googleapis.com\nlogging.googleapis.com\ncloudbilling.googleapis.com\ncompute.googleapis.com\nserviceusage.googleapis.com\n</code></pre>"},{"location":"Supported_Cloud_Providers/Google_Cloud_Platform/#requirements-for-google-cloud-connection","title":"Requirements for Google Cloud Connection","text":"<p>For a normal Google Cloud connection in Taikun, you need:</p> <p>1. Billing user permission</p> <p>2. IAM for folder:</p> <ul> <li> <p>Folder Admin</p> </li> <li> <p>Project Creator</p> </li> </ul> <p>3. Service Account's project must have:</p> <ul> <li> <p>API enabled (same as listed above)</p> </li> <li> <p>Billing enabled</p> </li> </ul>"},{"location":"Supported_Cloud_Providers/Google_Cloud_Platform/#adding-gcp-connection-to-taikun","title":"Adding GCP connection to Taikun","text":"<p>To add your GCP credentials in Taikun:</p> <p>1. Switch to the Cloud credentials in the left-hand navigation panel</p> <p>2. Hit the Add Cloud Credentials button in the top-right corner</p> <p>3. Specify the necessary parameters in the Google section:</p> <ul> <li> <p>Cloud Name \u2013 choose a name for your Cloud Credentials (3-30 characters, e.g. cloud-test)</p> </li> <li> <p>Import Project \u2013 Configure your GCE account without folder ID and billing account details</p> </li> <li> <p>Folder ID and Config file \u2013 find credentials in your GCE account (under My Security Credentials)</p> </li> <li> <p>Region \u2013 Choose a suitable region</p> </li> <li> <p>Zone Cont \u2013 choose availability for the region</p> </li> <li> <p>Billing Account \u2013 Here you will see the billings account which is configured with your project and user</p> </li> </ul> <p></p> <p>GCP Cloud Credentials</p> <p></p> <p>Add Cloud Credentials</p>"},{"location":"Supported_Cloud_Providers/Microsoft_Azure/","title":"Microsoft Azure","text":"<p>Microsoft Azure, commonly referred to as Azure, is a comprehensive cloud computing platform provided by Microsoft. It offers a wide range of services and tools for building, deploying, and managing applications and services through Microsoft\u2019s global network of data centers.</p> <p>Here you can learn more about Microsoft Azure.</p>"},{"location":"Supported_Cloud_Providers/Microsoft_Azure/#requirements-for-microsoft-azure","title":"Requirements for Microsoft Azure","text":"<p>Before adding the Azure account, you must create an application registration with commands.</p> <p>Here is a link on GitHub with Azure commands.</p> <p>!!! Note     The provided instructions are specific to Linux. It might look different from another operating system.</p> <p>1) If you haven\u2019t installed Azure CLI, you can do it with the following command:</p> <pre><code>sudo apt install azure-cli -y\n</code></pre> <p>2) Login</p> <pre><code>sudo apt-get install azure-cli\n</code></pre> <p>You will be redirected to an Azure page where you can choose your account:</p> <p></p> <p>Azure Web Login </p> <p>CLI output will be similar to this: </p> <pre><code>[\n  {\n    \"cloudName\": \"AzureCloud\",\n    \"id\": \"c0xxxxa5-xxx-4ecb-xxxx-f37bxxxx28d6\",\n    \"isDefault\": true,\n    \"name\": \"Bezplatn\u00e1 zku\u0161ebn\u00ed verze\",\n    \"state\": \"Enabled\",\n    \"tenantId\": \"32xxxxb3-xxx-46b3-xxxx-0exxxxc46d1\",\n    \"user\": {\n      \"name\": \"usermail@gmail.com\",\n      \"type\": \"user\"\n    }\n  }\n]\n</code></pre> <p>You\u2019ll need to fetch the Azure Subscription ID (\u201cid\u201d) and Azure Tenant ID (\u201ctenantID\u201d) fields from the output. Here\u2019s what we would use in our test instance:</p> <pre><code>\"id\": \"c0xxxxa5-xxx-4ecb-xxxx-f37bxxxx28d6\u201d\n</code></pre> <pre><code>\"tenantId\": \"32xxxxb3-xxx-46b3-xxxx-0exxxxc46d1\"\n</code></pre> <p>3) Create a new Azure App with the following command:</p> <pre><code>az ad app create --display-name kubernetes --identifier-uris http://kubernetes --homepage [http://example.com](http://example.com) --password CLIENT_SECRET\n</code></pre> <p>You\u2019ll need to use your Client Secret in this command, which can be deleted later (e.g. \u201cUe9)Qj^V\\\\~UYES3(C\u201d)</p> <p>CLI output will look like this: </p> <pre><code>{\n  \"acceptMappedClaims\": null,\n  \"addIns\": [],\n  \"allowGuestsSignIn\": null,\n  \"allowPassthroughUsers\": null,\n! \"appId\": \"7bxxxxc3-xxxx-4d74-xxxx-8c40xxxb558\", !\n  \"appLogoUrl\": null,\n  \"appPermissions\": null,\n  \"appRoles\": [],\n  \"applicationTemplateId\": null,\n  \"availableToOtherTenants\": false,\n  \"deletionTimestamp\": null,\n  \"displayName\": \"kubernetes\",\n  \"errorUrl\": null,\n  \"groupMembershipClaims\": null,\n  \"homepage\": \"http://example.com\",\n  \"identifierUris\": [\n    \"http://kubernetes\"\n  ],\n  }\n  ...\n  {\n    \"adminConsentDescription\": \"Allow the application to access kubernetes on behalf of the signed-in user.\",\n    \"adminConsentDisplayName\": \"Access kubernetes\",\n    \"id\": \"59xxx87-xxxx-47b8-xxxx-1708xxxxefcd\",\n    \"isEnabled\": true,\n    \"type\": \"User\",\n    \"userConsentDescription\": \"Allow the application to access kubernetes on your behalf.\",\n    \"userConsentDisplayName\": \"Access kubernetes\",\n    \"value\": \"user*impersonation\"\n  }\n...\n}\n</code></pre> <p>You\u2019ll need to use the \u201cappId\u201d parameter from this output. In our example, it would be: \"appId\": \"7bxxxxc3-xxxx-4d74-xxxx-8c40xxxb558\"</p> <p>4) Create a service principal for the app</p> <pre><code>az ad sp create --id appId\n</code></pre> <p>Use \u201cappId\u201d from the previous step here: </p> <pre><code>az ad sp create -id 7bxxxxc3-xxxx-4d74-xxxx-8c40xxxb558\n</code></pre> <p>CLI output example:</p> <pre><code>{\n  \"accountEnabled\": true,\n  ...\n}\n...\n\"objectId\": \"85xxxxcb-xxxx-4761-xxxx-63fxxxx515e\",\n  \"objectType\": \"ServicePrincipal\",\n  \"odata.metadata\": \"https://graph.windows.net/32xxxxb3-xxxx-46b3-xxxx-0e33xxxx46d1/$metadata#directoryObjects/@Element\",\n  \"odata.type\": \"Microsoft.DirectoryServices.ServicePrincipal\",\n}\n...\n</code></pre> <p>5) Create a role assignment:</p> <pre><code>az role assignment create --role \"Owner\" --assignee http://kubernetes --subscription SUBSCRIPTION_ID\n</code></pre> <p>In this case, you will use the subscription ID from step \u21162:</p> <pre><code>az role assignment create --role \"Owner\" --assignee http://kubernetes --subscription c0xxxxa5-xxx-4ecb-xxxx-f37bxxxx28d6\n</code></pre> <p>You\u2019ll find the following CLI output:</p> <pre><code>{\n  \"canDelegate\": null,\n  \"id\": \"/subscriptions/c0xxxxa5-xxx-4ecb-xxxx-f37bxxxx28d6/providers/Microsoft.Authorization/roleAssignments/4fxxxx7f-xxxx-4ccf-xxxx-7287xxxxfa14\",\n  \"name\": \"4fxxxx7f-xxxx-4ccf-xxxx-7287xxxxfa14\",\n  \"principalId\": \"85xxxxcb-xxxx-4761-xxxx-63ffxxxx515e\",\n  \"principalType\": \"ServicePrincipal\",\n  \"roleDefinitionId\": \"/subscriptions/c0xxxxa5-xxx-4ecb-xxxx-f37bxxxx28d6/providers/Microsoft.Authorization/roleDefinitions/8exxxx57-xxxx-443c-xxxx-2fe8xxxxb635\",\n  \"scope\": \"/subscriptions/c0xxxxa5-xxx-4ecb-xxxx-f37bxxxx28d6\",\n  \"type\": \"Microsoft.Authorization/roleAssignments\"\n}\n</code></pre> <p>Please be careful when inserting the credentials. You cannot add flavor and create a cluster if you add incorrect credentials.</p> <p>You can switch to Taikun and add your Azure credentials now:</p> <p></p> <p>Azure Cloud Credentials</p> <p>Follow these steps to add Azure Cloud Credentials:</p> <p>1. Switch to Cloud credentials in Taikun</p> <p>2. Hit Add Cloud Credentials in the top-right corner</p> <p>3. Specify the necessary parameters in the Azure section:</p> <ul> <li>Cloud Name \u2013 choose a name for your Cloud Credentials (3-30 characters, e.g. cloud-test)</li> <li>Azure Client ID</li> <li>Azure Client Secret</li> <li>Azure Tenant ID</li> </ul> <p></p> <p>Add Azure Cloud Credentials</p>"},{"location":"Supported_Cloud_Providers/Microsoft_Azure/#where-to-find-the-azure-credentials","title":"Where to find the Azure credentials","text":"<p>If you haven\u2019t created your application via CLI, you can find the guidelines here.</p> <p>Credentials for Azure are located in different tabs. Instructions on where to find them are posted below.</p> <p>Please be careful when inserting the credentials. You cannot add flavor and create a cluster if you add incorrect credentials.</p>"},{"location":"Supported_Cloud_Providers/Microsoft_Azure/#azure-client-and-tenant-id","title":"Azure Client and Tenant ID","text":"<ul> <li>Azure Active Directory \u2013 App registrations \u2013 All Applications \u2013 application -&gt; Application (client) ID (=Azure Client Id) and Directory (tenant) ID (Azure Tenant Id)</li> </ul> <p>Azure Active Directory</p> <p></p> <p>Azure Client Secrets</p> <ul> <li>Azure Active Directory \u2013 App registrations \u2013 All Applications \u2013 application \u2013 Certificates &amp; secrets \u2013 Client secrets -&gt; Value (=Azure Secret Id)</li> </ul> <p></p> <p>Azure Secret ID</p> <p></p> <p>Azure Client Secret</p> <p>!!! Warning     Client Secret is shown only for the first time, we recommend saving it somewhere else.</p>"},{"location":"Supported_Cloud_Providers/Microsoft_Azure/#azure-subscription-id","title":"Azure Subscription Id","text":"<p>!!! Note     Subscription is chosen from the drop-down selection, but you can find below where to find your Subscription ID.</p> <ul> <li>Subscriptions -&gt; Subscription ID (=Azure Subscription Id)</li> </ul> <p></p> <p>Azure Subscription</p> <p></p> <p>Azure Subscription ID</p> <p>More information is provided in the Azure guideline HERE.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/","title":"OpenStack","text":""},{"location":"Supported_Cloud_Providers/OpenStack/#requirements-for-openstack","title":"Requirements for OpenStack","text":"<p>To successfully connect your OpenStack account, the following configuration needs to be applied in your OpenStack environment:</p> <ul> <li> <p>An Ubuntu image must exist in the OpenStack environment. The requirement is an Ubuntu 20; we recommend using the most recent kernel (e.g., a base Ubuntu image with HWE kernel available here: <code>https://repo.itera.io/repository/images/taikun-image.qcow2</code>). To use an image in Taikun you need to use the tags taikun and ubuntu{number}. By default, Taikun takes an image with the latest <code>{number}</code>.</p> </li> <li> <p>The load-balancer member role is required to deploy and manage Kubernetes and load balancer in OpenStack. It grants users the necessary permissions to manage and configure the load balancer service. It allows registering and managing backend servers, configuring health monitoring, scaling the application infrastructure, and handling configuration settings and monitoring.</p> </li> <li> <p>A router between the public and internal networks should exist if you import a network from OpenStack. There should be internal access to the internal network, either from the router or directly. Additionally, DNS created in the Access Profiles section will be ignored when importing a network.</p> </li> </ul>"},{"location":"Supported_Cloud_Providers/OpenStack/#adding-openstack-connection-to-taikun","title":"Adding OpenStack connection to Taikun","text":""},{"location":"Supported_Cloud_Providers/OpenStack/#first-step","title":"First Step:","text":"<p>Switch to the Cloud credentials tab in Taikun.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#second-step","title":"Second Step:","text":"<p>Hit Add Cloud Credentials in the top-right corner and choose OpenStack from the list.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#third-step","title":"Third Step:","text":"<p>Specify the following parameters:</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#cloud-name","title":"Cloud Name:","text":"<p>Choose a name for your Cloud Credentials (3-30 characters, e.g. cloud-test).</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#user","title":"User:","text":"<p>Your OpenStack username.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#password","title":"Password:","text":"<p>Your password to OpenStack.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#url","title":"URL:","text":"<p>Endpoint-Identity (refer to the Where to find the OpenStack credentials below).</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#domain","title":"Domain:","text":"<p>Insert domain name.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#project","title":"Project:","text":"<p>Select an OpenStack Project.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#region","title":"Region:","text":"<p>Select an OpenStack Region.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#public-network","title":"Public Network:","text":"<p>Choose a network, if available.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#optional","title":"Optional:","text":"<ul> <li>Specify Availability Zone</li> <li>Volume Types</li> <li>Enable Import Network</li> </ul> <p>OpenStack Cloud Credentials</p> <p></p> <p>Add Cloud Credentials</p> <p>!!! Info      Alternatively, you can use Application credentials issued by the administrator of your OpenStack account.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#where-to-find-the-openstack-credentials","title":"Where to find the OpenStack credentials","text":"<p>After entering your OpenStack username and password, the other data from OpenStack will be added to Taikun automatically after filling in the URL.</p> <p>To find the URL follow these steps:</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#first-step_1","title":"First Step:","text":"<p>Log into your OpenStack account.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#second-step_1","title":"Second Step:","text":"<p>Locate the Project \u2013 API Access section in the left-hand navigation panel.</p>"},{"location":"Supported_Cloud_Providers/OpenStack/#third-step_1","title":"Third Step:","text":"<p>Find the Identity row and copy its Service Endpoint.</p>"},{"location":"Supported_Cloud_Providers/Proxmox/","title":"Proxmox","text":"<p>Proxmox VE (Virtual Environment) is an open-source server virtualization management platform. It allows you to create and manage virtual machines (VMs) and containers on a cluster of physical servers. Proxmox VE is based on Debian Linux and uses KVM (Kernel-based Virtual Machine) as the hypervisor and LXC (Linux Containers) for lightweight virtualization.  </p>"},{"location":"Supported_Cloud_Providers/Proxmox/#adding-proxmox-connection-to-taikun","title":"Adding Proxmox connection to Taikun","text":"<p>!!! Note     Please note that only users with a Partner role in Taikun can add Proxmox credentials for security purposes.</p> <p>1. Switch to the Cloud credentials tab in Taikun.</p> <p>2. Click on the Add Cloud Credentials button in the top-right corner.</p> <p>3. Specify the following parameters in the Proxmox section:</p> <ul> <li>Cloud Name \u2013 choose a name for your Cloud Credentials (3-30 characters, e.g., proxmox-cloud-credentials).</li> <li>Proxmox API Host \u2013 Proxmox API Host is a software component that provides a programming interface for managing and interacting with Proxmox Virtual Environment (PVE) infrastructure. (e.g., https://taikun-proxmox-XXX.XXX.XX/api2/json)</li> <li>Proxmox Client ID \u2013 Proxmox Client ID refers to a unique identifier assigned to a client or user in the Proxmox Virtual Environment (PVE) system. (e.g., taikun@XX!XXX)</li> <li>Proxmox Client Secret \u2013 a secure string or key generated along with the Client ID and used for authentication and authorization.</li> <li>Storage \u2013 refers to the available space and resources for storing virtual machine data.</li> <li>VM Template Name \u2013 VM Template Name in Proxmox refers to a predefined, reusable configuration and setup for a virtual machine (VM).</li> <li>Hypervisors \u2013 Hypervisors are responsible for abstracting and virtualizing the underlying hardware resources, allowing multiple VMs to run concurrently on a single physical server.</li> <li>Public Network and Private Network:<ul> <li>Network Address and Mask (e.g., 78.XX.XX.12/17)</li> <li>Gateway (e.g., 78.XX.XX.11)</li> <li>Allocation Range (e.g., 78.XX.XX.22 \u2013 78.XX.XX.100)</li> <li>Bridge \u2013 select the appropriate bridge for your credentials.</li> </ul> </li> </ul> <p>4. Confirm your action.</p> <p></p> <p>Proxmox Cloud Credentials</p>"},{"location":"Supported_Cloud_Providers/Red_Hat_OpenShift/","title":"Red Hat OpenShift","text":"<p>OpenShift is a containerization and Kubernetes platform developed by Red Hat, a leading open-source software company. It is designed to simplify containerized applications\u2019 deployment, management, and scaling. OpenShift provides a comprehensive set of tools and features for building, deploying, and running container-based applications in a cloud-native and DevOps-friendly manner.</p>"},{"location":"Supported_Cloud_Providers/Red_Hat_OpenShift/#connecting-openshift-to-taikun","title":"Connecting OpenShift to Taikun","text":"<p>1. Switch to the Cloud credentials in Taikun.  </p> <p>2. Click the Add Cloud Credentials button in the top-right corner.  </p> <p>3. Select the Openshift option.  </p> <p>4. Specify the parameters:  </p> <ul> <li>Cloud Name \u2013 choose a name with 3-30 alphanumeric characters.  </li> <li>Kubeconfig File \u2013 a configuration file used by Kubernetes and OpenShift command-line tools to authenticate and interact with a cluster. It contains information about the cluster, user credentials, context, and other configuration details.  </li> <li>Pull Secret \u2013 a Kubernetes secret that stores authentication information required to pull container images from container registries. It contains the necessary credentials, such as username, password, or token, and the registry URL for accessing container images.  </li> <li>Storage Class \u2013 a resource that defines how the cluster\u2019s dynamic provisioning of storage volumes should work. It specifies parameters such as the storage backend, access modes, and other storage characteristics to be provisioned.  </li> <li>Base Domain \u2013 the root domain that is the foundation for creating route hostnames within your OpenShift cluster. Routes are used to expose applications running in OpenShift to the external world.  </li> </ul> <p>5. Click Add Cloud Credentials.  </p> <p></p> <p>OpenShift Cloud Credentials</p>"},{"location":"Supported_Cloud_Providers/VMware_Tanzu/","title":"VMware Tanzu","text":"<p>Tanzu is a suite of products and services developed by VMware to facilitate the deployment, management, and modernization of applications across multi-cloud and hybrid-cloud environments. It aims to simplify and accelerate the process of building, running, and managing applications using modern cloud-native technologies.</p>"},{"location":"Supported_Cloud_Providers/VMware_Tanzu/#adding-vmware-tanzu-connection-to-taikun","title":"Adding VMware Tanzu connection to Taikun","text":"<p>!!! Note     Only users with a Partner role in Taikun can add VMware Tanzu credentials for security purposes.</p> <p>1. Switch to the Cloud credentials tab in Taikun.</p> <p>2. Click on the Add Cloud Credentials button in the top-right corner.</p> <p>3. Specify the following parameters in the Tanzu section:</p> <ul> <li>Cloud Name \u2013 choose a name for your Cloud Credentials (3-30 characters, e.g. <code>tanzu-cloud-test</code>).</li> <li>URL \u2013 Endpoint-Identity (e.g. <code>https://stra-caas56.businesscube.cz</code>).</li> <li>User \u2013 your user name to Tanzu (e.g. <code>user</code>).</li> <li>Password \u2013 your password to Tanzu (e.g. <code>user234</code>).</li> <li>Namespace \u2013 specify namespace here.</li> <li>Volume Type \u2013 specify volume type.</li> <li>Continent \u2013 specify working continent.</li> </ul> <p>4. Confirm your action.</p> <p></p> <p>Tanzu Cloud Credentials</p>"},{"location":"Supported_Cloud_Providers/VMware_vSphere/","title":"VMware vSphere","text":"<p>Virtualization platform developed by VMware. It enables organizations to create and manage virtualized IT environments, including virtual machines (VMs), on a large scale. It also provides features such as virtualization of compute, storage, and networking resources.</p>"},{"location":"Supported_Cloud_Providers/VMware_vSphere/#connecting-vsphere-to-taikun","title":"Connecting vSphere to Taikun","text":"<ol> <li> <p>Switch to the Cloud credentials in Taikun.</p> </li> <li> <p>Hit the Add Cloud Credentials button in the top-right corner.</p> </li> <li> <p>Select the vSphere option.</p> </li> <li> <p>Specify the parameters:</p> </li> </ol>"},{"location":"Supported_Cloud_Providers/VMware_vSphere/#cloud-settings","title":"Cloud Settings","text":"<ul> <li>Cloud Name \u2013 choose a name with 3-30 alphanumeric characters.</li> <li>Region \u2013 select your region (grouping of resources within a data center or across multiple data centers).</li> </ul>"},{"location":"Supported_Cloud_Providers/VMware_vSphere/#vsphere-credentials","title":"vSphere Credentials","text":"<ul> <li>Username \u2013 fill in your username to authenticate and allow access to the vSphere environment.</li> <li>Password \u2013 fill in your vSphere password.</li> <li>API URL \u2013 fill in API URL, endpoint URL used to interact with the vSphere API.</li> </ul>"},{"location":"Supported_Cloud_Providers/VMware_vSphere/#vsphere-cloud-settings","title":"vSphere Cloud Settings","text":"<ul> <li>Data center \u2013 logical grouping of ESXi hosts and associated virtual machines.</li> <li>Resource Pool \u2013 aggregates physical resources (such as CPU and memory) across multiple ESXi hosts.</li> <li>Data Store \u2013 storage location where virtual machine files are stored (local disk, SAN, or NAS).</li> <li>DRS Enabled \u2013 Distributed Resource Scheduler dynamically balances CPU and memory across ESXi hosts.</li> <li>Hypervisors \u2013 enables virtualization by abstracting hardware and creating multiple VMs on a single host.</li> <li>VM Template \u2013 a master copy of a VM used to deploy consistent environments quickly.</li> </ul>"},{"location":"Supported_Cloud_Providers/VMware_vSphere/#public-network","title":"Public Network","text":"<ul> <li>Network Name \u2013 name assigned to a specific network.</li> <li>Network Address/Mask \u2013 defines the network and range of IP addresses.</li> <li>Gateway \u2013 IP address of the router connecting to external networks.</li> <li>Allocation Range \u2013 defines the range of usable IP addresses.</li> </ul> <p>!!! Info     A public network is accessible from outside the vSphere environment, such as the internet or an external corporate network.</p>"},{"location":"Supported_Cloud_Providers/VMware_vSphere/#private-network","title":"Private Network","text":"<ul> <li>Network Name \u2013 name assigned to a specific network.</li> <li>Network Address/Mask \u2013 defines the network and range of IP addresses.</li> <li>Gateway \u2013 IP address of the router connecting to external networks.</li> <li>Allocation Range \u2013 defines the range of usable IP addresses.</li> </ul> <p>!!! Info     A private network is isolated and accessible only within the vSphere environment itself.</p> <p>5. Click Add Cloud Credentials.</p> <p>!!! Note     To prevent IP collisions, ensure that the allocation ranges do not include the IP addresses of the hypervisors (10.3.1.11 and 10.3.1.12). It\u2019s recommended to start the allocation range for the network 10.3.1.0 from 10.3.1.20.</p> <p></p> <p>Taikun Requests</p>"},{"location":"Supported_Cloud_Providers/Zadara/","title":"Zadara","text":"<p>Zadara\u2019s storage infrastructure is built for high availability and reliability, offering a wide range of storage options, including block, file, and object storage, as well as specialized solutions for specific use cases such as high-performance computing (HPC) and big data analytics.</p>"},{"location":"Supported_Cloud_Providers/Zadara/#connecting-zadara-to-taikun-cloudworks","title":"Connecting Zadara to Taikun CloudWorks","text":"<p>1. Switch to the Cloud credentials in Taikun.</p> <p>2. Hit the Add Cloud Credentials button in the top-right corner.</p> <p>3. Select the Zadara option.</p> <p>4. Specify the parameters:</p> <ul> <li>Cloud Name: Choose a name with 3-30 alphanumeric characters.</li> <li>Zadara API URL: The endpoint URL for accessing Zadara\u2019s API. The API allows you to programmatically interact with and manage your Zadara storage resources, such as provisioning volumes, monitoring performance, and configuring settings.</li> <li>Zadara Key ID: Used for authentication and authorization purposes when accessing Zadara services through the API.</li> <li>Secret Access Key: Confidential credential used along with the Zadara Key ID for authentication when making API requests. It should be kept secure.</li> <li>Region: Specifies the location where your Zadara storage resources are provisioned.</li> <li>Zadara Zone Count: Determines the level of resilience and redundancy for storage. Increasing the zone count enhances data protection, while reducing it may optimize cost.</li> <li>Zadara Volume Type: Selects the appropriate volume type to match performance, cost, and feature requirements.</li> <li>Continent: Provides additional context about the physical location of your data.</li> </ul> <p>!!! Note     To guarantee optimal performance and compatibility with Zadara\u2019s storage solutions, ensure that compute resources are running on version 23.08.2 or higher.</p> <p>For configuration and management of VPC peering, it can be easily set up through the Zadara Graphical User Interface (GUI). Use Zadara official documentation for more information.</p>"},{"location":"Supported_Cloud_Providers/Zadara/#machine-image","title":"Machine Image","text":"<p>A zCompute machine image named with the prefix \"taikun\" (e.g., \"taikun-server\") can be created from the Zadara marketplace or directly from the following image:</p> <p>URL: Ubuntu Jammy 22.04 Image</p> <p>For more details, check the Zadara machine image documentation.</p>"},{"location":"Supported_Cloud_Providers/Zadara/#adding-a-zadara-cloud-credential","title":"Adding a Zadara Cloud Credential","text":"<p>Add Zadara Cloud Credentials</p>"},{"location":"Supported_Cloud_Providers/Zadara/#user-permissions-in-zadara","title":"User Permissions in Zadara","text":"<p>When creating a new user account in Zadara, select a permission level that defines access rights and privileges:</p>"},{"location":"Supported_Cloud_Providers/Zadara/#1-iamfullaccess","title":"1. IAMFullAccess:","text":"<ul> <li>Provides full access to the Identity and Access Management (IAM) capabilities within Zadara.</li> <li>Users can manage user accounts, groups, roles, and permissions within Zadara.</li> <li>Suitable for administrators managing user access across the organization.</li> </ul>"},{"location":"Supported_Cloud_Providers/Zadara/#2-memberfullaccess","title":"2. MemberFullAccess:","text":"<ul> <li>Provides full access to Zadara storage resources but not IAM capabilities.</li> <li>Users can create, modify, and delete storage resources (volumes, snapshots, file shares).</li> <li>Does not allow managing user accounts, groups, roles, or permissions.</li> </ul> <p>Adding a Member with Full Access:</p> <p></p> <p>Add Member with Full Access </p> <p>For further information on getting started with Zadara zCompute, check this blog post.</p>"},{"location":"Supported_Cloud_Providers/Zededa/","title":"Zededa","text":"<p>ZEDEDA is a cloud-based platform that simplifies applications\u2019 deployment, management, and security on edge devices. It acts as a central control point for distributing and updating software across diverse hardware, ensuring seamless operations. By adopting a zero-trust security model, ZEDEDA protects sensitive data and applications from cyber threats prevalent in edge environments. Importantly, it focuses solely on managing the infrastructure and application deployment, leaving data handling and processing to the user\u2019s specific requirements.</p> <p>Learn more about Zededa</p>"},{"location":"Supported_Cloud_Providers/Zededa/#adding-zededa-credentials-to-taikun","title":"Adding Zededa Credentials to Taikun","text":"<p>1. Switch to the Cloud credentials in Taikun.</p> <p>2. Click the Add Cloud Credentials button in the top-right corner.</p> <p>3. Zededa credentials are divided into three sections:</p>"},{"location":"Supported_Cloud_Providers/Zededa/#cloud-settings","title":"Cloud Settings","text":"<ul> <li>Cloud Name: Choose a name for your Cloud Credentials (3-30 characters, e.g., proxmox-cloud-credentials).</li> <li>Organization: Select your organization.</li> <li>API URL: Endpoint-Identity (e.g., <code>https://cloud.mycloud.com:32132</code>).</li> <li>API Token: Insert an authentication API token for verification.</li> <li>Project: Select a project if there are multiple options (e.g., <code>my-cloud-project</code>).</li> <li>Edge Node: An Edge Node is a computing device that processes data locally at the edge of a network (e.g., <code>test123</code>).</li> </ul>"},{"location":"Supported_Cloud_Providers/Zededa/#public-network","title":"Public Network","text":"<ul> <li>Interface Name: Defines a contract for managing user authentication and authorization operations.</li> <li>VLAN ID: Define a unique VLAN ID from 1 to 4094.</li> <li>Network Address and Mask: Define the Network address and Mask (e.g., <code>192.168.##.##/24</code>).</li> <li>Gateway: Define a gateway that manages and routes client requests to the appropriate backend services or systems (e.g., <code>192.168.##.##</code>).</li> <li>Allocation Range: Define the required allocation range for your cloud credentials (e.g., <code>192.168.##.10 to 192.168.##.20</code>).</li> </ul>"},{"location":"Supported_Cloud_Providers/Zededa/#private-network","title":"Private Network","text":"<ul> <li>Interface Name: Defines a contract for managing user authentication and authorization operations.</li> <li>VLAN ID: Define a unique VLAN ID from 1 to 4094.</li> <li>Network Address and Mask: Define the Network address and Mask (e.g., <code>192.168.##.##/28</code>).</li> <li>Gateway: Define a gateway that manages and routes client requests to the appropriate backend services or systems (e.g., <code>192.168.##.##</code>).</li> <li>Allocation Range: Define the required allocation range for your cloud credentials (e.g., <code>192.168.##.30 to 192.168.##.40</code>).</li> </ul> <p>4. Click on the Add Cloud Credentials button.</p>"},{"location":"Taikun_CloudWorks_Overview/ArtifactHub_Repositories/","title":"ArtifactHub Repositories","text":"<p>Artifact Hub allows publishers to list their content in an automated way. Publishers can add their repositories from the control panel, accessible from the top right menu after signing in. It\u2019s possible to create an organization and add repositories to it instead of adding them to the user\u2019s account. Repositories will be indexed periodically to always display the most up-to-date content.</p> <p>The following repository kinds are supported at the moment:</p> <ul> <li>Argo templates repositories</li> <li>Backstage plugins repositories</li> <li>Containers images repositories</li> <li>CoreDNS plugins repositories</li> <li>Falco rules repositories</li> <li>Headlamp plugins repositories</li> <li>Helm charts repositories</li> <li>Helm plugins repositories</li> <li>KCL modules repositories</li> <li>KEDA scalers repositories</li> <li>Keptn integrations repositories</li> <li>Knative client plugins repositories</li> <li>Krew kubectl plugins repositories</li> <li>KubeArmor policies repositories</li> <li>Kubewarden policies repositories</li> <li>Kyverno policies repositories</li> <li>OLM operators repositories</li> <li>OPA policies repositories</li> <li>Tekton pipelines repositories</li> <li>Tekton tasks repositories</li> <li>Tinkerbell actions repositories</li> </ul> <p>This guide also contains additional information about the following repository topics:</p> <ul> <li>Repositories guide<ul> <li>Verified publisher</li> <li>Official status</li> <li>Ownership claim</li> <li>Private repositories</li> </ul> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/ArtifactHub_Repositories/#verified-publisher","title":"Verified publisher","text":"<p>Repositories and the packages they provide can display a special label named <code>Verified publisher</code>. This label indicates that the repository publisher owns or has control over the repository. Users may rely on it to decide if they want to use a given package or not.</p> <p>Publishers can be verified through the artifacthub-repo.yml repository metadata file. In the repositories tab in the Artifact Hub control panel, the repository identifier is exposed on each repository\u2019s card (ID). To proceed with the verification, an <code>artifacthub-repo.yml</code> metadata file must be added to the repository including that ID in the <code>repositoryID</code> field. The next time the repository is processed, the verification will be checked and the flag will be enabled if it succeeds.</p> <p>Please note that the artifacthub-repo.yml metadata file must be located at the repository URL\u2019s path. In Helm repositories, for example, this means it must be located at the same level of the chart repository index.yaml file, and it must be served from the chart repository HTTP server as well.</p> <p>The verified publisher flag won\u2019t be set until the next time the repository is processed. Please keep in mind that the repository won\u2019t be processed if it hasn\u2019t changed since the last time it was processed. Depending on the repository kind, this is checked in a different way. For Helm HTTP-based repositories, we consider it has changed if the <code>index.yaml</code> file changes (the <code>generated</code> field is ignored when performing this check). For git-based repositories, it does when the hash of the last commit in the branch you set up changes.</p>"},{"location":"Taikun_CloudWorks_Overview/ArtifactHub_Repositories/#official-status","title":"Official status","text":"<p>In Artifact Hub, the <code>official</code> status means that the publisher owns the software a package primarily focuses on. If we consider the example of a chart used to install Consul, to obtain the <code>official</code> status, the publisher should be the owner of the Consul software (HashiCorp in this case), not just the chart. Similarly, a Tekton task used to perform operations on Google Cloud would need to be published by Google to be marked as <code>official</code>. In the case of a MySQL operator, only one published by MySQL/Oracle would be considered <code>official</code>.</p> <p>The <code>official</code> status can be granted at the repository or package level. When it is granted for a repository, all packages available on it will display the <code>official</code> badge, so all packages in the repository must be official. If only some of the packages in your repository are official, please list them in the <code>Official packages</code> field when submitting the official status request.</p> <p>Before applying for this status, please make sure your repository complies with the following requirements:</p> <ul> <li>The repository has already obtained the Verified publisher status.</li> <li>The user requesting the status is the publisher of the repository in Artifact Hub, or belongs to the organization publishing it.</li> <li>All official packages available in the repository provide a <code>README.md</code> file with some documentation that can be displayed on Artifact Hub.</li> </ul> <p>Once you have verified that the requirements are met, please file an issue using this template to apply.</p>"},{"location":"Taikun_CloudWorks_Overview/ArtifactHub_Repositories/#ownership-claim","title":"Ownership claim","text":"<p>Any user is free to add any repository they wish to Artifact Hub. In some situations, legit owners may want to claim ownership of an already published repository to publish it themselves. This process can be easily done in an automated way from the Artifact Hub control panel.</p> <p>First, an artifacthub-repo.yml metadata file must be added to the repository you want to claim ownership of. Only the <code>owners</code> section of the metadata file is required to be set up for this process. The <code>repositoryID</code> field can be omitted as the user claiming ownership doesn\u2019t know it yet. The user requesting the ownership claim must appear in the list of owners in the metadata file, and the email listed must match the one used to sign in to Artifact Hub. This information will be used during the process to verify that the requesting user actually owns the repository.</p> <p>Once the repository metadata file has been set up, you can proceed from the Artifact Hub control panel. In the repositories tab, click on <code>Claim Ownership</code>. You\u2019ll need to enter the repository you\u2019d like to claim ownership of, as well as the destination entity, which can be the user performing the request or an organization. If the metadata file was set up correctly, the process should complete successfully.</p> <p>Please note that the artifacthub-repo.yml metadata file must be located at the repository URL\u2019s path. In Helm repositories, for example, this means it must be located at the same level of the chart repository index.yaml file, and it must be served from the chart repository HTTP server as well.</p>"},{"location":"Taikun_CloudWorks_Overview/ArtifactHub_Repositories/#private-repositories","title":"Private repositories","text":"<p>Artifact Hub supports adding private repositories (except OLM OCI based). By default, this feature is disabled, but you can enable it in your own Artifact Hub deployment by setting the <code>hub.server.allowPrivateRepositories</code> configuration setting to <code>true</code>. When enabled, you\u2019ll be allowed to add the authentication credentials for the repository in the add/update repository modal in the control panel. Credentials are not exposed in the Artifact Hub UI, so users will need to get them separately. The installation instructions modal will display a warning to users when the package displayed belongs to a private repository.</p>"},{"location":"Taikun_CloudWorks_Overview/Container_images/","title":"Container images","text":"<p>Taikun CloudWorks provides users with a seamless way to monitor and manage containerized applications within their Kubernetes clusters. Users can follow these simple steps to gain insights into the container images running in a cluster.</p>"},{"location":"Taikun_CloudWorks_Overview/Container_images/#steps-to-check-all-container-images-in-a-cluster","title":"Steps to Check All Container Images in a Cluster","text":"<p>1. Navigate to \u2018Projects\u2019:</p> <p>Upon logging into Taikun CloudWorks, locate and click on the \u2018Projects\u2019 tab in the main navigation menu. This will take you to the list of available projects within your workspace.</p> <p>2. Open a Project:</p> <p>Select the specific project you are interested in from the list of projects and click on its name to open it. This action will direct you to the Project Dashboard.</p> <p>3. Access \u2018K8s Information\u2019:</p> <p>Find and click on the \u2018K8s Information\u2019 option within the Project Dashboard. This section provides comprehensive details about the Kubernetes cluster associated with the selected project.</p> <p>4. Navigate to the \u2018Container Images\u2019 Tab:</p> <p>In the \u2018K8s Information\u2019 section, locate and select the \u2018Container Images\u2019 tab from the menu. This tab offers a detailed overview of all container images currently running within the Kubernetes cluster associated with the chosen project.</p> <p>5. Review Container Image Information:</p> <p>Once in the \u2018Container Images\u2019 tab, users can review essential information about each container image, including details such as image name, version, and related metadata. This comprehensive view facilitates efficient monitoring and management of containerized applications within the cluster.</p>"},{"location":"Taikun_CloudWorks_Overview/Exposing_an_Application_Through_the_Bastion_Load_Balancer/","title":"Exposing an Application Through the Bastion Load Balancer","text":"<p>When you deploy an application to the cluster, you can expose it externally through the bastion, which serves as a load balancer. You must configure specific parameters in the application\u2019s settings to do this.</p>"},{"location":"Taikun_CloudWorks_Overview/Exposing_an_Application_Through_the_Bastion_Load_Balancer/#instructions","title":"Instructions","text":""},{"location":"Taikun_CloudWorks_Overview/Exposing_an_Application_Through_the_Bastion_Load_Balancer/#1-define-service-type-as-nodeport","title":"1. Define Service Type as NodePort","text":"<p>In the application\u2019s parameters, set the <code>service.type</code> to <code>NodePort</code>.</p> <pre><code>service:\n  type: NodePort\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Exposing_an_Application_Through_the_Bastion_Load_Balancer/#2-enable-ingress-for-the-application","title":"2. Enable Ingress for the Application","text":"<p>Enable ingress by setting <code>ingress.enabled</code> to <code>true</code>.</p> <pre><code>ingress:\n  enabled: true\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Exposing_an_Application_Through_the_Bastion_Load_Balancer/#3-specify-the-ingress-class-name","title":"3. Specify the Ingress Class Name","text":"<p>Define the ingress class name as <code>taikun</code>. This informs the cluster that the application should be exposed via the Taikun ingress controller.</p> <pre><code>ingressClassName: taikun\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Exposing_an_Application_Through_the_Bastion_Load_Balancer/#4-set-the-ingress-hostname","title":"4. Set the Ingress Hostname","text":"<p>For Standard Clusters</p> <p>If you\u2019re working with a standard cluster, define the <code>ingress.hostName</code> with the following pattern:</p> <pre><code>name.&lt;access-ip&gt;.nip.io\n</code></pre> <p>where: - <code>name</code> is a user-defined name for the application. - <code>&lt;access-ip&gt;</code> is the IP address through which the application will be accessible.</p> <p>Example:</p> <pre><code>ingress:\n  hostName: myapp.192.168.1.100.nip.io\n</code></pre> <p>In this example, <code>myapp</code> is the user-defined name, and <code>192.168.1.100</code> is the access IP.</p> <p>For vcluster</p> <p>If you are using vcluster, which has an access IP of the form <code>vc-01.vcluster.b916618f.nip.io</code>, you do not need to add <code>nip.io</code> to the <code>ingress.hostName</code>. Use the vcluster\u2019s specific hostname directly.</p> <p>Example:</p> <pre><code>ingress:\n  hostName: myapp.vc-01.vcluster.b916618f.nip.io\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Exposing_an_Application_Through_the_Bastion_Load_Balancer/#5-accessing-the-application","title":"5. Accessing the Application","text":"<p>Once the application is deployed and the above settings are configured, you can access the application using the URL:</p> <ul> <li>For standard clusters: <code>http://name.&lt;access-ip&gt;.nip.io</code></li> <li>For vcluster: <code>http://myapp.vc-01.vcluster.b916618f.nip.io</code></li> </ul> <p>Replace <code>name</code> and <code>&lt;access-ip&gt;</code> with the actual values used in your configuration.</p> <p>Example Configuration: Here\u2019s a sample of how the parameters might look in your application\u2019s YAML configuration:</p> <pre><code>service:\n  type: NodePort\n\ningress:\n  enabled: true\n  ingressClassName: taikun\n  hostName: myapp.192.168.1.100.nip.io\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Exposing_an_Application_Through_the_Bastion_Load_Balancer/#additional-notes","title":"Additional Notes","text":"<ul> <li>Ensure that the DNS resolution is correctly configured to point <code>&lt;access-ip&gt;</code> to your bastion\u2019s public IP.</li> <li>The <code>nip.io</code> service provides wildcard DNS for any IP address, simplifying access to your application.</li> <li>For vcluster, the hostname is already in the correct format, so no additional suffix is needed.</li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/","title":"Horizontal Pod Autoscaling in Kubernetes","text":"<p>In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand.</p> <p>Horizontal scaling means deploying more Pods to respond to increased load. This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods that are already running for the workload.</p> <p>If the load decreases and the number of Pods exceeds the configured minimum, the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet, or other similar resource) to scale back down.</p> <p>Horizontal pod autoscaling does not apply to objects that can\u2019t be scaled (for example: a DaemonSet.)</p> <p>The HorizontalPodAutoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the controller\u2019s behavior. The horizontal pod autoscaling controller, running within the Kubernetes control plane, periodically adjusts the desired scale of its target (for example, a Deployment) to match observed metrics such as average CPU utilization, average memory utilization, or any other custom metric you specify.</p> <p>There is a walkthrough example of using horizontal pod autoscaling.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#how-does-a-horizontalpodautoscaler-work","title":"How does a HorizontalPodAutoscaler work","text":"<p>HorizontalPodAutoscaler</p> <p>HorizontalPodAutoscaler controls the scale of a Deployment and its ReplicaSet</p> <p>Kubernetes implements horizontal pod autoscaling as a control loop that runs intermittently (it is not a continuous process). The interval is set by the --horizontal-pod-autoscaler-sync-period parameter to the kube-controller-manager (and the default interval is 15 seconds).</p> <p>Once during each period, the controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. The controller manager finds the target resource defined by the scaleTargetRef, then selects the pods based on the target resource\u2019s .spec.selector labels, and obtains the metrics from the resource metrics API (for per-pod resource metrics) or the custom metrics API (for all other metrics).</p> <p>For per-pod resource metrics (like CPU), the controller fetches the metrics from the resource metrics API for each Pod targeted by the HorizontalPodAutoscaler. Then, if a target utilization value is set, the controller calculates the utilization value as a percentage of the equivalent resource request on the containers in each Pod. The raw metric values are used directly if a target raw value is set. The controller then takes the mean of the utilization or the raw value (depending on the type of target specified) across all targeted Pods and produces a ratio used to scale the number of desired replicas. Please note that if some of the Pod\u2019s containers do not have the relevant resource request set, CPU utilization for the Pod will not be defined and the autoscaler will not take any action for that metric. See the algorithm details section below for more information about how the autoscaling algorithm works. For per-pod custom metrics, the controller functions similarly to per-pod resource metrics, except that it works with raw values, not utilization values. For object metrics and external metrics, a single metric is fetched, which describes the object in question. This metric is compared to the target value, to produce a ratio as above. In the autoscaling/v2 API version, this value can be divided by the number of Pods before the comparison. The common use for HorizontalPodAutoscaler is to configure it to fetch metrics from aggregated APIs (metrics.k8s.io, custom.metrics.k8s.io, or external.metrics.k8s.io). The metrics.k8s.io API is usually provided by an add-on named Metrics Server, which needs to be launched separately. For more information about resource metrics, see Metrics Server.</p> <p>Support for metrics APIs explains the stability guarantees and support status for these different APIs.</p> <p>The HorizontalPodAutoscaler controller accesses corresponding workload resources that support scaling (such as Deployments and StatefulSet). These resources each have a subresource named scale, an interface that allows you to dynamically set the number of replicas and examine each of their current states. For general information about subresources in the Kubernetes API, see Kubernetes API Concepts.</p> <p>Algorithm details From the most basic perspective, the HorizontalPodAutoscaler controller operates on the ratio between desired metric value and current metric value:</p> <p>desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )] For example, if the current metric value is 200m, and the desired value is 100m, the number of replicas will be doubled, since 200.0 / 100.0 == 2.0 If the current value is instead 50m, you\u2019ll halve the number of replicas, since 50.0 / 100.0 == 0.5. The control plane skips any scaling action if the ratio is sufficiently close to 1.0 (within a globally-configurable tolerance, 0.1 by default).</p> <p>When a targetAverageValue or targetAverageUtilization is specified, the currentMetricValue is computed by taking the average of the given metric across all Pods in the HorizontalPodAutoscaler\u2019s scale target.</p> <p>Before checking the tolerance and deciding on the final values, the control plane also considers whether any metrics are missing, and how many Pods are Ready. All Pods with a deletion timestamp set (objects with a deletion timestamp are in the process of being shut down / removed) are ignored, and all failed Pods are discarded.</p> <p>If a particular Pod is missing metrics, it is set aside for later; Pods with missing metrics will be used to adjust the final scaling amount.</p> <p>When scaling on CPU, if any pod has yet to become ready (it\u2019s still initializing, or possibly is unhealthy) or the most recent metric point for the pod was before it became ready, that pod is set aside as well.</p> <p>Due to technical constraints, the HorizontalPodAutoscaler controller cannot exactly determine the first time a pod becomes ready when determining whether to set aside certain CPU metrics. Instead, it considers a Pod \u201cnot yet ready\u201d if it\u2019s unready and transitioned to ready within a short, configurable window of time since it started. This value is configured with the --horizontal-pod-autoscaler-initial-readiness-delay flag, and its default is 30 seconds. Once a pod has become ready, it considers any transition to ready to be the first if it occurred within a longer, configurable time since it started. This value is configured with the --horizontal-pod-autoscaler-cpu-initialization-period flag, and its default is 5 minutes.</p> <p>The currentMetricValue / desiredMetricValue base scale ratio is then calculated using the remaining pods not set aside or discarded from above.</p> <p>If there were any missing metrics, the control plane recomputes the average more conservatively, assuming those pods were consuming 100% of the desired value in case of a scale down, and 0% in case of a scale up. This dampens the magnitude of any potential scale.</p> <p>Furthermore, if any not-yet-ready pods were present, and the workload would have scaled up without factoring in missing metrics or not-yet-ready pods, the controller conservatively assumes that the not-yet-ready pods are consuming 0% of the desired metric, further dampening the magnitude of a scale up.</p> <p>After factoring in the not-yet-ready pods and missing metrics, the controller recalculates the usage ratio. If the new ratio reverses the scale direction, or is within the tolerance, the controller doesn\u2019t take any scaling action. In other cases, the new ratio is used to decide any change to the number of Pods.</p> <p>Note that the original value for the average utilization is reported back via the HorizontalPodAutoscaler status, without factoring in the not-yet-ready pods or missing metrics, even when the new usage ratio is used.</p> <p>If multiple metrics are specified in a HorizontalPodAutoscaler, this calculation is done for each metric, and then the largest of the desired replica counts is chosen. If any of these metrics cannot be converted into a desired replica count (e.g. due to an error fetching the metrics from the metrics APIs) and a scale down is suggested by the metrics which can be fetched, scaling is skipped. This means that the HPA is still capable of scaling up if one or more metrics give a desiredReplicas greater than the current value.</p> <p>Finally, right before HPA scales the target, the scale recommendation is recorded. The controller considers all recommendations within a configurable window choosing the highest recommendation from within that window. This value can be configured using the --horizontal-pod-autoscaler-downscale-stabilization flag, which defaults to 5 minutes. This means that scaledowns will occur gradually, smoothing out the impact of rapidly fluctuating metric values.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#api-object","title":"API Object","text":"<p>The Horizontal Pod Autoscaler is an API resource in the Kubernetes autoscaling API group. The current stable version can be found in the autoscaling/v2 API version which includes support for scaling on memory and custom metrics. The new fields introduced in autoscaling/v2 are preserved as annotations when working with autoscaling/v1.</p> <p>When you create a HorizontalPodAutoscaler API object, make sure the name specified is a valid DNS subdomain name. More details about the API object can be found at HorizontalPodAutoscaler Object.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#stability-of-workload-scale","title":"Stability of workload scale","text":"<p>When managing the scale of a group of replicas using the HorizontalPodAutoscaler, it is possible that the number of replicas keeps fluctuating frequently due to the dynamic nature of the metrics evaluated. This is sometimes referred to as thrashing, or flapping. It\u2019s similar to the concept of hysteresis in cybernetics.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#autoscalling-during-rolling-update","title":"Autoscalling during rolling update","text":"<p>Kubernetes lets you perform a rolling update on a Deployment. In that case, the Deployment manages the underlying ReplicaSets for you. When you configure autoscaling for a Deployment, you bind a HorizontalPodAutoscaler to a single Deployment. The HorizontalPodAutoscaler manages the replicas field of the Deployment. The deployment controller is responsible for setting the replicas of the underlying ReplicaSets so that they add up to a suitable number during the rollout and also afterwards.</p> <p>If you perform a rolling update of a StatefulSet that has an autoscaled number of replicas, the StatefulSet directly manages its set of Pods (there is no intermediate resource similar to ReplicaSet).</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#support-for-resource-matrics","title":"Support for resource matrics","text":"<p>Any HPA target can be scaled based on the resource usage of the pods in the scaling target. When defining the pod specification the resource requests like cpu and memory should be specified. This is used to determine the resource utilization and used by the HPA controller to scale the target up or down. To use resource utilization based scaling specify a metric source like this:</p> <pre><code>type: Resource\nresource:\n  name: cpu\n  target:\n    type: Utilization\n    averageUtilization: 60\n</code></pre> <p>With this metric, the HPA controller will keep the average utilization of the pods in the scaling target at 60%. Utilization is the ratio between the current usage of a resource and the requested resources of the pod. See Algorithm for more details about how utilization is calculated and averaged.</p> <p>??? Note     Since the resource usages of all the containers are summed up the total pod utilization may not accurately represent the individual container resource usage. This could lead to situations where a single container might be running with high usage and the HPA will not scale out because the overall pod usage is still within acceptable limits.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#container-resources-metrics","title":"Container resources metrics","text":"<p>FEATURE STATE: Kubernetes v1.27 [beta]</p> <p>The HorizontalPodAutoscaler API also supports a container metric source where the HPA can track the resource usage of individual containers across a set of Pods, in order to scale the target resource. This lets you configure scaling thresholds for the containers that matter most in a particular Pod. For example, suppose you have a web application and a logging sidecar. In that case, you can scale based on the resource use of the web application, ignoring the sidecar container and its resource use.</p> <p>If you revise the target resource to have a new Pod specification with a different set of containers, you should revise the HPA spec if that newly added container is also used for scaling. If the specified container in the metric source is not present or only in a subset of the pods, then those pods are ignored, and the recommendation is recalculated. See the Algorithm for more details about the calculation. To use container resources for autoscaling define a metric source as follows:</p> <pre><code>type: Resource\nresource:\n  name: cpu\n  target:\n    type: Utilization\n    averageUtilization: 60\n</code></pre> <p>In the above example, the HPA controller scales the target such that the average utilization of the CPU in the application container of all the pods is 60%.</p> <p>??? Note     Suppose you change the name of a container that a HorizontalPodAutoscaler is tracking. In that case, you can make that change in a specific order to ensure scaling remains available and effective whilst the change is being applied. Before you update the resource that defines the container (such as a Deployment), you should update the associated HPA to track the new and old container names. This way, the HPA can calculate a scaling recommendation throughout the update process.</p> <p>Once you have rolled out the container name change to the workload resource, tidy up by removing the old container name from the HPA specification.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#scaling-on-custom-metrics","title":"Scaling on custom metrics","text":"<p>FEATURE STATE: Kubernetes v1.23 [stable]</p> <p>(the autoscaling/v2beta2 API version previously provided this ability as a beta feature)</p> <p>Provided that you use the autoscaling/v2 API version, you can configure a HorizontalPodAutoscaler to scale based on a custom metric (that is not built into Kubernetes or any Kubernetes component). The HorizontalPodAutoscaler controller then queries for these custom metrics from the Kubernetes API.</p> <p>See Support for metrics APIs for the requirements.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#scaling-on-multiple-metrics","title":"Scaling on multiple metrics","text":"<p>FEATURE STATE: Kubernetes v1.23 [stable]</p> <p>(the autoscaling/v2beta2 API version previously provided this ability as a beta feature)</p> <p>Provided that you use the autoscaling/v2 API version, you can specify multiple metrics for a HorizontalPodAutoscaler to scale on. Then, the HorizontalPodAutoscaler controller evaluates each metric, and proposes a new scale based on that metric. The HorizontalPodAutoscaler takes the maximum scale recommended for each metric and sets the workload to that size (provided that this isn\u2019t larger than the overall maximum that you configured).</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#support-for-metrics-apis","title":"Support for metrics APIs","text":"<p>By default, the HorizontalPodAutoscaler controller retrieves metrics from a series of APIs. In order for it to access these APIs, cluster administrators must ensure that:</p> <ul> <li>The API aggregation layer is enabled.</li> <li>The corresponding APIs are registered: For resource metrics, this is the metrics.k8s.io API, generally provided by metrics-server. It can be launched as a cluster add-on. For custom metrics, this is the custom.metrics.k8s.io API. It\u2019s provided by \u201cadapter\u201d API servers provided by metrics solution vendors. Check your metrics pipeline to see if a Kubernetes metrics adapter is available. For external metrics, this is the external.metrics.k8s.io API. It may be provided by the custom metrics adapters provided above. For more information on these metrics paths and their differences, please see the relevant design proposals for the HPA V2, custom.metrics.k8s.io and external.metrics.k8s.io.</li> </ul> <p>For examples of how to use them see the walkthrough for using custom metrics and the walkthrough for using external metrics.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#configurable-scaling-behavior","title":"Configurable scaling behavior","text":"<p>FEATURE STATE: Kubernetes v1.23 [stable]</p> <p>(the autoscaling/v2beta2 API version previously provided this ability as a beta feature)</p> <p>If you use the v2 HorizontalPodAutoscaler API, you can use the behavior field (see the API reference) to configure separate scale-up and scale-down behaviors. You specify these behaviors by setting scaleUp and/or scaleDown under the behavior field.</p> <p>You can specify a stabilization window that prevents flapping the replica count for a scaling target. Scaling policies also let you control the rate of change of replicas while scaling.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#scaling-policies","title":"Scaling policies","text":"<p>One or more scaling policies can be specified in the behavior section of the spec. When multiple policies are specified, the policy that allows the highest amount of change is the policy that is selected by default. The following example shows this behavior while scaling down:</p> <pre><code>behavior:\n  scaleDown:\n    policies:\n    - type: Pods\n      value: 4\n      periodSeconds: 60\n    - type: Percent\n      value: 10\n      periodSeconds: 60\n\n</code></pre> <p>periodSeconds indicates the length of time in the past for which the policy must hold true. The maximum value that you can set for periodSeconds is 1800 (half an hour). The first policy (Pods) allows at most 4 replicas to be scaled down in one minute. The second policy (Percent) allows at most 10% of the current replicas to be scaled down in one minute.</p> <p>Since, by default, the policy that allows the highest amount of change is selected, the second policy will only be used when the number of pod replicas is more than 40. With 40 or fewer replicas, the first policy will be applied. For instance if there are 80 replicas and the target has to be scaled down to 10 replicas then during the first step 8 replicas will be reduced. In the next iteration, when the number of replicas is 72, 10% of the pods is 7.2, but the number is rounded to 8. On each loop of the autoscaler controller, the number of pods to be changed is re-calculated based on the number of current replicas. When the number of replicas falls below 40, the first policy (Pods) is applied, and 4 replicas will be reduced at a time.</p> <p>The policy selection can be changed by specifying the selectPolicy field for a scaling direction. By setting the value to Min which would select the policy that allows the smallest change in the replica count. Setting the value to Disabled completely disables scaling in that direction.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#stabilization-window","title":"Stabilization window","text":"<p>The stabilization window restricts the flapping of the replica count when the scaling metrics keep fluctuating. The autoscaling algorithm uses this window to infer a previous desired state and avoid unwanted changes to the workload scale.</p> <p>For example, in the following example snippet, a stabilization window is specified for scaleDown.</p> <pre><code>behavior:\n  scaleDown:\n    stabilizationWindowSeconds: 300\n</code></pre> <p>When the metrics indicate that the target should be scaled down, the algorithm looks into previously computed desired states and uses the highest value from the specified interval. All desired states from the past 5 minutes will be considered in the above example.</p> <p>This approximates a rolling maximum and avoids having the scaling algorithm frequently remove Pods only to trigger recreating an equivalent Pod moments later.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#default-behavior","title":"Default Behavior","text":"<p>To use the custom scaling, not all fields have to be specified. Only values that need to be customized can be specified. These custom values are merged with default values, which match the existing behavior in the HPA algorithm.</p> <pre><code>behavior:\n  scaleDown:\n    stabilizationWindowSeconds: 300\n    policies:\n    - type: Percent\n      value: 100\n      periodSeconds: 15\n  scaleUp:\n    stabilizationWindowSeconds: 0\n    policies:\n    - type: Percent\n      value: 100\n      periodSeconds: 15\n    - type: Pods\n      value: 4\n      periodSeconds: 15\n    selectPolicy: Max\n</code></pre> <p>For scaling down the stabilization window is 300 seconds (or the value of the --horizontal-pod-autoscaler-downscale-stabilization flag if provided). There is only a single policy for scaling down, which allows 100% of the currently running replicas to be removed, which means the scaling target can be scaled down to the minimum allowed replicas. For scaling up, there is no stabilization window. When the metrics indicate that the target should be scaled up the target is scaled up immediately. There are 2 policies where 4 pods, or 100% of the currently running replicas, may at most be added every 15 seconds till the HPA reaches its steady state.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#example-change-downscale-stabilization-window","title":"Example: change downscale stabilization window","text":"<p>To provide a custom downscale stabilization window of 1 minute, the following behavior would be added to the HPA:</p> <pre><code>behavior:\n  scaleDown:\n    stabilizationWindowSeconds: 60\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#example-limit-scale-down-rate","title":"Example: limit scale-down rate","text":"<p>To limit the rate at which pods are removed by the HPA to 10% per minute, the following behavior would be added to the HPA:</p> <pre><code>behavior:\n  scaleDown:\n    policies:\n    - type: Percent\n      value: 10\n      periodSeconds: 60\n</code></pre> <p>To ensure that no more than 5 Pods are removed per minute, you can add a second scale-down policy with a fixed size of 5 and set selectPolicy to minimum. Setting selectPolicy to Min means that the autoscaler chooses the policy that affects the smallest number of Pods:</p> <pre><code>behavior:\n  scaleDown:\n    policies:\n    - type: Percent\n      value: 10\n      periodSeconds: 60\n    - type: Pods\n      value: 5\n      periodSeconds: 60\n    selectPolicy: Min\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#example-disable-scale-down","title":"Example: disable scale down","text":"<p>The selectPolicy value of Disabled turns off scaling in the given direction. So to prevent downscaling the following policy would be used:</p> <pre><code>behavior:\n  scaleDown:\n    selectPolicy: Disabled\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#support-for-horizontalpodautoscaler-in-kubectl","title":"Support for HorizontalPodAutoscaler in kubectl","text":"<p>HorizontalPodAutoscaler, like every API resource, is supported in a standard way by kubectl. You can create a new autoscaler using kubectl create command. You can list autoscalers by kubectl get hpa or get detailed description by kubectl describe hpa. Finally, you can delete an autoscaler using kubectl delete hpa.</p> <p>In addition, there is a special kubectl autoscale the command for creating a HorizontalPodAutoscaler object. For instance, executing kubectl autoscale rs foo --min=2 --max=5 --cpu-percent=80 will create an autoscaler for ReplicaSet foo, with target CPU utilization set to 80% and the number of replicas between 2 and 5.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#implicit-maintenance-mode-deactivation","title":"Implicit maintenance-mode deactivation","text":"<p>You can implicitly deactivate the HPA for a target without changing the HPA configuration itself. If the target\u2019s desired replica count is set to 0, and the HPA\u2019s minimum replica count is greater than 0, the HPA stops adjusting the target (and sets the ScalingActive Condition on itself to false) until you reactivate it by manually adjusting the target\u2019s desired replica count or HPA\u2019s minimum replica count.</p>"},{"location":"Taikun_CloudWorks_Overview/Horizontal_Pod_Autoscaling_in_Kubernetes/#migrating-deployments-and-statefulsets-to-horizontal-autoscaling","title":"Migrating Deployments and StatefulSets to horizontal autoscaling","text":"<p>When an HPA is enabled, it is recommended that the value of spec.replicas Deployment and/or StatefulSet be removed from their manifest(s). If this isn\u2019t done, any time a change to that object is applied, for example via kubectl apply -f deployment.yaml, this will instruct Kubernetes to scale the current number of Pods to the value of the spec.replicas key. This may not be desired and troublesome when an HPA is active.</p> <p>Keep in mind that the removal of spec.replicas Pod counts may be degraded one time as the default value of this key is 1 (reference Deployment Replicas). Upon the update, all Pods except 1 will begin their termination procedures. Any deployment application afterward will behave normally and respect a rolling update configuration as desired.</p>"},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/","title":"Importing Kubernetes Cluster","text":"<p>By importing a Kubernetes cluster, users can leverage Taikun CloudWorks' centralized management capabilities, streamlined resource allocation, and enhanced operational control.</p> <p>This guide walks you through the process of importing your existing Kubernetes cluster into Taikun CloudWorks, allowing for effortless management alongside your other cloud resources.</p>"},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#what-youll-gain","title":"What You\u2019ll Gain:","text":"<ul> <li>Streamlined management of your existing Kubernetes cluster within Taikun CloudWorks.</li> <li>A centralized platform for all your cloud infrastructure, including Kubernetes.</li> <li>Simplified deployments, monitoring, and troubleshooting.</li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#1-prerequisites","title":"1. Prerequisites","text":"<p>Before importing your existing Kubernetes cluster into Taikun CloudWorks, ensure you meet the following requirements:</p> <ul> <li>Access Credentials and Permissions: You must have the required credentials and administrator-level permissions for both the existing Kubernetes cluster and Taikun CloudWorks.</li> <li>Kubeconfig Format: The Kubeconfig file must be in YAML format with a <code>.yaml</code> or <code>.yml</code> extension.</li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#2-importing-a-kubernetes-cluster-into-taikun-cloudworks","title":"2. Importing a Kubernetes Cluster into Taikun CloudWorks","text":""},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#step-1-create-a-new-project","title":"Step 1: Create a New Project","text":"<ol> <li>Log in to Taikun CloudWorks.</li> <li>Navigate to the Projects section.</li> <li>Click the Add Project button.</li> </ol>"},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#step-2-provide-cluster-details","title":"Step 2: Provide Cluster Details","text":"<ul> <li>Enter the credentials and details of the existing Kubernetes cluster, including its address and authentication information.</li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#step-3-obtain-the-kubeconfig-file","title":"Step 3: Obtain the Kubeconfig File","text":"<ul> <li>Retrieve the kubeconfig file from the Kubernetes cluster. This file is necessary to establish a secure connection between Taikun CloudWorks and the imported cluster.</li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#step-4-complete-the-import-process","title":"Step 4: Complete the Import Process","text":"<ul> <li>Follow the on-screen instructions to finalize the import process.</li> <li>Taikun CloudWorks will establish a secure connection with the Kubernetes cluster, making it manageable from the platform.</li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#3-managing-your-kubernetes-cluster-in-taikun-cloudworks","title":"3. Managing Your Kubernetes Cluster in Taikun CloudWorks","text":""},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#resource-utilization","title":"Resource Utilization","text":"<ul> <li>Monitor resource utilization metrics such as CPU, memory, and storage.</li> <li>Optimize performance based on real-time insights.</li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#application-management","title":"Application Management","text":"<ul> <li>Use Taikun CloudWorks to manage applications deployed on the imported cluster.</li> <li>Access deployment details, monitor application health, and perform application-specific tasks from a centralized interface.</li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#cost-monitoring-and-billing","title":"Cost Monitoring and Billing","text":"<ul> <li>Track resource usage and associated costs using Taikun CloudWorks' cost monitoring features.</li> <li>Gain insights into consumption patterns for better budgeting and cost management.</li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Importing_Kubernetes_Cluster/#backup-and-recovery","title":"Backup and Recovery","text":"<ul> <li>Implement backup strategies and schedule automated backups to safeguard critical data and configurations.</li> <li>In case of data loss, use Taikun CloudWorks to streamline recovery processes.</li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/","title":"Ingress in Kubernetes","text":"<p>An API object that manages external access to the services in a cluster, typically HTTP.</p> <p>Ingress may provide load balancing, SSL termination and name-based virtual hosting.</p> <p>!!! Note     Ingress is frozen. New features are being added to the\u00a0Gateway API.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#terminology","title":"Terminology","text":"<p>For clarity, this guide defines the following terms:</p> <ul> <li> <p>Node: A worker machine in Kubernetes, part of a cluster.</p> </li> <li> <p>Cluster: A set of Nodes that run containerized applications managed by Kubernetes. For this example, and in most common Kubernetes deployments, nodes in the cluster are not part of the public internet.</p> </li> <li> <p>Edge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or a physical piece of hardware.</p> </li> <li> <p>Cluster network: A set of links, logical or physical, that facilitate communication within a cluster according to the Kubernetes\u00a0networking model.</p> </li> <li> <p>Service: A Kubernetes Service that identifies a set of Pods using\u00a0label selectors. Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#what-is-ingress","title":"What is Ingress?","text":"<p>Ingress\u00a0exposes HTTP and HTTPS routes from outside the cluster to\u00a0services\u00a0within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.</p> <p>Here is a simple example where an Ingress sends all its traffic to one Service:</p> <p></p> <p>Taikun Ingress</p> <p>An Ingress may be configured to give Services externally reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An\u00a0Ingress controller\u00a0is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.</p> <p>An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type\u00a0Service.Type=NodePort\u00a0or\u00a0Service.Type=LoadBalancer.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#prerequisites","title":"Prerequisites","text":"<p>You must have an\u00a0Ingress controller\u00a0to satisfy an Ingress. Only creating an Ingress resource has no effect.</p> <p>You may need to deploy an Ingress controller such as\u00a0ingress-nginx. You can choose from a number of\u00a0Ingress controllers.</p> <p>Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress controllers operate slightly differently.</p> <p>!!! Note     \u00a0Make sure you review your Ingress controller\u2019s documentation to understand the caveats of choosing it.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#the-ingress-resource","title":"The Ingress resource","text":"<p>A minimal Ingress resource example:</p> <p>service/networking/minimal-ingress.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: minimal-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx-example\n  rules:\n  - http:\n      paths:\n      - path: /testpath\n        pathType: Prefix\n        backend:\n          service:\n            name: test\n            port:\n              number: 80\n</code></pre> <p>An Ingress needs\u00a0<code>apiVersion</code>,\u00a0<code>kind</code>,\u00a0<code>metadata</code>\u00a0and\u00a0<code>spec</code>\u00a0fields. The name of an Ingress object must be a valid\u00a0DNS subdomain name. For general information about working with config files, see\u00a0deploying applications,\u00a0configuring containers,\u00a0managing resources. Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which is the\u00a0rewrite-target annotation. Different\u00a0Ingress controllers\u00a0support different annotations. Review the documentation for your choice of Ingress controller to learn which annotations are supported.</p> <p>The\u00a0Ingress spec\u00a0has all the information needed to configure a load balancer or proxy server. Most importantly, it contains a list of rules matched against all incoming requests. Ingress resource only supports rules for directing HTTP(S) traffic.</p> <p>If the\u00a0<code>ingressClassName</code>\u00a0is omitted, a\u00a0default Ingress class\u00a0should be defined.</p> <p>There are some ingress controllers, that work without the definition of a default\u00a0<code>IngressClass</code>. For example, the Ingress-NGINX controller can be configured with a\u00a0flag <code>--watch-ingress-without-class</code>. It is\u00a0recommended\u00a0though, to specify the default\u00a0<code>IngressClass</code>\u00a0as shown\u00a0below.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#ingress-rules","title":"Ingress rules","text":"<p>Each HTTP rule contains the following information:</p> <ul> <li> <p>An optional host. In this example, no host is specified, so the rule applies to all inbound HTTP traffic through the IP address specified. If a host is provided (for example, foo.bar.com), the rules apply to that host.</p> </li> <li> <p>A list of paths (for example,\u00a0<code>/testpath</code>), each of which has an associated backend defined with a\u00a0<code>service.name</code> and a\u00a0<code>service.port.name</code> or\u00a0<code>service.port.number</code>. Both the host and path must match the content of an incoming request before the load balancer directs traffic to the referenced Service.</p> </li> <li> <p>A backend is a combination of Service and port names as described in the\u00a0Service doc or a\u00a0custom resource backend by way of a\u00a0CRD. HTTP (and HTTPS) requests to the Ingress that match the host and path of the rule are sent to the listed backend.</p> </li> </ul> <p>A\u00a0<code>defaultBackend</code>\u00a0is often configured in an Ingress controller to service any requests that do not match a path in the spec.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#defaultbackend","title":"DefaultBackend","text":"<p>An Ingress with no rules sends all traffic to a single default backend and\u00a0<code>.spec.defaultBackend</code>\u00a0is the backend that should handle requests in that case. The\u00a0<code>defaultBackend</code>\u00a0is conventionally a configuration option of the\u00a0Ingress controller\u00a0and is not specified in your Ingress resources. If no\u00a0<code>.spec.rules</code>\u00a0are specified,\u00a0<code>.spec.defaultBackend</code>\u00a0must be specified. If\u00a0<code>defaultBackend</code>\u00a0is not set, the handling of requests that do not match any of the rules will be up to the ingress controller (consult the documentation for your ingress controller to find out how it handles this case).</p> <p>If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#resource-backends","title":"Resource backends","text":"<p>A\u00a0<code>Resource</code>\u00a0backend is an ObjectRef to another Kubernetes resource within the same namespace as the Ingress object. A\u00a0<code>Resource</code>\u00a0is a mutually exclusive setting with Service, and will fail validation if both are specified. A common usage for a\u00a0<code>Resource</code>\u00a0backend is to ingress data to an object storage backend with static assets.</p> <p>service/networking/ingress-resource-backend.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-resource-backend\nspec:\n  defaultBackend:\n    resource:\n      apiGroup: k8s.example.com\n      kind: StorageBucket\n      name: static-assets\n  rules:\n    - http:\n        paths:\n          - path: /icons\n            pathType: ImplementationSpecific\n            backend:\n              resource:\n                apiGroup: k8s.example.com\n                kind: StorageBucket\n                name: icon-assets\n\n</code></pre> <p>After creating the Ingress above, you can view it with the following command:</p> <pre><code>kubectl describe ingress ingress-resource-backend\n</code></pre> <pre><code>Name:             ingress-resource-backend\nNamespace:        default\nAddress:\nDefault backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets\nRules:\n  Host        Path  Backends\n  ----        ----  --------\n  *\n              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets\nAnnotations:  &lt;none&gt;\nEvents:       &lt;none&gt;\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#path-types","title":"Path types","text":"<p>Each path in an Ingress is required to have a corresponding path type. Paths that do not include an explicit\u00a0<code>pathType</code>\u00a0will fail validation. There are three supported path types:</p> <ul> <li> <p><code>ImplementationSpecific</code>: With this path type, matching is up to the IngressClass. Implementations can treat this as a separate\u00a0<code>pathType</code> or treat it identically to\u00a0<code>Prefix</code> or\u00a0<code>Exact</code> path types.</p> </li> <li> <p><code>Exact</code>: Matches the URL path exactly and with case sensitivity.</p> </li> <li> <p><code>Prefix</code>: Matches based on a URL path prefix split by\u00a0<code>/</code>. Matching is case-sensitive and done on a path element-by-element basis. A path element refers to the list of labels in the path split by the\u00a0/\u00a0separator. A request is a match for path\u00a0p\u00a0if every\u00a0p\u00a0is an element-wise prefix of\u00a0p\u00a0of the request path. Note:\u00a0If the last element of the path is a substring of the last element in the request path, it is not a match (for example:\u00a0<code>/foo/bar</code> matches\u00a0<code>/foo/bar/baz</code>, but does not match\u00a0<code>/foo/barbaz</code>).</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#examples","title":"Examples","text":"Kind Path(s) Request Path(s) Matches? Prefix <code>/</code> (all paths) Yes Exact <code>/foo</code> <code>/foo</code> Yes Exact <code>/foo</code> <code>/bar</code> No Exact <code>/foo</code> <code>/foo/</code> No Exact <code>/foo/</code> <code>/foo</code> No Prefix <code>/foo</code> <code>/foo, /foo/</code> Yes Prefix <code>/foo/</code> <code>/foo, /foo/</code> Yes Prefix <code>/aaa/bb</code> <code>/aaa/bbb</code> No Prefix <code>/aaa/bbb</code> <code>/aaa/bbb</code> Yes Prefix <code>/aaa/bbb/</code> <code>/aaa/bbb</code> Yes, ignores trailing slash Prefix <code>/aaa/bbb</code> <code>/aaa/bbb/</code> Yes, matches trailing slash Prefix <code>/aaa/bbb</code> <code>/aaa/bbb/ccc</code> Yes, matches subpath Prefix <code>/aaa/bbb</code> <code>/aaa/bbbxyz</code> No, does not match string prefix Prefix <code>/, /aaa</code> <code>/aaa/ccc</code> Yes, matches <code>/aaa</code> prefix Prefix <code>/, /aaa, /aaa/bbb</code> <code>/aaa/bbb</code> Yes, matches <code>/aaa/bbb</code> prefix Prefix <code>/, /aaa, /aaa/bbb</code> <code>/ccc</code> Yes, matches <code>/</code> prefix Prefix <code>/aaa</code> <code>/ccc</code> No, uses default backend Mixed <code>/foo (Prefix), /foo (Exact)</code> <code>/foo</code> Yes, prefers Exact"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#multiple-matches","title":"Multiple matches","text":"<p>In some cases, multiple paths within an Ingress will match a request. In those cases precedence will be given first to the longest matching path. If two paths are still equally matched, precedence will be given to paths with an exact path type over prefix path type.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#hostname-wildcards","title":"Hostname wildcards","text":"<p>Hosts can be precise matches (for example \u201c<code>foo.bar.com</code>\u201d) or a wildcard (for example \u201c<code>*.foo.com</code>\u201d). Precise matches require that the HTTP\u00a0<code>host</code>\u00a0header matches the\u00a0<code>host</code>\u00a0field. Wildcard matches require the HTTP\u00a0<code>host</code>\u00a0header is equal to the suffix of the wildcard rule.</p> Host Host Header Match? <code>*.foo.com</code> <code>bar.foo.com</code> Matches based on shared suffix <code>*.foo.com</code> <code>baz.bar.foo.com</code> No match, wildcard only covers a single DNS label <code>*.foo.com</code> <code>foo.com</code> No match, wildcard only covers a single DNS label <p>service/networking/ingress-wildcard-host.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-wildcard-host\nspec:\n  rules:\n  - host: \"foo.bar.com\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/bar\"\n        backend:\n          service:\n            name: service1\n            port:\n              number: 80\n  - host: \"*.foo.com\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/foo\"\n        backend:\n          service:\n            name: service2\n            port:\n              number: 80\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#ingress-class","title":"Ingress class","text":"<p>Ingresses can be implemented by different controllers, often with various configurations. Each Ingress should specify a class, a reference to an IngressClass resource that contains additional configuration including the name of the controller that should implement the class.</p> <p>service/networking/external-lb.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: external-lb\nspec:\n  controller: example.com/ingress-controller\n  parameters:\n    apiGroup: k8s.example.com\n    kind: IngressParameters\n    name: external-lb\n</code></pre> <p>The\u00a0<code>.spec.parameters</code>\u00a0field of an IngressClass lets you reference another resource that provides configuration related to that IngressClass.</p> <p>The specific type of parameters to use depends on the ingress controller that you specify in the\u00a0<code>.spec.controller</code>\u00a0field of the IngressClass.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#ingressclass-scope","title":"IngressClass scope","text":"<p>Depending on your ingress controller, you may be able to use parameters that you set cluster-wide, or just for one namespace.</p> <ul> <li> <p>Cluster</p> </li> <li> <p>Namespaced</p> </li> </ul> <p>The default scope for IngressClass parameters is cluster-wide.</p> <p>If you set the\u00a0<code>.spec.parameters</code>\u00a0field and don\u2019t set\u00a0<code>.spec.parameters.scope</code>, or if you set\u00a0<code>.spec.parameters.scope</code>\u00a0to\u00a0<code>Cluster</code>, then the IngressClass refers to a cluster-scoped resource. The\u00a0<code>kind</code>\u00a0(in combination with the\u00a0<code>apiGroup</code>) of the parameters refers to a cluster-scoped API (possibly a custom resource), and the\u00a0<code>name</code>\u00a0of the parameters identifies a specific cluster-scoped resource for that API.</p> <p>For example:</p> <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: external-lb-1\nspec:\n  controller: example.com/ingress-controller\n  parameters:\n    # The parameters for this IngressClass are specified in a\n    # ClusterIngressParameter (API group k8s.example.net) named\n    # \"external-config-1\". This definition tells Kubernetes to\n    # look for a cluster-scoped parameter resource.\n    scope: Cluster\n    apiGroup: k8s.example.net\n    kind: ClusterIngressParameter\n    name: external-config-1\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#deprecated-annotation","title":"Deprecated annotation","text":"<p>Before the IngressClass resource and\u00a0<code>ingressClassName</code>\u00a0the field was added in Kubernetes 1.18, and Ingress classes were specified with a\u00a0<code>kubernetes.io/ingress.class</code>\u00a0annotation on the Ingress. This annotation was never formally defined but was widely supported by Ingress controllers.</p> <p>The newer\u00a0<code>ingressClassName</code>\u00a0field on Ingresses replaces that annotation but is not a direct equivalent. While the annotation was generally used to reference the name of the Ingress controller that should implement the Ingress, the field refers to an IngressClass resource that contains additional Ingress configuration, including the name of the Ingress controller.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#default-ingressclass","title":"Default IngressClass","text":"<p>You can mark a particular IngressClass as default for your cluster. Setting the\u00a0<code>ingressclass.kubernetes.io/is-default-class</code>\u00a0annotation to\u00a0<code>true</code>\u00a0on an IngressClass resource will ensure that new Ingresses without an\u00a0<code>ingressClassName</code>\u00a0field specified will be assigned this default IngressClass.</p> <p>!!! Info      If you have more than one IngressClass marked as the default for your cluster, the admission controller prevents creating new Ingress objects that don\u2019t have an\u00a0<code>ingressClassName</code>\u00a0specified. You can resolve this by ensuring that at most 1 IngressClass is marked as default in your cluster.</p> <p>There are some ingress controllers, that work without the definition of a default\u00a0<code>IngressClass</code>. For example, the Ingress-NGINX controller can be configured with a\u00a0flag <code>--watch-ingress-without-class</code>. It is\u00a0recommended\u00a0though, to specify the default\u00a0<code>IngressClass</code>:</p> <p><code>service/networking/default-ingressclass.yaml</code> </p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n  name: nginx-example\n  annotations:\n    ingressclass.kubernetes.io/is-default-class: \"true\"\nspec:\n  controller: k8s.io/ingress-nginx\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#types-of-ingress","title":"Types of Ingress","text":""},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#ingress-backed-by-a-single-service","title":"Ingress backed by a single Service","text":"<p>There are existing Kubernetes concepts that allow you to expose a single Service (see\u00a0alternatives). You can also do this with an Ingress by specifying a\u00a0default backend\u00a0with no rules.</p> <p>service/networking/test-ingress.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: test-ingress\nspec:\n  defaultBackend:\n    service:\n      name: test\n      port:\n        number: 80\n</code></pre> <p>If you create it using\u00a0<code>kubectl apply -f</code>\u00a0you should be able to view the state of the Ingress you added:</p> <pre><code>kubectl get ingress test-ingress\n\nNAME           CLASS         HOSTS   ADDRESS         PORTS   AGE\ntest-ingress   external-lb   *       203.0.113.123   80      59s\n</code></pre> <p>Where\u00a0<code>203.0.113.123</code>\u00a0is the IP allocated by the Ingress controller to satisfy this Ingress.</p> <p>!!! Note     Ingress controllers and load balancers may take a minute or two to allocate an IP address. Until that time, you often see the address listed as\u00a0<code>&lt;pending&gt;</code>.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#simple-fanout","title":"Simple fanout","text":"<p>A fanout configuration routes traffic from a single IP address to more than one Service, based on the HTTP URI being requested. An Ingress allows you to keep the number of load balancers down to a minimum. For example, a setup like:</p> <p></p> <p>Taikun - Ingress Fan Out</p> <p>It would require an Ingress such as:</p> <p>service/networking/simple-fanout-example.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: simple-fanout-example\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - path: /foo\n        pathType: Prefix\n        backend:\n          service:\n            name: service1\n            port:\n              number: 4200\n      - path: /bar\n        pathType: Prefix\n        backend:\n          service:\n            name: service2\n            port:\n              number: 8080\n</code></pre> <p>When you create the Ingress with\u00a0<code>kubectl apply -f</code>:</p> <pre><code>kubectl describe ingress simple-fanout-example\n\nName:             simple-fanout-example\nNamespace:        default\nAddress:          178.91.123.132\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\nRules:\n  Host         Path  Backends\n  ----         ----  --------\n  foo.bar.com\n               /foo   service1:4200 (10.8.0.90:4200)\n               /bar   service2:8080 (10.8.0.91:8080)\nEvents:\n  Type     Reason  Age                From                     Message\n  ----     ------  ----               ----                     -------\n  Normal   ADD     22s                loadbalancer-controller  default/test\n</code></pre> <p>The Ingress controller provisions an implementation-specific load balancer that satisfies the Ingress, as long as the Services (<code>service1</code>,\u00a0<code>service2</code>) exist. When it has done so, you can see the address of the load balancer at the Address field.</p> <p>!!! Note     Depending on the\u00a0Ingress controller\u00a0you are using, you may need to create a default-http-backend\u00a0Service.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#name-based-virtual-hosting","title":"Name based virtual hosting","text":"<p>Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.</p> <p></p> <p>Taikun - Ingress Name-Based Virtual hosting</p> <p>The following Ingress tells the backing load balancer to route requests based on the\u00a0Host header.</p> <p>service/networking/name-virtual-host-ingress.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: name-virtual-host-ingress\nspec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: service1\n            port:\n              number: 80\n  - host: bar.foo.com\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: service2\n            port:\n              number: 80\n</code></pre> <p>If you create an Ingress resource without any hosts defined in the rules, then any web traffic to the IP address of your Ingress controller can be matched without a name-based virtual host being required.</p> <p>For example, the following Ingress routes traffic requested for\u00a0<code>first.bar.com</code>\u00a0to\u00a0<code>service1</code>,\u00a0<code>second.bar.com</code>\u00a0to\u00a0<code>service2</code>, and any traffic whose request host header doesn\u2019t match\u00a0<code>first.bar.com</code>\u00a0and\u00a0<code>second.bar.com</code>\u00a0to\u00a0<code>service3</code>.</p> <p>service/networking/name-virtual-host-ingress-no-third-host.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: name-virtual-host-ingress-no-third-host\nspec:\n  rules:\n  - host: first.bar.com\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: service1\n            port:\n              number: 80\n  - host: second.bar.com\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: service2\n            port:\n              number: 80\n  - http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: service3\n            port:\n              number: 80\n</code></pre> <p>TLS</p> <p>You can secure an Ingress by specifying a\u00a0Secret\u00a0that contains a TLS private key and certificate. The Ingress resource only supports a single TLS port, 443, and assumes TLS termination at the ingress point (traffic to the Service and its Pods is in plaintext). If the TLS configuration section in an Ingress specifies different hosts, they are multiplexed on the same port according to the hostname specified through the SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret must contain keys named\u00a0<code>tls.crt</code>\u00a0and\u00a0<code>tls.key</code>\u00a0that contains the certificate and private key to use for TLS. For example:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: testsecret-tls\n  namespace: default\ndata:\n  tls.crt: base64 encoded cert\n  tls.key: base64 encoded key\ntype: kubernetes.io/tls\n</code></pre> <p>Referencing this secret in an Ingress tells the Ingress controller to secure the channel from the client to the load balancer using TLS. You need to make sure the TLS secret you created came from a certificate that contains a Common Name (CN), also known as a Fully Qualified Domain Name (FQDN) for\u00a0<code>https-example.foo.com</code>.</p> <p>!!! Note     Keep in mind that TLS will not work on the default rule because the certificates would have to be issued for all the possible sub-domains. Therefore,\u00a0<code>hosts</code>\u00a0in the\u00a0<code>tls</code>\u00a0section need to explicitly match the\u00a0<code>host</code>\u00a0in the\u00a0<code>rules</code>\u00a0section.</p> <p>service/networking/tls-example-ingress.yaml!</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tls-example-ingress\nspec:\n  tls:\n  - hosts:\n      - https-example.foo.com\n    secretName: testsecret-tls\n  rules:\n  - host: https-example.foo.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: service1\n            port:\n              number: 80\n</code></pre> <p>!!! Note     There is a gap between TLS features supported by various Ingress controllers. Please refer to documentation on\u00a0nginx,\u00a0GCE, or any other platform-specific Ingress controller to understand how TLS works in your environment.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#load-balancing","title":"Load balancing","text":"<p>An Ingress controller is bootstrapped with some load balancing policy settings that it applies to all Ingress, such as the load balancing algorithm, backend weight scheme, and others. More advanced load balancing concepts (e.g. persistent sessions, dynamic weights) are not yet exposed through the Ingress. You can instead get these features through the load balancer used for a Service.</p> <p>It\u2019s also worth noting that even though health checks are not exposed directly through the Ingress, there exist parallel concepts in Kubernetes such as\u00a0readiness probes\u00a0that allow you to achieve the same end result. Please review the controller-specific documentation to see how they handle health checks (for example:\u00a0nginx, or\u00a0GCE).</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#updating-an-ingress","title":"Updating an Ingress","text":"<p>To update an existing Ingress to add a new Host, you can update it by editing the resource:</p> <pre><code>kubectl describe ingress test\n\nName:             test\nNamespace:        default\nAddress:          178.91.123.132\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\nRules:\n  Host         Path  Backends\n  ----         ----  --------\n  foo.bar.com\n               /foo   service1:80 (10.8.0.90:80)\nAnnotations:\n  nginx.ingress.kubernetes.io/rewrite-target:  /\nEvents:\n  Type     Reason  Age                From                     Message\n  ----     ------  ----               ----                     -------\n  Normal   ADD     35s                loadbalancer-controller  default/test\n\nkubectl edit ingress test\n</code></pre> <p>This pops up an editor with the existing configuration in YAML format. Modify it to include the new Host:</p> <pre><code>spec:\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: service1\n            port:\n              number: 80\n        path: /foo\n        pathType: Prefix\n  - host: bar.baz.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: service2\n            port:\n              number: 80\n        path: /foo\n        pathType: Prefix\n..\n\n</code></pre> <p>After you save your changes, kubectl updates the resource in the API server, which tells the Ingress controller to reconfigure the load balancer.</p> <p>Verify this:</p> <pre><code>kubectl describe ingress test\n\nName:             test\nNamespace:        default\nAddress:          178.91.123.132\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\nRules:\n  Host         Path  Backends\n  ----         ----  --------\n  foo.bar.com\n               /foo   service1:80 (10.8.0.90:80)\n  bar.baz.com\n               /foo   service2:80 (10.8.0.91:80)\nAnnotations:\n  nginx.ingress.kubernetes.io/rewrite-target:  /\nEvents:\n  Type     Reason  Age                From                     Message\n  ----     ------  ----               ----                     -------\n  Normal   ADD     45s                loadbalancer-controller  default/test\n</code></pre> <p>You can achieve the same outcome by invoking\u00a0kubectl replace -f\u00a0on a modified Ingress YAML file.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#failing-across-availability-zones","title":"Failing across availability zones","text":"<p>Techniques for spreading traffic across failure domains differ between cloud providers. Please check the documentation of the relevant\u00a0Ingress controller\u00a0for details.</p>"},{"location":"Taikun_CloudWorks_Overview/Ingress_in_Kubernetes/#alternatives","title":"Alternatives","text":"<p>You can expose a Service in multiple ways that don\u2019t directly involve the Ingress resource:</p> <ul> <li> <p>Use\u00a0Service.Type=LoadBalancer</p> </li> <li> <p>Use\u00a0Service.Type=NodePort</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/","title":"Kubernetes DNS Pod Service","text":""},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#dns-for-services-and-pods","title":"DNS for Services and Pods","text":"<p>Kubernetes creates DNS records for Services and Pods. You can contact Services with consistent DNS names instead of IP addresses.</p> <p>Kubernetes publishes information about Pods and Services which is used to program DNS. Kubelet configures Pods\u2019 DNS so that running containers can lookup Services by name rather than IP.</p> <p>Services defined in the cluster are assigned DNS names. By default, a client Pod\u2019s DNS search list includes the Pod\u2019s own namespace and the cluster\u2019s default domain.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#namespaces-of-services","title":"Namespaces of Services","text":"<p>A DNS query may return different results based on the namespace of the Pod making it. DNS queries that don\u2019t specify a namespace are limited to the Pod\u2019s namespace. Access Services in other namespaces by specifying it in the DNS query.</p> <p>For example, consider a Pod in a\u00a0<code>test</code>\u00a0namespace. A\u00a0<code>data</code>\u00a0Service is in the\u00a0<code>prod</code>\u00a0namespace.</p> <p>A query for\u00a0<code>data</code>\u00a0returns no results, because it uses the Pod\u2019s\u00a0<code>test</code>\u00a0namespace.</p> <p>A query for\u00a0<code>data.prod</code>\u00a0returns the intended result, because it specifies the namespace.</p> <p>DNS queries may be expanded using the Pod\u2019s\u00a0<code>/etc/resolv.conf</code>. Kubelet configures this file for each Pod. For example, a query for just\u00a0<code>data</code>\u00a0may be expanded to\u00a0<code>data.test.svc.cluster.local</code>. The values of the\u00a0<code>search</code>\u00a0option are used to expand queries. To learn more about DNS queries, see\u00a0the\u00a0<code>resolv.conf</code>\u00a0manual page.</p> <pre><code>nameserver 10.32.0.10\nsearch &lt;namespace&gt;.svc.cluster.local svc.cluster.local cluster.local\noptions ndots:5\n</code></pre> <p>In summary, a Pod in the\u00a0test\u00a0namespace can successfully resolve either\u00a0<code>data.prod</code>\u00a0or\u00a0<code>data.prod.svc.cluster.local</code>.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#dns-records","title":"DNS Records","text":"<p>What objects get DNS records?</p> <ol> <li> <p>Services</p> </li> <li> <p>Pods</p> </li> </ol> <p>The following sections detail the supported DNS record types and layout that is supported. Any other layout or names or queries that happen to work are considered implementation details and are subject to change without warning. For more up-to-date specifications, see\u00a0Kubernetes DNS-Based Service Discovery.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#services","title":"Services","text":""},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#aaaaa-records","title":"A/AAAA records","text":"<p>\u201cNormal\u201d (not headless) Services are assigned DNS A and/or AAAA records, depending on the IP family or families of the Service, with a name of the form\u00a0<code>my-svc.my-namespace.svc.cluster-domain.example</code>. This resolves to the cluster IP of the Service.</p> <p>Headless Services\u00a0(without a cluster IP) Services are also assigned DNS A and/or AAAA records, with a name of the form\u00a0<code>my-svc.my-namespace.svc.cluster-domain.example</code>. Unlike normal Services, this resolves to the set of IPs of all of the Pods selected by the Service. Clients are expected to consume the set or else use standard round-robin selection from the set.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#srv-records","title":"SRV records","text":"<p>SRV Records are created for named ports that are part of normal or headless services. For each named port, the SRV record has the form\u00a0<code>_port-name._port-protocol.my-svc.my-namespace.svc.cluster-domain.example</code>. For a regular Service, this resolves to the port number and the domain name:\u00a0<code>my-svc.my-namespace.svc.cluster-domain.example</code>. For a headless Service, this resolves to multiple answers, one for each Pod that is backing the Service, and contains the port number and the domain name of the Pod of the form\u00a0<code>hostname.my-svc.my-namespace.svc.cluster-domain.example</code>.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#pods","title":"Pods","text":""},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#aaaaa-records_1","title":"A/AAAA records","text":"<p>Kube-DNS versions, before the implementation of the\u00a0DNS specification, had the following DNS resolution:</p> <pre><code>pod-ipv4-address.my-namespace.pod.cluster-domain.example.\n</code></pre> <p>For example, if a Pod in the\u00a0<code>default</code>\u00a0namespace has the IP address 172.17.0.3, and the domain name for your cluster is\u00a0<code>cluster.local</code>, then the Pod has a DNS name:</p> <pre><code>172-17-0-3.default.pod.cluster.local.\n</code></pre> <p>Any Pods exposed by a Service have the following DNS resolution available:</p> <pre><code>pod-ipv4-address.service-name.my-namespace.svc.cluster-domain.example.\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#pods-hostname-and-subdomain-fields","title":"Pod\u2019s hostname and subdomain fields","text":"<p>Currently, when a Pod is created, its hostname (as observed from within the Pod) is the Pod\u2019s\u00a0<code>metadata.name</code>\u00a0value.</p> <p>The Pod spec has an optional\u00a0<code>hostname</code>\u00a0field, which can be used to specify a different hostname. When specified, it takes precedence over the Pod\u2019s name to be the hostname of the Pod (again, as observed from within the Pod). For example, given a Pod with\u00a0<code>spec.hostname</code>\u00a0set to\u00a0<code>\"my-host\"</code>, the Pod will have its hostname set to\u00a0<code>\"my-host\"</code>.</p> <p>The Pod spec also has an optional\u00a0<code>subdomain</code>\u00a0field, which can be used to indicate that the pod is part of a sub-group of the namespace. For example, a Pod with\u00a0<code>spec.hostname</code>\u00a0set to\u00a0<code>\"foo\"</code>, and\u00a0<code>spec.subdomain</code>\u00a0set to\u00a0<code>\"bar\"</code>, in namespace\u00a0<code>\"my-namespace\"</code>, will have its hostname set to\u00a0<code>\"foo\"</code>\u00a0and its fully qualified domain name (FQDN) set to\u00a0<code>\"foo.bar.my-namespace.svc.cluster.local\"</code>\u00a0(once more, as observed from within the Pod).</p> <p>If a headless Service exists in the same namespace as the Pod, with the same name as the subdomain, the cluster\u2019s DNS Server also returns A and/or AAAA records for the Pod\u2019s fully qualified hostname.</p> <p>Example:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: busybox-subdomain\nspec:\n  selector:\n    name: busybox\n  clusterIP: None\n  ports:\n  - name: foo # name is not required for single-port Services\n    port: 1234\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox1\n  labels:\n    name: busybox\nspec:\n  hostname: busybox-1\n  subdomain: busybox-subdomain\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    name: busybox\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox2\n  labels:\n    name: busybox\nspec:\n  hostname: busybox-2\n  subdomain: busybox-subdomain\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    name: busybox\n</code></pre> <p>Given the above Service\u00a0<code>\"busybox-subdomain\"</code>\u00a0and the Pods, which set\u00a0<code>spec.subdomain</code>\u00a0to\u00a0<code>\"busybox-subdomain\"</code>, the first Pod will see its own FQDN as\u00a0<code>\"busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example\"</code>. DNS serves A and/or AAAA records at that name, pointing to the Pod\u2019s IP. Both Pods \u201c<code>busybox1</code>\u201d And \u201c<code>busybox2</code>\u201d will have their own address records.</p> <p>An\u00a0EndpointSlice\u00a0can specify the DNS hostname for any endpoint address, along with its IP.</p> <p>??? Note     \u00a0Because A and AAAA records are not created for Pod names,\u00a0<code>hostname</code>\u00a0is required for the Pod\u2019s A or AAAA record to be created. A Pod with no\u00a0<code>hostname</code>\u00a0but with\u00a0<code>subdomain</code>\u00a0will only create the A or AAAA record for the headless Service (<code>busybox-subdomain.my-namespace.svc.cluster-domain.example</code>), pointing to the Pods\u2019 IP addresses. Also, the Pod needs to be ready in order to have a record unless\u00a0<code>publishNotReadyAddresses=True</code>\u00a0is set on the Service.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#pods-sethostnameasfqdn-field","title":"Pod\u2019s setHostnameAsFQDN field","text":"<p>FEATURE STATE: <code>Kubernetes v1.22 [stable]</code></p> <p>When a Pod is configured to have a fully qualified domain name (FQDN), its hostname is the short hostname. For example, if you have a Pod with a fully qualified domain name\u00a0busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example, then by default the\u00a0hostname\u00a0command inside that Pod returns\u00a0busybox-1\u00a0and the\u00a0hostname \u2013fqdn\u00a0the command returns the FQDN.</p> <p>When you set\u00a0<code>setHostnameAsFQDN: true</code>\u00a0in the Pod spec, the kubelet writes the Pod\u2019s FQDN into the hostname for that Pod\u2019s namespace. In this case, both\u00a0<code>hostname</code>\u00a0and\u00a0<code>hostname --fqdn</code>\u00a0return the Pod\u2019s FQDN.</p> <p>??? Note     In Linux, the hostname field of the kernel (the\u00a0nodename\u00a0field of\u00a0struct utsname) is limited to 64 characters.</p> <p>If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in\u00a0<code>Pending</code>\u00a0status (<code>ContainerCreating</code>\u00a0as seen by\u00a0<code>kubectl</code>) generating error events, such as Failed to construct FQDN from Pod hostname and cluster domain, FQDN\u00a0<code>long-FQDN</code>\u00a0is too long (64 characters is the max, 70 characters requested). One way of improving user experience for this scenario is to create an\u00a0admission webhook controller\u00a0to control FQDN size when users create top level objects, for example, Deployment.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#pods-dns-policy","title":"Pod\u2019s DNS Policy","text":"<p>DNS policies can be set on a per-Pod basis. Currently Kubernetes supports the following Pod-specific DNS policies. These policies are specified in the\u00a0<code>dnsPolicy</code>\u00a0field of a Pod Spec.</p> <ul> <li> <p>\u201c<code>Default</code>\u201c: The Pod inherits the name resolution configuration from the node that the Pods run on. See\u00a0related discussion\u00a0for more details.</p> </li> <li> <p>\u201c<code>ClusterFirst</code>\u201c: Any DNS query that does not match the configured cluster domain suffix, such as \u201c<code>www.kubernetes.io</code>\u201c, is forwarded to an upstream nameserver by the DNS server. Cluster administrators may have extra stub-domain and upstream DNS servers configured. See\u00a0related discussion\u00a0for details on how DNS queries are handled in those cases.</p> </li> <li> <p>\u201c<code>ClusterFirstWithHostNet</code>\u201c: For Pods running with hostNetwork, you should explicitly set its DNS policy to \u201c<code>ClusterFirstWithHostNet</code>\u201c. Otherwise, Pods running with hostNetwork and\u00a0<code>\"ClusterFirst\"</code>\u00a0will fallback to the behavior of the\u00a0<code>\"Default\"</code>\u00a0policy.</p> </li> </ul> <p>??? Note     This is not supported on Windows. See\u00a0below\u00a0for details</p> <ul> <li>\u201c<code>None</code>\u201c: It allows a Pod to ignore DNS settings from the Kubernetes environment. All DNS settings are supposed to be provided using the\u00a0<code>dnsConfig</code>\u00a0field in the Pod Spec. See\u00a0Pod\u2019s DNS config\u00a0subsection below.</li> </ul> <p>!!! Note     \u201cDefault\u201d is not the default DNS policy. If\u00a0<code>dnsPolicy</code>\u00a0is not explicitly specified, then \u201cClusterFirst\u201d is used.</p> <p>The example below shows a Pod with its DNS policy set to \u201c<code>ClusterFirstWithHostNet</code>\u201d because it has\u00a0<code>hostNetwork</code>\u00a0set to\u00a0<code>true</code>.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\n  namespace: default\nspec:\n  containers:\n  - image: busybox:1.28\n    command:\n      - sleep\n      - \"3600\"\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  restartPolicy: Always\n  hostNetwork: true\n  dnsPolicy: ClusterFirstWithHostNet\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#pods-dns-config","title":"Pod\u2019s DNS Config","text":"<p>FEATURE STATE: <code>Kubernetes v1.14 [stable]</code></p> <p>Pod\u2019s DNS Config allows users more control on the DNS settings for a Pod.</p> <p>The\u00a0<code>dnsConfig</code>\u00a0field is optional and it can work with any\u00a0<code>dnsPolicy</code>\u00a0settings. However, when a Pod\u2019s\u00a0<code>dnsPolicy</code>\u00a0is set to \u201c<code>None</code>\u201c, the\u00a0<code>dnsConfig</code>\u00a0field has to be specified.</p> <p>Below are the properties a user can specify in the\u00a0<code>dnsConfig</code>\u00a0field:</p> <ul> <li> <p><code>nameservers</code>: a list of IP addresses that will be used as DNS servers for the Pod. There can be at most 3 IP addresses specified. When the Pod\u2019s\u00a0<code>dnsPolicy</code>\u00a0is set to \u201c<code>None</code>\u201c, the list must contain at least one IP address, otherwise this property is optional. The servers listed will be combined to the base nameservers generated from the specified DNS policy with duplicate addresses removed.</p> </li> <li> <p><code>searches</code>: a list of DNS search domains for hostname lookup in the Pod. This property is optional. When specified, the provided list will be merged into the base search domain names generated from the chosen DNS policy. Duplicate domain names are removed. Kubernetes allows up to 32 search domains.</p> </li> <li> <p><code>options</code>: an optional list of objects where each object may have a\u00a0<code>name</code>\u00a0property (required) and a\u00a0<code>value</code>\u00a0property (optional). The contents in this property will be merged to the options generated from the specified DNS policy. Duplicate entries are removed.</p> </li> </ul> <p>The following is an example Pod with custom DNS settings:</p> <p>service/networking/custom-dns.yaml</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: default\n  name: dns-example\nspec:\n  containers:\n    - name: test\n      image: nginx\n  dnsPolicy: \"None\"\n  dnsConfig:\n    nameservers:\n      - 192.0.2.1 # this is an example\n    searches:\n      - ns1.svc.cluster-domain.example\n      - my.dns.search.suffix\n    options:\n      - name: ndots\n        value: \"2\"\n      - name: edns0\n</code></pre> <p>When the Pod above is created, the container\u00a0<code>test</code>\u00a0gets the following contents in its\u00a0<code>/etc/resolv.conf</code>\u00a0file:</p> <pre><code>nameserver 192.0.2.1\nsearch ns1.svc.cluster-domain.example my.dns.search.suffix\noptions ndots:2 edns0\n</code></pre> <p>For IPv6 setup, the search path and name server should be set up like this:</p> <pre><code>kubectl exec -it dns-example -- cat /etc/resolv.conf\n</code></pre> <p>The output is similar to this:</p> <pre><code>nameserver 2001:db8:30::a\nsearch default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example\noptions ndots:5\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#dns-search-domain-list-limits","title":"DNS search domain list limits","text":"<p>FEATURE STATE: <code>Kubernetes 1.28 [stable]</code></p> <p>Kubernetes itself does not limit the DNS Config until the length of the search domain list exceeds 32 or the total length of all search domains exceeds 2048. This limit applies to the node\u2019s resolver configuration file, the Pod\u2019s DNS Config, and the merged DNS Config respectively.</p> <p>!!! Note     Some container runtimes of earlier versions may have their own restrictions on the number of DNS search domains. Depending on the container runtime environment, the pods with a large number of DNS search domains may get stuck in the pending state.</p> <pre><code>It is known that containerd v1.5.5 or earlier and CRI-O v1.21 or earlier have this problem.\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_DNS_Pod_Service/#dns-resolution-on-windows-nodes","title":"DNS resolution on Windows nodes","text":"<ul> <li> <p>ClusterFirstWithHostNet is not supported for Pods that run on Windows nodes. Windows treats all names with a\u00a0<code>.</code>\u00a0as a FQDN and skips FQDN resolution.</p> </li> <li> <p>On Windows, there are multiple DNS resolvers that can be used. As these come with slightly different behaviors, using the\u00a0<code>Resolve-DNSName</code>\u00a0powershell cmdlet for name query resolutions is recommended.</p> </li> <li> <p>On Linux, you have a DNS suffix list, which is used after resolution of a name as fully qualified has failed. On Windows, you can only have 1 DNS suffix, which is the DNS suffix associated with that Pod\u2019s namespace (example:\u00a0<code>mydns.svc.cluster.local</code>). Windows can resolve FQDNs, Services, or network name which can be resolved with this single suffix. For example, a Pod spawned in the\u00a0<code>default</code>\u00a0namespace, will have the DNS suffix\u00a0<code>default.svc.cluster.local</code>. Inside a Windows Pod, you can resolve both\u00a0<code>kubernetes.default.svc.cluster.local</code>\u00a0and\u00a0<code>kubernetes</code>, but not the partially qualified names (<code>kubernetes.default</code>\u00a0or\u00a0<code>kubernetes.default.svc</code>).</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/","title":"Kubernetes Persistent Volumes","text":"<p>This document describes\u00a0persistent volumes\u00a0in Kubernetes. Familiarity with\u00a0volumes,\u00a0StorageClasses\u00a0and\u00a0VolumeAttributesClasses\u00a0is suggested.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#introduction","title":"Introduction","text":"<p>Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.</p> <p>A\u00a0PersistentVolume\u00a0(PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using\u00a0Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.</p> <p>A\u00a0PersistentVolumeClaim\u00a0(PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany, ReadWriteMany, or ReadWriteOncePod, see\u00a0AccessModes).</p> <p>While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the\u00a0StorageClass\u00a0resource.</p> <p>See the\u00a0detailed walkthrough with working examples.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#lifecycle-of-a-volume-and-claim","title":"Lifecycle of a volume and claim","text":"<p>PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#provisioning","title":"Provisioning","text":"<p>There are two ways PVs may be provisioned: statically or dynamically.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#static","title":"Static","text":"<p>A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#dynamic","title":"Dynamic","text":"<p>When none of the static PVs the administrator created match a user\u2019s PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a\u00a0storage class\u00a0and the administrator must have created and configured that class for dynamic provisioning to occur. Claims that request the class\u00a0<code>\"\"</code>\u00a0effectively disable dynamic provisioning for themselves.</p> <p>To enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the\u00a0<code>DefaultStorageClass</code> admission controller\u00a0on the API server. This can be done, for example, by ensuring that\u00a0<code>DefaultStorageClass</code>\u00a0is among the comma-delimited, ordered list of values for the\u00a0<code>--enable-admission-plugins</code>\u00a0flag of the API server component. For more information on API server command-line flags, check\u00a0kube-apiserver\u00a0documentation.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#binding","title":"Binding","text":"<p>A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the control plane watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.</p> <p>Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#using","title":"Using","text":"<p>Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.</p> <p>Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a\u00a0<code>persistentVolumeClaim</code>\u00a0section in a Pod\u2019s\u00a0<code>volumes</code>\u00a0block. See\u00a0Claims As Volumes\u00a0for more details on this.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#storage-object-in-use-protection","title":"Storage Object in Use Protection","text":"<p>The purpose of the Storage Object in Use Protection feature is to ensure that PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs) that are bound to PVCs are not removed from the system, as this may result in data loss.</p> <p>Note:\u00a0PVC is in active use by a Pod when a Pod object exists that is using the PVC.</p> <p>If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. PVC removal is postponed until the PVC is no longer actively used by any Pods. Also, if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is postponed until the PV is no longer bound to a PVC.</p> <p>You can see that a PVC is protected when the PVC\u2019s status is\u00a0<code>Terminating</code>\u00a0and the\u00a0<code>Finalizers</code>\u00a0list includes\u00a0<code>kubernetes.io/pvc-protection</code>:</p> <pre><code>kubectl describe pvc hostpath\nName:          hostpath\nNamespace:     default\nStorageClass:  example-hostpath\nStatus:        Terminating\nVolume:\nLabels:        &lt;none&gt;\nAnnotations:   volume.beta.kubernetes.io/storage-class=example-hostpath\n               volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath\nFinalizers:    [kubernetes.io/pvc-protection]\n...\n</code></pre> <p>You can see that a PV is protected when the PV\u2019s status is\u00a0<code>Terminating</code>\u00a0and the\u00a0<code>Finalizers</code>\u00a0list includes\u00a0<code>kubernetes.io/pv-protection</code>\u00a0too:</p> <pre><code>kubectl describe pv task-pv-volume\nName:            task-pv-volume\nLabels:          type=local\nAnnotations:     &lt;none&gt;\nFinalizers:      [kubernetes.io/pv-protection]\nStorageClass:    standard\nStatus:          Terminating\nClaim:\nReclaim Policy:  Delete\nAccess Modes:    RWO\nCapacity:        1Gi\nMessage:\nSource:\n    Type:          HostPath (bare host directory volume)\n    Path:          /tmp/data\n    HostPathType:\nEvents:            &lt;none&gt;\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#reclaiming","title":"Reclaiming","text":"<p>When a user is done with their volume, they can delete the PVC objects from the API that allows the reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#retain","title":"Retain","text":"<p>The\u00a0<code>Retain</code>\u00a0reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered \u201creleased\u201d. But it is not yet available for another claim because the previous claimant\u2019s data remains on the volume. An administrator can manually reclaim the volume with the following steps.</p> <ol> <li> <p>Delete the PersistentVolume. The associated storage asset in external infrastructure still exists after the PV is deleted.</p> </li> <li> <p>Manually clean up the data on the associated storage asset accordingly.</p> </li> <li> <p>Manually delete the associated storage asset.</p> </li> </ol> <p>If you want to reuse the same storage asset, create a new PersistentVolume with the same storage asset definition.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#delete","title":"Delete","text":"<p>For volume plugins that support the\u00a0<code>Delete</code>\u00a0reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure. Volumes that were dynamically provisioned inherit the\u00a0reclaim policy of their StorageClass, which defaults to\u00a0<code>Delete</code>. The administrator should configure the StorageClass according to users\u2019 expectations; otherwise, the PV must be edited or patched after it is created. See\u00a0Change the Reclaim Policy of a PersistentVolume.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#recycle","title":"Recycle","text":"<p>Warning:\u00a0The\u00a0<code>Recycle</code>\u00a0reclaim policy is deprecated. Instead, the recommended approach is to use dynamic provisioning.</p> <p>If supported by the underlying volume plugin, the\u00a0<code>Recycle</code>\u00a0reclaim policy performs a basic scrub (<code>rm -rf /thevolume/*</code>) on the volume and makes it available again for a new claim.</p> <p>However, an administrator can configure a custom recycler Pod template using the Kubernetes controller manager command line arguments as described in the\u00a0reference. The custom recycler Pod template must contain a\u00a0<code>volumes</code>\u00a0specification, as shown in the example below:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pv-recycler\n  namespace: default\nspec:\n  restartPolicy: Never\n  volumes:\n  - name: vol\n    hostPath:\n      path: /any/path/it/will/be/replaced\n  containers:\n  - name: pv-recycler\n    image: \"registry.k8s.io/busybox\"\n    command: [\"/bin/sh\", \"-c\", \"test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \\\"$(ls -A /scrub)\\\" || exit 1\"]\n    volumeMounts:\n    - name: vol\n      mountPath: /scrub\n</code></pre> <p>However, the particular path specified in the custom recycler Pod template in the\u00a0<code>volumes</code>\u00a0part is replaced with the particular path of the volume that is being recycled.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#persistentvolume-deletion-protection-finalizer","title":"PersistentVolume deletion protection finalizer","text":"<p>FEATURE STATE: <code>Kubernetes v1.23 [alpha]</code></p> <p>Finalizers can be added on a PersistentVolume to ensure that PersistentVolumes having\u00a0<code>Delete</code>\u00a0reclaim policy are deleted only after the backing storage are deleted.</p> <p>The newly introduced finalizers\u00a0<code>kubernetes.io/pv-controller</code>\u00a0and\u00a0<code>external-provisioner.volume.kubernetes.io/finalizer</code>\u00a0are only added to dynamically provisioned volumes.</p> <p>The finalizer\u00a0<code>kubernetes.io/pv-controller</code>\u00a0is added to in-tree plugin volumes. The following is an example</p> <pre><code>kubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78\nName:            pvc-74a498d6-3929-47e8-8c02-078c1ece4d78\nLabels:          &lt;none&gt;\nAnnotations:     kubernetes.io/createdby: vsphere-volume-dynamic-provisioner\n                 pv.kubernetes.io/bound-by-controller: yes\n                 pv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volume\nFinalizers:      [kubernetes.io/pv-protection kubernetes.io/pv-controller]\nStorageClass:    vcp-sc\nStatus:          Bound\nClaim:           default/vcp-pvc-1\nReclaim Policy:  Delete\nAccess Modes:    RWO\nVolumeMode:      Filesystem\nCapacity:        1Gi\nNode Affinity:   &lt;none&gt;\nMessage:\nSource:\n    Type:               vSphereVolume (a Persistent Disk resource in vSphere)\n    VolumePath:         [vsanDatastore] d49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-dynamic-pvc-74a498d6-3929-47e8-8c02-078c1ece4d78.vmdk\n    FSType:             ext4\n    StoragePolicyName:  vSAN Default Storage Policy\nEvents:                 &lt;none&gt;\n</code></pre> <p>The finalizer\u00a0<code>external-provisioner.volume.kubernetes.io/finalizer</code>\u00a0is added for CSI volumes. The following is an example:</p> <pre><code>Name:            pvc-2f0bab97-85a8-4552-8044-eb8be45cf48d\nLabels:          &lt;none&gt;\nAnnotations:     pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com\nFinalizers:      [kubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer]\nStorageClass:    fast\nStatus:          Bound\nClaim:           demo-app/nginx-logs\nReclaim Policy:  Delete\nAccess Modes:    RWO\nVolumeMode:      Filesystem\nCapacity:        200Mi\nNode Affinity:   &lt;none&gt;\nMessage:\nSource:\n    Type:              CSI (a Container Storage Interface (CSI) volume source)\n    Driver:            csi.vsphere.vmware.com\n    FSType:            ext4\n    VolumeHandle:      44830fa8-79b4-406b-8b58-621ba25353fd\n    ReadOnly:          false\n    VolumeAttributes:      storage.kubernetes.io/csiProvisionerIdentity=1648442357185-8081-csi.vsphere.vmware.com\n                           type=vSphere CNS Block Volume\nEvents:                &lt;none&gt;\n</code></pre> <p>When the\u00a0<code>CSIMigration{provider}</code>\u00a0a feature flag is enabled for a specific in-tree volume plugin, the\u00a0<code>kubernetes.io/pv-controller</code>\u00a0finalizer is replaced by the\u00a0<code>external-provisioner.volume.kubernetes.io/finalizer</code>\u00a0finalizer.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#reserving-a-persistentvolume","title":"Reserving a PersistentVolume","text":"<p>The control plane can\u00a0bind PersistentVolumeClaims to matching PersistentVolumes\u00a0in the cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.</p> <p>By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding between that specific PV and PVC. If the PersistentVolume exists and has not reserved PersistentVolumeClaims through its\u00a0<code>claimRef</code>\u00a0field, then the PersistentVolume and PersistentVolumeClaim will be bound.</p> <p>The binding happens regardless of some volume-matching criteria, including node affinity. The control plane still checks that\u00a0storage class, access modes, and requested storage size are valid.</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: foo-pvc\n  namespace: foo\nspec:\n  storageClassName: \"\" # Empty string must be explicitly set otherwise default StorageClass will be set\n  volumeName: foo-pv\n</code></pre> <p>This method does not guarantee any binding privileges to the PersistentVolume. If other PersistentVolumeClaims could use the PV that you specify, you first need to reserve that storage volume. Specify the relevant PersistentVolumeClaim in the\u00a0<code>claimRef</code>\u00a0field of the PV so that other PVCs can not bind to it.</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: foo-pv\nspec:\n  storageClassName: \"\"\n  claimRef:\n    name: foo-pvc\n    namespace: foo\n  ...\n</code></pre> <p>This is useful if you want to consume PersistentVolumes that have their\u00a0<code>persistentVolumeReclaimPolicy</code>\u00a0set to\u00a0<code>Retain</code>, including cases where you are reusing an existing PV.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#expanding-persistent-volumes-claims","title":"Expanding Persistent Volumes Claims","text":"<p>FEATURE STATE: <code>Kubernetes v1.24 [stable]</code></p> <p>Support for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand the following types of volumes:</p> <ul> <li> <p>azureFile (deprecated)</p> </li> <li> <p>csi</p> </li> <li> <p>flexVolume (deprecated)</p> </li> <li> <p>rbd (deprecated)</p> </li> <li> <p>portworxVolume (deprecated)</p> </li> </ul> <p>You can only expand a PVC if its storage class\u2019s\u00a0<code>allowVolumeExpansion</code>\u00a0field is set to true.</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: example-vol-default\nprovisioner: vendor-name.example/magicstorage\nparameters:\n  resturl: \"http://192.168.10.100:8080\"\n  restuser: \"\"\n  secretNamespace: \"\"\n  secretName: \"\"\nallowVolumeExpansion: true\n</code></pre> <p>To request a larger volume for a PVC, edit the PVC object and specify a larger size. This triggers expansion of the volume that backs the underlying PersistentVolume. A new PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.</p> <p>Warning:\u00a0Directly editing the size of a PersistentVolume can prevent an automatic resize of that volume. If you edit the capacity of a PersistentVolume, and then edit the\u00a0<code>.spec</code>\u00a0of a matching PersistentVolumeClaim to make the size of the PersistentVolumeClaim match the PersistentVolume, then no storage resize happens. The Kubernetes control plane will see that the desired state of both resources matches, conclude that the backing volume size has been manually increased and that no resize is necessary.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#csi-volume-expansion","title":"CSI Volume expansion","text":"<p>FEATURE STATE: <code>Kubernetes v1.24 [stable]</code></p> <p>Support for expanding CSI volumes is enabled by default but it also requires a specific CSI driver to support volume expansion. Refer to documentation of the specific CSI driver for more information.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#resizing-a-volume-containing-a-file-system","title":"Resizing a volume containing a file system","text":"<p>You can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.</p> <p>When a volume contains a file system, the file system is only resized when a new Pod is using the PersistentVolumeClaim in\u00a0<code>ReadWrite</code>\u00a0mode. File system expansion is either done when a Pod is starting up or when a Pod is running and the underlying file system supports online expansion.</p> <p>FlexVolumes (deprecated since Kubernetes v1.23) allows resizing if the driver is configured with the\u00a0<code>RequiresFSResize</code>\u00a0capability to\u00a0<code>true</code>. The FlexVolume can be resized on Pod restart.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#resizing-an-in-use-persistentvolumeclaim","title":"Resizing an in-use PersistentVolumeClaim","text":"<p>FEATURE STATE: <code>Kubernetes v1.24 [stable]</code></p> <p>In this case, you don\u2019t need to delete and recreate a Pod or deployment that is using an existing PVC. Any in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded. This feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that uses the PVC before the expansion can be completed.</p> <p>Similar to other volume types \u2013 FlexVolume volumes can also be expanded when in use by a Pod.</p> <p>Note:\u00a0FlexVolume resize is possible only when the underlying driver supports resize.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#recovering-from-failure-when-expanding-volumes","title":"Recovering from Failure when Expanding Volumes","text":"<p>If a user specifies a new size that is too big to be satisfied by an underlying storage system, the expansion of PVC will be continuously retried until the user or cluster administrator takes some action. This can be undesirable and hence Kubernetes provides the following methods of recovering from such failures.</p> <ul> <li> <p>Manually with Cluster Administrator access</p> </li> <li> <p>By requesting expansion to smaller size</p> </li> </ul> <p>If expanding underlying storage fails, the cluster administrator can manually recover the Persistent Volume Claim (PVC) state and cancel the resize requests. Otherwise, the resize requests are continuously retried by the controller without administrator intervention.</p> <ol> <li> <p>Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC) with\u00a0<code>Retain</code>\u00a0reclaim policy.</p> </li> <li> <p>Delete the PVC. Since PV has\u00a0<code>Retain</code>\u00a0reclaim policy \u2013 we will not lose any data when we recreate the PVC.</p> </li> <li> <p>Delete the\u00a0<code>claimRef</code>\u00a0entry from PV specs, so that new PVC can bind to it. This should make the PV\u00a0<code>Available</code>.</p> </li> <li> <p>Re-create the PVC with a smaller size than PV and set\u00a0<code>volumeName</code>\u00a0field of the PVC to the name of the PV. This should bind new PVC to existing PV.</p> </li> <li> <p>Don\u2019t forget to restore the reclaim policy of the PV.</p> </li> </ol>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#types-of-persistent-volumes","title":"Types of Persistent Volumes","text":"<p>PersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:</p> <ul> <li> <p><code>csi</code>\u00a0\u2013 Container Storage Interface (CSI)</p> </li> <li> <p><code>fc</code>\u00a0\u2013 Fibre Channel (FC) storage</p> </li> <li> <p><code>hostPath</code>\u00a0\u2013 HostPath volume (for single node testing only; WILL NOT WORK in a multi-node cluster; consider using\u00a0<code>local</code>\u00a0volume instead)</p> </li> <li> <p><code>iscsi</code>\u00a0\u2013 iSCSI (SCSI over IP) storage</p> </li> <li> <p><code>local</code>\u00a0\u2013 local storage devices mounted on nodes.</p> </li> <li> <p><code>nfs</code>\u00a0\u2013 Network File System (NFS) storage</p> </li> </ul> <p>The following types of PersistentVolume are deprecated. This means that support is still available but will be removed in a future Kubernetes release.</p> <ul> <li> <p><code>azureFile</code>\u00a0\u2013 Azure File (deprecated\u00a0in v1.21)</p> </li> <li> <p><code>flexVolume</code>\u00a0\u2013 FlexVolume (deprecated\u00a0in v1.23)</p> </li> <li> <p><code>portworxVolume</code>\u00a0\u2013 Portworx volume (deprecated\u00a0in v1.25)</p> </li> <li> <p><code>vsphereVolume</code>\u00a0\u2013 vSphere VMDK volume (deprecated\u00a0in v1.19)</p> </li> <li> <p><code>cephfs</code>\u00a0\u2013 CephFS volume (deprecated\u00a0in v1.28)</p> </li> <li> <p><code>rbd</code>\u00a0\u2013 Rados Block Device (RBD) volume (deprecated\u00a0in v1.28)</p> </li> </ul> <p>Older versions of Kubernetes also supported the following in-tree PersistentVolume types:</p> <ul> <li> <p><code>awsElasticBlockStore</code>\u00a0\u2013 AWS Elastic Block Store (EBS) (not available\u00a0in v1.27)</p> </li> <li> <p><code>azureDisk</code>\u00a0\u2013 Azure Disk (not available\u00a0in v1.27)</p> </li> <li> <p><code>cinder</code>\u00a0\u2013 Cinder (OpenStack block storage) (not available\u00a0in v1.26)</p> </li> <li> <p><code>photonPersistentDisk</code>\u00a0\u2013 Photon controller persistent disk. (not available\u00a0starting v1.15)</p> </li> <li> <p><code>scaleIO</code>\u00a0\u2013 ScaleIO volume. (not available\u00a0starting v1.21)</p> </li> <li> <p><code>flocker</code>\u00a0\u2013 Flocker storage. (not available\u00a0starting v1.25)</p> </li> <li> <p><code>quobyte</code>\u00a0\u2013 Quobyte volume. (not available\u00a0starting v1.25)</p> </li> <li> <p><code>storageos</code>\u00a0\u2013 StorageOS volume. (not available\u00a0starting v1.25)</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#persistent-volumes","title":"Persistent Volumes","text":"<p>Each PV contains a spec and status, which is the specification and status of the volume. The name of a PersistentVolume object must be a valid\u00a0DNS subdomain name.</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv0003\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Recycle\n  storageClassName: slow\n  mountOptions:\n    - hard\n    - nfsvers=4.1\n  nfs:\n    path: /tmp\n    server: 172.17.0.2\n</code></pre> <p>Note:\u00a0Helper programs relating to the volume type may be required for the consumption of a PersistentVolume within a cluster. In this example, the PersistentVolume is of type NFS and the helper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#capacity","title":"Capacity","text":"<p>Generally, a PV will have a specific storage capacity. This is set using the PV\u2019s\u00a0<code>capacity</code>\u00a0attribute which is a\u00a0Quantity\u00a0value.</p> <p>Currently, storage size is the only resource that can be set or requested. Future attributes may include IOPS, throughput, etc.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#volume-mode","title":"Volume Mode","text":"<p>FEATURE STATE: <code>Kubernetes v1.18 [stable]</code></p> <p>Kubernetes supports two\u00a0<code>volumeModes</code>\u00a0of PersistentVolumes:\u00a0<code>Filesystem</code>\u00a0and\u00a0<code>Block</code>.</p> <p><code>volumeMode</code>\u00a0is an optional API parameter.\u00a0<code>Filesystem</code>\u00a0is the default mode used when\u00a0<code>volumeMode</code>\u00a0the parameter is omitted.</p> <p>A volume with\u00a0<code>volumeMode: Filesystem</code>\u00a0is\u00a0mounted\u00a0into Pods in a directory. If the volume is backed by a block device and the device is empty, Kubernetes creates a filesystem on the device before mounting it for the first time.</p> <p>You can set the value of\u00a0<code>volumeMode</code>\u00a0to\u00a0<code>Block</code>\u00a0to use a volume as a raw block device. Such volume is presented into a Pod as a block device, without any filesystem on it. This mode is useful to provide a Pod the fastest possible way to access a volume, without any filesystem layer between the Pod and the volume. On the other hand, the application running in the Pod must know how to handle a raw block device. See\u00a0Raw Block Volume Support\u00a0for an example on how to use a volume with\u00a0<code>volumeMode: Block</code>\u00a0in a Pod.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#access-modes","title":"Access Modes","text":"<p>A PersistentVolume can be mounted on a host in any way supported by the resource provider. As shown in the table below, providers will have different capabilities and each PV\u2019s access modes are set to the specific modes supported by that particular volume. For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV\u2019s capabilities.</p> <p>The access modes are:<code>ReadWriteOnce</code>the volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access the volume when the pods are running on the same node. For single pod access, please see ReadWriteOncePod.<code>ReadOnlyMany</code>the volume can be mounted as read-only by many nodes.<code>ReadWriteMany</code>the volume can be mounted as read-write by many nodes.<code>ReadWriteOncePod</code></p> <p>FEATURE STATE: <code>Kubernetes v1.29 [stable]</code>the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it.</p> <p>Note:</p> <p>The\u00a0<code>ReadWriteOncePod</code>\u00a0access mode is only supported for\u00a0CSI\u00a0volumes and Kubernetes version 1.22+. To use this feature you will need to update the following\u00a0CSI sidecars\u00a0to these versions or greater:</p> <ul> <li> <p>csi-provisioner:v3.0.0+</p> </li> <li> <p>csi-attacher:v3.3.0+</p> </li> <li> <p>csi-resizer:v1.3.0+</p> </li> </ul> <p>In the CLI, the access modes are abbreviated to:</p> <ul> <li> <p>RWO \u2013 ReadWriteOnce</p> </li> <li> <p>ROX \u2013 ReadOnlyMany</p> </li> <li> <p>RWX \u2013 ReadWriteMany</p> </li> <li> <p>RWOP \u2013 ReadWriteOncePod</p> </li> </ul> <p>Note:\u00a0Kubernetes uses volume access modes to match PersistentVolumeClaims and PersistentVolumes. In some cases, the volume access modes also constrain where the PersistentVolume can be mounted. Volume access modes do\u00a0not\u00a0enforce write protection once the storage has been mounted. Even if the access modes are specified as ReadWriteOnce, ReadOnlyMany, or ReadWriteMany, they don\u2019t set any constraints on the volume. For example, even if a PersistentVolume is created as ReadOnlyMany, it is no guarantee that it will be read-only. If the access modes are specified as ReadWriteOncePod, the volume is constrained and can be mounted on only a single Pod.</p> <p>Important!\u00a0A volume can only be mounted using one access mode at a time, even if it supports many.</p> Volume Plugin ReadWriteOnce ReadOnlyMany ReadWriteMany ReadWriteOncePod AzureFile \u2713 \u2713 \u2713 - CephFS \u2713 \u2713 \u2713 - CSI depends on the driver depends on the driver depends on the driver depends on the driver FC \u2713 \u2713 - - FlexVolume \u2713 \u2713 depends on the driver - HostPath \u2713 - - - iSCSI \u2713 \u2713 - - NFS \u2713 \u2713 \u2713 - RBD \u2713 \u2713 - - VsphereVolume \u2713 - - (works when Pods are collocated) - PortworxVolume \u2713 - \u2713 -"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#class","title":"Class","text":"<p>A PV can have a class, which is specified by setting the\u00a0<code>storageClassName</code>\u00a0attribute to the name of a\u00a0StorageClass. A PV of a particular class can only be bound to PVCs requesting that class. A PV with no\u00a0<code>storageClassName</code>\u00a0has no class and can only be bound to PVCs that request no particular class.</p> <p>In the past, the annotation\u00a0<code>volume.beta.kubernetes.io/storage-class</code>\u00a0was used instead of the\u00a0<code>storageClassName</code>\u00a0attribute. This annotation is still working; however, it will become fully deprecated in a future Kubernetes release.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#reclaim-policy","title":"Reclaim Policy","text":"<p>Current reclaim policies are:</p> <ul> <li> <p>Retain \u2014 manual reclamation</p> </li> <li> <p>Recycle \u2014 basic scrub (<code>rm -rf /thevolume/*</code>)</p> </li> <li> <p>Delete \u2014 delete the volume</p> </li> </ul> <p>For Kubernetes 1.29, only\u00a0<code>nfs</code>\u00a0and\u00a0<code>hostPath</code>\u00a0volume types support recycling.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#mount-options","title":"Mount Options","text":"<p>A Kubernetes administrator can specify additional mount options for when a Persistent Volume is mounted on a node.</p> <p>Note:\u00a0Not all Persistent Volume types support mount options.</p> <p>The following volume types support mount options:</p> <ul> <li> <p><code>azureFile</code></p> </li> <li> <p><code>cephfs</code>\u00a0(deprecated\u00a0in v1.28)</p> </li> <li> <p><code>cinder</code>\u00a0(deprecated\u00a0in v1.18)</p> </li> <li> <p><code>iscsi</code></p> </li> <li> <p><code>nfs</code></p> </li> <li> <p><code>rbd</code>\u00a0(deprecated\u00a0in v1.28)</p> </li> <li> <p><code>vsphereVolume</code></p> </li> </ul> <p>Mount options are not validated. If a mount option is invalid, the mount fails.</p> <p>In the past, the annotation\u00a0<code>volume.beta.kubernetes.io/mount-options</code>\u00a0was used instead of the\u00a0<code>mountOptions</code>\u00a0attribute. This annotation is still working; however, it will become fully deprecated in a future Kubernetes release.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#node-affinity","title":"Node Affinity","text":"<p>Note:\u00a0For most volume types, you do not need to set this field. You need to explicitly set this for\u00a0local\u00a0volumes.</p> <p>A PV can specify node affinity to define constraints that limit what nodes this volume can be accessed from. Pods that use a PV will only be scheduled to nodes that are selected by the node affinity. To specify node affinity, set\u00a0<code>nodeAffinity</code>\u00a0in the\u00a0<code>.spec</code>\u00a0of a PV. The\u00a0PersistentVolume\u00a0API reference has more details on this field.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#phase","title":"Phase","text":"<p>A PersistentVolume will be in one of the following phases:<code>Available</code>a free resource that is not yet bound to a claim<code>Bound</code>the volume is bound to a claim<code>Released</code>the claim has been deleted, but the associated storage resource is not yet reclaimed by the cluster<code>Failed</code>the volume has failed its (automated) reclamation</p> <p>You can see the name of the PVC bound to the PV using\u00a0<code>kubectl describe persistentvolume &lt;name&gt;</code>.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#phase-transition-timestamp","title":"Phase transition timestamp","text":"<p>FEATURE STATE: <code>Kubernetes v1.29 [beta]</code></p> <p>The\u00a0<code>.status</code>\u00a0field for a PersistentVolume can include an alpha\u00a0<code>lastPhaseTransitionTime</code>\u00a0field. This field records the timestamp of when the volume last transitioned its phase. For newly created volumes the phase is set to\u00a0<code>Pending</code>\u00a0and\u00a0<code>lastPhaseTransitionTime</code>\u00a0is set to the current time.</p> <p>Note:\u00a0You need to enable the\u00a0<code>PersistentVolumeLastPhaseTransitionTime</code> feature gate\u00a0to use or see the\u00a0<code>lastPhaseTransitionTime</code>\u00a0field.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#persistentvolumeclaims","title":"PersistentVolumeClaims","text":"<p>Each PVC contains a spec and status, which is the specification and status of the claim. The name of a PersistentVolumeClaim object must be a valid\u00a0DNS subdomain name.</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: myclaim\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 8Gi\n  storageClassName: slow\n  selector:\n    matchLabels:\n      release: \"stable\"\n    matchExpressions:\n      - {key: environment, operator: In, values: [dev]}\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#access-modes_1","title":"Access Modes","text":"<p>Claims use\u00a0the same conventions as volumes\u00a0when requesting storage with specific access modes.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#volume-modes","title":"Volume Modes","text":"<p>Claims use\u00a0the same convention as volumes\u00a0to indicate the consumption of the volume as either a filesystem or block device.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#resources","title":"Resources","text":"<p>Claims, like Pods, can request specific quantities of a resource. In this case, the request is for storage. The same\u00a0resource model\u00a0applies to both volumes and claims.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#selector","title":"Selector","text":"<p>Claims can specify a\u00a0label selector\u00a0to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim. The selector can consist of two fields:</p> <ul> <li> <p><code>matchLabels</code>\u00a0\u2013 the volume must have a label with this value</p> </li> <li> <p><code>matchExpressions</code>\u00a0\u2013 a list of requirements made by specifying key, list of values, and operator that relates the key and values. Valid operators include In, NotIn, Exists, and DoesNotExist.</p> </li> </ul> <p>All of the requirements, from both\u00a0<code>matchLabels</code>\u00a0and\u00a0<code>matchExpressions</code>, are ANDed together \u2013 they must all be satisfied in order to match.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#class_1","title":"Class","text":"<p>A claim can request a particular class by specifying the name of a\u00a0StorageClass\u00a0using the attribute\u00a0<code>storageClassName</code>. Only PVs of the requested class, ones with the same\u00a0<code>storageClassName</code>\u00a0as the PVC, can be bound to the PVC.</p> <p>PVCs don\u2019t necessarily have to request a class. A PVC with its\u00a0<code>storageClassName</code>\u00a0set equal to\u00a0<code>\"\"</code>\u00a0is always interpreted to be requesting a PV with no class, so it can only be bound to PVs with no class (no annotation or one set equal to\u00a0<code>\"\"</code>). A PVC with no\u00a0<code>storageClassName</code>\u00a0is not quite the same and is treated differently by the cluster, depending on whether the\u00a0<code>DefaultStorageClass</code>\u00a0admission plugin\u00a0is turned on.</p> <ul> <li> <p>If the admission plugin is turned on, the administrator may specify a default StorageClass. All PVCs that have no\u00a0<code>storageClassName</code>\u00a0can be bound only to PVs of that default. Specifying a default StorageClass is done by setting the annotation\u00a0<code>storageclass.kubernetes.io/is-default-class</code>\u00a0equal to\u00a0<code>true</code>\u00a0in a StorageClass object. If the administrator does not specify a default, the cluster responds to PVC creation as if the admission plugin were turned off. If more than one default StorageClass is specified, the newest default is used when the PVC is dynamically provisioned.</p> </li> <li> <p>If the admission plugin is turned off, there is no notion of a default StorageClass. All PVCs that have\u00a0<code>storageClassName</code>\u00a0set to\u00a0<code>\"\"</code>\u00a0can be bound only to PVs that have\u00a0<code>storageClassName</code>\u00a0also set to\u00a0<code>\"\"</code>. However, PVCs with missing\u00a0<code>storageClassName</code>\u00a0can be updated later once default StorageClass becomes available. If the PVC gets updated it will no longer bind to PVs that have\u00a0<code>storageClassName</code>\u00a0also set to\u00a0<code>\"\"</code>.</p> </li> </ul> <p>See\u00a0retroactive default StorageClass assignment\u00a0for more details.</p> <p>Depending on the installation method, a default StorageClass may be deployed to a Kubernetes cluster by addon manager during installation.</p> <p>When a PVC specifies a\u00a0<code>selector</code>\u00a0in addition to requesting a StorageClass, the requirements are ANDed together: only a PV of the requested class and with the requested labels may be bound to the PVC.</p> <p>Note:\u00a0Currently, a PVC with a non-empty\u00a0<code>selector</code>\u00a0can\u2019t have a PV dynamically provisioned for it.</p> <p>In the past, the annotation\u00a0<code>volume.beta.kubernetes.io/storage-class</code>\u00a0was used instead of\u00a0<code>storageClassName</code>\u00a0attribute. This annotation is still working; however, it won\u2019t be supported in a future Kubernetes release.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#retroactive-default-storageclass-assignment","title":"Retroactive default StorageClass assignment","text":"<p>FEATURE STATE: <code>Kubernetes v1.28 [stable]</code></p> <p>You can create a PersistentVolumeClaim without specifying a\u00a0<code>storageClassName</code>\u00a0for the new PVC, and you can do so even when no default StorageClass exists in your cluster. In this case, the new PVC creates as you defined it, and the\u00a0<code>storageClassName</code>\u00a0of that, PVC remains unset until default becomes available.</p> <p>When a default StorageClass becomes available, the control plane identifies any existing PVCs without\u00a0<code>storageClassName</code>. For the PVCs that either have an empty value for\u00a0<code>storageClassName</code>\u00a0or do not have this key, the control plane then updates those PVCs to set\u00a0<code>storageClassName</code>\u00a0to match the new default StorageClass. If you have an existing PVC where the\u00a0<code>storageClassName</code>\u00a0is\u00a0<code>\"\"</code>, and you configure a default StorageClass, then this PVC will not get updated.</p> <p>In order to keep binding to PVs with\u00a0<code>storageClassName</code>\u00a0set to\u00a0<code>\"\"</code>\u00a0(while a default StorageClass is present), you need to set the\u00a0<code>storageClassName</code>\u00a0of the associated PVC to\u00a0<code>\"\"</code>.</p> <p>This behavior helps administrators change the default StorageClass by removing the old one first and then creating or setting another one. This brief window while there is no default causes PVCs without\u00a0<code>storageClassName</code>\u00a0created at that time to not have any default, but due to the retroactive default StorageClass assignment, this way of changing defaults is safe.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#claims-as-volumes","title":"Claims As Volumes","text":"<p>Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod\u2019s namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: myfrontend\n      image: nginx\n      volumeMounts:\n      - mountPath: \"/var/www/html\"\n        name: mypd\n  volumes:\n    - name: mypd\n      persistentVolumeClaim:\n        claimName: myclaim\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#a-note-on-namespaces","title":"A Note on Namespaces","text":"<p>PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with \u201cMany\u201d modes (<code>ROX</code>,\u00a0<code>RWX</code>) is only possible within one namespace.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#persistentvolumes-typed-hostpath","title":"PersistentVolumes typed\u00a0<code>hostPath</code>","text":"<p>A\u00a0<code>hostPath</code>\u00a0PersistentVolume uses a file or directory on the Node to emulate network-attached storage. See\u00a0an example of\u00a0<code>hostPath</code>\u00a0typed volume.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#raw-block-volume-support","title":"Raw Block Volume Support","text":"<p>FEATURE STATE: <code>Kubernetes v1.18 [stable]</code></p> <p>The following volume plugins support raw block volumes, including dynamic provisioning where applicable:</p> <ul> <li> <p>CSI</p> </li> <li> <p>FC (Fibre Channel)</p> </li> <li> <p>iSCSI</p> </li> <li> <p>Local volume</p> </li> <li> <p>OpenStack Cinder</p> </li> <li> <p>RBD (deprecated)</p> </li> <li> <p>RBD (Ceph Block Device; deprecated)</p> </li> <li> <p>VsphereVolume</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#persistentvolume-using-a-raw-block-volume","title":"PersistentVolume using a Raw Block Volume","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: block-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Block\n  persistentVolumeReclaimPolicy: Retain\n  fc:\n    targetWWNs: [\"50060e801049cfd1\"]\n    lun: 0\n    readOnly: false\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#persistentvolumeclaim-requesting-a-raw-block-volume","title":"PersistentVolumeClaim requesting a Raw Block Volume","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: block-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Block\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#pod-specification-adding-raw-block-device-path-in-the-container","title":"Pod specification adding Raw Block Device path in the container","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-block-volume\nspec:\n  containers:\n    - name: fc-container\n      image: fedora:26\n      command: [\"/bin/sh\", \"-c\"]\n      args: [ \"tail -f /dev/null\" ]\n      volumeDevices:\n        - name: data\n          devicePath: /dev/xvda\n  volumes:\n    - name: data\n      persistentVolumeClaim:\n        claimName: block-pvc\n</code></pre> <p>Note:\u00a0When adding a raw block device for a Pod, you specify the device path in the container instead of a mount path.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#binding-block-volumes","title":"Binding Block Volumes","text":"<p>If a user requests a raw block volume by indicating this using the\u00a0<code>volumeMode</code>\u00a0field in the PersistentVolumeClaim spec, the binding rules differ slightly from previous releases that didn\u2019t consider this mode as part of the spec. Listed is a table of possible combinations the user and admin might specify for requesting a raw block device. The table indicates if the volume will be bound or not given the combinations: Volume binding matrix for statically provisioned volumes:</p> PV volumeMode PVC volumeMode Result unspecified unspecified BIND unspecified Block NO BIND unspecified Filesystem BIND Block unspecified NO BIND Block Block BIND Block Filesystem NO BIND Filesystem Filesystem BIND Filesystem Block NO BIND Filesystem unspecified BIND <p>Note:\u00a0Only statically provisioned volumes are supported for alpha release. Administrators should take care to consider these values when working with raw block devices.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#volume-snapshot-and-restore-volume-from-snapshot-support","title":"Volume Snapshot and Restore Volume from Snapshot Support","text":"<p>FEATURE STATE: <code>Kubernetes v1.20 [stable]</code></p> <p>Volume snapshots only support the out-of-tree CSI volume plugins. For details, see\u00a0Volume Snapshots. In-tree volume plugins are deprecated. You can read about the deprecated volume plugins in the\u00a0Volume Plugin FAQ.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#create-a-persistentvolumeclaim-from-a-volume-snapshot","title":"Create a PersistentVolumeClaim from a Volume Snapshot","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: restore-pvc\nspec:\n  storageClassName: csi-hostpath-sc\n  dataSource:\n    name: new-snapshot-test\n    kind: VolumeSnapshot\n    apiGroup: snapshot.storage.k8s.io\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#volume-cloning","title":"Volume Cloning","text":"<p>Volume Cloning\u00a0only available for CSI volume plugins.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#create-persistentvolumeclaim-from-an-existing-pvc","title":"Create PersistentVolumeClaim from an existing PVC","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cloned-pvc\nspec:\n  storageClassName: my-csi-plugin\n  dataSource:\n    name: existing-src-pvc-name\n    kind: PersistentVolumeClaim\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#volume-populators-and-data-sources","title":"Volume populators and data sources","text":"<p>FEATURE STATE: <code>Kubernetes v1.24 [beta]</code></p> <p>Kubernetes supports custom volume populators. To use custom volume populators, you must enable the\u00a0<code>AnyVolumeDataSource</code> feature gate\u00a0for the kube-apiserver and kube-controller-manager.</p> <p>Volume populators take advantage of a PVC spec field called\u00a0<code>dataSourceRef</code>. Unlike the\u00a0<code>dataSource</code>\u00a0field, which can only contain either a reference to another PersistentVolumeClaim or to a VolumeSnapshot, the\u00a0<code>dataSourceRef</code>\u00a0field can contain a reference to any object in the same namespace, except for core objects other than PVCs. For clusters that have the feature gate enabled, use of the\u00a0<code>dataSourceRef</code>\u00a0is preferred over\u00a0<code>dataSource</code>.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#cross-namespace-data-sources","title":"Cross namespace data sources","text":"<p>FEATURE STATE: <code>Kubernetes v1.26 [alpha]</code></p> <p>Kubernetes supports cross-namespace volume data sources. To use cross-namespace volume data sources, you must enable the\u00a0<code>AnyVolumeDataSource</code>\u00a0and\u00a0<code>CrossNamespaceVolumeDataSource</code> feature gates\u00a0for the kube-apiserver and kube-controller-manager. Also, you must enable the\u00a0<code>CrossNamespaceVolumeDataSource</code>\u00a0feature gate for the csi-provisioner.</p> <p>Enabling the\u00a0<code>CrossNamespaceVolumeDataSource</code>\u00a0feature gate allows you to specify a namespace in the dataSourceRef field.</p> <p>Note:\u00a0When you specify a namespace for a volume data source, Kubernetes checks for a ReferenceGrant in the other namespace before accepting the reference. ReferenceGrant is part of the\u00a0<code>gateway.networking.k8s.io</code>\u00a0extension APIs. See\u00a0ReferenceGrant\u00a0in the Gateway API documentation for details. This means that you must extend your Kubernetes cluster with at least ReferenceGrant from the Gateway API before you can use this mechanism.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#data-source-references","title":"Data source references","text":"<p>The\u00a0<code>dataSourceRef</code>\u00a0field behaves almost the same as the\u00a0<code>dataSource</code>\u00a0field. If one is specified while the other is not, the API server will give both fields the same value. Neither field can be changed after creation, and attempting to specify different values for the two fields will result in a validation error. Therefore the two fields will always have the same contents.</p> <p>There are two differences between the\u00a0<code>dataSourceRef</code>\u00a0field and the\u00a0<code>dataSource</code>\u00a0field that users should be aware of:</p> <ul> <li> <p>The\u00a0<code>dataSource</code>\u00a0field ignores invalid values (as if the field was blank) while the\u00a0<code>dataSourceRef</code>\u00a0field never ignores values and will cause an error if an invalid value is used. Invalid values are any core object (objects with no apiGroup) except for PVCs.</p> </li> <li> <p>The\u00a0<code>dataSourceRef</code>\u00a0field may contain different types of objects, while the\u00a0<code>dataSource</code>\u00a0field only allows PVCs and VolumeSnapshots.</p> </li> </ul> <p>When the\u00a0<code>CrossNamespaceVolumeDataSource</code>\u00a0feature is enabled, there are additional differences:</p> <ul> <li> <p>The\u00a0<code>dataSource</code>\u00a0field only allows local objects, while the\u00a0<code>dataSourceRef</code>\u00a0field allows objects in any namespaces.</p> </li> <li> <p>When namespace is specified,\u00a0<code>dataSource</code>\u00a0and\u00a0<code>dataSourceRef</code>\u00a0are not synced.</p> </li> </ul> <p>Users should always use\u00a0<code>dataSourceRef</code>\u00a0on clusters that have the feature gate enabled, and fall back to\u00a0<code>dataSource</code>\u00a0on clusters that do not. It is not necessary to look at both fields under any circumstance. The duplicated values with slightly different semantics exist only for backwards compatibility. In particular, a mixture of older and newer controllers are able to interoperate because the fields are the same.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#using-volume-populators","title":"Using volume populators","text":"<p>Volume populators are\u00a0controllers\u00a0that can create non-empty volumes, where the contents of the volume are determined by a Custom Resource. Users create a populated volume by referring to a Custom Resource using the\u00a0<code>dataSourceRef</code>\u00a0field:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: populated-pvc\nspec:\n  dataSourceRef:\n    name: example-name\n    kind: ExampleDataSource\n    apiGroup: example.storage.k8s.io\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre> <p>Because volume populators are external components, attempts to create a PVC that uses one can fail if not all the correct components are installed. External controllers should generate events on the PVC to provide feedback on the status of the creation, including warnings if the PVC cannot be created due to some missing component.</p> <p>You can install the alpha\u00a0volume data source validator\u00a0controller into your cluster. That controller generates warning Events on a PVC in the case that no populator is registered to handle that kind of data source. When a suitable populator is installed for a PVC, it\u2019s the responsibility of that populator controller to report Events that relate to volume creation and issues during the process.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#using-a-cross-namespace-volume-data-source","title":"Using a cross-namespace volume data source","text":"<p>FEATURE STATE: <code>Kubernetes v1.26 [alpha]</code></p> <p>Create a ReferenceGrant to allow the namespace owner to accept the reference. You define a populated volume by specifying a cross-namespace volume data source using the\u00a0<code>dataSourceRef</code>\u00a0field. You must already have a valid ReferenceGrant in the source namespace:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1beta1\nkind: ReferenceGrant\nmetadata:\n  name: allow-ns1-pvc\n  namespace: default\nspec:\n  from:\n  - group: \"\"\n    kind: PersistentVolumeClaim\n    namespace: ns1\n  to:\n  - group: snapshot.storage.k8s.io\n    kind: VolumeSnapshot\n    name: new-snapshot-demo\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: foo-pvc\n  namespace: ns1\nspec:\n  storageClassName: example\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  dataSourceRef:\n    apiGroup: snapshot.storage.k8s.io\n    kind: VolumeSnapshot\n    name: new-snapshot-demo\n    namespace: default\n  volumeMode: Filesystem\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Persistent_Volumes/#writing-portable-configuration","title":"Writing Portable Configuration","text":"<p>If you\u2019re writing configuration templates or examples that run on a wide range of clusters and need persistent storage, it is recommended that you use the following pattern:</p> <ul> <li> <p>Include PersistentVolumeClaim objects in your bundle of config (alongside Deployments, ConfigMaps, etc).</p> </li> <li> <p>Do not include PersistentVolume objects in the config, since the user instantiating the config may not have permission to create PersistentVolumes.</p> </li> <li> <p>Give the user the option of providing a storage class name when instantiating the template.</p> </li> <li> <p>If the user provides a storage class name, put that value into the\u00a0<code>persistentVolumeClaim.storageClassName</code>\u00a0field. This will cause the PVC to match the right storage class if the cluster has StorageClasses enabled by the admin.</p> </li> <li> <p>If the user does not provide a storage class name, leave the\u00a0<code>persistentVolumeClaim.storageClassName</code>\u00a0field as nil. This will cause a PV to be automatically provisioned for the user with the default StorageClass in the cluster. Many cluster environments have a default StorageClass installed, or administrators can create their own default StorageClass.</p> </li> <li> <p>In your tooling, watch for PVCs that are not getting bound after some time and surface this to the user, as this may indicate that the cluster has no dynamic storage support (in which case the user should create a matching PV) or the cluster has no storage system (in which case the user cannot deploy config requiring PVCs).</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/","title":"Kubernetes Storage Classes","text":""},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#storage-classes","title":"Storage Classes","text":"<p>This document describes the concept of a StorageClass in Kubernetes. Familiarity with\u00a0volumes\u00a0and\u00a0persistent volumes\u00a0is suggested.</p> <p>A StorageClass provides a way for administrators to describe the\u00a0classes\u00a0of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is unopinionated about what classes represent.</p> <p>The Kubernetes concept of a storage class is similar to \u201cprofiles\u201d in some other storage system designs.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#storageclass-objects","title":"StorageClass objects","text":"<p>Each StorageClass contains the fields\u00a0<code>provisioner</code>,\u00a0<code>parameters</code>, and\u00a0<code>reclaimPolicy</code>, which are used when a PersistentVolume belonging to the class needs to be dynamically provisioned to satisfy a PersistentVolumeClaim (PVC).</p> <p>The name of a StorageClass object is significant, and is how users can request a particular class. Administrators set the name and other parameters of a class when first creating StorageClass objects.</p> <p>As an administrator, you can specify a default StorageClass that applies to any PVCs that don\u2019t request a specific class. For more details, see the\u00a0PersistentVolumeClaim concept.</p> <p>Here\u2019s an example of a StorageClass:</p> <p>storage/storageclass-low-latency.yaml</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: low-latency\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"false\"\nprovisioner: csi-driver.example-vendor.example\nreclaimPolicy: Retain # default value is Delete\nallowVolumeExpansion: true\nmountOptions:\n  - discard # this might enable UNMAP / TRIM at the block storage layer\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  guaranteedReadWriteLatency: \"true\" # provider-specific\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#default-storageclass","title":"Default StorageClass","text":"<p>You can mark a StorageClass as the default for your cluster. For instructions on setting the default StorageClass, see\u00a0Change the default StorageClass.</p> <p>When a PVC does not specify a\u00a0<code>storageClassName</code>, the default StorageClass is used.</p> <p>If you set the\u00a0<code>storageclass.kubernetes.io/is-default-class</code>\u00a0annotation to true on more than one StorageClass in your cluster, and you then create a PersistentVolumeClaim with no\u00a0<code>storageClassName</code>\u00a0set, Kubernetes uses the most recently created default StorageClass.</p> <p>Note:\u00a0You should try to only have one StorageClass in your cluster that is marked as the default. The reason that Kubernetes allows you to have multiple default StorageClasses is to allow for seamless migration.</p> <p>You can create a PersistentVolumeClaim without specifying a\u00a0<code>storageClassName</code>\u00a0for the new PVC, and you can do so even when no default StorageClass exists in your cluster. In this case, the new PVC creates as you defined it, and the\u00a0<code>storageClassName</code>\u00a0of that PVC remains unset until a default becomes available.</p> <p>You can have a cluster without any default StorageClass. If you don\u2019t mark any StorageClass as default (and one hasn\u2019t been set for you by, for example, a cloud provider), then Kubernetes cannot apply that defaulting for PersistentVolumeClaims that need it.</p> <p>If or when a default StorageClass becomes available, the control plane identifies any existing PVCs without\u00a0<code>storageClassName</code>. For the PVCs that either have an empty value for\u00a0<code>storageClassName</code>\u00a0or do not have this key, the control plane then updates those PVCs to set\u00a0<code>storageClassName</code>\u00a0to match the new default StorageClass. If you have an existing PVC where the\u00a0<code>storageClassName</code>\u00a0is\u00a0<code>\"\"</code>, and you configure a default StorageClass, then this PVC will not get updated.</p> <p>In order to keep binding to PVs with\u00a0<code>storageClassName</code>\u00a0set to\u00a0<code>\"\"</code>\u00a0(while a default StorageClass is present), you need to set the\u00a0<code>storageClassName</code>\u00a0of the associated PVC to\u00a0<code>\"\"</code>.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#provisioner","title":"Provisioner","text":"<p>Each StorageClass has a provisioner that determines what volume plugin is used for provisioning PVs. This field must be specified.</p> Volume Plugin Internal Provisioner Config Example AzureFile \u2713 Azure File CephFS - - FC - - FlexVolume - - iSCSI - - Local - Local NFS - NFS PortworxVolume \u2713 Portworx Volume RBD - Ceph RBD VsphereVolume \u2713 vSphere <p>You are not restricted to specifying the \u201cinternal\u201d provisioners listed here (whose names are prefixed with \u201ckubernetes.io\u201d and shipped alongside Kubernetes). You can also run and specify external provisioners, which are independent programs that follow a\u00a0specification\u00a0defined by Kubernetes. Authors of external provisioners have full discretion over where their code lives, how the provisioner is shipped, how it needs to be run, what volume plugin it uses (including Flex), etc. The repository\u00a0kubernetes-sigs/sig-storage-lib-external-provisioner\u00a0houses a library for writing external provisioners that implements the bulk of the specification. Some external provisioners are listed under the repository\u00a0kubernetes-sigs/sig-storage-lib-external-provisioner.</p> <p>For example, NFS doesn\u2019t provide an internal provisioner, but an external provisioner can be used. There are also cases when 3rd party storage vendors provide their own external provisioner.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#reclaim-policy","title":"Reclaim policy","text":"<p>PersistentVolumes that are dynamically created by a StorageClass will have the\u00a0reclaim policy\u00a0specified in the\u00a0<code>reclaimPolicy</code>\u00a0field of the class, which can be either\u00a0<code>Delete</code>\u00a0or\u00a0<code>Retain</code>. If no\u00a0<code>reclaimPolicy</code>\u00a0is specified when a StorageClass object is created, it will default to\u00a0<code>Delete</code>.</p> <p>PersistentVolumes that are created manually and managed via a StorageClass will have whatever reclaim policy they were assigned at creation.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#volume-expansion","title":"Volume expansion","text":"<p>PersistentVolumes can be configured to be expandable. This allows you to resize the volume by editing the corresponding PVC object, requesting a new larger amount of storage.</p> <p>The following types of volumes support volume expansion, when the underlying StorageClass has the field\u00a0<code>allowVolumeExpansion</code>\u00a0set to true.</p> Volume type Required Kubernetes version for volume expansion AzureFile 1.11 CSI 1.24 FC 1.13 FlexVolume 1.11 PortworxVolume 1.11 rbd 1.11 <p>Note:\u00a0You can only use the volume expansion feature to grow a Volume, not to shrink it.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#mount-options","title":"Mount options","text":"<p>PersistentVolumes that are dynamically created by a StorageClass will have the mount options specified in the\u00a0<code>mountOptions</code>\u00a0field of the class.</p> <p>If the volume plugin does not support mount options but mount options are specified, provisioning will fail. Mount options are\u00a0not\u00a0validated on either the class or PV. If a mount option is invalid, the PV mount fails.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#volume-binding-mode","title":"Volume binding mode","text":"<p>The\u00a0<code>volumeBindingMode</code>\u00a0field controls when\u00a0volume binding and dynamic provisioning\u00a0should occur. When unset,\u00a0<code>Immediate</code>\u00a0mode is used by default.</p> <p>The\u00a0<code>Immediate</code>\u00a0mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created. For storage backends that are topology-constrained and not globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod\u2019s scheduling requirements. This may result in unschedulable Pods.</p> <p>A cluster administrator can address this issue by specifying the\u00a0<code>WaitForFirstConsumer</code>\u00a0mode which will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created. PersistentVolumes will be selected or provisioned conforming to the topology that is specified by the Pod\u2019s scheduling constraints. These include, but are not limited to,\u00a0resource requirements,\u00a0node selectors,\u00a0pod affinity and anti-affinity, and\u00a0taints and tolerations.</p> <p>The following plugins support\u00a0<code>WaitForFirstConsumer</code>\u00a0with dynamic provisioning:</p> <ul> <li>CSI volumes, provided that the specific CSI driver supports this</li> </ul> <p>The following plugins support\u00a0<code>WaitForFirstConsumer</code>\u00a0with pre-created PersistentVolume binding:</p> <ul> <li> <p>CSI volumes, provided that the specific CSI driver supports this</p> </li> <li> <p><code>local</code></p> </li> </ul> <p>Note:</p> <p>If you choose to use\u00a0<code>WaitForFirstConsumer</code>, do not use\u00a0<code>nodeName</code>\u00a0in the Pod spec to specify node affinity. If\u00a0<code>nodeName</code>\u00a0is used in this case, the scheduler will be bypassed and PVC will remain in\u00a0<code>pending</code>\u00a0state.</p> <p>Instead, you can use node selector for\u00a0<code>kubernetes.io/hostname</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod---\ntitle: \"Kubernetes Storage Classes\"\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#storage-classes_1","title":"Storage Classes","text":"<p>This document describes the concept of a StorageClass in Kubernetes. Familiarity with\u00a0volumes\u00a0and\u00a0persistent volumes\u00a0is suggested.</p> <p>A StorageClass provides a way for administrators to describe the\u00a0classes\u00a0of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is unopinionated about what classes represent.</p> <p>The Kubernetes concept of a storage class is similar to \u201cprofiles\u201d in some other storage system designs.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#storageclass-objects_1","title":"StorageClass objects","text":"<p>Each StorageClass contains the fields\u00a0<code>provisioner</code>,\u00a0<code>parameters</code>, and\u00a0<code>reclaimPolicy</code>, which are used when a PersistentVolume belonging to the class needs to be dynamically provisioned to satisfy a PersistentVolumeClaim (PVC).</p> <p>The name of a StorageClass object is significant, and is how users can request a particular class. Administrators set the name and other parameters of a class when first creating StorageClass objects.</p> <p>As an administrator, you can specify a default StorageClass that applies to any PVCs that don\u2019t request a specific class. For more details, see the\u00a0PersistentVolumeClaim concept.</p> <p>Here\u2019s an example of a StorageClass:</p> <p>storage/storageclass-low-latency.yaml</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: low-latency\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"false\"\nprovisioner: csi-driver.example-vendor.example\nreclaimPolicy: Retain # default value is Delete\nallowVolumeExpansion: true\nmountOptions:\n  - discard # this might enable UNMAP / TRIM at the block storage layer\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  guaranteedReadWriteLatency: \"true\" # provider-specific\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#default-storageclass_1","title":"Default StorageClass","text":"<p>You can mark a StorageClass as the default for your cluster. For instructions on setting the default StorageClass, see\u00a0Change the default StorageClass.</p> <p>When a PVC does not specify a\u00a0<code>storageClassName</code>, the default StorageClass is used.</p> <p>If you set the\u00a0<code>storageclass.kubernetes.io/is-default-class</code>\u00a0annotation to true on more than one StorageClass in your cluster, and you then create a PersistentVolumeClaim with no\u00a0<code>storageClassName</code>\u00a0set, Kubernetes uses the most recently created default StorageClass.</p> <p>Note:\u00a0You should try to only have one StorageClass in your cluster that is marked as the default. The reason that Kubernetes allows you to have multiple default StorageClasses is to allow for seamless migration.</p> <p>You can create a PersistentVolumeClaim without specifying a\u00a0<code>storageClassName</code>\u00a0for the new PVC, and you can do so even when no default StorageClass exists in your cluster. In this case, the new PVC creates as you defined it, and the\u00a0<code>storageClassName</code>\u00a0of that PVC remains unset until a default becomes available.</p> <p>You can have a cluster without any default StorageClass. If you don\u2019t mark any StorageClass as default (and one hasn\u2019t been set for you by, for example, a cloud provider), then Kubernetes cannot apply that defaulting for PersistentVolumeClaims that need it.</p> <p>If or when a default StorageClass becomes available, the control plane identifies any existing PVCs without\u00a0<code>storageClassName</code>. For the PVCs that either have an empty value for\u00a0<code>storageClassName</code>\u00a0or do not have this key, the control plane then updates those PVCs to set\u00a0<code>storageClassName</code>\u00a0to match the new default StorageClass. If you have an existing PVC where the\u00a0<code>storageClassName</code>\u00a0is\u00a0<code>\"\"</code>, and you configure a default StorageClass, then this PVC will not get updated.</p> <p>In order to keep binding to PVs with\u00a0<code>storageClassName</code>\u00a0set to\u00a0<code>\"\"</code>\u00a0(while a default StorageClass is present), you need to set the\u00a0<code>storageClassName</code>\u00a0of the associated PVC to\u00a0<code>\"\"</code>.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#provisioner_1","title":"Provisioner","text":"<p>Each StorageClass has a provisioner that determines what volume plugin is used for provisioning PVs. This field must be specified.</p> Volume Plugin Internal Provisioner Config Example AzureFile \u2713 Azure File CephFS - - FC - - FlexVolume - - iSCSI - - Local - Local NFS - NFS PortworxVolume \u2713 Portworx Volume RBD - Ceph RBD VsphereVolume \u2713 vSphere <p>You are not restricted to specifying the \u201cinternal\u201d provisioners listed here (whose names are prefixed with \u201ckubernetes.io\u201d and shipped alongside Kubernetes). You can also run and specify external provisioners, which are independent programs that follow a\u00a0specification\u00a0defined by Kubernetes. Authors of external provisioners have full discretion over where their code lives, how the provisioner is shipped, how it needs to be run, what volume plugin it uses (including Flex), etc. The repository\u00a0kubernetes-sigs/sig-storage-lib-external-provisioner\u00a0houses a library for writing external provisioners that implements the bulk of the specification. Some external provisioners are listed under the repository\u00a0kubernetes-sigs/sig-storage-lib-external-provisioner.</p> <p>For example, NFS doesn\u2019t provide an internal provisioner, but an external provisioner can be used. There are also cases when 3rd party storage vendors provide their own external provisioner.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#reclaim-policy_1","title":"Reclaim policy","text":"<p>PersistentVolumes that are dynamically created by a StorageClass will have the\u00a0reclaim policy\u00a0specified in the\u00a0<code>reclaimPolicy</code>\u00a0field of the class, which can be either\u00a0<code>Delete</code>\u00a0or\u00a0<code>Retain</code>. If no\u00a0<code>reclaimPolicy</code>\u00a0is specified when a StorageClass object is created, it will default to\u00a0<code>Delete</code>.</p> <p>PersistentVolumes that are created manually and managed via a StorageClass will have whatever reclaim policy they were assigned at creation.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#volume-expansion_1","title":"Volume expansion","text":"<p>PersistentVolumes can be configured to be expandable. This allows you to resize the volume by editing the corresponding PVC object, requesting a new larger amount of storage.</p> <p>The following types of volumes support volume expansion, when the underlying StorageClass has the field\u00a0<code>allowVolumeExpansion</code>\u00a0set to true.</p> Volume type Required Kubernetes version for volume expansion AzureFile 1.11 CSI 1.24 FC 1.13 FlexVolume 1.11 PortworxVolume 1.11 rbd 1.11 <p>Note:\u00a0You can only use the volume expansion feature to grow a Volume, not to shrink it.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#mount-options_1","title":"Mount options","text":"<p>PersistentVolumes that are dynamically created by a StorageClass will have the mount options specified in the\u00a0<code>mountOptions</code>\u00a0field of the class.</p> <p>If the volume plugin does not support mount options but mount options are specified, provisioning will fail. Mount options are\u00a0not\u00a0validated on either the class or PV. If a mount option is invalid, the PV mount fails.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#volume-binding-mode_1","title":"Volume binding mode","text":"<p>The\u00a0<code>volumeBindingMode</code>\u00a0field controls when\u00a0volume binding and dynamic provisioning\u00a0should occur. When unset,\u00a0<code>Immediate</code>\u00a0mode is used by default.</p> <p>The\u00a0<code>Immediate</code>\u00a0mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created. For storage backends that are topology-constrained and not globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod\u2019s scheduling requirements. This may result in unschedulable Pods.</p> <p>A cluster administrator can address this issue by specifying the\u00a0<code>WaitForFirstConsumer</code>\u00a0mode which will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created. PersistentVolumes will be selected or provisioned conforming to the topology that is specified by the Pod\u2019s scheduling constraints. These include, but are not limited to,\u00a0resource requirements,\u00a0node selectors,\u00a0pod affinity and anti-affinity, and\u00a0taints and tolerations.</p> <p>The following plugins support\u00a0<code>WaitForFirstConsumer</code>\u00a0with dynamic provisioning:</p> <ul> <li>CSI volumes, provided that the specific CSI driver supports this</li> </ul> <p>The following plugins support\u00a0<code>WaitForFirstConsumer</code>\u00a0with pre-created PersistentVolume binding:</p> <ul> <li> <p>CSI volumes, provided that the specific CSI driver supports this</p> </li> <li> <p><code>local</code></p> </li> </ul> <p>Note:</p> <p>If you choose to use\u00a0<code>WaitForFirstConsumer</code>, do not use\u00a0<code>nodeName</code>\u00a0in the Pod spec to specify node affinity. If\u00a0<code>nodeName</code>\u00a0is used in this case, the scheduler will be bypassed and PVC will remain in\u00a0<code>pending</code>\u00a0state.</p> <p>Instead, you can use node selector for\u00a0<code>kubernetes.io/hostname</code>:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: task-pv-pod\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: kube-01\n  volumes:\n    - name: task-pv-storage\n      persistentVolumeClaim:\n        claimName: task-pv-claim\n  containers:\n    - name: task-pv-container\n      image: nginx\n      ports:\n        - containerPort: 80\n          name: \"http-server\"\n      volumeMounts:\n        - mountPath: \"/usr/share/nginx/html\"\n          name: task-pv-storage\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#allowed-topologies","title":"Allowed topologies","text":"<p>When a cluster operator specifies the\u00a0<code>WaitForFirstConsumer</code>\u00a0volume binding mode, it is no longer necessary to restrict provisioning to specific topologies in most situations. However, if still required,\u00a0<code>allowedTopologies</code>\u00a0can be specified.</p> <p>This example demonstrates how to restrict the topology of provisioned volumes to specific zones and should be used as a replacement for the\u00a0<code>zone</code>\u00a0and\u00a0<code>zones</code>\u00a0parameters for the supported plugins.</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: standard\nprovisioner: kubernetes.io/example\nparameters:\n  type: pd-standard\nvolumeBindingMode: WaitForFirstConsumer\nallowedTopologies:\n- matchLabelExpressions:\n  - key: topology.kubernetes.io/zone\n    values:\n    - us-central-1a\n    - us-central-1b\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#parameters","title":"Parameters","text":"<p>StorageClasses have parameters that describe volumes belonging to the storage class. Different parameters may be accepted depending on the\u00a0<code>provisioner</code>. When a parameter is omitted, some default is used.</p> <p>There can be at most 512 parameters defined for a StorageClass. The total length of the parameters object including its keys and values cannot exceed 256 KiB.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#aws-ebs","title":"AWS EBS","text":"<p>Kubernetes 1.29 does not include a\u00a0<code>awsElasticBlockStore</code>\u00a0volume type.</p> <p>The AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the v1.27 release.</p> <p>The Kubernetes project suggests that you use the\u00a0AWS EBS\u00a0out-of-tree storage driver instead.</p> <p>Here is an example StorageClass for the AWS EBS CSI driver:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ebs-sc\nprovisioner: ebs.csi.aws.com\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  csi.storage.k8s.io/fstype: xfs\n  type: io1\n  iopsPerGB: \"50\"\n  encrypted: \"true\"\nallowedTopologies:\n- matchLabelExpressions:\n  - key: topology.ebs.csi.aws.com/zone\n    values:\n    - us-east-2c\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#nfs","title":"NFS","text":"<p>To configure NFS storage, you can use the in-tree driver or the\u00a0NFS CSI driver for Kubernetes\u00a0(recommended).</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: example-nfs\nprovisioner: example.com/external-nfs\nparameters:\n  server: nfs-server.example.com\n  path: /share\n  readOnly: \"false\"\n</code></pre> <ul> <li> <p><code>server</code>: Server is the hostname or IP address of the NFS server.</p> </li> <li> <p><code>path</code>: Path that is exported by the NFS server.</p> </li> <li> <p><code>readOnly</code>: A flag indicating whether the storage will be mounted as read only (default false).</p> </li> </ul> <p>Kubernetes doesn\u2019t include an internal NFS provisioner. You need to use an external provisioner to create a StorageClass for NFS. Here are some examples:</p> <ul> <li> <p>NFS Ganesha server and external provisioner</p> </li> <li> <p>NFS subdir external provisioner</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#vsphere","title":"vSphere","text":"<p>There are two types of provisioners for vSphere storage classes:</p> <ul> <li> <p>CSI provisioner:\u00a0<code>csi.vsphere.vmware.com</code></p> </li> <li> <p>vCP provisioner:\u00a0<code>kubernetes.io/vsphere-volume</code></p> </li> </ul> <p>In-tree provisioners are\u00a0deprecated. For more information on the CSI provisioner, see\u00a0Kubernetes vSphere CSI Driver\u00a0and\u00a0vSphereVolume CSI migration.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#csi-provisioner","title":"CSI Provisioner","text":"<p>The vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters. For an example, refer to the\u00a0vSphere CSI repository.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#vcp-provisioner","title":"vCP Provisioner","text":"<p>The following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.</p> <ol> <li> <p>Create a StorageClass with a user specified disk format.<code>apiVersion</code><code>: storage.k8s.io/v1</code><code>kind</code><code>: StorageClass</code><code>metadata</code><code>:</code><code>name</code><code>: fast</code><code>provisioner</code><code>: kubernetes.io/vsphere-volume</code><code>parameters</code><code>:</code><code>diskformat</code><code>: zeroedthick\u00a0diskformat</code>:\u00a0<code>thin</code>,\u00a0<code>zeroedthick</code>\u00a0and\u00a0<code>eagerzeroedthick</code>. Default:\u00a0<code>\"thin\"</code>.</p> </li> <li> <p>Create a StorageClass with a disk format on a user specified datastore.<code>apiVersion</code><code>: storage.k8s.io/v1</code><code>kind</code><code>: StorageClass</code><code>metadata</code><code>:</code><code>name</code><code>: fast</code><code>provisioner</code><code>: kubernetes.io/vsphere-volume</code><code>parameters</code><code>:</code><code>diskformat</code><code>: zeroedthick</code><code>datastore</code><code>: VSANDatastore\u00a0datastore</code>: The user can also specify the datastore in the StorageClass. The volume will be created on the datastore specified in the StorageClass, which in this case is\u00a0<code>VSANDatastore</code>. This field is optional. If the datastore is not specified, then the volume will be created on the datastore specified in the vSphere config file used to initialize the vSphere Cloud Provider.</p> </li> <li> <p>Storage Policy Management inside Kubernetes</p> </li> <li> <p>Using existing vCenter SPBM policyOne of the most important features of vSphere for Storage Management is policy-based Management. Storage Policy Based Management (SPBM) is a storage policy framework that provides a single unified control plane across a broad range of data services and storage solutions. SPBM enables vSphere administrators to overcome upfront storage provisioning challenges, such as capacity planning, differentiated service levels, and capacity headroom management. The SPBM policies can be specified in the StorageClass using the\u00a0<code>storagePolicyName</code>\u00a0parameter.</p> </li> <li> <p>Virtual SAN policy support inside KubernetesVsphere Infrastructure (VI) Admins will be able to specify custom Virtual SAN Storage Capabilities during dynamic volume provisioning. You can now define storage requirements, such as performance and availability, through storage capabilities during dynamic volume provisioning. The storage capability requirements are converted into a Virtual SAN policy, which is then pushed down to the Virtual SAN layer when a persistent volume (virtual disk) is created. The virtual disk is distributed across the Virtual SAN datastore to meet the requirements. You can see\u00a0Storage Policy Based Management for dynamic provisioning of volumes\u00a0for more details on using storage policies for persistent volume management.</p> </li> </ol> <p>There are a few\u00a0vSphere examples\u00a0that you can try out for persistent volume management inside Kubernetes for vSphere.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#ceph-rbd-deprecated","title":"Ceph RBD (deprecated)","text":"<p>Note:</p> <p>FEATURE STATE: <code>Kubernetes v1.28 [deprecated]</code></p> <p>This internal provisioner of Ceph RBD is deprecated. Please use the\u00a0CephFS RBD CSI driver.</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast\nprovisioner: kubernetes.io/rbd\nparameters:\n  monitors: 10.16.153.105:6789\n  adminId: kube\n  adminSecretName: ceph-secret\n  adminSecretNamespace: kube-system\n  pool: kube\n  userId: kube\n  userSecretName: ceph-secret-user\n  userSecretNamespace: default\n  fsType: ext4\n  imageFormat: \"2\"\n  imageFeatures: \"layering\"\n</code></pre> <ul> <li> <p><code>monitors</code>: Ceph monitors, comma delimited. This parameter is required.</p> </li> <li> <p><code>adminId</code>: Ceph client ID that is capable of creating images in the pool. The default is \u201cadmin\u201d.</p> </li> <li> <p><code>adminSecretName</code>: Secret Name for\u00a0<code>adminId</code>. This parameter is required. The provided secret must have the type \u201ckubernetes.io/rbd.\u201d</p> </li> <li> <p><code>adminSecretNamespace</code>: The namespace for\u00a0<code>adminSecretName</code>. The default is \u201cdefault\u201d.</p> </li> <li> <p><code>pool</code>: Ceph RBD pool. The default is \u201crbd\u201d.</p> </li> <li> <p><code>userId</code>: Ceph client ID that is used to map the RBD image. The default\u00a0is the same as\u00a0<code>admin</code>.</p> </li> <li> <p><code>userSecretName</code>: The name of Ceph Secret for\u00a0<code>userId</code>\u00a0to map the RBD image. It must exist in the same namespace as PVCs. This parameter is required. The provided secret must have type \u201ckubernetes.io/rbd,\u201d for example created in this way:<code>kubectl create secret generic ceph-secret --type=\"kubernetes.io/rbd\"</code><code>\\</code><code>--from-literal=key='QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ=='</code><code>\\</code><code>--namespace=kube-system</code></p> </li> <li> <p><code>userSecretNamespace</code>: The namespace for\u00a0<code>userSecretName</code>.</p> </li> <li> <p><code>fsType</code>: fsType that is supported by Kubernetes. Default:\u00a0<code>\"ext4\"</code>.</p> </li> <li> <p><code>imageFormat</code>: Ceph RBD image format, \u201c1\u201d or \u201c2\u201d. The default is \u201c2\u201d.</p> </li> <li> <p><code>imageFeatures</code>: This parameter is optional and should only be used if you set\u00a0<code>imageFormat</code>\u00a0to \u201c2\u201d. Currently, supported features are\u00a0<code>layering</code>\u00a0only. The default is \u201c\u201d, and no features are turned on.</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#azure-disk","title":"Azure Disk","text":"<p>Kubernetes 1.29 does not include an\u00a0<code>azureDisk</code>\u00a0volume type.</p> <p>The\u00a0<code>azureDisk</code>\u00a0in-tree storage driver was deprecated in the Kubernetes v1.19 release and removed entirely in the v1.27 release.</p> <p>The Kubernetes project suggests using the\u00a0Azure Disk\u00a0third-party storage driver instead.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#azure-file-deprecated","title":"Azure File (deprecated)","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: azurefile\n  provisioner: kubernetes.io/azure-file\n  parameters:\n    skuName: Standard_LRS\n      location: eastus\n        storageAccount: azure_storage_account_name\n</code></pre> <ul> <li> <p><code>skuName</code>: Azure storage account SKU tier. The default is empty.</p> </li> <li> <p><code>location</code>: Azure storage account location. The default is empty.</p> </li> <li> <p><code>storageAccount</code>: Azure storage account name. The default is empty. If a storage account is not provided, all storage accounts associated with the resource group are searched to find one that matches\u00a0<code>skuName</code>\u00a0and\u00a0<code>location</code>. If a storage account is provided, it must reside in the same resource group as the cluster, and\u00a0<code>skuName</code>\u00a0and\u00a0<code>location</code>\u00a0are ignored.</p> </li> <li> <p><code>secretNamespace</code>: the namespace of the secret that contains the Azure Storage Account Name and Key. The default is the same as the Pod.</p> </li> <li> <p><code>secretName</code>: the name of the secret that contains the Azure Storage Account Name and Key. Default is\u00a0<code>azure-storage-account-&lt;accountName&gt;-secret</code></p> </li> <li> <p><code>readOnly</code>: a flag indicating whether the storage will be mounted as read-only. Defaults to false, which means a read/write mount. This setting will impact the\u00a0<code>ReadOnly</code>\u00a0setting in VolumeMounts as well.</p> </li> </ul> <p>During storage provisioning, a secret named secretName is created for the mounting credentials. If the cluster has enabled both\u00a0RBAC\u00a0and\u00a0Controller Roles, add the\u00a0create permission of resource\u00a0secret for cluster role\u00a0system:controller:persistent-volume-binder.</p> <p>In a multi-tenancy context, it is strongly recommended that you set the value for\u00a0secretNamespace explicitly; otherwise, other users may be able to read the storage account credentials.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#portworx-volume-deprecated","title":"Portworx volume (deprecated)","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: portworx-io-priority-high\nprovisioner: kubernetes.io/portworx-volume\nparameters:\n  repl: \"1\"\n  snap_interval: \"70\"\n  priority_io: \"high\"\n</code></pre> <ul> <li> <p><code>fs</code>: filesystem to be laid out:\u00a0<code>none/xfs/ext4</code>\u00a0(default:\u00a0<code>ext4</code>).</p> </li> <li> <p><code>block_size</code>: block size in Kbytes (default:\u00a0<code>32</code>).</p> </li> <li> <p><code>repl</code>: number of synchronous replicas to be provided in the form of replication factor\u00a0<code>1..3</code>\u00a0(default:\u00a0<code>1</code>) A string is expected here i.e.,<code>\"1\"</code>\u00a0and not\u00a0<code>1</code>.</p> </li> <li> <p><code>priority_io</code>: determines whether the volume will be created from higher performance or a lower priority storage\u00a0<code>high/medium/low</code>\u00a0(default:\u00a0<code>low</code>).</p> </li> <li> <p><code>snap_interval:</code>The clock/time interval in minutes for when to trigger snapshots. Snapshots are incremental based on the difference with the prior snapshot; 0 disables snaps (default:\u00a00). A string is expected here, i.e., \u201c70\u201d and not\u00a070.</p> </li> <li> <p><code>aggregation_level</code>: specifies the number of chunks the volume would be distributed into, 0 indicates a non-aggregated volume (default:\u00a0<code>0</code>). A string is expected here i.e.,\u00a0<code>\"0\"</code>\u00a0and not\u00a0<code>0</code></p> </li> <li> <p><code>ephemeral</code>: specifies whether the volume should be cleaned up after unmounting or should be persistent.\u00a0emptyDir\u00a0use case can set this value to true, and\u00a0persistent volumes\u00a0use cases such as for databases like Cassandra, should be set to false,\u00a0<code>true/false</code>\u00a0(default\u00a0<code>false</code>). A string is expected here, i.e.\u00a0<code>\"true\"</code>\u00a0and not\u00a0<code>true</code>.</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Storage_Volumes/#local","title":"Local","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: local-storage\nprovisioner: kubernetes.io/no-provisioner\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <p>Local volumes do not support dynamic provisioning in Kubernetes 1.29. However, a StorageClass should still be created to delay volume binding until a Pod is actually scheduled to the appropriate node. The\u00a0WaitForFirstConsumer volume binding mode specifies this.</p> <p>Delaying volume binding allows the scheduler to consider all of a Pod\u2019s scheduling constraints when choosing an appropriate Persistent Volume for a PersistentVolumeClaim. spec:   nodeSelector:     kubernetes.io/hostname: kube-01   volumes:     - name: task-pv-storage       persistentVolumeClaim:         claimName: task-pv-claim   containers:     - name: task-pv-container       image: nginx       ports:         - containerPort: 80           name: \"http-server\"       volumeMounts:         - mountPath: \"/usr/share/nginx/html\"           name: task-pv-storage</p> <pre><code>\n# Allowed topologies\n\nWhen a cluster operator specifies the\u00a0`WaitForFirstConsumer`\u00a0volume binding mode, it is no longer necessary to restrict provisioning to specific topologies in most situations. However, if still required,\u00a0`allowedTopologies`\u00a0can be specified.\n\nThis example demonstrates how to restrict the topology of provisioned volumes to specific zones and should be used as a replacement for the\u00a0`zone`\u00a0and\u00a0`zones`\u00a0parameters for the supported plugins.\n\n</code></pre> <p>apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:   name: standard provisioner: kubernetes.io/example parameters:   type: pd-standard volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions:   - key: topology.kubernetes.io/zone     values:     - us-central-1a     - us-central-1b</p> <pre><code>\n# Parameters\n\nStorageClasses have parameters that describe volumes belonging to the storage class. Different parameters may be accepted depending on the\u00a0`provisioner`. When a parameter is omitted, some default is used.\n\nThere can be at most 512 parameters defined for a StorageClass. The total length of the parameters object including its keys and values cannot exceed 256 KiB.\n\n## AWS EBS\n\nKubernetes 1.29 does not include a\u00a0`awsElasticBlockStore`\u00a0volume type.\n\nThe AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the v1.27 release.\n\nThe Kubernetes project suggests that you use the\u00a0[AWS EBS](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)\u00a0out-of-tree storage driver instead.\n\nHere is an example StorageClass for the AWS EBS CSI driver:\n\n</code></pre> <p>apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:   name: ebs-sc provisioner: ebs.csi.aws.com volumeBindingMode: WaitForFirstConsumer parameters:   csi.storage.k8s.io/fstype: xfs   type: io1   iopsPerGB: \"50\"   encrypted: \"true\" allowedTopologies: - matchLabelExpressions:   - key: topology.ebs.csi.aws.com/zone     values:     - us-east-2c</p> <pre><code>\n## NFS\n\nTo configure NFS storage, you can use the in-tree driver or the\u00a0[NFS CSI driver for Kubernetes](https://github.com/kubernetes-csi/csi-driver-nfs#readme)\u00a0(recommended).\n\n</code></pre> <p>apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:   name: example-nfs provisioner: example.com/external-nfs parameters:   server: nfs-server.example.com   path: /share   readOnly: \"false\"</p> <pre><code>\n* `server`: Server is the hostname or IP address of the NFS server.\n\n* `path`: Path that is exported by the NFS server.\n\n* `readOnly`: A flag indicating whether the storage will be mounted as read only (default false).\n\nKubernetes doesn\u2019t include an internal NFS provisioner. You need to use an external provisioner to create a StorageClass for NFS. Here are some examples:\n\n* [NFS Ganesha server and external provisioner](https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner)\n\n* [NFS subdir external provisioner](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)\n\n## vSphere\n\nThere are two types of provisioners for vSphere storage classes:\n\n* [CSI provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#vsphere-provisioner-csi):\u00a0`csi.vsphere.vmware.com`\n\n* [vCP provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#vcp-provisioner):\u00a0`kubernetes.io/vsphere-volume`\n\nIn-tree provisioners are\u00a0[deprecated](https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi). For more information on the CSI provisioner, see\u00a0[Kubernetes vSphere CSI Driver](https://vsphere-csi-driver.sigs.k8s.io/)\u00a0and\u00a0[vSphereVolume CSI migration](https://kubernetes.io/docs/concepts/storage/volumes/#vsphere-csi-migration).\n\n### CSI Provisioner\n\nThe vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters. For an example, refer to the\u00a0[vSphere CSI repository](https://github.com/kubernetes-sigs/vsphere-csi-driver/blob/master/example/vanilla-k8s-RWM-filesystem-volumes/example-sc.yaml).\n\n### vCP Provisioner\n\nThe following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.\n\n1. Create a StorageClass with a user specified disk format.**`apiVersion`**`: storage.k8s.io/v1\u00a0`**`kind`**`: StorageClass\u00a0`**`metadata`**`:\u00a0`**`name`**`: fast\u00a0`**`provisioner`**`: kubernetes.io/vsphere-volume\u00a0`**`parameters`**`:\u00a0`**`diskformat`**`: zeroedthick\u00a0diskformat`:\u00a0`thin`,\u00a0`zeroedthick`\u00a0and\u00a0`eagerzeroedthick`. Default:\u00a0`\"thin\"`.\n\n2. Create a StorageClass with a disk format on a user specified datastore.**`apiVersion`**`: storage.k8s.io/v1\u00a0`**`kind`**`: StorageClass\u00a0`**`metadata`**`:\u00a0`**`name`**`: fast\u00a0`**`provisioner`**`: kubernetes.io/vsphere-volume\u00a0`**`parameters`**`:\u00a0`**`diskformat`**`: zeroedthick\u00a0`**`datastore`**`: VSANDatastore\u00a0datastore`: The user can also specify the datastore in the StorageClass. The volume will be created on the datastore specified in the StorageClass, which in this case is\u00a0`VSANDatastore`. This field is optional. If the datastore is not specified, then the volume will be created on the datastore specified in the vSphere config file used to initialize the vSphere Cloud Provider.\n\n3. Storage Policy Management inside Kubernetes\n\n   * Using existing vCenter SPBM policyOne of the most important features of vSphere for Storage Management is policy-based Management. Storage Policy Based Management (SPBM) is a storage policy framework that provides a single unified control plane across a broad range of data services and storage solutions. SPBM enables vSphere administrators to overcome upfront storage provisioning challenges, such as capacity planning, differentiated service levels, and capacity headroom management. The SPBM policies can be specified in the StorageClass using the\u00a0`storagePolicyName`\u00a0parameter.\n\n   * Virtual SAN policy support inside KubernetesVsphere Infrastructure (VI) Admins will be able to specify custom Virtual SAN Storage Capabilities during dynamic volume provisioning. You can now define storage requirements, such as performance and availability, through storage capabilities during dynamic volume provisioning. The storage capability requirements are converted into a Virtual SAN policy, which is then pushed down to the Virtual SAN layer when a persistent volume (virtual disk) is created. The virtual disk is distributed across the Virtual SAN datastore to meet the requirements. You can see\u00a0[Storage Policy Based Management for dynamic provisioning of volumes](https://github.com/vmware-archive/vsphere-storage-for-kubernetes/blob/fa4c8b8ad46a85b6555d715dd9d27ff69839df53/documentation/policy-based-mgmt.md)\u00a0for more details on using storage policies for persistent volume management.\n\nThere are a few\u00a0[vSphere examples](https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere)\u00a0that you can try out for persistent volume management inside Kubernetes for vSphere.\n\n## Ceph RBD (deprecated)\n\n**Note:**\n\n**FEATURE STATE:**\u00a0`Kubernetes v1.28 [deprecated]`\n\nThis internal provisioner of Ceph RBD is deprecated. Please use the\u00a0[CephFS RBD CSI driver](https://github.com/ceph/ceph-csi).\n\n</code></pre> <p>apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:   name: fast provisioner: kubernetes.io/rbd parameters:   monitors: 10.16.153.105:6789   adminId: kube   adminSecretName: ceph-secret   adminSecretNamespace: kube-system   pool: kube   userId: kube   userSecretName: ceph-secret-user   userSecretNamespace: default   fsType: ext4   imageFormat: \"2\"   imageFeatures: \"layering\"</p> <pre><code>\n* `monitors`: Ceph monitors, comma delimited. This parameter is required.\n\n* `adminId`: Ceph client ID that is capable of creating images in the pool. The default is \u201cadmin\u201d.\n\n* `adminSecretName`: Secret Name for\u00a0`adminId`. This parameter is required. The provided secret must have the type \u201ckubernetes.io/rbd.\u201d\n\n* `adminSecretNamespace`: The namespace for\u00a0`adminSecretName`. The default is \u201cdefault\u201d.\n\n* `pool`: Ceph RBD pool. The default is \u201crbd\u201d.\n\n* `userId`: Ceph client ID that is used to map the RBD image. The default\u00a0is the same as\u00a0`admin`.\n\n* `userSecretName`: The name of Ceph Secret for\u00a0`userId`\u00a0to map the RBD image. It must exist in the same namespace as PVCs. This parameter is required. The provided secret must have type \u201ckubernetes.io/rbd,\u201d for example created in this way:`kubectl create secret generic ceph-secret --type=\"kubernetes.io/rbd\"\u00a0`**`\\\u00a0`**`--from-literal=key='QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ=='\u00a0`**`\\\u00a0`**`--namespace=kube-system`\n\n* `userSecretNamespace`: The namespace for\u00a0`userSecretName`.\n\n* `fsType`: fsType that is supported by Kubernetes. Default:\u00a0`\"ext4\"`.\n\n* `imageFormat`: Ceph RBD image format, \u201c1\u201d or \u201c2\u201d. The default is \u201c2\u201d.\n\n* `imageFeatures`: This parameter is optional and should only be used if you set\u00a0`imageFormat`\u00a0to \u201c2\u201d. Currently, supported features are\u00a0`layering`\u00a0only. The default is \u201c\u201d, and no features are turned on.\n\n## Azure Disk\n\nKubernetes 1.29 does not include an\u00a0`azureDisk`\u00a0volume type.\n\nThe\u00a0`azureDisk`\u00a0in-tree storage driver was deprecated in the Kubernetes v1.19 release and removed entirely in the v1.27 release.\n\nThe Kubernetes project suggests using the\u00a0[Azure Disk](https://github.com/kubernetes-sigs/azuredisk-csi-driver)\u00a0third-party storage driver instead.\n\n## Azure File (deprecated)\n\n</code></pre> <p>apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:   name: azurefile   provisioner: kubernetes.io/azure-file   parameters:     skuName: Standard_LRS       location: eastus         storageAccount: azure_storage_account_name</p> <pre><code>\n* `skuName`: Azure storage account SKU tier. The default is empty.\n\n* `location`: Azure storage account location. The default is empty.\n\n* `storageAccount`: Azure storage account name. The default is empty. If a storage account is not provided, all storage accounts associated with the resource group are searched to find one that matches\u00a0`skuName`\u00a0and\u00a0`location`. If a storage account is provided, it must reside in the same resource group as the cluster, and\u00a0`skuName`\u00a0and\u00a0`location`\u00a0are ignored.\n\n* `secretNamespace`: the namespace of the secret that contains the Azure Storage Account Name and Key. The default is the same as the Pod.\n\n* `secretName`: the name of the secret that contains the Azure Storage Account Name and Key. Default is\u00a0`azure-storage-account-&lt;accountName&gt;-secret`\n\n* `readOnly`: a flag indicating whether the storage will be mounted as read-only. Defaults to false, which means a read/write mount. This setting will impact the\u00a0`ReadOnly`\u00a0setting in VolumeMounts as well.\n\nDuring storage provisioning, a secret named secretName is created for the mounting credentials. If the cluster has enabled both\u00a0[RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)\u00a0and\u00a0[Controller Roles](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#controller-roles), add the\u00a0create permission of resource\u00a0secret for cluster role\u00a0system:controller:persistent-volume-binder.\n\nIn a multi-tenancy context, it is strongly recommended that you set the value for\u00a0secretNamespace explicitly; otherwise, other users may be able to read the storage account credentials.\n\n## Portworx volume (deprecated)\n\n</code></pre> <p>apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:   name: portworx-io-priority-high provisioner: kubernetes.io/portworx-volume parameters:   repl: \"1\"   snap_interval: \"70\"   priority_io: \"high\"</p> <pre><code>\n* `fs`: filesystem to be laid out:\u00a0`none/xfs/ext4`\u00a0(default:\u00a0`ext4`).\n\n* `block_size`: block size in Kbytes (default:\u00a0`32`).\n\n* `repl`: number of synchronous replicas to be provided in the form of replication factor\u00a0`1..3`\u00a0(default:\u00a0`1`) A string is expected here i.e.,`\"1\"`\u00a0and not\u00a0`1`.\n\n* `priority_io`: determines whether the volume will be created from higher performance or a lower priority storage\u00a0`high/medium/low`\u00a0(default:\u00a0`low`).\n\n* `snap_interval:`The clock/time interval in minutes for when to trigger snapshots. Snapshots are incremental based on the difference with the prior snapshot; 0 disables snaps (default:\u00a00). A string is expected here, i.e., \u201c70\u201d and not\u00a070.\n\n* `aggregation_level`: specifies the number of chunks the volume would be distributed into, 0 indicates a non-aggregated volume (default:\u00a0`0`). A string is expected here i.e.,\u00a0`\"0\"`\u00a0and not\u00a0`0`\n\n* `ephemeral`: specifies whether the volume should be cleaned up after unmounting or should be persistent.\u00a0emptyDir\u00a0use case can set this value to true, and\u00a0persistent volumes\u00a0use cases such as for databases like Cassandra, should be set to false,\u00a0`true/false`\u00a0(default\u00a0`false`). A string is expected here, i.e.\u00a0`\"true\"`\u00a0and not\u00a0`true`.\n\n## Local\n\n</code></pre> <p>apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:   name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer ```</p> <p>Local volumes do not support dynamic provisioning in Kubernetes 1.29. However, a StorageClass should still be created to delay volume binding until a Pod is actually scheduled to the appropriate node. The\u00a0WaitForFirstConsumer volume binding mode specifies this.</p> <p>Delaying volume binding allows the scheduler to consider all of a Pod\u2019s scheduling constraints when choosing an appropriate Persistent Volume for a PersistentVolumeClaim.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Taints_and_Tolerations/","title":"Kubernetes Taints and Tolerations","text":"<p>Node affinity\u00a0is a property of\u00a0Pods\u00a0that\u00a0attracts\u00a0them to a set of\u00a0nodes\u00a0(either as a preference or a hard requirement).\u00a0Taints\u00a0are the opposite \u2014 they allow a node to repel a set of pods.</p> <p>Tolerations\u00a0are applied to pods. Tolerations allow the scheduler to schedule pods with matching taints. Tolerations allow scheduling but don\u2019t guarantee scheduling: the scheduler also\u00a0evaluates other parameters\u00a0as part of its function.</p> <p>Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Taints_and_Tolerations/#concepts","title":"Concepts","text":"<p>You add a taint to a node using\u00a0kubectl taint. For example,</p> <p>kubectl taint nodes node1 key1=value1:NoSchedule</p> <p>places a taint on node\u00a0<code>node1</code>. The taint has key\u00a0<code>key1</code>, value\u00a0<code>value1</code>, and taint effect\u00a0<code>NoSchedule</code>. This means that no pod will be able to schedule onto\u00a0<code>node1</code>\u00a0unless it has a matching toleration.</p> <p>To remove the taint added by the command above, you can run:</p> <pre><code>kubectl taint nodes node1 key1=value1:NoSchedule-\n</code></pre> <p>You specify a toleration for a pod in the PodSpec. Both of the following tolerations \u201cmatch\u201d the taint created by the\u00a0<code>kubectl taint</code>\u00a0line above, and thus a pod with either toleration would be able to schedule onto\u00a0<code>node1</code>:</p> <pre><code>tolerations:\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoSchedule\"\n</code></pre> <pre><code>tolerations:\n- key: \"key1\"\n  operator: \"Exists\"\n  effect: \"NoSchedule\"\n</code></pre> <p>Here's an example of a pod that uses tolerations:</p> <p>pods/pod-with-toleration.yaml</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  tolerations:\n  - key: \"example-key\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n</code></pre> <p>The default value for\u00a0<code>operator</code>\u00a0is\u00a0<code>Equal</code>.</p> <p>A toleration \u201cmatches\u201d a taint if the keys are the same and the effects are the same, and:</p> <ul> <li> <p>the\u00a0<code>operator</code>\u00a0is\u00a0<code>Exists</code>\u00a0(in which case no\u00a0<code>value</code>\u00a0should be specified), or</p> </li> <li> <p>the\u00a0<code>operator</code>\u00a0is\u00a0<code>Equal</code>\u00a0and the values should be equal.</p> </li> </ul> <p>Note: There are two special cases: An empty\u00a0<code>key</code>\u00a0with operator\u00a0<code>Exists</code>\u00a0matches all keys, values and effects which means this will tolerate everything. An empty\u00a0<code>effect</code>\u00a0matches all effects with key\u00a0<code>key1</code>.</p> <p>The above example used\u00a0<code>effect</code>\u00a0of\u00a0<code>NoSchedule</code>. Alternatively, you can use\u00a0<code>effect</code>\u00a0of\u00a0<code>PreferNoSchedule</code>.</p> <p>The allowed values for the\u00a0<code>effect</code>\u00a0field are:<code>NoExecute</code>This affects pods that are already running on the node as follows:</p> <ul> <li> <p>Pods that do not tolerate the taint are evicted immediately</p> </li> <li> <p>Pods that tolerate the taint without specifying\u00a0<code>tolerationSeconds</code>\u00a0in their toleration specification remain bound forever</p> </li> <li> <p>Pods that tolerate the taint with a specified\u00a0<code>tolerationSeconds</code>\u00a0remain bound for the specified amount of time. After that time elapses, the node lifecycle controller evicts the Pods from the node.</p> </li> </ul> <p><code>NoSchedule</code>No new Pods will be scheduled on the tainted node unless they have a matching toleration. Pods currently running on the node are\u00a0not\u00a0evicted.<code>PreferNoSchedulePreferNoSchedule</code>\u00a0is a \u201cpreference\u201d or \u201csoft\u201d version of\u00a0<code>NoSchedule</code>. The control plane will\u00a0try\u00a0to avoid placing a Pod that does not tolerate the taint on the node, but it is not guaranteed.</p> <p>You can put multiple taints on the same node and multiple tolerations on the same pod. The way Kubernetes processes multiple taints and tolerations is like a filter: start with all of a node\u2019s taints, then ignore the ones for which the pod has a matching toleration; the remaining un-ignored taints have the indicated effects on the pod. In particular,</p> <ul> <li> <p>if there is at least one un-ignored taint with effect\u00a0<code>NoSchedule</code>\u00a0then Kubernetes will not schedule the pod onto that node</p> </li> <li> <p>if there is no un-ignored taint with effect\u00a0<code>NoSchedule</code>\u00a0but there is at least one un-ignored taint with effect\u00a0<code>PreferNoSchedule</code>\u00a0then Kubernetes will\u00a0try\u00a0to not schedule the pod onto the node</p> </li> <li> <p>if there is at least one un-ignored taint with effect\u00a0<code>NoExecute</code>\u00a0then the pod will be evicted from the node (if it is already running on the node), and will not be scheduled onto the node (if it is not yet running on the node).</p> </li> </ul> <p>For example, imagine you taint a node like this</p> <pre><code>kubectl taint nodes node1 key1=value1:NoSchedule\nkubectl taint nodes node1 key1=value1:NoExecute\nkubectl taint nodes node1 key2=value2:NoSchedule\n</code></pre> <p>A pod has two tolerations</p> <pre><code>tolerations:\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoSchedule\"\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoExecute\"\n</code></pre> <p>In this case, the pod will not be able to schedule onto the node, because there is no toleration matching the third taint. But it will be able to continue running if it is already running on the node when the taint is added, because the third taint is the only one of the three that is not tolerated by the pod.</p> <p>Normally, if a taint with effect\u00a0<code>NoExecute</code>\u00a0is added to a node, then any pods that do not tolerate the taint will be evicted immediately, and pods that do tolerate the taint will never be evicted. However, a toleration with\u00a0<code>NoExecute</code>\u00a0effect can specify an optional\u00a0<code>tolerationSeconds</code>\u00a0field that dictates how long the pod will stay bound to the node after the taint is added. For example,</p> <pre><code>tolerations:\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoExecute\"\n  tolerationSeconds: 3600\n</code></pre> <p>means that if this pod is running and a matching taint is added to the node, then the pod will stay bound to the node for 3600 seconds, and then be evicted. If the taint is removed before that time, the pod will not be evicted.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Taints_and_Tolerations/#example-use-cases","title":"Example Use Cases","text":"<p>Taints and tolerations are a flexible way to steer pods\u00a0away\u00a0from nodes or evict pods that shouldn\u2019t be running. A few of the use cases are</p> <ul> <li> <p>Dedicated Nodes: If you want to dedicate a set of nodes for exclusive use by a particular set of users, you can add a taint to those nodes (say,\u00a0<code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>) and then add a corresponding toleration to their pods (this would be done most easily by writing a custom\u00a0admission controller). The pods with the tolerations will then be allowed to use the tainted (dedicated) nodes as well as any other nodes in the cluster. If you want to dedicate the nodes to them\u00a0and\u00a0ensure they\u00a0only\u00a0use the dedicated nodes, then you should additionally add a label similar to the taint to the same set of nodes (e.g.\u00a0<code>dedicated=groupName</code>), and the admission controller should additionally add a node affinity to require that the pods can only schedule onto nodes labeled with\u00a0<code>dedicated=groupName</code>.</p> </li> <li> <p>Nodes with Special Hardware: In a cluster where a small subset of nodes have specialized hardware (for example GPUs), it is desirable to keep pods that don\u2019t need the specialized hardware off of those nodes, thus leaving room for later-arriving pods that do need the specialized hardware. This can be done by tainting the nodes that have the specialized hardware (e.g.\u00a0<code>kubectl taint nodes nodename special=true:NoSchedule</code>\u00a0or\u00a0<code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>) and adding a corresponding toleration to pods that use the special hardware. As in the dedicated nodes use case, it is probably easiest to apply the tolerations using a custom\u00a0admission controller. For example, it is recommended to use\u00a0Extended Resources\u00a0to represent the special hardware, taint your special hardware nodes with the extended resource name and run the\u00a0ExtendedResourceToleration\u00a0admission controller. Now, because the nodes are tainted, no pods without the toleration will schedule on them. But when you submit a pod that requests the extended resource, the\u00a0<code>ExtendedResourceToleration</code>\u00a0admission controller will automatically add the correct toleration to the pod and that pod will schedule on the special hardware nodes. This will make sure that these special hardware nodes are dedicated for pods requesting such hardware and you don\u2019t have to manually add tolerations to your pods.</p> </li> <li> <p>Taint based Evictions: A per-pod-configurable eviction behavior when there are node problems, which is described in the next section.</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Taints_and_Tolerations/#taint-based-evictions","title":"Taint based Evictions","text":"<p>FEATURE STATE: <code>Kubernetes v1.18 [stable]</code></p> <p>The node controller automatically taints a Node when certain conditions are true. The following taints are built in:</p> <ul> <li> <p><code>node.kubernetes.io/not-ready</code>: Node is not ready. This corresponds to the NodeCondition\u00a0<code>Ready</code>\u00a0being \u201c<code>False</code>\u201c.</p> </li> <li> <p><code>node.kubernetes.io/unreachable</code>: Node is unreachable from the node controller. This corresponds to the NodeCondition\u00a0<code>Ready</code>\u00a0being \u201c<code>Unknown</code>\u201c.</p> </li> <li> <p><code>node.kubernetes.io/memory-pressure</code>: Node has memory pressure.</p> </li> <li> <p><code>node.kubernetes.io/disk-pressure</code>: Node has disk pressure.</p> </li> <li> <p><code>node.kubernetes.io/pid-pressure</code>: Node has PID pressure.</p> </li> <li> <p><code>node.kubernetes.io/network-unavailable</code>: Node\u2019s network is unavailable.</p> </li> <li> <p><code>node.kubernetes.io/unschedulable</code>: Node is unschedulable.</p> </li> <li> <p><code>node.cloudprovider.kubernetes.io/uninitialized</code>: When the kubelet is started with \u201cexternal\u201d cloud provider, this taint is set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet removes this taint.</p> </li> </ul> <p>In case a node is to be drained, the node controller or the kubelet adds relevant taints with\u00a0<code>NoExecute</code>\u00a0effect. This effect is added by default for the\u00a0<code>node.kubernetes.io/not-ready</code>\u00a0and\u00a0<code>node.kubernetes.io/unreachable</code>\u00a0taints. If the fault condition returns to normal, the kubelet or node controller can remove the relevant taint(s).</p> <p>In some cases when the node is unreachable, the API server is unable to communicate with the kubelet on the node. The decision to delete the pods cannot be communicated to the kubelet until communication with the API server is re-established. In the meantime, the pods that are scheduled for deletion may continue to run on the partitioned node.</p> <p>Note:\u00a0The control plane limits the rate of adding new taints to nodes. This rate limiting manages the number of evictions that are triggered when many nodes become unreachable at once (for example: if there is a network disruption).</p> <p>You can specify\u00a0<code>tolerationSeconds</code>\u00a0for a Pod to define how long that Pod stays bound to a failing or unresponsive Node.</p> <p>For example, you might want to keep an application with a lot of local state bound to node for a long time in the event of network partition, hoping that the partition will recover and thus the pod eviction can be avoided. The toleration you set for that Pod might look like:</p> <pre><code>tolerations:\n- key: \"node.kubernetes.io/unreachable\"\n  operator: \"Exists\"\n  effect: \"NoExecute\"\n  tolerationSeconds: 6000\n</code></pre> <p>Note: Kubernetes automatically adds a toleration for\u00a0<code>node.kubernetes.io/not-ready</code>\u00a0and\u00a0<code>node.kubernetes.io/unreachable</code>\u00a0with\u00a0<code>tolerationSeconds=300</code>, unless you, or a controller, set those tolerations explicitly.</p> <p>These automatically-added tolerations mean that Pods remain bound to Nodes for 5 minutes after one of these problems is detected.</p> <p>DaemonSet\u00a0pods are created with\u00a0<code>NoExecute</code>\u00a0tolerations for the following taints with no\u00a0<code>tolerationSeconds</code>:</p> <pre><code>- node.kubernetes.io/unreachable\n- node.kubernetes.io/not-ready\n</code></pre> <p>This ensures that DaemonSet pods are never evicted due to these problems.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Taints_and_Tolerations/#taint-nodes-by-condition","title":"Taint Nodes by Condition","text":"<p>The control plane, using the node\u00a0controller, automatically creates taints with a\u00a0<code>NoSchedule</code>\u00a0effect for\u00a0node conditions.</p> <p>The scheduler checks taints, not node conditions, when it makes scheduling decisions. This ensures that node conditions don\u2019t directly affect scheduling. For example, if the\u00a0<code>DiskPressure</code>\u00a0node condition is active, the control plane adds the\u00a0<code>node.kubernetes.io/disk-pressure</code>\u00a0taint and does not schedule new pods onto the affected node. If the\u00a0<code>MemoryPressure</code>\u00a0node condition is active, the control plane adds the\u00a0<code>node.kubernetes.io/memory-pressure</code>\u00a0taint.</p> <p>You can ignore node conditions for newly created pods by adding the corresponding Pod tolerations. The control plane also adds the\u00a0<code>node.kubernetes.io/memory-pressure</code>\u00a0toleration on pods that have a\u00a0QoS class\u00a0other than\u00a0<code>BestEffort</code>. This is because Kubernetes treats pods in the\u00a0<code>Guaranteed</code>\u00a0or\u00a0<code>Burstable</code>\u00a0QoS classes (even pods with no memory request set) as if they are able to cope with memory pressure, while new\u00a0<code>BestEffort</code>\u00a0pods are not scheduled onto the affected node.</p> <p>The DaemonSet controller automatically adds the following\u00a0<code>NoSchedule</code>\u00a0tolerations to all daemons, to prevent DaemonSets from breaking.</p> <pre><code>- node.kubernetes.io/memory-pressure\n- node.kubernetes.io/disk-pressure\n- node.kubernetes.io/pid-pressure\u00a0(1.14 or later)\n- node.kubernetes.io/unschedulable\u00a0(1.10 or later)\n- node.kubernetes.io/network-unavailable\u00a0(host network only)\n</code></pre> <p>Adding these tolerations ensures backward compatibility. You can also add arbitrary tolerations to DaemonSets.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/","title":"Volume Snapshots","text":"<p>In Kubernetes, a\u00a0VolumeSnapshot\u00a0represents a snapshot of a volume on a storage system. This document assumes that you are already familiar with Kubernetes\u00a0persistent volumes.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#introduction","title":"Introduction","text":"<p>Similar to how API resources\u00a0<code>PersistentVolume</code>\u00a0and\u00a0<code>PersistentVolumeClaim</code>\u00a0are used to provision volumes for users and administrators,\u00a0<code>VolumeSnapshotContent</code>\u00a0and\u00a0<code>VolumeSnapshot</code>\u00a0API resources are provided to create volume snapshots for users and administrators.</p> <p>A\u00a0<code>VolumeSnapshotContent</code>\u00a0is a snapshot taken from a volume in the cluster that has been provisioned by an administrator. It is a resource in the cluster, just like a PersistentVolume is a cluster resource.</p> <p>A\u00a0<code>VolumeSnapshot</code>\u00a0It is a request for a snapshot of a volume by a user. It is similar to a PersistentVolumeClaim.</p> <p><code>VolumeSnapshotClass</code>\u00a0allows you to specify different attributes belonging to a\u00a0<code>VolumeSnapshot</code>. These attributes may differ among snapshots taken from the same volume on the storage system and, therefore cannot be expressed by using the same\u00a0<code>StorageClass</code>\u00a0of a\u00a0<code>PersistentVolumeClaim</code>.</p> <p>Volume snapshots provide Kubernetes users a standardized way to copy a volume\u2019s contents at a particular time without creating an entirely new volume. This functionality enables, for example, database administrators to back up databases before editing or deleting modifications.</p> <p>Users need to be aware of the following when using this feature:</p> <ul> <li> <p>API Objects\u00a0<code>VolumeSnapshot</code>,\u00a0<code>VolumeSnapshotContent</code>, and\u00a0<code>VolumeSnapshotClass</code>\u00a0are\u00a0CRDs, not part of the core API.</p> </li> <li> <p><code>VolumeSnapshot</code>\u00a0support is only available for CSI drivers.</p> </li> <li> <p>As part of the deployment process of\u00a0<code>VolumeSnapshot</code>, the Kubernetes team provides a snapshot controller to be deployed into the control plane, and a sidecar helper container called csi-snapshotter to be deployed together with the CSI driver. The snapshot controller watches\u00a0<code>VolumeSnapshot</code>\u00a0and\u00a0<code>VolumeSnapshotContent</code>\u00a0objects and is responsible for the creation and deletion of\u00a0<code>VolumeSnapshotContent</code>\u00a0object. The sidecar csi-snapshotter watches\u00a0<code>VolumeSnapshotContent</code>\u00a0objects and triggers\u00a0<code>CreateSnapshot</code>\u00a0and\u00a0<code>DeleteSnapshot</code>\u00a0operations against a CSI endpoint.</p> </li> <li> <p>A validating webhook server also provides tightened validation on snapshot objects. It should be installed by the Kubernetes distros along with the snapshot controller and CRDs, not CSI drivers, in all Kubernetes clusters with enabled snapshot features.</p> </li> <li> <p>CSI drivers may or may not have implemented the volume snapshot functionality. The CSI drivers that have provided support for volume snapshot will likely use the csi-snapshotter. See\u00a0CSI Driver documentation\u00a0for details.</p> </li> <li> <p>The CRDs and snapshot controller installations are the responsibility of the Kubernetes distribution.</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#lifecycle-of-a-volume-snapshot-and-volume-snapshot-content","title":"Lifecycle of a volume snapshot and volume snapshot content","text":"<p><code>VolumeSnapshotContents</code>\u00a0are resources in the cluster.\u00a0<code>VolumeSnapshots</code>\u00a0are requests for those resources. The interaction between\u00a0<code>VolumeSnapshotContents</code>\u00a0and\u00a0<code>VolumeSnapshots</code>\u00a0follow this lifecycle:</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#provisioning-volume-snapshot","title":"Provisioning Volume Snapshot","text":"<p>There are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#pre-provisioned","title":"Pre-provisioned","text":"<p>A cluster administrator creates a number of\u00a0<code>VolumeSnapshotContents</code>. They carry the details of the real volume snapshot on the storage system which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#dynamic","title":"Dynamic","text":"<p>Instead of using a pre-existing snapshot, you can request that a snapshot to be dynamically taken from a PersistentVolumeClaim. The\u00a0VolumeSnapshotClass\u00a0specifies storage provider-specific parameters to use when taking a snapshot.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#binding","title":"Binding","text":"<p>The snapshot controller handles the binding of a\u00a0<code>VolumeSnapshot</code>\u00a0object with an appropriate\u00a0<code>VolumeSnapshotContent</code>\u00a0object, in both pre-provisioned and dynamically provisioned scenarios. The binding is a one-to-one mapping.</p> <p>In the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the requested VolumeSnapshotContent object is created.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#persistent-volume-claim-as-snapshot-source-protection","title":"Persistent Volume Claim as Snapshot Source Protection","text":"<p>The purpose of this protection is to ensure that in-use\u00a0PersistentVolumeClaim\u00a0API objects are not removed from the system while a snapshot is being taken from it (as this may result in data loss).</p> <p>While a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim is in-use. If you delete a PersistentVolumeClaim API object in active use as a snapshot source, the PersistentVolumeClaim object is not removed immediately. Instead, removal of the PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#delete","title":"Delete","text":"<p>Deletion is triggered by deleting the\u00a0<code>VolumeSnapshot</code>\u00a0object, and the\u00a0<code>DeletionPolicy</code>\u00a0will be followed. If the\u00a0<code>DeletionPolicy</code>\u00a0is\u00a0<code>Delete</code>, then the underlying storage snapshot will be deleted along with the\u00a0<code>VolumeSnapshotContent</code>\u00a0object. If the\u00a0<code>DeletionPolicy</code>\u00a0is\u00a0<code>Retain</code>, then both the underlying snapshot and\u00a0<code>VolumeSnapshotContent</code>\u00a0remain.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#volumesnapshots","title":"VolumeSnapshots","text":"<p>Each VolumeSnapshot contains a spec and a status.</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: new-snapshot-test\nspec:\n  volumeSnapshotClassName: csi-hostpath-snapclass\n  source:\n    persistentVolumeClaimName: pvc-test\n</code></pre> <p><code>persistentVolumeClaimName</code>\u00a0is the name of the PersistentVolumeClaim data source for the snapshot. This field is required for dynamically provisioning a snapshot.</p> <p>A volume snapshot can request a particular class by specifying the name of a\u00a0VolumeSnapshotClass\u00a0using the attribute\u00a0<code>volumeSnapshotClassName</code>. If nothing is set, then the default class is used if available.</p> <p>For pre-provisioned snapshots, you need to specify a\u00a0<code>volumeSnapshotContentName</code>\u00a0as the source for the snapshot as shown in the following example. The\u00a0<code>volumeSnapshotContentName</code>\u00a0source field is required for pre-provisioned snapshots.</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: test-snapshot\nspec:\n  source:\n    volumeSnapshotContentName: test-content\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#volume-snapshot-contents","title":"Volume Snapshot Contents","text":"<p>Each VolumeSnapshotContent contains a spec and status. In dynamic provisioning, the snapshot common controller creates\u00a0<code>VolumeSnapshotContent</code>\u00a0objects. Here is an example:</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotContent\nmetadata:\n  name: snapcontent-72d9a349-aacd-42d2-a240-d775650d2455\nspec:\n  deletionPolicy: Delete\n  driver: hostpath.csi.k8s.io\n  source:\n    volumeHandle: ee0cfb94-f8d4-11e9-b2d8-0242ac110002\n  sourceVolumeMode: Filesystem\n  volumeSnapshotClassName: csi-hostpath-snapclass\n  volumeSnapshotRef:\n    name: new-snapshot-test\n    namespace: default\n    uid: 72d9a349-aacd-42d2-a240-d775650d2455\n</code></pre> <p><code>volumeHandle</code>\u00a0is the unique identifier of the volume created on the storage backend and returned by the CSI driver during the volume creation. This field is required for dynamically provisioning a snapshot. It specifies the volume source of the snapshot.</p> <p>For pre-provisioned snapshots, you (as cluster administrator) are responsible for creating the\u00a0<code>VolumeSnapshotContent</code>\u00a0object as follows.</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotContent\nmetadata:\n  name: new-snapshot-content-test\nspec:\n  deletionPolicy: Delete\n  driver: hostpath.csi.k8s.io\n  source:\n    snapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002\n  sourceVolumeMode: Filesystem\n  volumeSnapshotRef:\n    name: new-snapshot-test\n    namespace: default\n</code></pre> <p><code>snapshotHandle</code>\u00a0is the unique identifier of the volume snapshot created on the storage backend. This field is required for the pre-provisioned snapshots. It specifies the CSI snapshot id on the storage system that this\u00a0<code>VolumeSnapshotContent</code>\u00a0represents.</p> <p><code>sourceVolumeMode</code>\u00a0is the mode of the volume whose snapshot is taken. The value of the\u00a0<code>sourceVolumeMode</code>\u00a0field can be either\u00a0<code>Filesystem</code>\u00a0or\u00a0<code>Block</code>. If the source volume mode is not specified, Kubernetes treats the snapshot as if the source volume\u2019s mode is unknown.</p> <p><code>volumeSnapshotRef</code>\u00a0is the reference of the corresponding\u00a0<code>VolumeSnapshot</code>. Note that when the\u00a0<code>VolumeSnapshotContent</code>\u00a0is being created as a pre-provisioned snapshot, the\u00a0<code>VolumeSnapshot</code>\u00a0referenced in\u00a0<code>volumeSnapshotRef</code>\u00a0might not exist yet.</p>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#converting-the-volume-mode-of-a-snapshot","title":"Converting the volume mode of a Snapshot","text":"<p>If the\u00a0<code>VolumeSnapshots</code>\u00a0API installed on your cluster supports the\u00a0<code>sourceVolumeMode</code>\u00a0field, then the API has the capability to prevent unauthorized users from converting the mode of a volume.</p> <p>To check if your cluster has capability for this feature, run the following command:</p> <pre><code>$ kubectl get crd volumesnapshotcontent -o yaml\n</code></pre> <p>If you want to allow users to create a\u00a0<code>PersistentVolumeClaim</code>\u00a0from an existing\u00a0<code>VolumeSnapshot</code>, but with a different volume mode than the source, the annotation\u00a0<code>snapshot.storage.kubernetes.io/allow-volume-mode-change: \"true\"</code>needs to be added to the\u00a0<code>VolumeSnapshotContent</code>\u00a0that corresponds to the\u00a0<code>VolumeSnapshot</code>.</p> <p>For pre-provisioned snapshots,\u00a0<code>spec.sourceVolumeMode</code>\u00a0needs to be populated by the cluster administrator.</p> <p>An example\u00a0<code>VolumeSnapshotContent</code>\u00a0resource with this feature enabled would look like this:</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotContent\nmetadata:\n  name: new-snapshot-content-test\n  annotations:\n    - snapshot.storage.kubernetes.io/allow-volume-mode-change: \"true\"\nspec:\n  deletionPolicy: Delete\n  driver: hostpath.csi.k8s.io\n  source:\n    snapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002\n  sourceVolumeMode: Filesystem\n  volumeSnapshotRef:\n    name: new-snapshot-test\n    namespace: default\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Kubernetes_Volume_Snapshots/#provisioning-volumes-from-snapshots","title":"Provisioning Volumes from Snapshots","text":"<p>You can provision a new volume, pre-populated with data from a snapshot, by using the\u00a0dataSource\u00a0field in the\u00a0<code>PersistentVolumeClaim</code>\u00a0object.</p> <p>For more details, see\u00a0Volume Snapshot and Restore Volume from Snapshot.</p>"},{"location":"Taikun_CloudWorks_Overview/Metrics/","title":"Design","text":"<p>This document describes the design and interaction between the\u00a0custom resource definitions\u00a0that the Prometheus Operator manages.</p> <p>The custom resources managed by the Prometheus Operator are:</p> <ul> <li> <p>Prometheus</p> </li> <li> <p>Alertmanager</p> </li> <li> <p>ThanosRuler</p> </li> <li> <p>ServiceMonitor</p> </li> <li> <p>PodMonitor</p> </li> <li> <p>Probe</p> </li> <li> <p>PrometheusRule</p> </li> <li> <p>AlertmanagerConfig</p> </li> <li> <p>PrometheusAgent</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Metrics/#prometheus","title":"Prometheus","text":"<p>The\u00a0<code>Prometheus</code>\u00a0custom resource definition (CRD) declaratively defines a desired\u00a0Prometheus\u00a0setup to run in a Kubernetes cluster. It provides options to configure the number of replicas, persistent storage, and Alertmanagers to which the deployed Prometheus instances send alerts to.</p> <p>For each\u00a0<code>Prometheus</code>\u00a0resource, the Operator deploys one or several\u00a0<code>StatefulSet</code>\u00a0objects in the same namespace (the number of statefulsets is equal to the number of shards but by default it is 1).</p> <p>The CRD defines via label and namespace selectors which\u00a0<code>ServiceMonitor</code>,\u00a0<code>PodMonitor</code>\u00a0and\u00a0<code>Probe</code>\u00a0objects should be associated to the deployed Prometheus instances. The CRD also defines which\u00a0<code>PrometheusRules</code>\u00a0objects should be reconciled. The operator continuously reconciles the custom resources and generates one or several\u00a0<code>Secret</code>\u00a0objects holding the Prometheus configuration. A\u00a0<code>config-reloader</code>\u00a0container running as a sidecar in the Prometheus pod detects any change to the configuration and reloads Prometheus if needed.</p>"},{"location":"Taikun_CloudWorks_Overview/Metrics/#alertmanager","title":"Alertmanager","text":"<p>The\u00a0<code>Alertmanager</code>\u00a0custom resource definition (CRD) declaratively defines a desired\u00a0Alertmanager\u00a0setup to run in a Kubernetes cluster. It provides options to configure the number of replicas and persistent storage.</p> <p>For each\u00a0<code>Alertmanager</code>\u00a0resource, the Operator deploys a\u00a0<code>StatefulSet</code>\u00a0in the same namespace. The Alertmanager pods are configured to mount a\u00a0<code>Secret</code>\u00a0called\u00a0<code>alertmanager-&lt;alertmanager-name&gt;</code>\u00a0which holds the Alertmanager configuration under the key\u00a0<code>alertmanager.yaml</code>.</p> <p>When there are two or more configured replicas, the Operator runs the Alertmanager instances in high-availability mode.</p>"},{"location":"Taikun_CloudWorks_Overview/Metrics/#thanosruler","title":"ThanosRuler","text":"<p>The\u00a0<code>ThanosRuler</code>\u00a0custom resource definition (CRD) declaratively defines a desired\u00a0Thanos Ruler\u00a0setup to run in a Kubernetes cluster. With Thanos Ruler recording and alerting rules can be processed across multiple Prometheus instances.</p> <p>A\u00a0<code>ThanosRuler</code>\u00a0instance requires at least one query endpoint which points to the location of Thanos Queriers or Prometheus instances.</p> <p>Further information can also be found in the\u00a0Thanos section.</p>"},{"location":"Taikun_CloudWorks_Overview/Metrics/#servicemonitor","title":"ServiceMonitor","text":"<p>The\u00a0<code>ServiceMonitor</code>\u00a0custom resource definition (CRD) allows to declaratively define how a dynamic set of services should be monitored. Which services are selected to be monitored with the desired configuration is defined using label selections. This allows an organization to introduce conventions around how metrics are exposed, and then following these conventions new services are automatically discovered, without the need to reconfigure the system.</p> <p>For Prometheus to monitor any application within Kubernetes an\u00a0<code>Endpoints</code>\u00a0object needs to exist.\u00a0<code>Endpoints</code>\u00a0objects are essentially lists of IP addresses. Typically an\u00a0<code>Endpoints</code>\u00a0object is populated by a\u00a0<code>Service</code>\u00a0object. A\u00a0<code>Service</code>\u00a0object discovers\u00a0<code>Pod</code>s by a label selector and adds those to the\u00a0<code>Endpoints</code>\u00a0object.</p> <p>A\u00a0<code>Service</code>\u00a0may expose one or more service ports, which are backed by a list of multiple endpoints that point to a\u00a0<code>Pod</code>\u00a0in the common case. This is reflected in the respective\u00a0<code>Endpoints</code>\u00a0object as well.</p> <p>The\u00a0<code>ServiceMonitor</code>\u00a0object introduced by the Prometheus Operator in turn discovers those\u00a0<code>Endpoints</code>\u00a0objects and configures Prometheus to monitor those\u00a0<code>Pod</code>s.</p> <p>The\u00a0<code>endpoints</code>\u00a0section of the\u00a0<code>ServiceMonitorSpec</code>, is used to configure which ports of these\u00a0<code>Endpoints</code>\u00a0are going to be scraped for metrics, and with which parameters. For advanced use cases one may want to monitor ports of backing\u00a0<code>Pod</code>s, which are not directly part of the service endpoints. Therefore when specifying an endpoint in the\u00a0<code>endpoints</code>\u00a0section, they are strictly used.</p> <p>Note:\u00a0<code>endpoints</code>\u00a0(lowercase) is the field in the\u00a0<code>ServiceMonitor</code>\u00a0CRD, while\u00a0<code>Endpoints</code>\u00a0(capitalized) is the Kubernetes object kind.</p> <p>Both\u00a0<code>ServiceMonitors</code>\u00a0as well as discovered targets may come from any namespace. This is important to allow cross-namespace monitoring use cases, e.g. for meta-monitoring. Using the\u00a0<code>ServiceMonitorNamespaceSelector</code>\u00a0of the\u00a0<code>PrometheusSpec</code>, one can restrict the namespaces\u00a0<code>ServiceMonitor</code>s are selected from by the respective Prometheus server. Using the\u00a0<code>namespaceSelector</code>\u00a0of the\u00a0<code>ServiceMonitorSpec</code>, one can restrict the namespaces the\u00a0<code>Endpoints</code>\u00a0objects are allowed to be discovered from.</p> <p>One can discover targets in all namespaces like this:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: example-app\nspec:\n  selector:\n    matchLabels:\n      app: example-app\n  endpoints:\n  - port: web\n  namespaceSelector:\n    any: true\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Metrics/#podmonitor","title":"PodMonitor","text":"<p>The\u00a0<code>PodMonitor</code>\u00a0custom resource definition (CRD) allows to declaratively define how a dynamic set of pods should be monitored. Which pods are selected to be monitored with the desired configuration is defined using label selections. This allows an organization to introduce conventions around how metrics are exposed, and then following these conventions new pods are automatically discovered, without the need to reconfigure the system.</p> <p>A\u00a0<code>Pod</code>\u00a0is a collection of one or more containers which can expose Prometheus metrics on a number of ports.</p> <p>The\u00a0<code>PodMonitor</code>\u00a0object introduced by the Prometheus Operator discovers these pods and generates the relevant configuration for the Prometheus server in order to monitor them.</p> <p>The\u00a0<code>PodMetricsEndpoints</code>\u00a0section of the\u00a0<code>PodMonitorSpec</code>, is used to configure which ports of a pod are going to be scraped for metrics, and with which parameters.</p> <p>Both\u00a0<code>PodMonitors</code>\u00a0as well as discovered targets may come from any namespace. This is important to allow cross-namespace monitoring use cases, e.g. for meta-monitoring. Using the\u00a0<code>namespaceSelector</code>\u00a0of the\u00a0<code>PodMonitorSpec</code>, one can restrict the namespaces the\u00a0<code>Pods</code>\u00a0are allowed to be discovered from.</p> <p>Once can discover targets in all namespaces like this:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: example-app\nspec:\n  selector:\n    matchLabels:\n      app: example-app\n  podMetricsEndpoints:\n  - port: web\n  namespaceSelector:\n    any: true\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Metrics/#probe","title":"Probe","text":"<p>The\u00a0<code>Probe</code>\u00a0custom resource definition (CRD) allows to declarative define how groups of ingresses and static targets should be monitored. Besides the target, the\u00a0<code>Probe</code>\u00a0object requires a\u00a0<code>prober</code>\u00a0which is the service that monitors the target and provides metrics for Prometheus to scrape. Typically, this is achieved using the\u00a0blackbox exporter.</p>"},{"location":"Taikun_CloudWorks_Overview/Metrics/#prometheusrule","title":"PrometheusRule","text":"<p>The\u00a0<code>PrometheusRule</code>\u00a0custom resource definition (CRD) declaratively defines desired Prometheus rules to be consumed by Prometheus or Thanos Ruler instances.</p> <p>Alerts and recording rules are reconciled by the Operator and dynamically loaded without requiring any restart of Prometheus/Thanos Ruler.</p>"},{"location":"Taikun_CloudWorks_Overview/Metrics/#alertmanagerconfig","title":"AlertmanagerConfig","text":"<p>The\u00a0<code>AlertmanagerConfig</code>\u00a0custom resource definition (CRD) declaratively specifies subsections of the Alertmanager configuration, allowing routing of alerts to custom receivers, and setting inhibition rules. The\u00a0<code>AlertmanagerConfig</code>\u00a0can be defined on a namespace level providing an aggregated configuration to Alertmanager. An example on how to use it is provided below. Please be aware that this CRD is not stable yet.</p> <pre><code>apiVersion: monitoring.coreos.com/v1alpha1\nkind: AlertmanagerConfig\nmetadata:\n  name: config-example\n  labels:\n    alertmanagerConfig: example\nspec:\n  route:\n    groupBy: ['job']\n    groupWait: 30s\n    groupInterval: 5m\n    repeatInterval: 12h\n    receiver: 'webhook'\n  receivers:\n  - name: 'webhook'\n    webhookConfigs:\n    - url: 'http://example.com/'\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Metrics/#prometheusagent","title":"PrometheusAgent","text":"<p>The\u00a0<code>PrometheusAgent</code>\u00a0custom resource definition (CRD) declaratively defines a desired\u00a0Prometheus Agent\u00a0setup to run in a Kubernetes cluster.</p> <p>Similar to the binaries of Prometheus Server and Prometheus Agent, the\u00a0<code>Prometheus</code>\u00a0and\u00a0<code>PrometheusAgent</code>\u00a0CRs are also similar. Inspired in the Agent binary, the Agent CR has several configuration options redacted when compared with regular Prometheus CR, e.g. alerting, PrometheusRules selectors, remote-read, storage and thanos sidecars.</p> <p>A more extensive read explaining why Agent support was done with a whole new CRD can be seen\u00a0here.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Plugins/","title":"Network Plugins","text":"<p>Kubernetes 1.29 supports\u00a0Container Network Interface\u00a0(CNI) plugins for cluster networking. You must use a CNI plugin that is compatible with your cluster and that suits your needs. Different plugins are available (both open- and closed- source) in the wider Kubernetes ecosystem.</p> <p>A CNI plugin is required to implement the\u00a0Kubernetes network model.</p> <p>You must use a CNI plugin that is compatible with the\u00a0v0.4.0\u00a0or later releases of the CNI specification. The Kubernetes project recommends using a plugin that is compatible with the\u00a0v1.0.0\u00a0CNI specification (plugins can be compatible with multiple spec versions).</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Plugins/#installation","title":"Installation","text":"<p>A Container Runtime, in the networking context, is a daemon on a node configured to provide CRI Services for kubelet. In particular, the Container Runtime must be configured to load the CNI plugins required to implement the Kubernetes network model.</p> <p>Note:</p> <p>Prior to Kubernetes 1.24, the CNI plugins could also be managed by the kubelet using the\u00a0cni-bin-dir\u00a0and\u00a0network-plugin\u00a0command-line parameters. These command-line parameters were removed in Kubernetes 1.24, with management of the CNI no longer in scope for kubelet.</p> <p>See\u00a0Troubleshooting CNI plugin-related errors\u00a0if you are facing issues following the removal of dockershim.</p> <p>For specific information about how a Container Runtime manages the CNI plugins, see the documentation for that Container Runtime, for example:</p> <ul> <li> <p>containerd</p> </li> <li> <p>CRI-O</p> </li> </ul> <p>For specific information about how to install and manage a CNI plugin, see the documentation for that plugin or\u00a0networking provider.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Plugins/#network-plugin-requirements","title":"Network Plugin Requirements","text":"<p>For plugin developers and users who regularly build or deploy Kubernetes, the plugin may also need specific configuration to support kube-proxy. The iptables proxy depends on iptables, and the plugin may need to ensure that container traffic is made available to iptables. For example, if the plugin connects containers to a Linux bridge, the plugin must set the\u00a0net/bridge/bridge-nf-call-iptables\u00a0sysctl to\u00a01\u00a0to ensure that the iptables proxy functions correctly. If the plugin does not use a Linux bridge, but uses something like Open vSwitch or some other mechanism instead, it should ensure container traffic is appropriately routed for the proxy.</p> <p>By default, if no kubelet network plugin is specified, the\u00a0noop\u00a0plugin is used, which sets\u00a0net/bridge/bridge-nf-call-iptables=1\u00a0to ensure simple configurations (like Docker with a bridge) work correctly with the iptables proxy.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Plugins/#loopback-cni","title":"Loopback CNI","text":"<p>In addition to the CNI plugin installed on the nodes for implementing the Kubernetes network model, Kubernetes also requires the container runtimes to provide a loopback interface\u00a0lo, which is used for each sandbox (pod sandboxes, vm sandboxes, \u2026). Implementing the loopback interface can be accomplished by re-using the\u00a0CNI loopback plugin.\u00a0or by developing your own code to achieve this (see\u00a0this example from CRI-O).</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Plugins/#support-hostport","title":"Support hostPort","text":"<p>The CNI networking plugin supports\u00a0hostPort. You can use the official\u00a0portmap\u00a0plugin offered by the CNI plugin team or use your own plugin with portMapping functionality.</p> <p>If you want to enable\u00a0hostPort\u00a0support, you must specify\u00a0portMappings capability\u00a0in your\u00a0cni-conf-dir. For example:</p> <pre><code>{\n  \"name\": \"k8s-pod-network\",\n  \"cniVersion\": \"0.4.0\",\n  \"plugins\": [\n    {\n      \"type\": \"calico\",\n      \"log_level\": \"info\",\n      \"datastore_type\": \"kubernetes\",\n      \"nodename\": \"127.0.0.1\",\n      \"ipam\": {\n        \"type\": \"host-local\",\n        \"subnet\": \"usePodCidr\"\n      },\n      \"policy\": {\n        \"type\": \"k8s\"\n      },\n      \"kubernetes\": {\n        \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\"\n      }\n    },\n    {\n      \"type\": \"portmap\",\n      \"capabilities\": {\"portMappings\": true},\n      \"externalSetMarkChain\": \"KUBE-MARK-MASQ\"\n    }\n  ]\n}\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Network_Plugins/#support-traffic-shaping","title":"Support traffic shaping","text":"<p>Experimental Feature</p> <p>The CNI networking plugin also supports pod ingress and egress traffic shaping. You can use the official\u00a0bandwidth\u00a0plugin offered by the CNI plugin team or use your own plugin with bandwidth control functionality.</p> <p>If you want to enable traffic shaping support, you must add the\u00a0bandwidth\u00a0plugin to your CNI configuration file (default\u00a0/etc/cni/net.d) and ensure that the binary is included in your CNI bin dir (default\u00a0/opt/cni/bin).</p> <pre><code>{\n  \"name\": \"k8s-pod-network\",\n  \"cniVersion\": \"0.4.0\",\n  \"plugins\": [\n    {\n      \"type\": \"calico\",\n      \"log_level\": \"info\",\n      \"datastore_type\": \"kubernetes\",\n      \"nodename\": \"127.0.0.1\",\n      \"ipam\": {\n        \"type\": \"host-local\",\n        \"subnet\": \"usePodCidr\"\n      },\n      \"policy\": {\n        \"type\": \"k8s\"\n      },\n      \"kubernetes\": {\n        \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\"\n      }\n    },\n    {\n      \"type\": \"bandwidth\",\n      \"capabilities\": {\"bandwidth\": true}\n    }\n  ]\n}\n</code></pre> <p>Now you can add the\u00a0kubernetes.io/ingress-bandwidth\u00a0and\u00a0kubernetes.io/egress-bandwidth\u00a0annotations to your Pod. For example:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubernetes.io/ingress-bandwidth: 1M\n    kubernetes.io/egress-bandwidth: 1M\n...\n</code></pre>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/","title":"Network Policies","text":"<p>If you want to control traffic flow at the IP address or port level for TCP, UDP, and SCTP protocols, then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster. NetworkPolicies are an application-centric construct which allow you to specify how a\u00a0pod\u00a0is allowed to communicate with various network \u201centities\u201d (we use the word \u201centity\u201d here to avoid overloading the more common terms such as \u201cendpoints\u201d and \u201cservices\u201d, which have specific Kubernetes connotations) over the network. NetworkPolicies apply to a connection with a pod on one or both ends, and are not relevant to other connections.</p> <p>The entities that a Pod can communicate with are identified through a combination of the following 3 identifiers:</p> <ol> <li> <p>Other pods that are allowed (exception: a pod cannot block access to itself)</p> </li> <li> <p>Namespaces that are allowed</p> </li> <li> <p>IP blocks (exception: traffic to and from the node where a Pod is running is always allowed, regardless of the IP address of the Pod or the node)</p> </li> </ol> <p>When defining a pod- or namespace- based NetworkPolicy, you use a\u00a0selector\u00a0to specify what traffic is allowed to and from the Pod(s) that match the selector.</p> <p>Meanwhile, when IP based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#prerequisites","title":"Prerequisites","text":"<p>Network policies are implemented by the\u00a0network plugin. To use network policies, you must be using a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#the-two-sorts-of-pod-isolation","title":"The Two Sorts of Pod Isolation","text":"<p>There are two sorts of isolation for a pod: isolation for egress, and isolation for ingress. They concern what connections may be established. \u201cIsolation\u201d here is not absolute, rather it means \u201csome restrictions apply\u201d. The alternative, \u201cnon-isolated for $direction\u201d, means that no restrictions apply in the stated direction. The two sorts of isolation (or not) are declared independently, and are both relevant for a connection from one pod to another.</p> <p>By default, a pod is non-isolated for egress; all outbound connections are allowed. A pod is isolated for egress if there is any NetworkPolicy that both selects the pod and has \u201cEgress\u201d in its\u00a0<code>policyTypes</code>; we say that such a policy applies to the pod for egress. When a pod is isolated for egress, the only allowed connections from the pod are those allowed by the\u00a0<code>egress</code>\u00a0list of some NetworkPolicy that applies to the pod for egress. Reply traffic for those allowed connections will also be implicitly allowed. The effects of those\u00a0<code>egress</code>\u00a0lists combine additively.</p> <p>By default, a pod is non-isolated for ingress; all inbound connections are allowed. A pod is isolated for ingress if there is any NetworkPolicy that both selects the pod and has \u201cIngress\u201d in its\u00a0<code>policyTypes</code>; we say that such a policy applies to the pod for ingress. When a pod is isolated for ingress, the only allowed connections into the pod are those from the pod\u2019s node and those allowed by the\u00a0<code>ingress</code>\u00a0list of some NetworkPolicy that applies to the pod for ingress. Reply traffic for those allowed connections will also be implicitly allowed. The effects of those\u00a0<code>ingress</code>\u00a0lists combine additively.</p> <p>Network policies do not conflict; they are additive. If any policy or policies apply to a given pod for a given direction, the connections allowed in that direction from that pod is the union of what the applicable policies allow. Thus, order of evaluation does not affect the policy result.</p> <p>For a connection from a source pod to a destination pod to be allowed, both the egress policy on the source pod and the ingress policy on the destination pod need to allow the connection. If either side does not allow the connection, it will not happen.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#the-networkpolicy-resource","title":"The NetworkPolicy resource","text":"<p>See the\u00a0NetworkPolicy\u00a0reference for a full definition of the resource.</p> <p>An example NetworkPolicy might look like this:</p> <p>service/networking/networkpolicy.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: test-network-policy\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - ipBlock:\n        cidr: 172.17.0.0/16\n        except:\n        - 172.17.1.0/24\n    - namespaceSelector:\n        matchLabels:\n          project: myproject\n    - podSelector:\n        matchLabels:\n          role: frontend\n    ports:\n    - protocol: TCP\n      port: 6379\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.0/24\n    ports:\n    - protocol: TCP\n      port: 5978\n</code></pre> <p>Note:\u00a0POSTing this to the API server for your cluster will have no effect unless your chosen networking solution supports network policy.</p> <p>Mandatory Fields: As with all other Kubernetes config, a NetworkPolicy needs\u00a0<code>apiVersion</code>,\u00a0<code>kind</code>, and\u00a0<code>metadata</code>\u00a0fields. For general information about working with config files, see\u00a0Configure a Pod to Use a ConfigMap, and\u00a0Object Management.</p> <p>spec: NetworkPolicy\u00a0spec\u00a0has all the information needed to define a particular network policy in the given namespace.</p> <p>podSelector: Each NetworkPolicy includes a\u00a0<code>podSelector</code>\u00a0which selects the grouping of pods to which the policy applies. The example policy selects pods with the label \u201crole=db\u201d. An empty\u00a0<code>podSelector</code>\u00a0selects all pods in the namespace.</p> <p>policyTypes: Each NetworkPolicy includes a\u00a0<code>policyTypes</code>\u00a0list which may include either\u00a0<code>Ingress</code>,\u00a0<code>Egress</code>, or both. The\u00a0<code>policyTypes</code>\u00a0field indicates whether or not the given policy applies to ingress traffic to selected pod, egress traffic from selected pods, or both. If no\u00a0<code>policyTypes</code>\u00a0are specified on a NetworkPolicy then by default\u00a0<code>Ingress</code>\u00a0will always be set and\u00a0<code>Egress</code>\u00a0will be set if the NetworkPolicy has any egress rules.</p> <p>ingress: Each NetworkPolicy may include a list of allowed\u00a0<code>ingress</code>\u00a0rules. Each rule allows traffic which matches both the\u00a0<code>from</code>\u00a0and\u00a0<code>ports</code>\u00a0sections. The example policy contains a single rule, which matches traffic on a single port, from one of three sources, the first specified via an\u00a0<code>ipBlock</code>, the second via a\u00a0<code>namespaceSelector</code>\u00a0and the third via a\u00a0<code>podSelector</code>.</p> <p>egress: Each NetworkPolicy may include a list of allowed\u00a0<code>egress</code>\u00a0rules. Each rule allows traffic which matches both the\u00a0<code>to</code>\u00a0and\u00a0<code>ports</code>\u00a0sections. The example policy contains a single rule, which matches traffic on a single port to any destination in\u00a0<code>10.0.0.0/24</code>.</p> <p>So, the example NetworkPolicy:</p> <ol> <li> <p>isolates\u00a0<code>role=db</code>\u00a0pods in the\u00a0<code>default</code>\u00a0namespace for both ingress and egress traffic (if they weren\u2019t already isolated)</p> </li> <li> <p>(Ingress rules) allows connections to all pods in the\u00a0<code>default</code>\u00a0namespace with the label\u00a0<code>role=db</code>\u00a0on TCP port 6379 from:</p> </li> <li> <p>any pod in the\u00a0<code>default</code>\u00a0namespace with the label\u00a0<code>role=frontend</code></p> </li> <li> <p>any pod in a namespace with the label\u00a0<code>project=myproject</code></p> </li> <li> <p>IP addresses in the ranges\u00a0<code>172.17.0.0</code>\u2013<code>172.17.0.255</code>\u00a0and\u00a0<code>172.17.2.0</code>\u2013<code>172.17.255.255</code>\u00a0(ie, all of\u00a0<code>172.17.0.0/16</code>\u00a0except\u00a0<code>172.17.1.0/24</code>)</p> </li> <li> <p>(Egress rules) allows connections from any pod in the\u00a0<code>default</code>\u00a0namespace with the label\u00a0<code>role=db</code>\u00a0to CIDR\u00a0<code>10.0.0.0/24</code>\u00a0on TCP port 5978</p> </li> </ol> <p>See the\u00a0Declare Network Policy\u00a0walkthrough for further examples.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#behavior-of-to-and-from-selectors","title":"Behavior of\u00a0to\u00a0and\u00a0from\u00a0selectors","text":"<p>There are four kinds of selectors that can be specified in an\u00a0<code>ingress</code> <code>from</code>\u00a0section or\u00a0<code>egress</code> <code>to</code>\u00a0section:</p> <p>podSelector: This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources or egress destinations.</p> <p>namespaceSelector: This selects particular namespaces for which all Pods should be allowed as ingress sources or egress destinations.</p> <p>namespaceSelector and podSelector: A single\u00a0<code>to</code>/<code>from</code>\u00a0entry that specifies both\u00a0<code>namespaceSelector</code>\u00a0and\u00a0<code>podSelector</code>\u00a0selects particular Pods within particular namespaces. Be careful to use correct YAML syntax. For example:</p> <pre><code>...\n ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          user: alice\n      podSelector:\n        matchLabels:\n          role: client\n...\n</code></pre> <p>This policy contains a single\u00a0<code>from</code>\u00a0element allowing connections from Pods with the label\u00a0<code>role=client</code>\u00a0in namespaces with the label\u00a0<code>user=alice</code>. But the following policy is different:</p> <pre><code>  ...\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          user: alice\n    - podSelector:\n        matchLabels:\n          role: client\n  ...\n</code></pre> <p>It contains two elements in the\u00a0<code>from</code>\u00a0array, and allows connections from Pods in the local Namespace with the label\u00a0<code>role=client</code>,\u00a0or\u00a0from any Pod in any namespace with the label\u00a0<code>user=alice</code>.</p> <p>When in doubt, use\u00a0<code>kubectl describe</code>\u00a0to see how Kubernetes has interpreted the policy.</p> <p>ipBlock: This selects particular IP CIDR ranges to allow as ingress sources or egress destinations. These should be cluster-external IPs, since Pod IPs are ephemeral and unpredictable.</p> <p>Cluster ingress and egress mechanisms often require rewriting the source or destination IP of packets. In cases where this happens, it is not defined whether this happens before or after NetworkPolicy processing, and the behavior may be different for different combinations of network plugin, cloud provider,\u00a0<code>Service</code>\u00a0implementation, etc.</p> <p>In the case of ingress, this means that in some cases you may be able to filter incoming packets based on the actual original source IP, while in other cases, the \u201csource IP\u201d that the NetworkPolicy acts on may be the IP of a\u00a0<code>LoadBalancer</code>\u00a0or of the Pod\u2019s node, etc.</p> <p>For egress, this means that connections from pods to\u00a0<code>Service</code>\u00a0IPs that get rewritten to cluster-external IPs may or may not be subject to\u00a0<code>ipBlock</code>-based policies.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#default-policies","title":"Default policies","text":"<p>By default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to and from pods in that namespace. The following examples let you change the default behavior in that namespace.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#default-deny-all-ingress-traffic","title":"Default deny all ingress traffic","text":"<p>You can create a \u201cdefault\u201d ingress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any ingress traffic to those pods.</p> <p>service/networking/network-policy-default-deny-ingress.yaml</p> <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n</code></pre> <p>This ensures that even pods that aren\u2019t selected by any other NetworkPolicy will still be isolated for ingress. This policy does not affect isolation for egress from any pod.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#allow-all-ingress-traffic","title":"Allow all ingress traffic","text":"<p>If you want to allow all incoming connections to all pods in a namespace, you can create a policy that explicitly allows that.</p> <p>service/networking/network-policy-allow-all-ingress.yaml</p> <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-all-ingress\nspec:\n  podSelector: {}\n  ingress:\n  - {}\n  policyTypes:\n  - Ingress\n</code></pre> <p>With this policy in place, no additional policy or policies can cause any incoming connection to those pods to be denied. This policy has no effect on isolation for egress from any pod.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#default-deny-all-egress-traffic","title":"Default deny all egress traffic","text":"<p>You can create a \u201cdefault\u201d egress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any egress traffic from those pods.</p> <p><code>service/networking/network-policy-default-deny-egress.yaml</code> </p> <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-egress\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n</code></pre> <p>This ensures that even pods that aren\u2019t selected by any other NetworkPolicy will not be allowed egress traffic. This policy does not change the ingress isolation behavior of any pod.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#allow-all-egress-traffic","title":"Allow all egress traffic","text":"<p>If you want to allow all connections from all pods in a namespace, you can create a policy that explicitly allows all outgoing connections from pods in that namespace.</p> <p>service/networking/network-policy-allow-all-egress.yaml</p> <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-all-egress\nspec:\n  podSelector: {}\n  egress:\n  - {}\n  policyTypes:\n  - Egress\n</code></pre> <p>With this policy in place, no additional policy or policies can cause any outgoing connection from those pods to be denied. This policy has no effect on isolation for ingress to any pod.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#default-deny-all-ingress-and-all-egress-traffic","title":"Default deny all ingress and all egress traffic","text":"<p>You can create a \u201cdefault\u201d policy for a namespace which prevents all ingress AND egress traffic by creating the following NetworkPolicy in that namespace.</p> <p>service/networking/network-policy-default-deny-all.yaml</p> <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre> <p>This ensures that even pods that aren\u2019t selected by any other NetworkPolicy will not be allowed ingress or egress traffic.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#network-traffic-filtering","title":"Network traffic filtering","text":"<p>NetworkPolicy is defined for\u00a0layer 4\u00a0connections (TCP, UDP, and optionally SCTP). For all the other protocols, the behaviour may vary across network plugins.</p> <p>Note:\u00a0You must be using a\u00a0CNI\u00a0plugin that supports SCTP protocol NetworkPolicies.</p> <p>When a\u00a0<code>deny all</code>\u00a0network policy is defined, it is only guaranteed to deny TCP, UDP and SCTP connections. For other protocols, such as ARP or ICMP, the behaviour is undefined. The same applies to allow rules: when a specific pod is allowed as ingress source or egress destination, it is undefined what happens with (for example) ICMP packets. Protocols such as ICMP may be allowed by some network plugins and denied by others.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#targeting-a-range-of-ports","title":"Targeting a range of ports","text":"<p>FEATURE STATE:\u00a0Kubernetes v1.25 [stable]</p> <p>When writing a NetworkPolicy, you can target a range of ports instead of a single port.</p> <p>This is achievable with the usage of the\u00a0<code>endPort</code>\u00a0field, as the following example:</p> <p>service/networking/networkpolicy-multiport-egress.yaml</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: multi-port-egress\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n    - Egress\n  egress:\n    - to:\n        - ipBlock:\n            cidr: 10.0.0.0/24\n      ports:\n        - protocol: TCP\n          port: 32000\n          endPort: 32768\n</code></pre> <p>The above rule allows any Pod with label\u00a0<code>role=db</code>\u00a0on the namespace\u00a0<code>default</code>\u00a0to communicate with any IP within the range\u00a0<code>10.0.0.0/24</code>\u00a0over TCP, provided that the target port is between the range 32000 and 32768.</p> <p>The following restrictions apply when using this field:</p> <ul> <li> <p>The\u00a0<code>endPort</code>\u00a0field must be equal to or greater than the\u00a0<code>port</code>\u00a0field.</p> </li> <li> <p><code>endPort</code>\u00a0can only be defined if\u00a0<code>port</code>\u00a0is also defined.</p> </li> <li> <p>Both ports must be numeric.</p> </li> </ul> <p>Note:\u00a0Your cluster must be using a\u00a0CNI\u00a0plugin that supports the\u00a0<code>endPort</code>\u00a0field in NetworkPolicy specifications. If your\u00a0network plugin\u00a0does not support the\u00a0<code>endPort</code>\u00a0field and you specify a NetworkPolicy with that, the policy will be applied only for the single\u00a0<code>port</code>\u00a0field.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#targeting-multiple-namespaces-by-label","title":"Targeting multiple namespaces by label","text":"<p>In this scenario, your\u00a0<code>Egress</code>\u00a0NetworkPolicy targets more than one namespace using their label names. For this to work, you need to label the target namespaces. For example:</p> <p>kubectl label namespace frontend namespace=frontend kubectl label namespace backend namespace=backend</p> <p>Add the labels under\u00a0namespaceSelector\u00a0in your NetworkPolicy document. For example:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: egress-namespaces\nspec:\n  podSelector:\n    matchLabels:\n      app: myapp\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchExpressions:\n        - key: namespace\n          operator: In\n          values: [\"frontend\", \"backend\"]\n</code></pre> <p>Note:\u00a0It is not possible to directly specify the name of the namespaces in a NetworkPolicy. You must use a\u00a0<code>namespaceSelector</code>\u00a0with\u00a0<code>matchLabels</code>\u00a0or\u00a0<code>matchExpressions</code>\u00a0to select the namespaces based on their labels.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#targeting-a-namespace-by-its-name","title":"Targeting a Namespace by its name","text":"<p>The Kubernetes control plane sets an immutable label\u00a0kubernetes.io/metadata.name\u00a0on all namespaces, the value of the label is the namespace name.</p> <p>While NetworkPolicy cannot target a namespace by its name with some object field, you can use the standardized label to target a specific namespace.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#pod-lifecycle","title":"Pod lifecycle","text":"<p>Note:\u00a0The following applies to clusters with a conformant networking plugin and a conformant implementation of NetworkPolicy.</p> <p>When a new NetworkPolicy object is created, it may take some time for a network plugin to handle the new object. If a pod that is affected by a NetworkPolicy is created before the network plugin has completed NetworkPolicy handling, that pod may be started unprotected, and isolation rules will be applied when the NetworkPolicy handling is completed.</p> <p>Once the NetworkPolicy is handled by a network plugin,</p> <ol> <li> <p>All newly created pods affected by a given NetworkPolicy will be isolated before they are started. Implementations of NetworkPolicy must ensure that filtering is effective throughout the Pod lifecycle, even from the very first instant that any container in that Pod is started. Because they are applied at Pod level, NetworkPolicies apply equally to init containers, sidecar containers, and regular containers.</p> </li> <li> <p>Allow rules will be applied eventually after the isolation rules (or may be applied at the same time). In the worst case, a newly created pod may have no network connectivity at all when it is first started, if isolation rules were already applied, but no allow rules were applied yet.</p> </li> </ol> <p>Every created NetworkPolicy will be handled by a network plugin eventually, but there is no way to tell from the Kubernetes API when exactly that happens.</p> <p>Therefore, pods must be resilient against being started up with different network connectivity than expected. If you need to make sure the pod can reach certain destinations before being started, you can use an\u00a0init container\u00a0to wait for those destinations to be reachable before kubelet starts the app containers.</p> <p>Every NetworkPolicy will be applied to all selected pods eventually. Because the network plugin may implement NetworkPolicy in a distributed manner, it is possible that pods may see a slightly inconsistent view of network policies when the pod is first created, or when pods or policies change. For example, a newly-created pod that is supposed to be able to reach both Pod A on Node 1 and Pod B on Node 2 may find that it can reach Pod A immediately, but cannot reach Pod B until a few seconds later.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#networkpolicy-and-hostnetwork-pods","title":"NetworkPolicy and\u00a0hostNetwork\u00a0pods","text":"<p>NetworkPolicy behaviour for\u00a0hostNetwork\u00a0pods is undefined, but it should be limited to 2 possibilities:</p> <ul> <li> <p>The network plugin can distinguish\u00a0hostNetwork\u00a0pod traffic from all other traffic (including being able to distinguish traffic from different\u00a0<code>hostNetwork</code>\u00a0pods on the same node), and will apply NetworkPolicy to\u00a0<code>hostNetwork</code>\u00a0pods just like it does to pod-network pods.</p> </li> <li> <p>The network plugin cannot properly distinguish\u00a0hostNetwork\u00a0pod traffic, and so it ignores\u00a0hostNetwork\u00a0pods when matching\u00a0podSelector\u00a0and\u00a0namespaceSelector. Traffic to/from\u00a0hostNetwork\u00a0pods is treated the same as all other traffic to/from the node IP. (This is the most common implementation.)</p> </li> </ul> <p>This applies when</p> <ol> <li> <p>a hostNetwork pod is selected by spec.podSelector. ... spec: podSelector: matchLabels: role: client ...</p> </li> <li> <p>a hostNetwork pod is selected by a podSelector or namespaceSelector in an ingress or egress rule. ... ingress: - from: - podSelector: matchLabels: role: client ...    At the same time, since hostNetwork pods have the same IP addresses as the nodes they reside on, their connections will be treated as node connections. For example, you can allow traffic from a hostNetwork Pod using an ipBlock rule.</p> </li> </ol> <p>At the same time, since\u00a0<code>hostNetwork</code>\u00a0pods have the same IP addresses as the nodes they reside on, their connections will be treated as node connections. For example, you can allow traffic from a\u00a0<code>hostNetwork</code>\u00a0Pod using an\u00a0<code>ipBlock</code>\u00a0rule.</p>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#what-you-cant-do-with-network-policies-at-least-not-yet","title":"What you can\u2019t do with network policies (at least, not yet)","text":"<p>As of Kubernetes 1.29, the following functionality does not exist in the NetworkPolicy API, but you might be able to implement workarounds using Operating System components (such as SELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingress controllers, Service Mesh implementations) or admission controllers. In case you are new to network security in Kubernetes, its worth noting that the following User Stories cannot (yet) be implemented using the NetworkPolicy API.</p> <ul> <li> <p>Forcing internal cluster traffic to go through a common gateway (this might be best served with a service mesh or other proxy).</p> </li> <li> <p>Anything TLS related (use a service mesh or ingress controller for this).</p> </li> <li> <p>Node specific policies (you can use CIDR notation for these, but you cannot target nodes by their Kubernetes identities specifically).</p> </li> <li> <p>Targeting of services by name (you can, however, target pods or namespaces by their\u00a0labels, which is often a viable workaround).</p> </li> <li> <p>Creation or management of \u201cPolicy requests\u201d that are fulfilled by a third party.</p> </li> <li> <p>Default policies which are applied to all namespaces or pods (there are some third party Kubernetes distributions and projects which can do this).</p> </li> <li> <p>Advanced policy querying and reachability tooling.</p> </li> <li> <p>The ability to log network security events (for example connections that are blocked or accepted).</p> </li> <li> <p>The ability to explicitly deny policies (currently the model for NetworkPolicies are deny by default, with only the ability to add allow rules).</p> </li> <li> <p>The ability to prevent loopback or incoming host traffic (Pods cannot currently block localhost access, nor do they have the ability to block access from their resident node).</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Network_Policies/#networkpolicys-impact-on-existing-connections","title":"NetworkPolicy\u2019s impact on existing connections","text":"<p>When the set of NetworkPolicies that applies to an existing connection changes \u2013 this could happen either due to a change in NetworkPolicies or if the relevant labels of the namespaces/pods selected by the policy (both subject and peers) are changed in the middle of an existing connection \u2013 it is implementation defined as to whether the change will take effect for that existing connection or not. Example: A policy is created that leads to denying a previously allowed connection, the underlying network plugin implementation is responsible for defining if that new policy will close the existing connections or not. It is recommended not to modify policies/pods/namespaces in ways that might affect existing connections.</p>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/","title":"High Availability and Resilient Deployment Options","text":"<p>We recognize that Sonatype Nexus Repository is often mission-critical for your business. Be ready for the unexpected by using a\u00a0high availability (HA) or resilient deployment option\u00a0that can protect your business and your data in the event of disaster or outages. With comprehensive\u00a0backup and restoration tasks\u00a0and a growing list of deployment architecture examples and guidance, Nexus Repository Pro can help you reduce your Recovery Time Objectives and Recovery Point Objectives.</p>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#external-postgresql-database-option","title":"External PostgreSQL Database Option","text":"<p>Sonatype Nexus Repository Pro allows you to use an external PostgreSQL database. By externalizing your database, you can take advantage of a number of benefits:</p> <ul> <li> <p>Performance and scalability improvements</p> </li> <li> <p>Leverage the benefits of managed, fault-tolerant cloud databases (e.g., AWS Aurora, RDS, and Azure)</p> </li> <li> <p>Improved compatibility with container orchestration (e.g., Kubernetes and OpenShift)</p> </li> <li> <p>Full availability for writes during backups</p> </li> <li> <p>Fault-tolerant cloud deployments with multi-Availability Zone cloud deployment models</p> </li> <li> <p>Simpler and easier disaster recovery procedures</p> </li> </ul>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#saml-authentication-and-single-sign-on","title":"SAML Authentication and Single Sign-On","text":"<p>Sonatype Nexus Repository Pro integrates with SAML Identity Providers to allow identity, authentication, and authorization to be managed centrally. Using SAML, Sonatype Nexus Repository acts as a service provider which receives users\u2019 authentication and authorization information from external Identity Providers. Administrators can manage users and roles in one place, and users can sign in with Single Sign-On credentials.</p>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#user-token-support","title":"User Token Support","text":"<p>When using Apache Maven with Sonatype Nexus Repository Pro, the user credentials for accessing the repository manager have to be stored in the user\u2019s\u00a0<code>settings.xml</code>\u00a0file. Like a\u00a0<code>pom.xml</code>\u00a0your\u00a0<code>settings.xml</code>\u00a0is file that contains your user preferences. The Maven framework has the ability to encrypt passwords within the\u00a0<code>settings.xml</code>, but the need for it to be reversible in order to be used limits its security.</p> <p>Other build systems use similar approaches and can benefit from the usage of user tokens as well. Sonatype Nexus Repository Pro\u2019s user token feature establishes a two-part token for the user. Usage of the token acts as a substitute method for authentication that would normally require passing your username and password in plain text.</p> <p>This is especially useful for scenarios where single sign-on solutions like LDAP are used for authentication against the repository manager and other systems and the plain text username and password cannot be stored in the\u00a0<code>settings.xml</code>\u00a0following security policies. In this scenario the generated user tokens can be used instead.</p>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#content-replication","title":"Content Replication","text":"<p>Teams today rarely work from a single location, so why should artifacts be stuck in one?\u00a0Content replication\u00a0allows you to make artifacts readily available across distributed teams. With content replication, you can manage what binaries are copied from one instance and pre-emptively pulled via HTTP to other instances. Here\u2019s how it works:</p> <ol> <li> <p>New assets are published to the Sonatype Nexus Repository source instance</p> </li> <li> <p>The replication task runs on the target instance on a schedule (each minute) to identify new assets</p> </li> <li> <p>The replication task issues proxy requests for each new asset to replicate them to the target instance</p> </li> <li> <p>Users on the target instance now have access to newly added artifacts on the target instance</p> </li> </ol>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#staging-and-build-promotion","title":"Staging and Build Promotion","text":"<p>In modern software development, it is imperative to thoroughly test software before it is deployed to a production system or externally accessible repository. Mostly commonly a release candidate will first be deployed to a staging system which is a close copy of the production system so it can undergo a series of rigorous tests before a decision is made to promote it to production or return it to development.</p> <p>The staging functionality in Sonatype Nexus Repository Pro supports promotion of software components matching your organization\u2019s software development life cycle phases by moving those components between repositories. This allows the creation of isolated release candidates that can be discarded or promoted to make it possible to support the decisions that go into certifying a release.</p>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#typical-staging-workflow","title":"Typical Staging Workflow","text":""},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#tagging","title":"Tagging","text":"<p>Tagging provides the ability to mark a set of components with a tag so they can be logically associated to each other. The usage of the tags is up to you but the most common scenarios would be a CI build ID for a project (e.g. project-abc-build-142) or a higher level release train when you are coordinating multiple projects together as a single unit (e.g. release-train-13). Tagging is used extensively by the\u00a0Staging\u00a0feature.</p>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#importexport","title":"Import/Export","text":"<p>Import/Export provides the capability to copy components between repositories or Sonatype Nexus Repository instances.</p> <p>Some common use cases to utilize Import/Export include:</p> <ol> <li> <p>Gradual migration of content from Nexus Repository 2</p> </li> <li> <p>Consolidation of Sonatype Nexus Repository 3 instances</p> </li> <li> <p>Transfer components between disconnected Sonatype Nexus Repository instances</p> </li> </ol> <p>These tasks keep track of the last run results and if run again, they will skip files that were processed previously. These tasks work with HA-C setup.</p> <p>A temporary file system location is needed for these tasks to export to and import from. Please also ensure the Sonatype Nexus Repository instance running the task has sufficient disk space for export and blob storage for import before running this task to avoid disk full issues.</p>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#group-blob-stores","title":"Group Blob Stores","text":"<p>A group blob store combines multiple blob stores together so that they act as a single blob store for repositories. Members of the group can be added or removed. Fill Policies are selectable algorithms that determine to which group member a blob is written. These features significantly increase the flexibility customers have in using and upgrading their storage. More information can be found in the\u00a0Storage Guide.</p> <p>Below is an example of a group blob store:</p> <p></p>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#deployment-to-group-repositories","title":"Deployment to Group Repositories","text":"<p>Group deployment allows developers and CI servers to use a single URL, the URL of a group repository, to both push and pull content.</p> <p>Without this feature developers have to use two URLs; one for pushing content, one for pulling content. For some formats, these URLs can\u2019t be saved to configuration and have to be manually entered.</p> <p>When a group repository is being configured an administrator will be able to select a hosted repository that the group can delegate push requests to. When content is pushed the group repository will automatically route that content to the delegated hosted repository.</p>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#docker","title":"Docker","text":"<p>Docker format has several important advantages related to the Group Deployment Feature.</p> <ol> <li> <p>Reduced ports\u00a0\u2013 Docker uses ports rather than the repository URL. This means that each repository, that needs to be accessible from the Docker client, must open up at least one port (2 ports if HTTP and HTTPS are used).</p> </li> <li> <p>Reduced storage\u00a0\u2013 Docker images are made up of multiple layers. Some of those layers will be from public remotes and some will be internally created. When an image is pushed to a repository the Docker client checks to see which layers are already stored and skips those. Before group deployment developers would push to a hosted repository and the Docker client would not find the public layers and would push those as well. With group deployment that will no longer happen and only proprietary layers will be pushed.</p> </li> <li> <p>Simplified client configuration\u00a0\u2013 Docker doesn\u2019t provide a way to specify different endpoints for pushing and pulling content. A developer or someone creating a CI build would have to remember them. Also because Docker relies on ports it is hard to remember which port relates to which repository and these need to be looked up.</p> </li> <li> <p>Simplified reverse proxy configuration\u00a0\u2013 Most Docker users have a reverse proxy between the client and the server. Reducing the number of repositories that need to be accessible makes this setup easier to maintain.</p> </li> </ol>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#repository-health-check-rhc","title":"Repository Health Check (RHC)","text":"<p>Sonatype Nexus Repository users can now automatically identify open source security risks at the earliest stages of their DevOps pipeline. Specifically, the\u00a0RHC feature\u00a0empowers software development teams into important capabilities:</p> <ul> <li> <p>Prioritizes the list of vulnerable components by severity and impact, detailing how many times each component was downloaded from the repository manager by developers in the past 30 days.</p> </li> <li> <p>Provides actionable guidance on which components housed in the repository manager should be upgraded or replaced.</p> </li> </ul> <p></p>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#change-repository-blob-store","title":"Change Repository Blob Store","text":"<p>Change repository blob store is a task that allows changing the blob store of a given repository. It moves the blobs from the chosen repository to a different blob store.</p> <p>Some common use cases to utilize the Change Repository Blob Store task:</p> <ol> <li> <p>A blob store reaching maximum capacity. You could use this task to move repository content freeing up space for other repositories in the original blob store.</p> </li> <li> <p>Decommissioning of a blob store.</p> </li> </ol>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#azure-blob-store","title":"Azure Blob Store","text":"<p>The Azure Blob Store allows assets to be stored in an Azure Storage Account container. Sonatype Nexus Repository Pro can take advantage of the storage features that Azure provides such as replication, configurable performance profiles, and access control when running from within the Azure cloud.</p>"},{"location":"Taikun_CloudWorks_Overview/Nexus_Funcionality/#atlassian-crowd-support","title":"Atlassian Crowd Support","text":"<p>Atlassian Crowd is a single sign-on and identity management product that many organizations use to consolidate user accounts and control which users and groups have access to which applications. Atlassian Crowd support is a feature preinstalled and ready to configure in Sonatype Nexus Repository Pro. Sonatype Nexus Repository contains a security realm that allows you to configure the repository manager to authenticate against an Atlassian Crowd instance.</p>"},{"location":"Taikun_CloudWorks_Overview/Prometheus_for_Taikun_CloudWorks/","title":"Prometheus for Taikun CloudWorks","text":"<p>The\u00a0Alertmanager\u00a0handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integration such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts.</p> <p>The following describes the core concepts the Alertmanager implements. Consult the\u00a0configuration documentation\u00a0to learn how to use them in more detail.</p>"},{"location":"Taikun_CloudWorks_Overview/Prometheus_for_Taikun_CloudWorks/#grouping","title":"Grouping","text":"<p>Grouping categorizes alerts of similar nature into a single notification. This is especially useful during larger outages when many systems fail at once and hundreds to thousands of alerts may be firing simultaneously.</p> <p>Example:\u00a0Dozens or hundreds of instances of a service are running in your cluster when a network partition occurs. Half of your service instances can no longer reach the database. Alerting rules in Prometheus were configured to send an alert for each service instance if it cannot communicate with the database. As a result hundreds of alerts are sent to Alertmanager.</p> <p>As a user, one only wants to get a single page while still being able to see exactly which service instances were affected. Thus one can configure Alertmanager to group alerts by their cluster and alertname so it sends a single compact notification.</p> <p>Grouping of alerts, timing for the grouped notifications, and the receivers of those notifications are configured by a routing tree in the configuration file.</p>"},{"location":"Taikun_CloudWorks_Overview/Prometheus_for_Taikun_CloudWorks/#inhibition","title":"Inhibition","text":"<p>Inhibition is a concept of suppressing notifications for certain alerts if certain other alerts are already firing.</p> <p>Example:\u00a0An alert is firing that informs that an entire cluster is not reachable. Alertmanager can be configured to mute all other alerts concerning this cluster if that particular alert is firing. This prevents notifications for hundreds or thousands of firing alerts that are unrelated to the actual issue.</p> <p>Inhibitions are configured through the Alertmanager\u2019s configuration file.</p>"},{"location":"Taikun_CloudWorks_Overview/Prometheus_for_Taikun_CloudWorks/#silences","title":"Silences","text":"<p>Silences are a straightforward way to simply mute alerts for a given time. A silence is configured based on matchers, just like the routing tree. Incoming alerts are checked whether they match all the equality or regular expression matchers of an active silence. If they do, no notifications will be sent out for that alert.</p> <p>Silences are configured in the web interface of the Alertmanager.</p>"},{"location":"Taikun_CloudWorks_Overview/Prometheus_for_Taikun_CloudWorks/#client-behavior","title":"Client behavior","text":"<p>The Alertmanager has\u00a0special requirements\u00a0for behavior of its client. Those are only relevant for advanced use cases where Prometheus is not used to send alerts.</p>"},{"location":"Taikun_CloudWorks_Overview/Prometheus_for_Taikun_CloudWorks/#high-availability","title":"High Availability","text":"<p>Alertmanager supports configuration to create a cluster for high availability. This can be configured using the\u00a0\u2013cluster-*\u00a0flags.</p> <p>It\u2019s important not to load balance traffic between Prometheus and its Alertmanagers, but instead, point Prometheus to a list of all Alertmanagers.</p>"},{"location":"Taikun_CloudWorks_Overview/Serverless_Kubernetes/","title":"Serverless Kubernetes","text":"<p>Serverless Container Orchestration:\u00a0Taikun CloudWorks facilitates seamless serverless container orchestration, extending Kubernetes capabilities. This feature allows users to deploy containerized applications effortlessly, abstracting away infrastructure concerns. The system intelligently scales instances based on demand, automatically adjusting resources to ensure optimal performance and cost efficiency.</p> <p>Auto-scaling:\u00a0Experience dynamic auto-scaling with Taikun CloudWorks. The platform intelligently adjusts the number of container instances in response to varying workloads. This ensures that your applications have the resources they need during peak times while scaling down to zero during inactivity.</p> <p>Event-Driven Architecture:\u00a0Taikun CloudWorks empowers an event-driven architecture, enabling services to respond to various triggers. Whether it\u2019s HTTP requests, messages from external systems, or changes in data stores, the platform seamlessly integrates with events, enhancing the flexibility and responsiveness of your applications.</p> <p>Build and Serving Components:\u00a0Benefit from the streamlined Build, Eventing, and Serving components within Taikun CloudWorks. The Build component facilitates the effortless creation of container images from source code. Eventing manages the event-driven aspects, while the Serving component takes care of the deployment and scaling of serverless applications, simplifying the development process.</p> <p>Integration with Cloud Services:\u00a0Taikun CloudWorks provides native integration with a range of cloud services. This includes seamless connections to managed databases, storage solutions, and monitoring tools, enhancing the overall functionality and versatility of your serverless Kubernetes applications.</p> <p>Developer-Friendly Workflow:\u00a0Enjoy a developer-friendly experience with Taikun CloudWorks. Whether through an intuitive user interface or powerful command-line tools, the platform ensures that developers can easily deploy, manage, and monitor their serverless applications, streamlining the entire development lifecycle.</p> <p>Monitoring and Logging:\u00a0Gain insights into your applications with built-in monitoring and logging features. Taikun CloudWorks equips users with the tools needed to track performance metrics and analyze logs, facilitating efficient troubleshooting and optimization of serverless applications.</p>"},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/","title":"Services, Load Balancing, and Networking in Kubernetes","text":""},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#the-kubernetes-network-model","title":"The Kubernetes network model","text":"<p>Every\u00a0<code>Pod</code>\u00a0in a cluster gets its own unique cluster-wide IP address. This means you do not need to explicitly create links between\u00a0<code>Pods</code>\u00a0and you almost never need to deal with mapping container ports to host ports. This creates a clean, backwards-compatible model where\u00a0<code>Pods</code>\u00a0can be treated much like VMs or physical hosts from the perspectives of port allocation, naming, service discovery,\u00a0load balancing, application configuration, and migration.</p> <p>Kubernetes imposes the following fundamental requirements on any networking implementation (barring any intentional network segmentation policies):</p> <ul> <li> <p>pods can communicate with all other pods on any other\u00a0node\u00a0without NAT</p> </li> <li> <p>agents on a node (e.g. system daemons, kubelet) can communicate with all pods on that node</p> </li> </ul> <p>Note: For those platforms that support\u00a0<code>Pods</code>\u00a0running in the host network (e.g. Linux), when pods are attached to the host network of a node they can still communicate with all pods on all nodes without NAT.</p> <p>This model is not only less complex overall, but it is principally compatible with the desire for Kubernetes to enable low-friction porting of apps from VMs to containers. If your job previously ran in a VM, your VM had an IP and could talk to other VMs in your project. This is the same basic model.</p> <p>Kubernetes IP addresses exist at the\u00a0<code>Pod</code>\u00a0scope \u2013 containers within a\u00a0<code>Pod</code>\u00a0share their network namespaces \u2013 including their IP address and MAC address. This means that containers within a\u00a0<code>Pod</code>\u00a0can all reach each other\u2019s ports on\u00a0<code>localhost</code>. This also means that containers within a\u00a0<code>Pod</code>\u00a0must coordinate port usage, but this is no different from processes in a VM. This is called the \u201cIP-per-pod\u201d model.</p> <p>How this is implemented is a detail of the particular container runtime in use.</p> <p>It is possible to request ports on the\u00a0<code>Node</code>\u00a0itself which forward to your\u00a0<code>Pod</code>\u00a0(called host ports), but this is a very niche operation. How that forwarding is implemented is also a detail of the container runtime. The\u00a0<code>Pod</code>\u00a0itself is blind to the existence or non-existence of host ports.</p> <p>Kubernetes networking addresses four concerns:</p> <ul> <li> <p>Containers within a Pod\u00a0use networking to communicate\u00a0via loopback.</p> </li> <li> <p>Cluster networking provides communication between different Pods.</p> </li> <li> <p>The\u00a0Service\u00a0API lets you\u00a0expose an application running in Pods\u00a0to be reachable from outside your cluster.</p> </li> <li> <p>Ingress\u00a0provides extra functionality specifically for exposing HTTP applications, websites and APIs.</p> </li> <li> <p>Gateway API\u00a0is an\u00a0add-on\u00a0that provides an expressive, extensible, and role-oriented family of API kinds for modeling service networking.</p> </li> <li> <p>You can also use Services to\u00a0publish services only for consumption inside your cluster.</p> </li> </ul> <p>The\u00a0Connecting Applications with Services\u00a0tutorial lets you learn about Services and Kubernetes networking with a hands-on example.</p> <p>Cluster Networking\u00a0explains how to set up networking for your cluster, and also provides an overview of the technologies involved.</p>"},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#service","title":"Service","text":"<p>Expose an application running in your cluster behind a single outward-facing endpoint, even when the workload is split across multiple backends.</p>"},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#ingress","title":"Ingress","text":"<p>Make your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API.</p>"},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#ingress-controllers","title":"Ingress Controllers","text":"<p>In order for an\u00a0Ingress\u00a0to work in your cluster, there must be an\u00a0ingress controller\u00a0running. You need to select at least one ingress controller and make sure it is set up in your cluster. This page lists common ingress controllers that you can deploy.</p>"},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#gateway-api","title":"Gateway API","text":"<p>Gateway API is a family of API kinds that provide dynamic infrastructure provisioning and advanced traffic routing.</p>"},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#endpointslices","title":"EndpointSlices","text":"<p>The EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to handle large numbers of backends, and allows the cluster to update its list of healthy backends efficiently.</p>"},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#network-policies","title":"Network Policies","text":"<p>If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), NetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between Pods and the outside world. Your cluster must use a network plugin that supports NetworkPolicy enforcement.</p>"},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#dns-for-services-and-pods","title":"DNS for Services and Pods","text":"<p>Your workload can discover Services within your cluster using DNS; this page explains how that works.</p>"},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#ipv4ipv6-dual-stack","title":"IPv4/IPv6 dual-stack","text":"<p>Kubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or dual stack networking with both network families active. This page explains how.</p>"},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#topology-aware-routing","title":"Topology Aware Routing","text":"<p>Topology Aware Routing\u00a0provides a mechanism to help keep network traffic within the zone where it originated. Preferring same-zone traffic between Pods in your cluster can help with reliability, performance (network latency and throughput), or cost.</p>"},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#networking-on-windows","title":"Networking on Windows","text":""},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#service-clusterip-allocation","title":"Service ClusterIP allocation","text":""},{"location":"Taikun_CloudWorks_Overview/Service%2C_Load_Balancing%2C_and_Networking_in_Kubernetes/#service-internal-traffic-policy","title":"Service Internal Traffic Policy","text":"<p>If two Pods in your cluster want to communicate, and both Pods are actually running on the same node, use\u00a0Service Internal Traffic Policy\u00a0to keep network traffic within that node. Avoiding a round trip via the cluster network can help with reliability, performance (network latency and throughput), or cost.</p>"},{"location":"Taikun_CloudWorks_Overview/Taikun_CloudWorks_Architecture/","title":"Taikun CloudWorks \u2013 Architecture overview","text":"<p>Taikun CloudWorks is a Platform as a Service (PaaS) solution designed to streamline the management of applications, Kubernetes clusters, and virtual machines (VMs) across Public, Private, and Hybrid Cloud environments. This comprehensive platform empowers users with advanced capabilities for Kubernetes and container management, catering to diverse deployment needs, whether online or on-premise.</p>"},{"location":"Taikun_CloudWorks_Overview/Taikun_CloudWorks_Architecture/#key-features","title":"Key features","text":"<p>Deployment Flexibility:\u00a0Taikun CloudWorks offers flexibility in deployment options, allowing users to choose between using it as an online service or deploying it on-premise according to their specific requirements.</p> <p>Containerized Solution:\u00a0The architecture of Taikun CloudWorks is built around containerization principles, facilitating easy deployment and scalability. It is designed to run as a containerized solution, ensuring consistency across different environments and simplifying management tasks.</p> <p>Compatibility with Kubernetes:\u00a0Taikun CloudWorks is seamlessly integrated with Kubernetes, leveraging its robust orchestration capabilities for managing containerized applications. Users have the option to deploy Taikun CloudWorks on top of vanilla Kubernetes or managed Kubernetes services, such as those offered by cloud providers.</p> <p>Apps Management:\u00a0The platform provides comprehensive tools for managing applications deployed in containerized environments. Users can easily deploy, monitor, and scale their applications using intuitive interfaces and automation features provided by Taikun CloudWorks.</p> <p>Managed Services:\u00a0Taikun CloudWorks extends its capabilities beyond traditional PaaS offerings by providing managed services such as Database as a Service (DBaaS) and other specialized services. With DBaaS, users can easily provision, manage, and scale databases without the complexity of manual configuration and maintenance tasks. Additionally, other managed services may include caching solutions, messaging queues, monitoring tools, and more, allowing users to offload operational burdens and focus on developing and deploying their applications effectively. These managed services are integrated seamlessly into the platform, providing a unified management experience and ensuring optimal performance, reliability, and security for critical components of the application stack.</p> <p>Kubernetes Management:\u00a0Taikun CloudWorks enhances Kubernetes management capabilities, offering features for cluster provisioning, configuration management, monitoring, and maintenance. Users can efficiently manage their Kubernetes clusters across different cloud environments from a centralized dashboard.</p> <p>VM Management:\u00a0In addition to Kubernetes management, Taikun CloudWorks provides functionalities for managing virtual machines (VMs). Users can provision, monitor, and manage VMs alongside containerized workloads, ensuring seamless integration and flexibility in resource allocation.</p> <p>Multi-Cloud Support:\u00a0Taikun CloudWorks supports deployment across Public, Private, and Hybrid Cloud environments, enabling users to leverage resources from various cloud providers while maintaining consistent management experiences.</p> <p>Security and Compliance:\u00a0Security features are integrated into the architecture of Taikun CloudWorks to ensure data protection and compliance with industry standards. This includes access control mechanisms, encryption protocols, and auditing capabilities to safeguard sensitive information and maintain regulatory compliance.</p> <p>Scalability and High Availability:\u00a0The architecture is designed to be highly scalable and fault-tolerant, allowing for the dynamic allocation of resources based on demand. Load balancing and auto-scaling mechanisms are implemented to ensure optimal performance and availability of applications and infrastructure components.</p> <p>Taikun CloudWorks offers a flexible and robust PaaS solution for managing applications, Kubernetes clusters, and VMs across diverse cloud environments. Its containerized architecture, compatibility with Kubernetes, and comprehensive management features make it a versatile choice for organizations seeking efficient and scalable cloud management solutions.</p>"},{"location":"Taikun_CloudWorks_Overview/Taikun_CloudWorks_Operators/","title":"Taikun CloudWorks Operators","text":"<p>An operator in Taikun CloudWorks is a specialized controller that enhances the capabilities of the platform by automating the management and operation of specific applications or workloads. This operator is tailored to work seamlessly within the Taikun environment, providing an efficient and automated way to deploy, scale, and manage complex applications.</p> <p>Installing an Operator within Taikun is as straightforward as deploying it as an application using YAML manifests, similar to managing apps. Taikun simplifies the installation process for various functionalities.</p> <p>In Taikun CloudWorks, the deployment and management of Operators are seamlessly incorporated into the platform\u2019s functionality. The process involves leveraging the OIn Taikun CloudWorks, operator deployment, and management, which are seamlessly incorporated into the platform\u2019s functionality. The process involves leveraging the Operator Lifecycle Manager (OLM), a tool that simplifies the installation and ongoing management of Operators within the cluster.t of Operators within the cluster.</p> <p>For cluster administrators, the installation process involves using the web console or the Command-Line Interface (CLI). Subscribing an Operator to one or more namespaces ensures that the Operator becomes readily available to developers throughout the cluster. This streamlined process guarantees the efficient deployment and accessibility of Operators within the Taikun environment.</p>"},{"location":"Taikun_CloudWorks_Overview/Taikun_CloudWorks_Operators/#pod-placement-of-operator-workloads","title":"Pod placement of Operator workloads","text":"<p>By default, Operator Lifecycle Manager (OLM) places pods on arbitrary worker nodes when installing an Operator or deploying Operand workloads. As an administrator, you can use projects with a combination of node selectors, taints, and tolerations to control the placement of Operators and Operands to specific nodes.</p> <p>Controlling pod placement of Operator and Operand workloads has the following prerequisites:</p> <ol> <li> <p>Determine a node or set of nodes to target for the pods per your requirements. If available, note an existing label, such as\u00a0<code>node-role.kubernetes.io/app</code>, that identifies the node or nodes. Otherwise, add a label, such as\u00a0<code>myoperator</code>, by using a compute machine set or editing the node directly. You will use this label later as the node selector for your project.</p> </li> <li> <p>If you want to ensure that only pods with a certain label are allowed to run on the nodes<code>can</code> while steering unrelated workloads to other nodes, add a taint to the node or nodes by using a compute machine set or editing the node directly. Use an effect that ensures that new pods that do not match the taint cannot be scheduled on the nodes. For example, a\u00a0<code>myoperator:NoSchedule</code>\u00a0taint ensures that new pods that do not match the taint are not scheduled onto that node, but existing pods on the node are allowed to remain.</p> </li> <li> <p>Create a project that is configured with a default node selector and, if you added a taint, a matching toleration.</p> </li> </ol>"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/","title":"Taikun Compatibility Matrix","text":""},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#calico-compatibility","title":"Calico Compatibility","text":"Calico Version APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 3.25.1 Yes N/A N/A N/A 3.26.3 N/A Yes N/A N/A 3.26.4 N/A N/A Yes Yes 3.27.2 N/A N/A N/A N/A"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#kube-proxy-compatibility","title":"Kube-proxy Compatibility","text":"Kube-proxy version APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 1.26.4 Yes N/A N/A N/A 1.27.7 N/A Yes N/A N/A 1.28.6 N/A N/A Yes N/A"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#nodelocaldns-compatibility","title":"NodelocalDNS Compatibility","text":"Node Local DNS Version APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 1.21.1 Yes Yes Yes N/A"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#cinder-csi-plug-in","title":"Cinder CSI Plug-in","text":"Cinder CSI Plugin Version APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 2.3.0 1.25.0 Yes Yas N/A N/A 2.28.2 1.28.2 N/A N/A Yes N/A 2.29.0 1.29.0 N/A N/A N/A Yes"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#openstack-cloud-controller-manager","title":"OpenStack Cloud Controller Manager","text":"OpenStack Cloud Controller Manager APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 1.3.0 Yes N/A N/A N/A 1.4.0 1.25.0 N/A Yes N/A N/A 2.28.4 1.28.2 N/A N/A Yes N/A 2.29.0 1.29 N/A N/A N/A Yes"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#metrics-server-compatibility","title":"Metrics Server Compatibility","text":"Metrics Server Compatibility APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 6.2.17 0.6.3 Yes N/A N/A N/A 6.6.0 0.6.4 N/A Yes N/A N/A 7.0.3 0.7.1 N/A N/A Yes Yes"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#coredns-compatibility","title":"CoreDNS Compatibility","text":"CoreDNS Version APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 1.22.0 1.10.1 Yes Yes Yes N/A 1.29.0 1.11.1 N/A N/A N/A Yes"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#cluster-autoscaler-compatibility","title":"Cluster Autoscaler Compatibility","text":"Autoscaler Version APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 2.22.0 2.22.0 N/A N/A N/A N/A 2.23.1 2.23.1 N/A Yes N/A N/A 2.24.0 N/A N/A Yes N/A"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#cert-exporter-compatibility","title":"Cert-exporter Compatibility","text":"Cert-exporter Version APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 2.7.0 N/A N/A N/A N/A 3.0.1 Yes Yes Yes N/A"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#taikun-ima-compatibility","title":"Taikun-Ima Compatibility","text":"Taikun-Ima Version APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 2.22.0 2.22.0 Yes N/A N/A N/A 2.31.1 2.31.1 N/A Yes N/A N/A 2.24.0 2.24.0 N/A N/A Yes N/A 2.25.0 2.25.0 N/A N/A N/A Yes"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#loki-stack-compatibility","title":"Loki-Stack Compatibility","text":"Loki-Stack Version APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 2.8.7 Yes N/A N/A N/A 2.10.1 N/A Yes Yes N/A"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#kube-prometheus-stack-compatibility","title":"Kube-prometheus-stack Compatibility","text":"Kube-prometheus-stack Version APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 41.7.3 Yes N/A N/A N/A 56.0.5 N/A Yes Yes N/A"},{"location":"Taikun_CloudWorks_Overview/Taikun_Compatibility_Matrix/#ingress-nginx-compatibility","title":"Ingress-nginx Compatibility","text":"ingress-nginx Version APP VERSION Kubernetes 1.26.4 Kubernetes 1.27.7 Kubernetes 1.28.6 Kubernetes 1.29 4.3.0 Yes Yes Yes N/A"},{"location":"Taikun_CloudWorks_Overview/Tasks_and_Pipelines_in_Taikun_CloudWorks/","title":"Tasks and Pipelines in Taikun CloudWorks","text":"<p>Taikun CloudWorks empowers to streamline the development lifecycle with its advanced capabilities in creating and managing CI/CD pipelines directly within its web interface.</p> <p>Key Features:</p> <ol> <li> <p>Tekton Integration:\u00a0Taikun CloudWorks seamlessly integrates with Tekton, a robust open-source framework for building CI/CD systems. Leveraging Tekton within Kubernetes clusters, Taikun provides a scalable and flexible solution to meet your continuous integration and continuous delivery needs.</p> </li> <li> <p>Effortless Pipeline Creation:\u00a0Taikun CloudWorks simplifies the pipeline creation process, allowing developers to define, customize, and manage CI/CD workflows effortlessly. With an intuitive interface, users can design pipelines tailored to their project requirements.</p> </li> <li> <p>Kubernetes Efficiency:\u00a0Tekton operates within Kubernetes clusters, ensuring a container-native approach to CI/CD. Taikun harnesses the power of Kubernetes to enhance scalability, reliability, and ease of deployment for your CI/CD processes.</p> </li> <li> <p>End-to-End Management:\u00a0Taikun CloudWorks provides comprehensive pipeline management capabilities. From initiating builds to automating deployment and monitoring, the platform ensures a smooth end-to-end experience for your development team.</p> </li> </ol>"},{"location":"Taikun_CloudWorks_Overview/Tasks_and_Pipelines_in_Taikun_CloudWorks/#tasks-and-pipelines","title":"Tasks and Pipelines","text":"<p>Building Blocks of Tekton CI/CD Workflow</p>"},{"location":"Taikun_CloudWorks_Overview/Tasks_and_Pipelines_in_Taikun_CloudWorks/#tekton-pipelines","title":"Tekton Pipelines","text":"<p>Tekton Pipelines is a Kubernetes extension that installs and runs on your Kubernetes cluster. It defines a set of Kubernetes\u00a0Custom Resources\u00a0that act as building blocks from which you can assemble CI/CD pipelines. Once installed, Tekton Pipelines becomes available via the Kubernetes CLI (kubectl) and via API calls, just like pods and other resources. Tekton is open-source and part of the\u00a0CD Foundation, a\u00a0Linux Foundation\u00a0project.</p>"},{"location":"Taikun_CloudWorks_Overview/Tasks_and_Pipelines_in_Taikun_CloudWorks/#tekton-pipelines-entities","title":"Tekton Pipelines entities","text":"<p>Tekton Pipelines defines the following entities:</p> Entity Description Task Defines a series of steps which launch specific build or delivery tools that ingest specific inputs and produce specific outputs. TaskRun Instantiates a Task for execution with specific inputs, outputs, and execution parameters. Can be invoked on its own or as part of a Pipeline. Pipeline Defines a series of Tasks that accomplish a specific build or delivery goal. Can be triggered by an event or invoked from a PipelineRun. PipelineRun Instantiates a Pipeline for execution with specific inputs, outputs, and execution parameters. PipelineResources (Deprecated) Defines locations for inputs ingested and outputs produced by the steps in Tasks. Run (alpha) Instantiates a Custom Task for execution when specific inputs."},{"location":"Taikun_CloudWorks_Overview/Tekton_Operators/","title":"Tekton Operators","text":""},{"location":"Taikun_CloudWorks_Overview/Tekton_Operators/#operator","title":"Operator","text":"<p>Manage Tekton CI/CD Building </p>"},{"location":"Taikun_CloudWorks_Overview/Tekton_Operators/#tekton-operator","title":"Tekton Operator","text":"<p>Tekton Operator is a Kubernetes extension that installs, upgrades and manages TektonCD\u00a0Pipelines,\u00a0Dashboards,\u00a0and\u00a0Triggers\u00a0(and other components) on any Kubernetes Cluster.</p>"},{"location":"Taikun_CloudWorks_Overview/Tekton_Operators/#tekton-operator-entities","title":"Tekton Operator entities","text":"<p>Tekton Operator defines the following entities:</p> Entity Description TektonConfig Configure Tekton components to be installed and managed. TektonPipeline Configure the Tekton Pipeline component to be installed and managed. TektonTrigger Configure the Tekton Trigger component to be installed and managed. TektonDashboard Configure the Tekton Dashboard component to be installed and managed. TektonResult Configure the Tekton Result component to be installed and managed. TektonChain Configure the Tekton Chain component to be installed and managed. OpenShiftPipelinesAsCode Configure the Pipelines as Code component to be installed and managed. TektonAddon Configure addons to be installed and managed."},{"location":"Taikun_CloudWorks_Overview/Tekton_Operators/#getting-started","title":"Getting started","text":"<p>To install an Operator, there are multiple ways</p> <ul> <li> <p>Install from Operator HubYou.\u00a0You can find the instructions\u00a0here. The lifecycle will be managed by Operator Lifecycle Manager (OLM).</p> </li> <li> <p>Install using the release file.\u00a0The latest version\u2019s release file can be found\u00a0here. In this case, you will have to manage the operator\u2019s lifecycle.</p> </li> <li> <p>Install from code.\u00a0You can clone the repositories and install the Operator. You can find the instructions\u00a0here</p> </li> </ul> <p>After installing the Operator, install the required Tekton Component, such as Tekton Pipeline and Tekton Triggers.</p> <p>Create an instance of\u00a0<code>TektonConfig</code>,\u00a0which will create the required components. You can find more details and the available configuration in\u00a0TektonConfig.</p> <p>NOTE:\u00a0TektonResult and\u00a0TektonChain are optional components and are not currently installed through TektonConfig. The installation steps are in their docs.</p>"},{"location":"Taikun_CloudWorks_Overview/Tekton_Operators/#understanding-tekton-operator","title":"Understanding Tekton Operator","text":"<p>Each Tekton Component has a Custom Resource that installs and manages the component.</p> <p>TektonConfig is a top-level Custom Resource that creates other components.</p> <p>So, the user needs to create TektonConfig with the required configurations, and it will install the necessary components.</p> <p>More about the Resources and their available configurations are found in their docs.</p> <ul> <li> <p>TektonConfig</p> </li> <li> <p>TektonPipeline</p> </li> <li> <p>TektonTrigger</p> </li> <li> <p>TektonDashboard</p> </li> <li> <p>TektonResult</p> </li> <li> <p>TektonChain</p> </li> <li> <p>TektonAddon</p> </li> <li> <p>OpenShiftPipelinesAsCode</p> </li> </ul> <p>To understand how Tekton Operator works, you can find the details\u00a0[here](https://github.com/tektoncd/operator/tree/release-v0.70.x/docs/TektonOperator.md</p>"},{"location":"Taikun_Dictionary/Our_Lingo/","title":"Our Lingo","text":""},{"location":"Taikun_Dictionary/Our_Lingo/#access-profile","title":"Access Profile","text":"<p>Profile using HTTP proxy, SSH (we support RSA, ECDSA or Ed25519), DNS or NTP to access your cluster, the condition is that you add it to the project during creation.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#alerting-profile","title":"Alerting Profile","text":"<p>Profile attached to a Project, provides notifications about alerts in your Project.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#kubernetes-profile","title":"Kubernetes Profile","text":"<p>Profile for enabling Kubernetes functions and features (such as Octavia, Load Balancer, or Proxy on Bastion).</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#audit-log","title":"Audit Log","text":"<p>Summary of changes made in Projects.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#backup-credentials","title":"Backup Credentials","text":"<p>Taikun uses S3-compatible backup. S3 (Simple Storage Service) Backup is a desktop application using Amazon\u2019s infrastructure for remote backups and secure online file storage. It features strong encryption, compression, easy access to your backed-up files, and built-in backup scheduling.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#backup-policy","title":"Backup Policy","text":"<p>Defines the ground rules for planning, executing, and validating backups and includes specific activities to ensure that critical data is backed up to secure storage media located in a secure location.</p> <ul> <li>Schedules = Policies.</li> <li>Backups = A copy created based on the policy.</li> <li>Restores = A copy from backup data.</li> </ul>"},{"location":"Taikun_Dictionary/Our_Lingo/#billing-rules","title":"Billing Rules","text":"<p>Rules for Chargeback. Every rule is based on an external source, metrics, and price.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#chargeback","title":"Chargeback","text":"<p>External billing set by Billing Rules. You can see the price for every rule for the selected time period.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#cloud","title":"Cloud","text":"<p>Global network of servers, each with a unique function. These servers are designed to either store and manage data, run applications, or deliver content or a service such as streaming videos, webmail, office productivity software, or social media. Cloud operates as a single ecosystem.</p> <ul> <li>Taikun supports OpenStack, Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP).</li> </ul>"},{"location":"Taikun_Dictionary/Our_Lingo/#cluster","title":"Cluster","text":"<p>Taikun Project \u2013 a set of computers that work together so that they can be viewed as a single system.</p> <ul> <li>Kubernetes cluster: Set of nodes that run containerized applications. Containerizing applications package an app with its dependencies and some necessary services. This way, Kubernetes clusters allow applications to be more easily developed, moved, and managed.</li> </ul>"},{"location":"Taikun_Dictionary/Our_Lingo/#cni","title":"CNI","text":"<p>Container Network Interface, consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#credentials","title":"Credentials","text":"<p>Data confirming the identity of the user or external application (e.g., username and password, username and API key, or authentication token that the identity service provides). In Taikun, you can find Cloud Credentials, Backup Credentials, Billing Credentials, and Showback Credentials.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#csm","title":"CSM","text":"<p>Customer Success Manager.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#endpoint","title":"Endpoint","text":"<p>Remote computing device that communicates back and forth with a network to which it is connected (usually URL address).</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#flavor","title":"Flavor","text":"<p>An available hardware configuration for a server. Defines the compute, memory, and storage capacity of nova computing instances.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#floating-ip","title":"Floating IP","text":"<p>A kind of virtual IP address that can be dynamically routed to any server in the same network. Multiple servers can own the same Floating IP address, but it can only be active on one server at any given time.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#import-network","title":"Import Network","text":"<p>Use the network already created in the Cloud (works for OpenStack).</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#keycloak","title":"Keycloak","text":"<p>Keycloak is open-source software for single sign-on with Identity and Access Management aimed at modern applications and services.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#kubeconfig-file","title":"Kubeconfig File","text":"<p>Kubeconfig file is used to configure access to Kubernetes when used in conjunction with the <code>kubectl</code> command-line tool (or other clients).</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services that facilitates both declarative configuration and automation.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#load-balancer","title":"Load Balancer","text":"<p>Refers to the process of distributing a set of tasks over a set of resources (computing units), aiming to optimize the response time and avoid unevenly overloading some compute nodes while others are idle.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#metrics","title":"Metrics","text":"<p>Measure specific characteristics in a countable manner. Metrics are used in monitoring and billing.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#monitoring","title":"Monitoring","text":"<p>The process to gather metrics about operations, ensuring everything functions as expected to support applications and services. By enabling monitoring, you can see Logs, Alerts, or Metrics.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#node","title":"Node","text":"<p>Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#octavia","title":"Octavia","text":"<p>Exposes the Service externally using the load balancers from OpenStack.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#pod","title":"Pod","text":"<p>Pod is the smallest execution unit in Kubernetes. A pod encapsulates one or more applications.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#proxy-on-bastion","title":"Proxy on Bastion","text":"<p>Exposes the Service on each Node\u2019s IP at a static port, the NodePort. You\u2019ll be able to contact the NodePort Service from outside the cluster by requesting <code>NodeIP:NodePort</code>.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#reboot","title":"Reboot","text":"<p>To reload the operating system of a computer, starting it up again.</p> <ul> <li>Hard reboot: The power to the system is physically turned off and back again, causing an initial boot.</li> <li>Soft reboot: The system restarts without the need to interrupt the power.</li> </ul>"},{"location":"Taikun_Dictionary/Our_Lingo/#role","title":"Role","text":"<p>Defines the rights and permissions granted to user accounts. In Taikun, there are Users, Managers (Owner), or Partners.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#showback","title":"Showback","text":"<p>Internal billing set by Showback Rules. You can see the price for every rule for the selected time period. You can also set an external source for billing.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#showback-rules","title":"Showback Rules","text":"<p>Rules for Showback. Every rule is based on metrics and price.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#subscription","title":"Subscription","text":"<p>An amount of money that you pay regularly to use Taikun. The price varies depending on the number of users, projects, or TCU.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#token","title":"Token","text":"<p>An alphanumeric text string that provides access to APIs and resources.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#user","title":"User","text":"<p>Digital representation of a person using Taikun. Each user is defined by a username (email) and access is granted by a token. The user is assigned a role with its permissions.</p>"},{"location":"Taikun_Dictionary/Our_Lingo/#webhook","title":"Webhook","text":"<p>A way web applications can communicate with each other, allowing real-time data to be sent from one application to another whenever a given event occurs.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/","title":"Taikun OCP \u2013 Skydive","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#overview","title":"Overview","text":"<p>Skydive is an open-source real-time network topology and protocols analyzer. It aims to provide a comprehensive understanding of what is happening in the network infrastructure.</p> <p>Skydive agents collect topology information, flow it, and forward it to a central agent for further analysis. All the information is stored in an Elasticsearch database.</p> <p>Skydive is SDN-agnostic but provides SDN drivers in order to enhance the topology and flows informations.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#topology-probes","title":"Topology Probes","text":"<p>Topology probes are used to construct the graph comprising:</p> <ul> <li> <p>Graph nodes: depicted as cycles (in contracted form) or areas (in expanded form)</p> </li> <li> <p>Graph edges: depicted as straight lines (with arrowheads when directional)</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#agent-topology-probes","title":"Agent Topology Probes","text":"<p>Probes which extract topological information from the host about host residing entities are agent probes:</p> <ul> <li> <p>Docker (docker)</p> </li> <li> <p>Ethtool (ethtool)</p> </li> <li> <p>LibVirt (libvirt)</p> </li> <li> <p>LLDP (lldp)</p> </li> <li> <p>Lxd (lxd)</p> </li> <li> <p>NetLINK (netlink)</p> </li> <li> <p>NetNS (netns)</p> </li> <li> <p>Neutron (neutron)</p> </li> <li> <p>OVSDB (ovsdb)</p> </li> <li> <p>Opencontrail (opencontrail)</p> </li> <li> <p>runC (runc)</p> </li> <li> <p>Socket Information (socketinfo)</p> </li> <li> <p>VPP (vpp)</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#analyzer-topology-probes","title":"Analyzer Topology Probes","text":"<p>Probes which extract topological information from a remote global entity are analyzer probes:</p> <ul> <li> <p>Istio (istio)</p> </li> <li> <p>Kubernetes (k8s)</p> </li> <li> <p>OVN (ovn)</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#k8s","title":"K8s","text":"<p>The k8s probe provides topological information.</p> <p>Graph nodes:</p> <ul> <li> <p>general: cluster, namespace</p> </li> <li> <p>compute: node, pod, container</p> </li> <li> <p>storage: persistentvolumeclaim (pvc), persistentvolume (pv), storageclass</p> </li> <li> <p>network: network policy, service, endpoints, ingress</p> </li> <li> <p>deployment: deployment, stateful set, replica set, replication controller, cronjob, job</p> </li> <li> <p>configuration: config map, secret</p> </li> </ul> <p>Graph edges:</p> <ul> <li> <p>k8s-k8s ownership (e.g. k8s.namespace \u2013 k8s.pod)</p> </li> <li> <p>k8s-k8s relationship (e.g. k8s.service \u2013 k8s.pod)</p> </li> <li> <p>k8s-physical relationship (e.g. k8s.node \u2013 host)</p> </li> </ul> <p>Graph node metadata:</p> <ul> <li> <p>Indexed fields: standard fields such as\u00a0<code>Type</code>,\u00a0<code>Name</code>\u00a0plus k8s specific such as\u00a0<code>K8s.Namespace</code></p> </li> <li> <p>stored-only fields: the entire content of k8s resource stored under\u00a0<code>K8s.Extra</code></p> </li> </ul> <p>Graph node status:</p> <ul> <li> <p>the\u00a0<code>Status</code>\u00a0node metadata field</p> </li> <li> <p>with values Up (white) / Down (red)</p> </li> <li> <p>currently implemented for resources: pod, persistent volume claim (PVC) and persistent volume (PV)</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#flow-probes-supported","title":"Flow Probes supported","text":"<p>Flow probes currently implemented :</p> <ul> <li> <p>sFlow</p> </li> <li> <p>AFPacket</p> </li> <li> <p>PCAP</p> </li> <li> <p>PCAP socket</p> </li> <li> <p>DPDK</p> </li> <li> <p>eBPF</p> </li> <li> <p>OpenvSwitch port mirroring</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#architecture","title":"Architecture","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#graph-engine","title":"Graph engine","text":"<p>Skydive relies on an event-based graph engine, meaning notifications are sent for each modification. Graphs expose notifications over WebSocket connections. Skydive supports multiple graph backends for the Graph. The\u00a0<code>memory</code>\u00a0backend will always be used by agents, while the analyzer backend can be chosen. Each modification is kept in the datastore so that we have a full history of the graph. This is really useful to troubleshoot even if interfaces do not exist anymore.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#topology-probes_1","title":"Topology probes","text":"<p>Fill the graph with the topology information collected. Multiple probes fill the graph in parallel. For example, probes fill graphs with network namespaces, Netlink, or OVSDB information.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#flow-table","title":"Flow table","text":"<p>Skydive keeps track of packets captured in flow tables. It allows Skydive to keep metrics for each flow. At a given frequency or when the flow expires (see the config file), the flows are forwarded from agents to analyzers and then to the datastore.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#flow-enhancer","title":"Flow enhancer","text":"<p>Each time a new flow is received by the analyzer the flow is enhanced with topology informations like where it has been captured, where it originates from, where the packet is going to.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#flow-probes","title":"Flow probes","text":"<p>Flow probes capture packets and fill agent flow tables. There are different ways to capture packets like sFlow, afpacket, PCAP, etc.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#gremlin-engine","title":"Gremlin engine","text":"<p>Skydive uses Gremlin language as its graph traversal language. The Skydive Gremlin implementation allows to use Gremlin for flow traversal purpose. The Gremlin engine can either retrieve informations from the datastore or from agents depending whether the request is about something is the past or for live monitoring/troubleshooting.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#etcd","title":"Etcd","text":"<p>Skydive uses Etcd to store API objects like captures. Agents are watching Etcd so that they can react on API calls.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#on-demand-probes","title":"On-demand probes","text":"<p>This component watches Etcd and the graph in order to start captures. So when a new capture is created by the API on-demande probe looks for graph nodes matching the Gremlin expression, and if so, start capturing traffic.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#getting-started","title":"Getting-started","text":"<p>There are multiple ways to easily deploy Skydive, in this section we are going to explain the most common ways.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#downloading-binary","title":"Downloading binary","text":"<p>The easiest way is to download a static binary of Skydive. There are two kind of binaries, one is built each time a feature or a bug fix is available\u00a0(continuous binary)\u00a0, the others are provided for each\u00a0release\u00a0.</p> <p>Since Skydive uses the same binary for all its component, one can use it as agent, analyzer or client.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#all-in-one-mode","title":"All-in-One mode","text":"<p>This mode start an analyzer and an agent at once.</p> <p><code>$ skydive allinone</code><code>[</code><code>--conf etc/skydive.yml]</code></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#agent-and-analyzer-separately","title":"Agent and Analyzer separately","text":"<p><code>skydive agent</code><code>[</code><code>--conf etc/skydive.yml]</code></p> <p><code>skydive analyzer</code><code>[</code><code>--conf etc/skydive.yml]</code></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#client","title":"Client","text":"<p><code>skydive client</code></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#vagrant-deployment","title":"Vagrant deployment","text":"<p>You can use Vagrant to deploy a Skydive environment with one virtual machine running both Skydive analyzer and Elasticsearch, and two virtual machines with the Skydive agent. This\u00a0<code>Vagrantfile</code>, hosted in\u00a0<code>contrib/vagrant</code>\u00a0of the Git repository, makes use of the\u00a0libvirt Vagrant provider]\u00a0and uses Fedora as the box image.</p> <p>cd contrib/vagrant <code>vagrant up</code></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#docker","title":"Docker","text":"<p>A Docker image is available on the\u00a0Skydive Docker Hub account\u00a0.</p> <p>To start the analyzer :</p> <p><code>docker run -p 8082:8082 skydive/skydive analyzer</code></p> <p>To start the agent :</p> <p><code>docker run --privileged --pid</code><code>=</code><code>host --net</code><code>=</code>host -p 8081:8081 \\ <code>-e SKYDIVE_ANALYZERS</code><code>=</code>localhost:8082 \\ <code>-v /var/run/docker.sock:/var/run/docker.sock skydive/skydive agent</code></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#docker-compose","title":"Docker Compose","text":"<p>Docker Compose\u00a0can also be used to automatically start an Elasticsearch container, a Skydive analyzer container and a Skydive agent container. The service definition is located in the\u00a0<code>contrib/docker</code>\u00a0folder of the Skydive sources.</p> <p><code>docker-compose up</code></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#openstackdevstack","title":"Openstack/Devstack","text":"<p>Skydive provides a DevStack plugin that can be used in order to have Skydive Agents/Analyzer set up with the proper probes by DevStack.</p> <p>For a single node setup adding the following lines to your local.conf file should be enough.</p> <p>enable_plugin skydive https://github.com/skydive-project/skydive.git</p> <p><code>enable_service skydive-agent skydive-analyzer</code></p> <p>The plugin accepts the following parameters:</p> <p><code># Address on which skydive analyzer process listens for connections.</code> <code># Must be in ip:port format</code> <code>#SKYDIVE_ANALYZER_LISTEN=</code></p> <p><code># Configure the skydive analyzer with the etcd server address</code> <code># IP_ADDRESS:12379</code> <code>#SKYDIVE_ANALYZER_ETCD=</code></p> <p><code># Inform the agent about the address on which analyzers are listening</code> <code># Must be in ip:port format</code> <code>#SKYDIVE_ANALYZERS=</code></p> <p><code># ip:port address on which skydive agent listens for connections.</code> <code>#SKYDIVE_AGENT_LISTEN=</code></p> <p><code># The path for the generated skydive configuration file</code> <code>#SKYDIVE_CONFIG_FILE=</code></p> <p><code># List of agent probes to be used by the agent</code> <code># Ex: netns netlink ovsdb</code> <code>#SKYDIVE_AGENT_PROBES=</code></p> <p><code># Remote port for ovsdb server.</code> <code>#SKYDIVE_OVSDB_REMOTE_PORT=6640</code></p> <p><code># Set the default log level, default: INFO</code> <code>#SKYDIVE_LOGLEVEL=DEBUG</code></p> <p><code># List of public interfaces for the agents to register in fabric</code> <code>#SKYDIVE_PUBLIC_INTERFACES=\"devstack1/eth0 devstack2/eth1\"</code></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#the-classical-two-nodes-deployment","title":"The classical two nodes deployment","text":"<p>Inside the\u00a0<code>devstack</code>\u00a0folder of the\u00a0Skydive sources\u00a0there are two local.conf files that can be used in order to deployment two Devstack with Skydive. The first file will install a full Devstack with Skydive analyzer and agent. The second one will install a compute Devstack with only the skydive agent.</p> <p>For Skydive to create a TOR object that links both Devstack, add the following line to your local.conf file :</p> <p><code>SKYDIVE_PUBLIC_INTERFACES</code><code>=</code><code>\"devstack1/eth0 devstack2/eth1\"</code></p> <p>where\u00a0<code>devstack1</code>\u00a0and\u00a0<code>devstack2</code>\u00a0are the hostnames of the two nodes followed by their respective public interface.</p> <p>Skydive will be set with the probes for OpenvSwitch and Neutron. It will be set to use Keystone as authentication mechanism, so the credentials will be the same than the admin.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#client-webui","title":"Client &amp; WebUI","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#client_1","title":"Client","text":"<p>Skydive client can be used to interact with Skydive Analyzer and Agents. Running it without any command will return all the commands available.</p> <p>skydive client Usage: <code>skydive client</code><code>[</code><code>command</code><code>]</code></p> <p>Available Commands:   alert         Manage alerts   capture       Manage captures   inject-packet Inject packets   pcap          Import flows from PCAP file   query         Issue Gremlin queries   shell         Shell Command Line Interface   status        Show analyzer status <code>topology      Request on topology</code><code>[</code>deprecated: use 'client query' instead]   user-metadata Manage user metadata</p> <p>Flags:       --analyzer string   analyzer address <code>-h, --help              help</code><code>for</code>client       --password string   password auth parameter       --username string   username auth parameter</p> <p>Global Flags: <code>-c, --conf stringArray        location of Skydive configuration files, default try loading /etc/skydive/skydive.yml</code><code>if</code>exist <code>-b, --config-backend string   configuration backend</code><code>(</code><code>defaults to file</code><code>)</code><code></code><code>(</code><code>default \"file\"</code><code>)</code></p> <p><code>Use \"skydive client [command] --help\"</code><code>for</code><code>more information about a command.</code></p> <p>Specifying the subcommand will give the usage of the subcommand.</p> <p>$ skydive client capture Manage captures</p> <p>Usage: <code>skydive client capture</code><code>[</code><code>command</code><code>]</code></p> <p>Available Commands:   create      Create capture   delete      Delete capture   get         Display capture   list        List captures</p> <p>Flags: <code>-h, --help   help</code><code>for</code>capture</p> <p>Global Flags:       --analyzer string         analyzer address <code>-c, --conf stringArray        location of Skydive configuration files, default try loading /etc/skydive/skydive.yml</code><code>if</code>exist <code>-b, --config-backend string   configuration backend</code><code>(</code><code>defaults to file</code><code>)</code><code></code><code>(</code><code>default \"file\"</code><code>)</code>       --password string         password auth parameter       --username string         username auth parameter</p> <p><code>Use \"skydive client capture [command] --help\"</code><code>for</code><code>more information about a command.</code></p> <p>If an authentication mechanism is defined in the configuration file the username and password parameter have to be used for each command. Environment variables\u00a0<code>SKYDIVE_USERNAME</code>\u00a0and\u00a0<code>SKYDIVE_PASSWORD</code>\u00a0can be used as default value for the username/password command line parameters.</p> <p>Skydive uses the Gremlin traversal language as a topology request language. Requests on the topology can be done as following :</p> <pre><code>$ skydive client query \"G.V().Has('Name', 'br-int', 'Type' ,'ovsbridge')\"\n[\n  {\n    \"Host\": \"pc48.home\",\n    \"ID\": \"1e4fc503-312c-4e4f-4bf5-26263ce82e0b\",\n    \"Metadata\": {\n      \"Name\": \"br-int\",\n      \"Type\": \"ovsbridge\",\n      \"UUID\": \"c80cf5a7-998b-49ca-b2b2-7a1d050facc8\"\n    }\n  }\n]\n</code></pre> <p>Refer to the\u00a0Gremlin section\u00a0for further explanations about the syntax and the functions available.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#webui","title":"WebUI","text":"<p>To access to the WebUI of agents or analyzer :</p> <p><code>http://&lt;address&gt;:&lt;port&gt;</code></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#history-and-datastore","title":"History and Datastore","text":"<p>Skydive can keep track of all the modifications of the topology and flows in a datastore. Skydive supports two backends:\u00a0Elasticsearch\u00a0and\u00a0OrientDB.</p> <p>In order to activate the history we need first to define the storage in the configuration\u00a0file</p> <p><code>Elasticsearch</code>\u00a0example:</p> <p>storage:   myelasticsearch:     driver: elasticsearch     host: 127.0.0.1:9200</p> <p>Then we need to use it as topology\u00a0backend:</p> <p>  topology:     backend: myelasticsearch</p> <p>and as Flow\u00a0backend</p> <p>  flow:     backend: myelasticsearch</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP_-_Skydive/#grafana","title":"Grafana","text":"<p>In order to use the Grafana datasource plugin we need to use a data store, please see the upper section.</p> <p>A docker image with the datasource plugin available can be use as following:</p> <p><code>docker run -d --name=grafana -p 3000:3000 skydive/skydive-grafana-datasource</code>open-source</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Barbican/Key_Manager_Overview/","title":"What is Barbican?","text":"<p>Barbican is the OpenStack Key Manager service. It provides secure storage, provisioning and management of secret data. This includes keying material such as Symmetric Keys, Asymmetric Keys, Certificates and raw binary data.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Barbican/Key_Manager_Overview/#api-guide","title":"API Guide","text":"<p>If you\u2019re trying to learn how to use barbican, you can start by reading about\u00a0Secrets in the Barbican API Guide.</p> <p>Once you\u2019re comfortable working with secrets you can dig into the rest of the API.</p> <ul> <li> <p>Cloud Administrator Guide \u2013 Key Manager service</p> </li> <li> <p>Access Control</p> </li> <li> <p>Barbican Service Management Utility</p> </li> <li> <p>Database Cleaning</p> </li> <li> <p>Key Manager Service Upgrade Guide</p> </li> <li> <p>CLI Reference</p> </li> <li> <p>barbican-status</p> </li> <li> <p>Key Manager service</p> </li> <li> <p>Key Manager service overview</p> </li> <li> <p>Install and configure</p> </li> <li> <p>Verify operation</p> </li> <li> <p>Next steps</p> </li> <li> <p>Setting up Barbican</p> </li> <li> <p>Using Keystone Middleware with Barbican</p> </li> <li> <p>Troubleshooting your Barbican Setup</p> </li> <li> <p>No Auth barbican</p> </li> <li> <p>Using Audit Middleware with Barbican</p> </li> <li> <p>Using Secret Store Plugins in Barbican</p> </li> <li> <p>barbican.conf</p> </li> <li> <p>Policy configuration</p> </li> <li> <p>Barbican for Developers</p> </li> <li> <p>Setting up a Barbican Development Environment</p> </li> <li> <p>Running Barbican on DevStack</p> </li> <li> <p>Contributing to Barbican</p> </li> <li> <p>Getting Involved</p> </li> <li> <p>Architecture</p> </li> <li> <p>Project Structure</p> </li> <li> <p>Dataflow</p> </li> <li> <p>Adding/Updating Dependencies</p> </li> <li> <p>Database Migrations</p> </li> <li> <p>Plugin Developers Guide</p> </li> <li> <p>Writing and Running Barbican Tests</p> </li> <li> <p>Barbican API Documentation</p> </li> <li> <p>User Guide</p> </li> <li> <p>API Reference</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Barbican/Key_Manager_Overview/#sample-files","title":"Sample Files","text":"<ul> <li> <p>Barbican Sample Configuration File</p> </li> <li> <p>Barbican Sample Policy</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Barbican/Key_Manager_Overview/#indices-and-tables","title":"Indices and tables","text":"<ul> <li> <p>Index</p> </li> <li> <p>Module Index</p> </li> <li> <p>Search Page</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/","title":"Block Devices","text":"<p>You can attach Ceph Block Device images to OpenStack instances through\u00a0<code>libvirt</code>, which configures the QEMU interface to\u00a0<code>librbd</code>. Ceph stripes block volumes across multiple OSDs within the cluster, which means that large volumes can realize better performance than local drives on a standalone server!</p> <p>To use Ceph Block Devices with OpenStack, you must install QEMU,\u00a0<code>libvirt</code>, and OpenStack first. We recommend using a separate physical node for your OpenStack installation. OpenStack recommends a minimum of 8GB of RAM and a quad-core processor. The following diagram depicts the OpenStack/Ceph technology stack.</p> <p></p> <p>Important To use Ceph Block Devices with OpenStack, you must have access to a running Ceph Storage Cluster.</p> <p>Three parts of OpenStack integrate with Ceph\u2019s block devices:</p> <ul> <li>Images</li> </ul> OpenStack Glance manages images for VMs. Images are immutable. OpenStack treats images as binary blobs and downloads them accordingly. <ul> <li>Volumes</li> </ul> Volumes are block devices. OpenStack uses volumes to boot VMs, or to attach volumes to running VMs. OpenStack manages volumes using Cinder services. <ul> <li>Guest Disks</li> </ul> Guest disks are guest operating system disks. By default, when you boot a virtual machine, its disk appears as a file on the file system of the hypervisor (usually under\u00a0 <p><code>/var/lib/nova/instances/&lt;uuid&gt;/</code></p> <p>). Prior to OpenStack Havana, the only way to boot a VM in Ceph was to use the boot-from-volume functionality of Cinder. However, now it is possible to boot every virtual machine inside Ceph directly without using Cinder, which is advantageous because it allows you to perform maintenance operations easily with the live-migration process. Additionally, if your hypervisor dies it is also convenient to trigger\u00a0</p> <p><code>nova\u00a0evacuate</code></p> <p>and reinstate the virtual machine elsewhere almost seamlessly. In doing so,\u00a0</p> <p>exclusive locks</p> <p>prevent multiple compute nodes from concurrently accessing the guest disk.</p> <p>You can use OpenStack Glance to store images as Ceph Block Devices, and you can use Cinder to boot a VM using a copy-on-write clone of an image.</p> <p>The instructions below detail the setup for Glance, Cinder and Nova, although they do not have to be used together. You may store images in Ceph block devices while running VMs using a local disk, or vice versa.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#create-a-pool","title":"CREATE A POOL","text":"<p>By default, Ceph block devices live within the\u00a0<code>rbd</code>\u00a0pool. You may use any suitable pool by specifying it explicitly. We recommend creating a pool for Cinder and a pool for Glance. Ensure your Ceph cluster is running, then create the pools.</p> <pre><code>ceph osd pool create volumes\nceph osd pool create images\nceph osd pool create backups\nceph osd pool create vms\n</code></pre> <p>See\u00a0Create a Pool\u00a0for detail on specifying the number of placement groups for your pools, and\u00a0Placement Groups\u00a0for details on the number of placement groups you should set for your pools.</p> <p>Newly created pools must be initialized prior to use. Use the\u00a0<code>rbd</code>\u00a0tool to initialize the pools:</p> <pre><code>rbd pool init volumes\nrbd pool init images\nrbd pool init backups\nrbd pool init vms\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#configure-openstack-ceph-clients","title":"CONFIGURE OPENSTACK CEPH CLIENTS","text":"<p>The nodes running\u00a0<code>glance-api</code>,\u00a0<code>cinder-volume</code>,\u00a0<code>nova-compute</code>\u00a0and\u00a0<code>cinder-backup</code>\u00a0act as Ceph clients. Each requires the\u00a0<code>ceph.conf</code>\u00a0file</p> <pre><code>ssh {your-openstack-server} sudo tee /etc/ceph/ceph.conf &lt;/etc/ceph/ceph.conf\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#install-ceph-client-packages","title":"INSTALL CEPH CLIENT PACKAGES","text":"<p>On the\u00a0<code>glance-api</code>\u00a0node, you will need the Python bindings for\u00a0<code>librbd</code>:</p> <pre><code>sudo apt-get install python-rbd\nsudo yum install python-rbd\n</code></pre> <p>On the\u00a0<code>nova-compute</code>,\u00a0<code>cinder-backup</code>\u00a0and on the\u00a0<code>cinder-volume</code>\u00a0node, use both the Python bindings and the client command line tools:</p> <pre><code>sudo apt-get install ceph-common\nsudo yum install ceph-common\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#setup-ceph-client-authentication","title":"SETUP CEPH CLIENT AUTHENTICATION","text":"<p>If you have\u00a0cephx authentication\u00a0enabled, create a new user for Nova/Cinder and Glance. Execute the following:</p> <pre><code>ceph auth get-or-create client.glance mon 'profile rbd' osd 'profile rbd pool=images' mgr 'profile rbd pool=images'\nceph auth get-or-create client.cinder mon 'profile rbd' osd 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd-read-only pool=images' mgr 'profile rbd pool=volumes, profile rbd pool=vms'\nceph auth get-or-create client.cinder-backup mon 'profile rbd' osd 'profile rbd pool=backups' mgr 'profile rbd pool=backups'\n</code></pre> <p>Add the keyrings for\u00a0<code>client.cinder</code>,\u00a0<code>client.glance</code>, and\u00a0<code>client.cinder-backup</code>\u00a0to the appropriate nodes and change their ownership:</p> <pre><code>ceph auth get-or-create client.glance | ssh {your-glance-api-server} sudo tee /etc/ceph/ceph.client.glance.keyring\nssh {your-glance-api-server} sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring\nceph auth get-or-create client.cinder | ssh {your-volume-server} sudo tee /etc/ceph/ceph.client.cinder.keyring\nssh {your-cinder-volume-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring\nceph auth get-or-create client.cinder-backup | ssh {your-cinder-backup-server} sudo tee /etc/ceph/ceph.client.cinder-backup.keyring\nssh {your-cinder-backup-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder-backup.keyring\n</code></pre> <p>Nodes running\u00a0<code>nova-compute</code>\u00a0need the keyring file for the\u00a0<code>nova-compute</code>\u00a0process:</p> <pre><code>ceph auth get-or-create client.cinder | ssh {your-nova-compute-server} sudo tee /etc/ceph/ceph.client.cinder.keyring\n</code></pre> <p>They also need to store the secret key of the\u00a0<code>client.cinder</code>\u00a0user in\u00a0<code>libvirt</code>. The libvirt process needs it to access the cluster while attaching a block device from Cinder.</p> <p>Create a temporary copy of the secret key on the nodes running\u00a0<code>nova-compute</code>:</p> <pre><code>ceph auth get-key client.cinder | ssh {your-compute-node} tee client.cinder.key\n</code></pre> <p>Then, on the compute nodes, add the secret key to\u00a0<code>libvirt</code>\u00a0and remove the temporary copy of the key:</p> <pre><code>uuidgen\n457eb676-33da-42ec-9a8c-9293d545c337\n\ncat &gt; secret.xml &lt;&lt;EOF\n&lt;secret ephemeral='no' private='no'&gt;\n  &lt;uuid&gt;457eb676-33da-42ec-9a8c-9293d545c337&lt;/uuid&gt;\n  &lt;usage type='ceph'&gt;\n    &lt;name&gt;client.cinder secret&lt;/name&gt;\n  &lt;/usage&gt;\n&lt;/secret&gt;\nEOF\nsudo virsh secret-define --file secret.xml\nSecret 457eb676-33da-42ec-9a8c-9293d545c337 created\nsudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) &amp;&amp; rm client.cinder.key secret.xml\n</code></pre> <p>Save the uuid of the secret for configuring\u00a0<code>nova-compute</code>\u00a0later.</p> <p>Important You don\u2019t necessarily need the UUID on all the compute nodes. However from a platform consistency perspective, it\u2019s better to keep the same UUID.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#configure-openstack-to-use-ceph","title":"CONFIGURE OPENSTACK TO USE CEPH","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#configuring-glance","title":"CONFIGURING GLANCE","text":"<p>Glance can use multiple back ends to store images. To use Ceph block devices by default, configure Glance like the following.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#kilo-and-after","title":"KILO AND AFTER","text":"<p>Edit\u00a0<code>/etc/glance/glance-api.conf</code>\u00a0and add under the\u00a0<code>[glance_store]</code>\u00a0section:</p> <pre><code>[glance_store]\nstores = rbd\ndefault_store = rbd\nrbd_store_pool = images\nrbd_store_user = glance\nrbd_store_ceph_conf = /etc/ceph/ceph.conf\nrbd_store_chunk_size = 8\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#enable-copy-on-write-cloning-of-images","title":"ENABLE COPY-ON-WRITE CLONING OF IMAGES","text":"<p>Note that this exposes the back end location via Glance\u2019s API, so the endpoint with this option enabled should not be publicly accessible.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#any-openstack-version-except-mitaka","title":"ANY OPENSTACK VERSION EXCEPT MITAKA","text":"<p>If you want to enable copy-on-write cloning of images, also add under the\u00a0<code>[DEFAULT]</code>\u00a0section:</p> <pre><code>show_image_direct_url = True\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#disable-cache-management-any-openstack-version","title":"DISABLE CACHE MANAGEMENT (ANY OPENSTACK VERSION)","text":"<p>Disable the Glance cache management to avoid images getting cached under\u00a0<code>/var/lib/glance/image-cache/</code>, assuming your configuration file has\u00a0<code>flavor\u00a0=\u00a0keystone+cachemanagement</code>:</p> <pre><code>[paste_deploy]\nflavor = keystone\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#image-properties","title":"IMAGE PROPERTIES","text":"<p>We recommend to use the following properties for your images:</p> <ul> <li><code>hw_scsi_model=virtio-scsi</code></li> </ul> add the virtio-scsi controller and get better performance and support for discard operation <ul> <li><code>hw_disk_bus=scsi</code></li> </ul> connect every cinder block devices to that controller <ul> <li><code>hw_qemu_guest_agent=yes</code></li> </ul> enable the QEMU guest agent <ul> <li><code>os_require_quiesce=yes</code></li> </ul> send fs-freeze/thaw calls through the QEMU guest agent"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#configuring-cinder","title":"CONFIGURING CINDER","text":"<p>OpenStack requires a driver to interact with Ceph block devices. You must also specify the pool name for the block device. On your OpenStack node, edit\u00a0<code>/etc/cinder/cinder.conf</code>\u00a0by adding:</p> <pre><code>[DEFAULT]\n...\nenabled_backends = ceph\nglance_api_version = 2\n...\n[ceph]\nvolume_driver = cinder.volume.drivers.rbd.RBDDriver\nvolume_backend_name = ceph\nrbd_pool = volumes\nrbd_ceph_conf = /etc/ceph/ceph.conf\nrbd_flatten_volume_from_snapshot = false\nrbd_max_clone_depth = 5\nrbd_store_chunk_size = 4\nrados_connect_timeout = -1\n</code></pre> <p>If you are using\u00a0cephx authentication, also configure the user and uuid of the secret you added to\u00a0<code>libvirt</code>\u00a0as documented earlier:</p> <pre><code>[ceph]\n...\nrbd_user = cinder\nrbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337\n</code></pre> <p>Note that if you are configuring multiple cinder back ends,\u00a0<code>glance_api_version\u00a0=\u00a02</code>\u00a0must be in the\u00a0<code>[DEFAULT]</code>\u00a0section.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#configuring-cinder-backup","title":"CONFIGURING CINDER BACKUP","text":"<p>OpenStack Cinder Backup requires a specific daemon so don\u2019t forget to install it. On your Cinder Backup node, edit\u00a0<code>/etc/cinder/cinder.conf</code>\u00a0and add:</p> <pre><code>backup_driver = cinder.backup.drivers.ceph\nbackup_ceph_conf = /etc/ceph/ceph.conf\nbackup_ceph_user = cinder-backup\nbackup_ceph_chunk_size = 134217728\nbackup_ceph_pool = backups\nbackup_ceph_stripe_unit = 0\nbackup_ceph_stripe_count = 0\nrestore_discard_excess_bytes = true\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#configuring-nova-to-attach-ceph-rbd-block-device","title":"CONFIGURING NOVA TO ATTACH CEPH RBD BLOCK DEVICE","text":"<p>In order to attach Cinder devices (either normal block or by issuing a boot from volume), you must tell Nova (and libvirt) which user and UUID to refer to when attaching the device. libvirt will refer to this user when connecting and authenticating with the Ceph cluster.</p> <pre><code>[libvirt]\n...\nrbd_user = cinder\nrbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337\n</code></pre> <p>These two flags are also used by the Nova ephemeral back end.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#configuring-nova","title":"CONFIGURING NOVA","text":"<p>In order to boot virtual machines directly from Ceph volumes, you must configure the ephemeral backend for Nova.</p> <p>It is recommended to enable the RBD cache in your Ceph configuration file; this has been enabled by default since the Giant release. Moreover, enabling the client admin socket allows the collection of metrics and can be invaluable for troubleshooting.</p> <p>This socket can be accessed on the hypervisor (Nova compute) node:</p> <pre><code>ceph daemon /var/run/ceph/ceph-client.cinder.19195.32310016.asok help\n</code></pre> <p>To enable RBD cache and admin sockets, ensure that on each hypervisor\u2019s\u00a0<code>ceph.conf</code>\u00a0contains:</p> <pre><code>[client]\n    rbd cache = true\n    rbd cache writethrough until flush = true\n    admin socket = /var/run/ceph/guests/$cluster-$type.$id.$pid.$cctid.asok\n    log file = /var/log/qemu/qemu-guest-$pid.log\n    rbd concurrent management ops = 20\n</code></pre> <p>Configure permissions for these directories:</p> <pre><code>mkdir -p /var/run/ceph/guests/ /var/log/qemu/\nchown qemu:libvirtd /var/run/ceph/guests /var/log/qemu/\n</code></pre> <p>Note that user\u00a0<code>qemu</code>\u00a0and group\u00a0<code>libvirtd</code>\u00a0can vary depending on your system. The provided example works for RedHat based systems.</p> <p>Tip If your virtual machine is already running you can simply restart it to enable the admin socket</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#restart-openstack","title":"RESTART OPENSTACK","text":"<p>To activate the Ceph block device driver and load the block device pool name into the configuration, you must restart the related OpenStack services. For Debian based systems execute these commands on the appropriate nodes:</p> <pre><code>sudo glance-control api restart\nsudo service nova-compute restart\nsudo service cinder-volume restart\nsudo service cinder-backup restart\n</code></pre> <p>For Red Hat based systems execute:</p> <pre><code>sudo service openstack-glance-api restart\nsudo service openstack-nova-compute restart\nsudo service openstack-cinder-volume restart\nsudo service openstack-cinder-backup restart\n</code></pre> <p>Once OpenStack is up and running, you should be able to create a volume and boot from it.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Block_Devices/#booting-from-a-block-device","title":"BOOTING FROM A BLOCK DEVICE","text":"<p>You can create a volume from an image using the Cinder command line tool:</p> <pre><code>cinder create --image-id {id of image} --display-name {name of volume} {size of volume}\n</code></pre> <p>You can use\u00a0qemu-img\u00a0to convert from one format to another. For example:</p> <pre><code>qemu-img convert -f {source-format} -O {output-format} {source-filename} {output-filename}\nqemu-img convert -f qcow2 -O raw precise-cloudimg.img precise-cloudimg.raw\n</code></pre> <p>When Glance and Cinder are both using Ceph block devices, the image is a copy-on-write clone, so new volumes are created quickly. In the OpenStack dashboard, you can boot from that volume by performing the following steps:</p> <p>     Launch a new instance.   </p> <p>     Choose the image associated to the copy-on-write clone.   </p> <p>     Select \u2018boot from volume\u2019.   </p> <p>     Select the volume you created.   </p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/","title":"ARCHITECTURE","text":"<p>Ceph\u00a0uniquely delivers\u00a0object, block, and file storage\u00a0in one unified system. Ceph is highly reliable, easy to manage, and free. The power of Ceph can transform your company\u2019s IT infrastructure and your ability to manage vast amounts of data. Ceph delivers extraordinary scalability\u2013thousands of clients accessing petabytes to exabytes of data. A\u00a0Ceph Node\u00a0leverages commodity hardware and intelligent daemons, and a\u00a0Ceph Storage Cluster\u00a0accommodates large numbers of nodes, which communicate with each other to replicate and redistribute data dynamically.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#the-ceph-storage-cluster","title":"THE CEPH STORAGE CLUSTER","text":"<p>Ceph provides an infinitely scalable\u00a0Ceph Storage Cluster\u00a0based upon\u00a0RADOS, a reliable, distributed storage service that uses the intelligence in each of its nodes to secure the data it stores and to provide that data to client. See Sage Weil\u2019s \u201cThe RADOS Object Store\u201d blog post for a brief explanation of RADOS and see\u00a0RADOS \u2013 A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters\u00a0for an exhaustive explanation of\u00a0RADOS.</p> <p>A Ceph Storage Cluster consists of multiple types of daemons:</p> <ul> <li> <p>Ceph Monitor</p> </li> <li> <p>Ceph OSD Daemon</p> </li> <li> <p>Ceph Manager</p> </li> <li> <p>Ceph Metadata Server</p> </li> </ul> <p>Ceph Monitors maintain the master copy of the cluster map, which they provide to Ceph clients. The existence of multiple monitors in the Ceph cluster ensures availability if one of the monitor daemons or its host fails.</p> <p>A Ceph OSD Daemon checks its own state and the state of other OSDs and reports back to monitors.</p> <p>A Ceph Manager serves as an endpoint for monitoring, orchestration, and plug-in modules.</p> <p>A Ceph Metadata Server (MDS) manages file metadata when CephFS is used to provide file services.</p> <p>Storage cluster clients and\u00a0Ceph OSD Daemons use the CRUSH algorithm to compute information about the location of data. Use of the CRUSH algoritm means that clients and OSDs are not bottlenecked by a central lookup table. Ceph\u2019s high-level features include a native interface to the Ceph Storage Cluster via\u00a0<code>librados</code>, and a number of service interfaces built on top of\u00a0<code>librados</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#storing-data","title":"STORING DATA","text":"<p>The Ceph Storage Cluster receives data from\u00a0Ceph Clients\u2013whether it comes through a\u00a0Ceph Block Device,\u00a0Ceph Object Storage, the\u00a0Ceph File System, or a custom implementation that you create by using\u00a0<code>librados</code>. The data received by the Ceph Storage Cluster is stored as RADOS objects. Each object is stored on an\u00a0Object Storage Device\u00a0(this is also called an \u201cOSD\u201d). Ceph OSDs control read, write, and replication operations on storage drives. The default BlueStore back end stores objects in a monolithic, database-like fashion.</p> <p></p> <p>Ceph OSD Daemons store data as objects in a flat namespace. This means that objects are not stored in a hierarchy of directories. An object has an identifier, binary data, and metadata consisting of name/value pairs.\u00a0Ceph Clients determine the semantics of the object data. For example, CephFS uses metadata to store file attributes such as the file owner, the created date, and the last modified date.</p> <p></p> <p>Note An object ID is unique across the entire cluster, not just the local filesystem.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#scalability-and-high-availability","title":"SCALABILITY AND HIGH AVAILABILITY","text":"<p>In traditional architectures, clients talk to a centralized component. This centralized component might be a gateway, a broker, an API, or a facade. A centralized component of this kind acts as a single point of entry to a complex subsystem. Architectures that rely upon such a centralized component have a single point of failure and incur limits to performance and scalability. If the centralized component goes down, the whole system becomes unavailable.</p> <p>Ceph eliminates this centralized component. This enables clients to interact with Ceph OSDs directly. Ceph OSDs create object replicas on other Ceph Nodes to ensure data safety and high availability. Ceph also uses a cluster of monitors to ensure high availability. To eliminate centralization, Ceph uses an algorithm called\u00a0CRUSH.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#crush-introduction","title":"CRUSH INTRODUCTION","text":"<p>Ceph Clients and Ceph OSD Daemons both use the\u00a0CRUSH\u00a0algorithm to compute information about object location instead of relying upon a central lookup table. CRUSH provides a better data management mechanism than do older approaches, and CRUSH enables massive scale by distributing the work to all the OSD daemons in the cluster and all the clients that communicate with them. CRUSH uses intelligent data replication to ensure resiliency, which is better suited to hyper-scale storage. The following sections provide additional details on how CRUSH works. For a detailed discussion of CRUSH, see\u00a0CRUSH \u2013 Controlled, Scalable, Decentralized Placement of Replicated Data.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#cluster-map","title":"CLUSTER MAP","text":"<p>In order for a Ceph cluster to function properly, Ceph Clients and Ceph OSDs must have current information about the cluster\u2019s topology. Current information is stored in the \u201cCluster Map\u201d, which is in fact a collection of five maps. The five maps that constitute the cluster map are:</p> <ol> <li> <p>The Monitor Map:\u00a0Contains the cluster\u00a0<code>fsid</code>, the position, the name, the address, and the TCP port of each monitor. The monitor map specifies the current epoch, the time of the monitor map\u2019s creation, and the time of the monitor map\u2019s last modification. To view a monitor map, run\u00a0<code>ceph\u00a0mon\u00a0dump</code>.</p> </li> <li> <p>The OSD Map:\u00a0Contains the cluster\u00a0<code>fsid</code>, the time of the OSD map\u2019s creation, the time of the OSD map\u2019s last modification, a list of pools, a list of replica sizes, a list of PG numbers, and a list of OSDs and their statuses (for example,\u00a0<code>up</code>,\u00a0<code>in</code>). To view an OSD map, run\u00a0<code>ceph\u00a0osd\u00a0dump</code>.</p> </li> <li> <p>The PG Map:\u00a0Contains the PG version, its time stamp, the last OSD map epoch, the full ratios, and the details of each placement group. This includes the PG ID, the\u00a0Up Set, the\u00a0Acting Set, the state of the PG (for example,\u00a0<code>active\u00a0+\u00a0clean</code>), and data usage statistics for each pool.</p> </li> <li> <p>The CRUSH Map:\u00a0Contains a list of storage devices, the failure domain hierarchy (for example,\u00a0<code>device</code>,\u00a0<code>host</code>,\u00a0<code>rack</code>,\u00a0<code>row</code>,\u00a0<code>room</code>), and rules for traversing the hierarchy when storing data. To view a CRUSH map, run\u00a0<code>ceph\u00a0osd\u00a0getcrushmap\u00a0-o\u00a0{filename}</code>\u00a0and then decompile it by running\u00a0<code>crushtool\u00a0-d\u00a0{comp-crushmap-filename}\u00a0-o\u00a0{decomp-crushmap-filename}</code>. Use a text editor or\u00a0<code>cat</code>\u00a0to view the decompiled map.</p> </li> <li> <p>The MDS Map:\u00a0Contains the current MDS map epoch, when the map was created, and the last time it changed. It also contains the pool for storing metadata, a list of metadata servers, and which metadata servers are\u00a0<code>up</code>\u00a0and\u00a0<code>in</code>. To view an MDS map, execute\u00a0<code>ceph\u00a0fs\u00a0dump</code>.</p> </li> </ol> <p>Each map maintains a history of changes to its operating state. Ceph Monitors maintain a master copy of the cluster map. This master copy includes the cluster members, the state of the cluster, changes to the cluster, and information recording the overall health of the Ceph Storage Cluster.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#high-availability-monitors","title":"HIGH AVAILABILITY MONITORS","text":"<p>A Ceph Client must contact a Ceph Monitor and obtain a current copy of the cluster map in order to read data from or to write data to the Ceph cluster.</p> <p>It is possible for a Ceph cluster to function properly with only a single monitor, but a Ceph cluster that has only a single monitor has a single point of failure: if the monitor goes down, Ceph clients will be unable to read data from or write data to the cluster.</p> <p>Ceph leverages a cluster of monitors in order to increase reliability and fault tolerance. When a cluster of monitors is used, however, one or more of the monitors in the cluster can fall behind due to latency or other faults. Ceph mitigates these negative effects by requiring multiple monitor instances to agree about the state of the cluster. To establish consensus among the monitors regarding the state of the cluster, Ceph uses the\u00a0Paxos\u00a0algorithm and a majority of monitors (for example, one in a cluster that contains only one monitor, two in a cluster that contains three monitors, three in a cluster that contains five monitors, four in a cluster that contains six monitors, and so on).</p> <p>See the\u00a0Monitor Config Reference\u00a0for more detail on configuring monitors.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#high-availability-authentication","title":"HIGH AVAILABILITY AUTHENTICATION","text":"<p>The\u00a0<code>cephx</code>\u00a0authentication system is used by Ceph to authenticate users and daemons and to protect against man-in-the-middle attacks.</p> <p>Note The\u00a0<code>cephx</code>\u00a0protocol does not address data encryption in transport (for example, SSL/TLS) or encryption at rest.</p> <p><code>cephx</code>\u00a0uses shared secret keys for authentication. This means that both the client and the monitor cluster keep a copy of the client\u2019s secret key.</p> <p>The\u00a0<code>cephx</code>\u00a0protocol makes it possible for each party to prove to the other that it has a copy of the key without revealing it. This provides mutual authentication and allows the cluster to confirm (1) that the user has the secret key and (2) that the user can be confident that the cluster has a copy of the secret key.</p> <p>As stated in\u00a0Scalability and High Availability, Ceph does not have any centralized interface between clients and the Ceph object store. By avoiding such a centralized interface, Ceph avoids the bottlenecks that attend such centralized interfaces. However, this means that clients must interact directly with OSDs. Direct interactions between Ceph clients and OSDs require authenticated connections. The\u00a0<code>cephx</code>\u00a0authentication system establishes and sustains these authenticated connections.</p> <p>The\u00a0<code>cephx</code>\u00a0protocol operates in a manner similar to\u00a0Kerberos.</p> <p>A user invokes a Ceph client to contact a monitor. Unlike Kerberos, each monitor can authenticate users and distribute keys, which means that there is no single point of failure and no bottleneck when using\u00a0<code>cephx</code>. The monitor returns an authentication data structure that is similar to a Kerberos ticket. This authentication data structure contains a session key for use in obtaining Ceph services. The session key is itself encrypted with the user\u2019s permanent secret key, which means that only the user can request services from the Ceph Monitors. The client then uses the session key to request services from the monitors, and the monitors provide the client with a ticket that authenticates the client against the OSDs that actually handle data. Ceph Monitors and OSDs share a secret, which means that the clients can use the ticket provided by the monitors to authenticate against any OSD or metadata server in the cluster.</p> <p>Like Kerberos tickets,\u00a0<code>cephx</code>\u00a0tickets expire. An attacker cannot use an expired ticket or session key that has been obtained surreptitiously. This form of authentication prevents attackers who have access to the communications medium from creating bogus messages under another user\u2019s identity and prevents attackers from altering another user\u2019s legitimate messages, as long as the user\u2019s secret key is not divulged before it expires.</p> <p>An administrator must set up users before using\u00a0<code>cephx</code>. In the following diagram, the\u00a0<code>client.admin</code>\u00a0user invokes\u00a0<code>ceph\u00a0auth\u00a0get-or-create-key</code>\u00a0from the command line to generate a username and secret key. Ceph\u2019s\u00a0<code>auth</code>\u00a0subsystem generates the username and key, stores a copy on the monitor(s), and transmits the user\u2019s secret back to the\u00a0<code>client.admin</code>\u00a0user. This means that the client and the monitor share a secret key.</p> <p>Note The\u00a0<code>client.admin</code>\u00a0user must provide the user ID and secret key to the user in a secure manner.</p> <p></p> <p>Here is how a client authenticates with a monitor. The client passes the user name to the monitor. The monitor generates a session key that is encrypted with the secret key associated with the\u00a0<code>username</code>. The monitor transmits the encrypted ticket to the client. The client uses the shared secret key to decrypt the payload. The session key identifies the user, and this act of identification will last for the duration of the session. The client requests a ticket for the user, and the ticket is signed with the session key. The monitor generates a ticket and uses the user\u2019s secret key to encrypt it. The encrypted ticket is transmitted to the client. The client decrypts the ticket and uses it to sign requests to OSDs and to metadata servers in the cluster.</p> <p></p> <p>The\u00a0<code>cephx</code>\u00a0protocol authenticates ongoing communications between the clients and Ceph daemons. After initial authentication, each message sent between a client and a daemon is signed using a ticket that can be verified by monitors, OSDs, and metadata daemons. This ticket is verified by using the secret shared between the client and the daemon.</p> <p></p> <p>This authentication protects only the connections between Ceph clients and Ceph daemons. The authentication is not extended beyond the Ceph client. If a user accesses the Ceph client from a remote host, cephx authentication will not be applied to the connection between the user\u2019s host and the client host.</p> <p>See\u00a0Cephx Config Guide\u00a0for more on configuration details.</p> <p>See\u00a0User Management\u00a0for more on user management.</p> <p>See\u00a0A Detailed Description of the Cephx Authentication Protocol\u00a0for more on the distinction between authorization and authentication and for a step-by-step explanation of the setup of\u00a0<code>cephx</code>\u00a0tickets and session keys.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#smart-daemons-enable-hyperscale","title":"SMART DAEMONS ENABLE HYPERSCALE","text":"<p>A feature of many storage clusters is a centralized interface that keeps track of the nodes that clients are permitted to access. Such centralized architectures provide services to clients by means of a double dispatch. At the petabyte-to-exabyte scale, such double dispatches are a significant bottleneck.</p> <p>Ceph obviates this bottleneck: Ceph\u2019s OSD Daemons AND Ceph clients are cluster-aware. Like Ceph clients, each Ceph OSD Daemon is aware of other Ceph OSD Daemons in the cluster. This enables Ceph OSD Daemons to interact directly with other Ceph OSD Daemons and to interact directly with Ceph Monitors. Being cluster-aware makes it possible for Ceph clients to interact directly with Ceph OSD Daemons.</p> <p>Because Ceph clients, Ceph monitors, and Ceph OSD daemons interact with one another directly, Ceph OSD daemons can make use of the aggregate CPU and RAM resources of the nodes in the Ceph cluster. This means that a Ceph cluster can easily perform tasks that a cluster with a centralized interface would struggle to perform. The ability of Ceph nodes to make use of the computing power of the greater cluster provides several benefits:</p> <ol> <li> <p>OSDs Service Clients Directly:\u00a0Network devices can support only a limited number of concurrent connections. Because Ceph clients contact Ceph OSD daemons directly without first connecting to a central interface, Ceph enjoys improved perfomance and increased system capacity relative to storage redundancy strategies that include a central interface. Ceph clients maintain sessions only when needed, and maintain those sessions with only particular Ceph OSD daemons, not with a centralized interface.</p> </li> <li> <p>OSD Membership and Status: When Ceph OSD Daemons join a cluster, they report their status. At the lowest level, the Ceph OSD Daemon status is\u00a0<code>up</code>\u00a0or\u00a0<code>down</code>: this reflects whether the Ceph OSD daemon is running and able to service Ceph Client requests. If a Ceph OSD Daemon is\u00a0<code>down</code>\u00a0and\u00a0<code>in</code>\u00a0the Ceph Storage Cluster, this status may indicate the failure of the Ceph OSD Daemon. If a Ceph OSD Daemon is not running because it has crashed, the Ceph OSD Daemon cannot notify the Ceph Monitor that it is\u00a0<code>down</code>. The OSDs periodically send messages to the Ceph Monitor (in releases prior to Luminous, this was done by means of\u00a0<code>MPGStats</code>, and beginning with the Luminous release, this has been done with\u00a0<code>MOSDBeacon</code>). If the Ceph Monitors receive no such message after a configurable period of time, then they mark the OSD\u00a0<code>down</code>. This mechanism is a failsafe, however. Normally, Ceph OSD Daemons determine if a neighboring OSD is\u00a0<code>down</code>\u00a0and report it to the Ceph Monitors. This contributes to making Ceph Monitors lightweight processes. See\u00a0Monitoring OSDs\u00a0and\u00a0Heartbeats\u00a0for additional details.</p> </li> <li> <p>Data Scrubbing:\u00a0To maintain data consistency, Ceph OSD Daemons scrub RADOS objects. Ceph OSD Daemons compare the metadata of their own local objects against the metadata of the replicas of those objects, which are stored on other OSDs. Scrubbing occurs on a per-Placement-Group basis, finds mismatches in object size and finds metadata mismatches, and is usually performed daily. Ceph OSD Daemons perform deeper scrubbing by comparing the data in objects, bit-for-bit, against their checksums. Deep scrubbing finds bad sectors on drives that are not detectable with light scrubs. See\u00a0Data Scrubbing\u00a0for details on configuring scrubbing.</p> </li> <li> <p>Replication:\u00a0Data replication involves a collaboration between Ceph Clients and Ceph OSD Daemons. Ceph OSD Daemons use the CRUSH algorithm to determine the storage location of object replicas. Ceph clients use the CRUSH algorithm to determine the storage location of an object, then the object is mapped to a pool and to a placement group, and then the client consults the CRUSH map to identify the placement group\u2019s primary OSD.After identifying the target placement group, the client writes the object to the identified placement group\u2019s primary OSD. The primary OSD then consults its own copy of the CRUSH map to identify secondary and tertiary OSDS, replicates the object to the placement groups in those secondary and tertiary OSDs, confirms that the object was stored successfully in the secondary and tertiary OSDs, and reports to the client that the object was stored successfully.</p> </li> </ol> <p></p> <p>By performing this act of data replication, Ceph OSD Daemons relieve Ceph clients of the burden of replicating data.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#dynamic-cluster-management","title":"DYNAMIC CLUSTER MANAGEMENT","text":"<p>In the\u00a0Scalability and High Availability\u00a0section, we explained how Ceph uses CRUSH, cluster topology, and intelligent daemons to scale and maintain high availability. Key to Ceph\u2019s design is the autonomous, self-healing, and intelligent Ceph OSD Daemon. Let\u2019s take a deeper look at how CRUSH works to enable modern cloud storage infrastructures to place data, rebalance the cluster, and adaptively place and balance data and recover from faults.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#about-pools","title":"ABOUT POOLS","text":"<p>The Ceph storage system supports the notion of \u2018Pools\u2019, which are logical partitions for storing objects.</p> <p>Ceph Clients retrieve a\u00a0Cluster Map\u00a0from a Ceph Monitor, and write RADOS objects to pools. The way that Ceph places the data in the pools is determined by the pool\u2019s\u00a0<code>size</code>\u00a0or number of replicas, the CRUSH rule, and the number of placement groups in the pool.</p> <p></p> <p>Pools set at least the following parameters:</p> <ul> <li> <p>Ownership/Access to Objects</p> </li> <li> <p>The Number of Placement Groups, and</p> </li> <li> <p>The CRUSH Rule to Use.</p> </li> </ul> <p>See\u00a0Set Pool Values\u00a0for details.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#mapping-pgs-to-osds","title":"MAPPING PGS TO OSDS","text":"<p>Each pool has a number of placement groups (PGs) within it. CRUSH dynamically maps PGs to OSDs. When a Ceph Client stores objects, CRUSH maps each RADOS object to a PG.</p> <p>This mapping of RADOS objects to PGs implements an abstraction and indirection layer between Ceph OSD Daemons and Ceph Clients. The Ceph Storage Cluster must be able to grow (or shrink) and redistribute data adaptively when the internal topology changes.</p> <p>If the Ceph Client \u201cknew\u201d which Ceph OSD Daemons were storing which objects, a tight coupling would exist between the Ceph Client and the Ceph OSD Daemon. But Ceph avoids any such tight coupling. Instead, the CRUSH algorithm maps each RADOS object to a placement group and then maps each placement group to one or more Ceph OSD Daemons. This \u201clayer of indirection\u201d allows Ceph to rebalance dynamically when new Ceph OSD Daemons and their underlying OSD devices come online. The following diagram shows how the CRUSH algorithm maps objects to placement groups, and how it maps placement groups to OSDs.</p> <p></p> <p>The client uses its copy of the cluster map and the CRUSH algorithm to compute precisely which OSD it will use when reading or writing a particular object.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#calculating-pg-ids","title":"CALCULATING PG IDS","text":"<p>When a Ceph Client binds to a Ceph Monitor, it retrieves the latest version of the\u00a0Cluster Map. When a client has been equipped with a copy of the cluster map, it is aware of all the monitors, OSDs, and metadata servers in the cluster.\u00a0However, even equipped with a copy of the latest version of the cluster map, the client doesn\u2019t know anything about object locations.</p> <p>Object locations must be computed.</p> <p>The client requires only the object ID and the name of the pool in order to compute the object location.</p> <p>Ceph stores data in named pools (for example, \u201cliverpool\u201d). When a client stores a named object (for example, \u201cjohn\u201d, \u201cpaul\u201d, \u201cgeorge\u201d, or \u201cringo\u201d) it calculates a placement group by using the object name, a hash code, the number of PGs in the pool, and the pool name. Ceph clients use the following steps to compute PG IDs.</p> <ol> <li> <p>The client inputs the pool name and the object ID. (for example: pool = \u201cliverpool\u201d and object-id = \u201cjohn\u201d)</p> </li> <li> <p>Ceph hashes the object ID.</p> </li> <li> <p>Ceph calculates the hash, modulo the number of PGs (for example:\u00a0<code>58</code>), to get a PG ID.</p> </li> <li> <p>Ceph uses the pool name to retrieve the pool ID: (for example: \u201cliverpool\u201d =\u00a0<code>4</code>)</p> </li> <li> <p>Ceph prepends the pool ID to the PG ID (for example:\u00a0<code>4.58</code>).</p> </li> </ol> <p>It is much faster to compute object locations than to perform object location query over a chatty session. The\u00a0CRUSH\u00a0algorithm allows a client to compute where objects are expected to be stored, and enables the client to contact the primary OSD to store or retrieve the objects.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#peering-and-sets","title":"PEERING AND SETS","text":"<p>In previous sections, we noted that Ceph OSD Daemons check each other\u2019s heartbeats and report back to Ceph Monitors. Ceph OSD daemons also \u2018peer\u2019, which is the process of bringing all of the OSDs that store a Placement Group (PG) into agreement about the state of all of the RADOS objects (and their metadata) in that PG. Ceph OSD Daemons\u00a0Report Peering Failure\u00a0to the Ceph Monitors. Peering issues usually resolve themselves; however, if the problem persists, you may need to refer to the\u00a0Troubleshooting Peering Failure\u00a0section.</p> <p>Note PGs that agree on the state of the cluster do not necessarily have the current data yet.</p> <p>The Ceph Storage Cluster was designed to store at least two copies of an object (that is,\u00a0<code>size\u00a0=\u00a02</code>), which is the minimum requirement for data safety. For high availability, a Ceph Storage Cluster should store more than two copies of an object (that is,\u00a0<code>size\u00a0=\u00a03</code>\u00a0and\u00a0<code>min\u00a0size\u00a0=\u00a02</code>) so that it can continue to run in a\u00a0<code>degraded</code>\u00a0state while maintaining data safety.</p> <p>Warning Although we say here that R2 (replication with two copies) is the minimum requirement for data safety, R3 (replication with three copies) is recommended. On a long enough timeline, data stored with an R2 strategy will be lost.</p> <p>As explained in the diagram in\u00a0Smart Daemons Enable Hyperscale, we do not name the Ceph OSD Daemons specifically (for example,\u00a0<code>osd.0</code>,\u00a0<code>osd.1</code>, etc.), but rather refer to them as\u00a0Primary,\u00a0Secondary, and so forth. By convention, the\u00a0Primary\u00a0is the first OSD in the\u00a0Acting Set, and is responsible for orchestrating the peering process for each placement group where it acts as the\u00a0Primary. The\u00a0Primary\u00a0is the\u00a0ONLY\u00a0OSD in a given placement group that accepts client-initiated writes to objects.</p> <p>The set of OSDs that is responsible for a placement group is called the\u00a0Acting Set. The term \u201cActing Set\u201d can refer either to the Ceph OSD Daemons that are currently responsible for the placement group, or to the Ceph OSD Daemons that were responsible for a particular placement group as of some epoch.</p> <p>The Ceph OSD daemons that are part of an\u00a0Acting Set\u00a0might not always be\u00a0<code>up</code>. When an OSD in the\u00a0Acting Set\u00a0is\u00a0<code>up</code>, it is part of the\u00a0Up Set. The\u00a0Up Set\u00a0is an important distinction, because Ceph can remap PGs to other Ceph OSD Daemons when an OSD fails.</p> <p>Note Consider a hypothetical\u00a0Acting Set\u00a0for a PG that contains\u00a0<code>osd.25</code>,\u00a0<code>osd.32</code>\u00a0and\u00a0<code>osd.61</code>. The first OSD (<code>osd.25</code>), is the\u00a0Primary. If that OSD fails, the Secondary (<code>osd.32</code>), becomes the\u00a0Primary, and\u00a0<code>osd.25</code>\u00a0is removed from the\u00a0Up Set.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#rebalancing","title":"REBALANCING","text":"<p>When you add a Ceph OSD Daemon to a Ceph Storage Cluster, the cluster map gets updated with the new OSD. Referring back to\u00a0Calculating PG IDs, this changes the cluster map. Consequently, it changes object placement, because it changes an input for the calculations. The following diagram depicts the rebalancing process (albeit rather crudely, since it is substantially less impactful with large clusters) where some, but not all of the PGs migrate from existing OSDs (OSD 1, and OSD 2) to the new OSD (OSD 3). Even when rebalancing, CRUSH is stable. Many of the placement groups remain in their original configuration, and each OSD gets some added capacity, so there are no load spikes on the new OSD after rebalancing is complete.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Architecture/#data-consistency","title":"DATA CONSISTENCY","text":"<p>As part of maintaining data consistency and cleanliness, Ceph OSDs also scrub objects within placement groups. That is, Ceph OSDs compare object metadata in one placement group with its replicas in placement groups stored in other OSDs. Scrubbing (usually performed daily) catches OSD bugs or filesystem errors, often as a result of hardware issues. OSDs also perform deeper scrubbing by comparing data in objects bit-for-bit. Deep scrubbing (by default performed weekly) finds bad blocks on a drive that weren\u2019t apparent in a light scrub.</p> <p>See\u00a0Data Scrubbing\u00a0for details on configuring scrubbing.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Disaster_Recovery/","title":"Restoring Mon Quorum","text":"<p>Under extenuating circumstances, the mons may lose quorum. If the mons cannot form quorum again, there is a manual procedure to get the quorum going again. The only requirement is that at least one mon is still healthy. The following steps will remove the unhealthy mons from quorum and allow you to form a quorum again with a single mon, then grow the quorum back to the original size.</p> <p>The\u00a0Rook kubectl Plugin\u00a0has a command\u00a0<code>restore-quorum</code>\u00a0that will walk you through the mon quorum automated restoration process.</p> <p>If the name of the healthy mon is\u00a0<code>c</code>, you would run the command:</p> <pre><code>kubectl rook-ceph mons restore-quorum c\n</code></pre> <p>See the\u00a0restore-quorum documentation\u00a0for more details.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Disaster_Recovery/#restoring-crds-after-deletion","title":"Restoring CRDs After Deletion","text":"<p>When the Rook CRDs are deleted, the Rook operator will respond to the deletion event to attempt to clean up the cluster resources. If any data appears present in the cluster, Rook will refuse to allow the resources to be deleted since the operator will refuse to remove the finalizer on the CRs until the underlying data is deleted. For more details, see the\u00a0dependency design doc.</p> <p>While it is good that the CRs will not be deleted and the underlying Ceph data and daemons continue to be available, the CRs will be stuck indefinitely in a\u00a0<code>Deleting</code>\u00a0state in which the operator will not continue to ensure cluster health. Upgrades will be blocked, further updates to the CRs are prevented, and so on. Since Kubernetes does not allow undeleting resources, the following procedure will allow you to restore the CRs to their prior state without even necessarily suffering cluster downtime.</p> <p>Note In the following commands, the affected\u00a0<code>CephCluster</code>\u00a0resource is called\u00a0<code>rook-ceph</code>. If yours is named differently, the commands will need to be adjusted.</p> <p>1. Scale down the operator.1</p> <pre><code>kubectl -n rook-ceph scale --replicas=0 deploy/rook-ceph-operator\n</code></pre> <p>2. Backup all Rook CRs and critical metadata</p> <pre><code># Store the `CephCluster` CR settings. Also, save other Rook CRs that are in terminating state.\nkubectl -n rook-ceph get cephcluster rook-ceph -o yaml &gt; cluster.yaml\n\n# Backup critical secrets and configmaps in case something goes wrong later in the procedure\nkubectl -n rook-ceph get secret -o yaml &gt; secrets.yaml\nkubectl -n rook-ceph get configmap -o yaml &gt; configmaps.yaml\n</code></pre> <p>3. Remove the owner references from all critical Rook resources that were referencing the\u00a0<code>CephCluster</code>\u00a0CR.</p> <p>a. Programmatically determine all such resources, using this command:</p> <pre><code># Determine the `CephCluster` UID\nROOK_UID=$(kubectl -n rook-ceph get cephcluster rook-ceph -o 'jsonpath={.metadata.uid}')\n# List all secrets, configmaps, services, deployments, and PVCs with that ownership UID.\nRESOURCES=$(kubectl -n rook-ceph get secret,configmap,service,deployment,pvc -o jsonpath='{range .items[?(@.metadata.ownerReferences[*].uid==\"'\"$ROOK_UID\"'\")]}{.kind}{\"/\"}{.metadata.name}{\"\\n\"}{end}')\n# Show the collected resources.\nkubectl -n rook-ceph get $RESOURCES\n</code></pre> <p>b. Verify that all critical resources are shown in the output.\u00a0The critical resources are these:</p> <ul> <li> <p>Secrets:\u00a0<code>rook-ceph-admin-keyring</code>,\u00a0<code>rook-ceph-config</code>,\u00a0<code>rook-ceph-mon</code>,\u00a0<code>rook-ceph-mons-keyring</code></p> </li> <li> <p>ConfigMap:\u00a0<code>rook-ceph-mon-endpoints</code></p> </li> <li> <p>Services:\u00a0<code>rook-ceph-mon-*</code>,\u00a0<code>rook-ceph-mgr-*</code></p> </li> <li> <p>Deployments:\u00a0<code>rook-ceph-mon-*</code>,\u00a0<code>rook-ceph-osd-*</code>,\u00a0<code>rook-ceph-mgr-*</code></p> </li> <li> <p>PVCs (if applicable):\u00a0<code>rook-ceph-mon-*</code>\u00a0and the OSD PVCs (named\u00a0<code>&lt;deviceset&gt;-*</code>, for example\u00a0<code>set1-data-*</code>)</p> </li> </ul> <p>c. For each listed resource, remove the\u00a0<code>ownerReferences</code>\u00a0metadata field, in order to unlink it from the deleting\u00a0<code>CephCluster</code>\u00a0CR. To do so programmatically, use the command:</p> <pre><code>for resource in $(kubectl -n rook-ceph get $RESOURCES -o name); do\n  kubectl -n rook-ceph patch $resource -p '{\"metadata\": {\"ownerReferences\":null}}'\ndone\n</code></pre> <p>d. For a manual alternative, issue\u00a0<code>kubectl edit</code>\u00a0on each resource, and remove the block matching:</p> <pre><code>ownerReferences:\n- apiVersion: ceph.rook.io/v1\n   blockOwnerDeletion: true\n   controller: true\n   kind: `CephCluster`\n   name: rook-ceph\n   uid: &lt;uid&gt;\n</code></pre> <p>4. Before completing this step, validate these things. Failing to do so could result in data loss.</p> <ul> <li> <p>Confirm that\u00a0<code>cluster.yaml</code>\u00a0contains the\u00a0<code>CephCluster</code>\u00a0CR.</p> </li> <li> <p>Confirm all critical resources listed above have had the\u00a0<code>ownerReference</code>\u00a0to the\u00a0<code>CephCluster</code>\u00a0CR removed</p> </li> </ul> <p>Remove the finalizer from the\u00a0<code>CephCluster</code>\u00a0resource. This will cause the resource to be immediately deleted by Kubernetes.</p> <pre><code>kubectl -n rook-ceph patch cephcluster/rook-ceph --type json --patch='[ { \"op\": \"remove\", \"path\": \"/metadata/finalizers\" } ]'\n</code></pre> <p>After the finalizer is removed, the\u00a0<code>CephCluster</code>\u00a0will be immediately deleted. If all owner references were properly removed, all ceph daemons will continue running and there will be no downtime.</p> <p>5. Create the\u00a0<code>CephCluster</code>\u00a0CR with the same settings as previously</p> <pre><code># Use the same cluster settings as exported in step 2.\nkubectl create -f cluster.yaml\n</code></pre> <p>6. If there are other CRs in terminating state such as CephBlockPools, CephObjectStores, or CephFilesystems, follow the above steps as well for those CRs:</p> <p>a. Backup the CR</p> <p>b. Remove the finalizer and confirm the CR is deleted (the underlying Ceph resources will be preserved)</p> <p>c. Create the CR again</p> <p>7. Scale up the operator</p> <pre><code>kubectl -n rook-ceph scale --replicas=1 deploy/rook-ceph-operator\n</code></pre> <p>8. Watch the operator log to confirm that the reconcile completes successfully.</p> <pre><code>kubectl -n rook-ceph logs -f deployment/rook-ceph-operator\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Disaster_Recovery/#adopt-an-existing-rook-ceph-cluster-into-a-new-kubernetes-cluster","title":"Adopt an existing Rook Ceph cluster into a new Kubernetes cluster","text":"<p>Situations this section can help resolve:</p> <ol> <li> <p>The Kubernetes environment underlying a running Rook Ceph cluster failed catastrophically, requiring a new Kubernetes environment in which the user wishes to recover the previous Rook Ceph cluster.</p> </li> <li> <p>The user wishes to migrate their existing Rook Ceph cluster to a new Kubernetes environment, and downtime can be tolerated.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Disaster_Recovery/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>A working Kubernetes cluster to which we will migrate the previous Rook Ceph cluster.</p> </li> <li> <p>At least one Ceph mon db is in quorum, and sufficient number of Ceph OSD is\u00a0<code>up</code>\u00a0and\u00a0<code>in</code>\u00a0before disaster.</p> </li> <li> <p>The previous Rook Ceph cluster is not running.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Disaster_Recovery/#overview-for-steps-below","title":"Overview for Steps below","text":"<ol> <li> <p>Start a new and clean Rook Ceph cluster, with old\u00a0<code>CephCluster</code> <code>CephBlockPool</code> <code>CephFilesystem</code> <code>CephNFS</code> <code>CephObjectStore</code>.</p> </li> <li> <p>Shut the new cluster down when it has been created successfully.</p> </li> <li> <p>Replace ceph-mon data with that of the old cluster.</p> </li> <li> <p>Replace\u00a0<code>fsid</code>\u00a0in\u00a0<code>secrets/rook-ceph-mon</code>\u00a0with that of the old one.</p> </li> <li> <p>Fix monmap in ceph-mon db.</p> </li> <li> <p>Fix ceph mon auth key.</p> </li> <li> <p>Disable auth.</p> </li> <li> <p>Start the new cluster, watch it resurrect.</p> </li> <li> <p>Fix admin auth key, and enable auth.</p> </li> <li> <p>Restart cluster for the final time.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Disaster_Recovery/#steps","title":"Steps","text":"<p>Assuming\u00a0<code>dataHostPathData</code>\u00a0is\u00a0<code>/var/lib/rook</code>, and the\u00a0<code>CephCluster</code>\u00a0trying to adopt is named\u00a0<code>rook-ceph</code>.</p> <ol> <li> <p>Make sure the old Kubernetes cluster is completely torn down and the new Kubernetes cluster is up and running without Rook Ceph.</p> </li> <li> <p>Backup\u00a0<code>/var/lib/rook</code>\u00a0in all the Rook Ceph nodes to a different directory. Backups will be used later.</p> </li> <li> <p>Pick a\u00a0<code>/var/lib/rook/rook-ceph/rook-ceph.config</code>\u00a0from any previous Rook Ceph node and save the old cluster\u00a0<code>fsid</code>\u00a0from its content.</p> </li> <li> <p>Remove\u00a0<code>/var/lib/rook</code>\u00a0from all the Rook Ceph nodes.</p> </li> <li> <p>Add identical\u00a0<code>CephCluster</code>\u00a0descriptor to the new Kubernetes cluster, especially identical\u00a0<code>spec.storage.config</code>\u00a0and\u00a0<code>spec.storage.nodes</code>, except\u00a0<code>mon.count</code>, which should be set to\u00a0<code>1</code>.</p> </li> <li> <p>Add identical\u00a0<code>CephFilesystem</code> <code>CephBlockPool</code> <code>CephNFS</code> <code>CephObjectStore</code>\u00a0descriptors (if any) to the new Kubernetes cluster.</p> </li> <li> <p>Install Rook Ceph in the new Kubernetes cluster.</p> </li> <li> <p>Watch the operator logs with\u00a0<code>kubectl -n rook-ceph logs -f rook-ceph-operator-xxxxxxx</code>, and wait until the orchestration has settled.</p> </li> <li> <p>STATE: Now the cluster will have\u00a0<code>rook-ceph-mon-a</code>,\u00a0<code>rook-ceph-mgr-a</code>, and all the auxiliary pods up and running, and zero (hopefully)\u00a0<code>rook-ceph-osd-ID-xxxxxx</code>\u00a0running.\u00a0<code>ceph -s</code>\u00a0output should report 1 mon, 1 mgr running, and all of the OSDs down, all PGs are in\u00a0<code>unknown</code>\u00a0state. Rook should not start any OSD daemon since all devices belongs to the old cluster (which have a different\u00a0<code>fsid</code>).</p> </li> <li> <p>Run\u00a0<code>kubectl -n rook-ceph exec -it rook-ceph-mon-a-xxxxxxxx bash</code>\u00a0to enter the\u00a0<code>rook-ceph-mon-a</code>\u00a0pod</p> </li> </ol> <pre><code>mon-a# cat /etc/ceph/keyring-store/keyring  # save this keyring content for later use\nmon-a# exit\n\n</code></pre> <p>11. Stop the Rook operator by running\u00a0<code>kubectl -n rook-ceph edit deploy/rook-ceph-operator</code>\u00a0and set\u00a0<code>replicas</code>\u00a0to\u00a0<code>0</code>.</p> <p>12. Stop cluster daemons by running\u00a0<code>kubectl -n rook-ceph delete deploy/X</code>\u00a0where X is every deployment in namespace\u00a0<code>rook-ceph</code>, except\u00a0<code>rook-ceph-operator</code>\u00a0and\u00a0<code>rook-ceph-tools</code>.</p> <p>13. Save the\u00a0<code>rook-ceph-mon-a</code>\u00a0address with\u00a0<code>kubectl -n rook-ceph get cm/rook-ceph-mon-endpoints -o yaml</code>\u00a0in the new Kubernetes cluster for later use.</p> <p>14. SSH to the host where\u00a0<code>rook-ceph-mon-a</code>\u00a0in the new Kubernetes cluster resides.</p> <p>a. Remove\u00a0<code>/var/lib/rook/mon-a</code></p> <p>b. Pick a healthy\u00a0<code>rook-ceph-mon-ID</code>\u00a0directory (<code>/var/lib/rook/mon-ID</code>) in the previous backup, copy to\u00a0<code>/var/lib/rook/mon-a</code>.\u00a0<code>ID</code>\u00a0is any healthy mon node ID of the old cluster.</p> <p>c. Replace\u00a0<code>/var/lib/rook/mon-a/keyring</code>\u00a0with the saved keyring, preserving only the\u00a0<code>[mon.]</code>\u00a0section, remove\u00a0<code>[client.admin]</code>\u00a0section.</p> <p>d. Run\u00a0<code>docker run -it --rm -v /var/lib/rook:/var/lib/rook ceph/ceph:v14.2.1-20190430 bash</code>. The Docker image tag should match the Ceph version used in the Rook cluster. The\u00a0<code>/etc/ceph/ceph.conf</code>\u00a0file needs to exist for\u00a0<code>ceph-mon</code>\u00a0to work.</p> <pre><code>touch /etc/ceph/ceph.conf\ncd /var/lib/rook\nceph-mon --extract-monmap monmap --mon-data ./mon-a/data  # Extract monmap from old ceph-mon db and save as monmap\nmonmaptool --print monmap  # Print the monmap content, which reflects the old cluster ceph-mon configuration.\nmonmaptool --rm a monmap  # Delete `a` from monmap.\nmonmaptool --rm b monmap  # Repeat, and delete `b` from monmap.\nmonmaptool --rm c monmap  # Repeat this pattern until all the old ceph-mons are removed\nmonmaptool --rm d monmap\nmonmaptool --rm e monmap\nmonmaptool --addv a [v2:10.77.2.216:3300,v1:10.77.2.216:6789] monmap   # Replace it with the rook-ceph-mon-a address you got from previous command.\nceph-mon --inject-monmap monmap --mon-data ./mon-a/data  # Replace monmap in ceph-mon db with our modified version.\nrm monmap\nexit\n</code></pre> <p>15. Tell Rook to run as old cluster by running\u00a0<code>kubectl -n rook-ceph edit secret/rook-ceph-mon</code>\u00a0and changing\u00a0<code>fsid</code>\u00a0to the original\u00a0<code>fsid</code>. Note that the\u00a0<code>fsid</code>\u00a0is base64 encoded and must not contain a trailing carriage return. For example:</p> <pre><code>echo -n a811f99a-d865-46b7-8f2c-f94c064e4356 | base64  # Replace with the fsid from your old cluster.\n</code></pre> <p>16. Disable authentication by running\u00a0<code>kubectl -n rook-ceph edit cm/rook-config-override</code>\u00a0and adding content below:</p> <pre><code>data:\nconfig: |\n    [global]\n    auth cluster required = none\n    auth service required = none\n    auth client required = none\n    auth supported = none\n</code></pre> <p>17. Bring the Rook Ceph operator back online by running\u00a0<code>kubectl -n rook-ceph edit deploy/rook-ceph-operator</code>\u00a0and set\u00a0<code>replicas</code>\u00a0to\u00a0<code>1</code>.</p> <p>18. Watch the operator logs with\u00a0<code>kubectl -n rook-ceph logs -f rook-ceph-operator-xxxxxxx</code>, and wait until the orchestration has settled.</p> <p>19. STATE: Now the new cluster should be up and running with authentication disabled.\u00a0<code>ceph -s</code>\u00a0should report 1 mon &amp; 1 mgr &amp; all of the OSDs up and running, and all PGs in either\u00a0<code>active</code>\u00a0or\u00a0<code>degraded</code>\u00a0state.</p> <p>20. Run\u00a0<code>kubectl -n rook-ceph exec -it rook-ceph-tools-XXXXXXX bash</code>\u00a0to enter tools pod:</p> <pre><code>vi key\n# [paste keyring content saved before, preserving only `[client admin]` section]\nceph auth import -i key\nrm key\n</code></pre> <p>21. Re-enable authentication by running\u00a0<code>kubectl -n rook-ceph edit cm/rook-config-override</code>\u00a0and removing auth configuration added in previous steps.</p> <p>22. Stop the Rook operator by running\u00a0<code>kubectl -n rook-ceph edit deploy/rook-ceph-operator</code>\u00a0and set\u00a0<code>replicas</code>\u00a0to\u00a0<code>0</code>.</p> <p>23. Shut down entire new cluster by running\u00a0<code>kubectl -n rook-ceph delete deploy/X</code>\u00a0where X is every deployment in namespace\u00a0<code>rook-ceph</code>, except\u00a0<code>rook-ceph-operator</code>\u00a0and\u00a0<code>rook-ceph-tools</code>, again. This time OSD daemons are present and should be removed too.</p> <p>24. Bring the Rook Ceph operator back online by running\u00a0<code>kubectl -n rook-ceph edit deploy/rook-ceph-operator</code>\u00a0and set\u00a0<code>replicas</code>\u00a0to\u00a0<code>1</code>.</p> <p>25. Watch the operator logs with\u00a0<code>kubectl -n rook-ceph logs -f rook-ceph-operator-xxxxxxx</code>, and wait until the orchestration has settled.</p> <p>26. STATE: Now the new cluster should be up and running with authentication enabled.\u00a0<code>ceph -s</code>\u00a0output should not change much comparing to previous steps.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Disaster_Recovery/#backing-up-and-restoring-a-cluster-based-on-pvcs-into-a-new-kubernetes-cluster","title":"Backing up and restoring a cluster based on PVCs into a new Kubernetes cluster","text":"<p>It is possible to migrate/restore an rook/ceph cluster from an existing Kubernetes cluster to a new one without resorting to SSH access or ceph tooling. This allows doing the migration using standard kubernetes resources only. This guide assumes the following:</p> <ol> <li> <p>You have a CephCluster that uses PVCs to persist mon and osd data. Let\u2019s call it the \u201cold cluster\u201d</p> </li> <li> <p>You can restore the PVCs as-is in the new cluster. Usually this is done by taking regular snapshots of the PVC volumes and using a tool that can re-create PVCs from these snapshots in the underlying cloud provider.\u00a0Velero\u00a0is one such tool.</p> </li> <li> <p>You have regular backups of the secrets and configmaps in the rook-ceph namespace. Velero provides this functionality too.</p> </li> </ol> <p>Do the following in the new cluster:</p> <ol> <li> <p>Stop the rook operator by scaling the deployment\u00a0<code>rook-ceph-operator</code>\u00a0down to zero:\u00a0<code>kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas 0</code>\u00a0and deleting the other deployments. An example command to do this is\u00a0<code>k -n rook-ceph delete deployment -l operator!=rook</code></p> </li> <li> <p>Restore the rook PVCs to the new cluster.</p> </li> <li> <p>Copy the keyring and fsid secrets from the old cluster:\u00a0<code>rook-ceph-mgr-a-keyring</code>,\u00a0<code>rook-ceph-mon</code>,\u00a0<code>rook-ceph-mons-keyring</code>,\u00a0<code>rook-ceph-osd-0-keyring</code>, \u2026</p> </li> <li> <p>Delete mon services and copy them from the old cluster:\u00a0<code>rook-ceph-mon-a</code>,\u00a0<code>rook-ceph-mon-b</code>, \u2026 Note that simply re-applying won\u2019t work because the goal here is to restore the\u00a0<code>clusterIP</code>\u00a0in each service and this field is immutable in\u00a0<code>Service</code>\u00a0resources.</p> </li> <li> <p>Copy the endpoints configmap from the old cluster:\u00a0<code>rook-ceph-mon-endpoints</code></p> </li> <li> <p>Scale the rook operator up again :\u00a0<code>kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas 1</code></p> </li> <li> <p>Wait until the reconciliation is over.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Disaster_Recovery/#restoring-the-rook-cluster-after-the-rook-namespace-is-deleted","title":"Restoring the Rook cluster after the Rook namespace is deleted","text":"<p>When the rook-ceph namespace is accidentally deleted, the good news is that the cluster can be restored. With the content in the directory\u00a0<code>dataDirHostPath</code>\u00a0and the original OSD disks, the ceph cluster could be restored with this guide.</p> <p>You need to manually create a ConfigMap and a Secret to make it work. The information required for the ConfigMap and Secret can be found in the\u00a0<code>dataDirHostPath</code>\u00a0directory.</p> <p>The first resource is the secret named\u00a0<code>rook-ceph-mon</code>\u00a0as seen in this example below:</p> <pre><code>apiVersion: v1\ndata:\n  ceph-secret: QVFCZ0h6VmorcVNhSGhBQXVtVktNcjcrczNOWW9Oa2psYkErS0E9PQ==\n  ceph-username: Y2xpZW50LmFkbWlu\n  fsid: M2YyNzE4NDEtNjE4OC00N2MxLWIzZmQtOTBmZDRmOTc4Yzc2\n  mon-secret: QVFCZ0h6VmorcVNhSGhBQXVtVktNcjcrczNOWW9Oa2psYkErS0E9PQ==\nkind: Secret\nmetadata:\n  finalizers:\n  - ceph.rook.io/disaster-protection\n  name: rook-ceph-mon\n  namespace: rook-ceph\n  ownerReferences: null\ntype: kubernetes.io/rook\n</code></pre> <p>The values for the secret can be found in\u00a0<code>$dataDirHostPath/rook-ceph/client.admin.keyring</code>\u00a0and\u00a0<code>$dataDirHostPath/rook-ceph/rook-ceph.config</code>.</p> <ul> <li> <p><code>ceph-secret</code>\u00a0and\u00a0<code>mon-secret</code>\u00a0are to be filled with the\u00a0<code>client.admin</code>\u2018s keyring contents.</p> </li> <li> <p><code>ceph-username</code>: set to the string\u00a0<code>client.admin</code></p> </li> <li> <p><code>fsid</code>: set to the original ceph cluster id.</p> </li> </ul> <p>All the fields in data section need to be encoded in base64. Coding could be done like this:</p> <p><code>echo -n \"string to code\" | base64 -i -</code></p> <p>Now save the secret as\u00a0<code>rook-ceph-mon.yaml</code>, to be created later in the restore.</p> <p>The second resource is the configmap named rook-ceph-mon-endpoints as seen in this example below:</p> <pre><code>apiVersion: v1\ndata:\n  csi-cluster-config-json: '[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"169.169.241.153:6789\",\"169.169.82.57:6789\",\"169.169.7.81:6789\"],\"namespace\":\"\"}]'\n  data: k=169.169.241.153:6789,m=169.169.82.57:6789,o=169.169.7.81:6789\n  mapping: '{\"node\":{\"k\":{\"Name\":\"10.138.55.111\",\"Hostname\":\"10.138.55.111\",\"Address\":\"10.138.55.111\"},\"m\":{\"Name\":\"10.138.55.120\",\"Hostname\":\"10.138.55.120\",\"Address\":\"10.138.55.120\"},\"o\":{\"Name\":\"10.138.55.112\",\"Hostname\":\"10.138.55.112\",\"Address\":\"10.138.55.112\"}}}'\n  maxMonId: \"15\"\nkind: ConfigMap\nmetadata:\n  finalizers:\n  - ceph.rook.io/disaster-protection\n  name: rook-ceph-mon-endpoints\n  namespace: rook-ceph\n  ownerReferences: null\n</code></pre> <p>The Monitor\u2019s service IPs are kept in the monitor data store and you need to create them by original ones. After you create this configmap with the original service IPs, the rook operator will create the correct services for you with IPs matching in the monitor data store. Along with monitor ids, their service IPs and mapping relationship of them can be found in dataDirHostPath/rook-ceph/rook-ceph.config, for example:</p> <pre><code>[global]\nfsid                = 3f271841-6188-47c1-b3fd-90fd4f978c76\nmon initial members = m o k\nmon host            = [v2:169.169.82.57:3300,v1:169.169.82.57:6789],[v2:169.169.7.81:3300,v1:169.169.7.81:6789],[v2:169.169.241.153:3300,v1:169.169.241.153:6789]\n</code></pre> <p><code>mon initial members</code>\u00a0and\u00a0<code>mon host</code>\u00a0are holding sequences of monitors\u2019 id and IP respectively; the sequence are going in the same order among monitors as a result you can tell which monitors have which service IP addresses. Modify your\u00a0<code>rook-ceph-mon-endpoints.yaml</code>\u00a0on fields\u00a0<code>csi-cluster-config-json</code>\u00a0and\u00a0<code>data</code>\u00a0based on the understanding of\u00a0<code>rook-ceph.config</code>\u00a0above. The field\u00a0<code>mapping</code>\u00a0tells rook where to schedule monitor\u2019s pods. you could search in\u00a0<code>dataDirHostPath</code>\u00a0in all Ceph cluster hosts for\u00a0<code>mon-m,mon-o,mon-k</code>. If you find\u00a0<code>mon-m</code>\u00a0in host\u00a0<code>10.138.55.120</code>, you should fill\u00a0<code>10.138.55.120</code>\u00a0in field\u00a0<code>mapping</code>\u00a0for\u00a0<code>m</code>. Others are the same. Update the\u00a0<code>maxMonId</code>\u00a0to be the max numeric ID of the highest monitor ID. For example, 15 is the 0-based ID for mon\u00a0<code>o</code>. Now save this configmap in the file rook-ceph-mon-endpoints.yaml, to be created later in the restore.</p> <p>Now that you have the info for the secret and the configmap, you are ready to restore the running cluster.</p> <p>Deploy Rook Ceph using the YAML files or Helm, with the same settings you had previously.</p> <pre><code>`kubectl create -f crds.yaml -f common.yaml -f operator.yaml`\n</code></pre> <p>After the operator is running, create the configmap and secret you have just crafted:</p> <pre><code>`kubectl create -f rook-ceph-mon.yaml -f rook-ceph-mon-endpoints.yaml`\n</code></pre> <p>Create your Ceph cluster CR (if possible, with the same settings as existed previously):</p> <pre><code>`kubectl create -f cluster.yaml`\n</code></pre> <p>Now your Rook Ceph cluster should be running again.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Encryption/","title":"IMAGE ENCRYPTION","text":"<p>Starting with the Pacific release, image-level encryption can be handled internally by RBD clients. This means you can set a secret key that will be used to encrypt a specific RBD image. This page describes the scope of the RBD encryption feature.</p> <p>Note The\u00a0<code>krbd</code>\u00a0kernel module does not support encryption at this time.</p> <p>Note External tools (e.g. dm-crypt, QEMU) can be used as well to encrypt an RBD image, and the feature set and limitation set for that use may be different than described here.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Encryption/#encryption-format","title":"ENCRYPTION FORMAT","text":"<p>By default, RBD images are not encrypted. To encrypt an RBD image, it needs to be formatted to one of the supported encryption formats. The format operation persists encryption metadata to the image. The encryption metadata usually includes information such as the encryption format and version, cipher algorithm and mode specification, as well as information used to secure the encryption key. The encryption key itself is protected by a user-kept secret (usually a passphrase), which is never persisted. The basic encryption format operation will require specifying the encryption format and a secret.</p> <p>Some of the encryption metadata may be stored as part of the image data, typically an encryption header will be written to the beginning of the raw image data. This means that the effective image size of the encrypted image may be lower than the raw image size. See the\u00a0Supported Formats\u00a0section for more details.</p> <p>Note Currently only flat images (i.e. not cloned) can be formatted. Clones of an encrypted image are inherently encrypted using the same format and secret.</p> <p>Note Any data written to the image prior to its format may become unreadable, though it may still occupy storage resources.</p> <p>Note Images with the\u00a0journal feature\u00a0enabled cannot be formatted and encrypted by RBD clients.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Encryption/#encryption-load","title":"ENCRYPTION LOAD","text":"<p>Formatting an image is a necessary pre-requisite for enabling encryption. However, formatted images will still be treated as raw unencrypted images by all of the RBD APIs. In particular, an encrypted RBD image can be opened by the same APIs as any other image, and raw unencrypted data can be read / written. Such raw IOs may risk the integrity of the encryption format, for example by overriding encryption metadata located at the beginning of the image.</p> <p>In order to safely perform encrypted IO on the formatted image, an additional\u00a0encryption load\u00a0operation should be applied after opening the image. The encryption load operation requires supplying the encryption format and a secret for unlocking the encryption key. Following a successful encryption load operation, all IOs for the opened image will be encrypted / decrypted. For a cloned image, this includes IOs for ancestor images as well. The encryption key will be stored in-memory by the RBD client until the image is closed.</p> <p>Note Once encryption has been loaded, no other encryption load / format operations can be applied to the context of the opened image.</p> <p>Note Once encryption has been loaded, API calls for retrieving the image size using the opened image context will return the effective image size.</p> <p>Note Encryption load can be automatically applied when mounting RBD images as block devices via\u00a0rbd-nbd.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Encryption/#supported-formats","title":"SUPPORTED FORMATS","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Encryption/#luks","title":"LUKS","text":"<p>Both LUKS1 and LUKS2 are supported. The data layout is fully compliant with the LUKS specification. Thus, images formatted by RBD can be loaded using external LUKS-supporting tools such as dm-crypt or QEMU. Furthermore, existing LUKS data, created outside of RBD, can be imported (by copying the raw LUKS data into the image) and loaded by RBD encryption.</p> <p>Note The LUKS formats are supported on Linux-based systems only.</p> <p>Note Currently, only AES-128 and AES-256 encryption algorithms are supported. Additionally, xts-plain64 is currently the only supported encryption mode.</p> <p>To use the LUKS format, start by formatting the image:</p> <pre><code>$ rbd encryption format {pool-name}/{image-name} {luks1|luks2} {passphrase-file} [\u2013cipher-alg {aes-128 | aes-256}]\n</code></pre> <p>The encryption format operation generates a LUKS header and writes it to the beginning of the image. The header is appended with a single keyslot holding a randomly-generated encryption key, and is protected by the passphrase read from\u00a0passphrase-file.</p> <p>Note If the content of\u00a0passphrase-file\u00a0ends with a newline character, it will be stripped off.</p> <p>By default, AES-256 in xts-plain64 mode (which is the current recommended mode, and the usual default for other tools) will be used. The format operation allows selecting AES-128 as well. Adding / removing passphrases is currently not supported by RBD, but can be applied to the raw RBD data using compatible tools such as cryptsetup.</p> <p>The LUKS header size can vary (up to 136MiB in LUKS2), but is usually up to 16MiB, depending on the version of\u00a0libcryptsetup\u00a0installed. For optimal performance, the encryption format will set the data offset to be aligned with the image object size. For example expect a minimum overhead of 8MiB if using an imageconfigured with an 8MiB object size.</p> <p>In LUKS1, sectors, which are the minimal encryption units, are fixed at 512 bytes. LUKS2 supports larger sectors, and for better performance we set the default sector size to the maximum of 4KiB. Writes which are either smaller than a sector, or are not aligned to a sector start, will trigger a guarded read-modify-write chain on the client, with a considerable latency penalty. A batch of such unaligned writes can lead to IO races which will further deteriorate performance. Thus it is advisable to avoid using RBD encryption in cases where incoming writes cannot be guaranteed to be sector-aligned.</p> <p>To mount a LUKS-encrypted image run:</p> <pre><code>$ rbd -p {pool-name} device map -t nbd -o encryption-format={luks1|luks2},encryption-passphrase-file={passphrase-file}\n</code></pre> <p>Note that for security reasons, both the encryption format and encryption load operations are CPU-intensive, and may take a few seconds to complete. For the encryption operations of actual image IO, assuming AES-NI is enabled, a relative small microseconds latency should be added, as well as a small increase in CPU utilization.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Integration_with_Keystone/","title":"Ceph Integration with Keystone","text":"<p>It is possible to integrate the Ceph Object Gateway with Keystone, the OpenStack identity service. This sets up the gateway to accept Keystone as the users authority. A user that Keystone authorizes to access the gateway will also be automatically created on the Ceph Object Gateway (if didn\u2019t exist beforehand). A token that Keystone validates will be considered as valid by the gateway.</p> <p>The following configuration options are available for Keystone integration:</p> <pre><code>[client.radosgw.gateway]\nrgw keystone api version = {keystone api version}\nrgw keystone url = {keystone server url:keystone server admin port}\nrgw keystone admin token = {keystone admin token}\nrgw keystone admin token path = {path to keystone admin token} #preferred\nrgw keystone accepted roles = {accepted user roles}\nrgw keystone token cache size = {number of tokens to cache}\nrgw keystone implicit tenants = {true for private tenant for each new user}\n</code></pre> <p>It is also possible to configure a Keystone service tenant, user &amp; password for Keystone (for v2.0 version of the OpenStack Identity API), similar to the way OpenStack services tend to be configured, this avoids the need for setting the shared secret\u00a0<code>rgw\u00a0keystone\u00a0admin\u00a0token</code>\u00a0in the configuration file, which is recommended to be disabled in production environments. The service tenant credentials should have admin privileges, for more details refer the\u00a0OpenStack Keystone documentation, which explains the process in detail. The requisite configuration options for are:</p> <pre><code>rgw keystone admin user = {keystone service tenant user name}\nrgw keystone admin password = {keystone service tenant user password}\nrgw keystone admin password = {keystone service tenant user password path} # preferred\nrgw keystone admin tenant = {keystone service tenant name}\n</code></pre> <p>A Ceph Object Gateway user is mapped into a Keystone\u00a0<code>tenant</code>. A Keystone user has different roles assigned to it on possibly more than a single tenant. When the Ceph Object Gateway gets the ticket, it looks at the tenant, and the user roles that are assigned to that ticket, and accepts/rejects the request according to the\u00a0<code>rgw\u00a0keystone\u00a0accepted\u00a0roles</code>\u00a0configurable.</p> <p>For a v3 version of the OpenStack Identity API you should replace\u00a0<code>rgw\u00a0keystone\u00a0admin\u00a0tenant</code>\u00a0with:</p> <pre><code>rgw keystone admin domain = {keystone admin domain name}\nrgw keystone admin project = {keystone admin project name}\n</code></pre> <p>For compatibility with previous versions of ceph, it is also possible to set\u00a0<code>rgw\u00a0keystone\u00a0implicit\u00a0tenants</code>\u00a0to either\u00a0<code>s3</code>\u00a0or\u00a0<code>swift</code>. This has the effect of splitting the identity space such that the indicated protocol will only use implicit tenants, and the other protocol will never use implicit tenants. Some older versions of ceph only supported implicit tenants with swift.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Integration_with_Keystone/#ocata-and-later","title":"OCATA (AND LATER)","text":"<p>Keystone itself needs to be configured to point to the Ceph Object Gateway as an object-storage endpoint:</p> <pre><code>openstack service create --name=swift \\\n                         --description=\"Swift Service\" \\\n                         object-store\n+-------------+----------------------------------+\n| Field       | Value                            |\n+-------------+----------------------------------+\n| description | Swift Service                    |\n| enabled     | True                             |\n| id          | 37c4c0e79571404cb4644201a4a6e5ee |\n| name        | swift                            |\n| type        | object-store                     |\n+-------------+----------------------------------+\n\nopenstack endpoint create --region RegionOne \\\n     --publicurl   \"http://radosgw.example.com:8080/swift/v1\" \\\n     --adminurl    \"http://radosgw.example.com:8080/swift/v1\" \\\n     --internalurl \"http://radosgw.example.com:8080/swift/v1\" \\\n     swift\n+--------------+------------------------------------------+\n| Field        | Value                                    |\n+--------------+------------------------------------------+\n| adminurl     | http://radosgw.example.com:8080/swift/v1 |\n| id           | e4249d2b60e44743a67b5e5b38c18dd3         |\n| internalurl  | http://radosgw.example.com:8080/swift/v1 |\n| publicurl    | http://radosgw.example.com:8080/swift/v1 |\n| region       | RegionOne                                |\n| service_id   | 37c4c0e79571404cb4644201a4a6e5ee         |\n| service_name | swift                                    |\n| service_type | object-store                             |\n+--------------+------------------------------------------+\n\n$ openstack endpoint show object-store\n+--------------+------------------------------------------+\n| Field        | Value                                    |\n+--------------+------------------------------------------+\n| adminurl     | http://radosgw.example.com:8080/swift/v1 |\n| enabled      | True                                     |\n| id           | e4249d2b60e44743a67b5e5b38c18dd3         |\n| internalurl  | http://radosgw.example.com:8080/swift/v1 |\n| publicurl    | http://radosgw.example.com:8080/swift/v1 |\n| region       | RegionOne                                |\n| service_id   | 37c4c0e79571404cb4644201a4a6e5ee         |\n| service_name | swift                                    |\n| service_type | object-store                             |\n+--------------+------------------------------------------+\n</code></pre> <p>Note If your radosgw\u00a0<code>ceph.conf</code>\u00a0sets the configuration option\u00a0<code>rgw\u00a0swift\u00a0account\u00a0in\u00a0url\u00a0=\u00a0true</code>, your\u00a0<code>object-store</code>\u00a0endpoint URLs must be set to include the suffix\u00a0<code>/v1/AUTH_%(tenant_id)s</code>\u00a0(instead of just\u00a0<code>/v1</code>).</p> <p>The Keystone URL is the Keystone admin RESTful API URL. The admin token is the token that is configured internally in Keystone for admin requests.</p> <p>OpenStack Keystone may be terminated with a self signed ssl certificate, in order for radosgw to interact with Keystone in such a case, you could either install Keystone\u2019s ssl certificate in the node running radosgw. Alternatively radosgw could be made to not verify the ssl certificate at all (similar to OpenStack clients with a\u00a0<code>--insecure</code>\u00a0switch) by setting the value of the configurable\u00a0<code>rgw\u00a0keystone\u00a0verify\u00a0ssl</code>\u00a0to false.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Integration_with_Keystone/#cross-projecttenant-access","title":"CROSS PROJECT(TENANT) ACCESS","text":"<p>In order to let a project (earlier called a \u2018tenant\u2019) access buckets belonging to a different project, the following config option needs to be enabled:</p> <pre><code>rgw swift account in url = true\n</code></pre> <p>The Keystone object-store endpoint must accordingly be configured to include the AUTH_%(project_id)s suffix:</p> <pre><code>openstack endpoint create --region RegionOne \\\n     --publicurl   \"http://radosgw.example.com:8080/swift/v1/AUTH_$(project_id)s\" \\\n     --adminurl    \"http://radosgw.example.com:8080/swift/v1/AUTH_$(project_id)s\" \\\n     --internalurl \"http://radosgw.example.com:8080/swift/v1/AUTH_$(project_id)s\" \\\n     swift\n+--------------+--------------------------------------------------------------+\n| Field        | Value                                                        |\n+--------------+--------------------------------------------------------------+\n| adminurl     | http://radosgw.example.com:8080/swift/v1/AUTH_$(project_id)s |\n| id           | e4249d2b60e44743a67b5e5b38c18dd3                             |\n| internalurl  | http://radosgw.example.com:8080/swift/v1/AUTH_$(project_id)s |\n| publicurl    | http://radosgw.example.com:8080/swift/v1/AUTH_$(project_id)s |\n| region       | RegionOne                                                    |\n| service_id   | 37c4c0e79571404cb4644201a4a6e5ee                             |\n| service_name | swift                                                        |\n| service_type | object-store                                                 |\n+--------------+--------------------------------------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Integration_with_Keystone/#keystone-integration-with-the-s3-api","title":"KEYSTONE INTEGRATION WITH THE S3 API","text":"<p>It is possible to use Keystone for authentication even when using the S3 API (with AWS-like access and secret keys), if the\u00a0<code>rgw\u00a0s3\u00a0auth\u00a0use\u00a0keystone</code>\u00a0option is set. For details, see\u00a0Authentication and ACLs.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Integration_with_Keystone/#service-token-support","title":"SERVICE TOKEN SUPPORT","text":"<p>Service tokens can be enabled to support RadosGW Keystone integration to allow expired tokens when coupled with a valid service token in the request.</p> <p>Enable the support with\u00a0<code>rgw\u00a0keystone\u00a0service\u00a0token\u00a0enabled</code>\u00a0and use the\u00a0<code>rgw\u00a0keystone\u00a0service\u00a0token\u00a0accepted\u00a0roles</code>\u00a0option to specify which roles are considered service roles.</p> <p>The\u00a0<code>rgw\u00a0keystone\u00a0expired\u00a0token\u00a0cache\u00a0expiration</code>\u00a0option can be used to tune the cache expiration for an expired token allowed with a service token, please note that this must be lower than the\u00a0<code>[token]/allow_expired_window</code>\u00a0option in the Keystone configuration.</p> <p>Enabling this will cause an expired token given in the X-Auth-Token header to be allowed if coupled with a X-Service-Token header that contains a valid token with the accepted roles. This can allow long running processes using a user token in X-Auth-Token to function beyond the expiration of the token.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Performance_Testing/","title":"Ceph Performance Testing","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Performance_Testing/#ceph-osd-bench","title":"CEPH OSD BENCH","text":"<p>The Ceph OSD Bench tool is a utility used for benchmarking the performance of Ceph Object Storage Daemons (OSDs). It allows you to simulate different types of workloads and measure the throughput, latency, and other performance metrics of your Ceph cluster.</p> <p>The\u00a0<code>ceph tell</code>\u00a0command is a versatile tool that can be used not only for performance testing but also for monitoring, maintenance, and automation tasks in Ceph clusters.</p> <p>Usage:</p> <pre><code>ceph tell &lt;name (type.id)&gt; &lt;command&gt; [options...]\n</code></pre> <p>List all available commands.</p> <p>Usage:</p> <pre><code>ceph tell &lt;name (type.id)&gt; help\n</code></pre> <p>Example:</p> <pre><code>[rook@tclru002 /]$ ceph tell osd.62 bench\n{\n    \"bytes_written\": 1073741824,\n    \"blocksize\": 4194304,\n    \"elapsed_sec\": 0.52926448199999998,\n    \"bytes_per_sec\": 2028743398.6548903,\n    \"iops\": 483.69011846897371\n}\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ceph/Ceph_Performance_Testing/#ceph-pool-bench","title":"CEPH POOL BENCH","text":"<p>In Ceph, the\u00a0<code>ceph bench</code>\u00a0command is used for benchmarking purposes to measure the performance of the storage cluster. It allows you to simulate various workloads and test the performance of Ceph in terms of read/write throughput, latency, and other metrics. Here are some common options available for the\u00a0<code>ceph bench</code>\u00a0command:</p> <pre><code>-t\u00a0N,\u00a0--concurrent-ios=N\n</code></pre> <p>Set number of concurrent I/O operations.</p> <pre><code>--show-time\n</code></pre> <p>Prefix output with date/time.</p> <pre><code>--no-verify\n</code></pre> <p>Do not verify contents of read objects.</p> <pre><code>--write-object\n</code></pre> <p>Write contents to the objects.</p> <pre><code>--write-omap\n</code></pre> <p>Write contents to the omap.</p> <pre><code>--write-xattr\n</code></pre> <p>Write contents to the extended attributes.</p> <p>Example:</p> <pre><code>[rook@tclru002 /]$ rados bench -p cinder.nvme 10 write\nhints = 1\nMaintaining 16 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 10 seconds or 0 objects\nObject prefix: benchmark_data_tclru002_41432\n  sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)\n    0       0         0         0         0         0           -           0\n    1      16       624       608   2431.73      2432   0.0494825   0.0260557\n    2      16      1260      1244   2487.61      2544   0.0239319   0.0255206\n    3      16      1957      1941   2587.56      2788   0.0159702   0.0246469\n    4      16      2671      2655   2654.54      2856   0.0139591   0.0239974\n    5      16      3412      3396   2716.33      2964   0.0219474   0.0234938\n    6      16      4133      4117   2744.19      2884   0.0276837   0.0232849\n    7      16      4847      4831    2760.1      2856   0.0160697   0.0231393\n    8      16      5581      5565   2782.02      2936   0.0177683   0.0229799\n    9      16      6293      6277   2789.29      2848   0.0175105    0.022901\n   10      16      6990      6974   2789.12      2788   0.0145607   0.0229147\nTotal time run:         10.0198\nTotal writes made:      6990\nWrite size:             4194304\nObject size:            4194304\nBandwidth (MB/sec):     2790.47\nStddev Bandwidth:       170.356\nMax bandwidth (MB/sec): 2964\nMin bandwidth (MB/sec): 2432\nAverage IOPS:           697\nStddev IOPS:            42.589\nMax IOPS:               741\nMin IOPS:               608\nAverage Latency(s):     0.022914\nStddev Latency(s):      0.00722401\nMax latency(s):         0.194613\nMin latency(s):         0.0103783\nCleaning up (deleting benchmark objects)\nRemoved 6990 objects\nClean up completed and total clean up time :0.511463\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Block_Storage_Overview-cinder/","title":"What is Cinder?","text":"<p>Cinder is the OpenStack Block Storage service for providing volumes to Nova virtual machines, Ironic bare metal hosts, containers and more. Some of the goals of Cinder are to be/have:</p> <ul> <li> <p>Component based architecture: Quickly add new behaviors</p> </li> <li> <p>Highly available: Scale to very serious workloads</p> </li> <li> <p>Fault-Tolerant: Isolated processes avoid cascading failures</p> </li> <li> <p>Recoverable: Failures should be easy to diagnose, debug, and rectify</p> </li> <li> <p>Open Standards: Be a reference implementation for a community-driven api</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Block_Storage_Overview-cinder/#for-end-users","title":"For end users","text":"<p>As an end user of Cinder, you\u2019ll use Cinder to create and manage volumes using the Horizon user interface, command line tools such as the\u00a0python-cinderclient, or by directly using the\u00a0REST API.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Block_Storage_Overview-cinder/#tools-for-using-cinder","title":"Tools for using Cinder","text":"<ul> <li> <p>Horizon: The official web UI for the OpenStack Project.</p> </li> <li> <p>OpenStack Client: The official CLI for OpenStack Projects. You should use this as your CLI for most things, it includes not just nova commands but also commands for most of the projects in OpenStack.</p> </li> <li> <p>Cinder Client: The\u00a0openstack\u00a0CLI is recommended, but there are some advanced features and administrative commands that are not yet available there. For CLI access to these commands, the\u00a0cinder\u00a0CLI can be used instead.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Block_Storage_Overview-cinder/#using-the-cinder-api","title":"Using the Cinder API","text":"<p>All features of Cinder are exposed via a REST API that can be used to build more complicated logic or automation with Cinder. This can be consumed directly or via various SDKs. The following resources can help you get started consuming the API directly.</p> <ul> <li> <p>Cinder API</p> </li> <li> <p>Cinder microversion history</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Block_Storage_Overview-cinder/#for-operators","title":"For operators","text":"<p>This section has details for deploying and maintaining Cinder services.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Block_Storage_Overview-cinder/#installing-cinder","title":"Installing Cinder","text":"<p>Cinder can be configured standalone using the configuration setting\u00a0<code>auth_strategy\u00a0=\u00a0noauth</code>, but in most cases you will want to at least have the\u00a0Keystone\u00a0Identity service and other\u00a0OpenStack services\u00a0installed.</p> <ul> <li>Installation Guide</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Block_Storage_Overview-cinder/#administrating-cinder","title":"Administrating Cinder","text":"<ul> <li>Cinder Administration</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/","title":"Manage Volumes via CLI","text":"<p>A volume is a detachable block storage device, similar to a USB hard drive. You can attach a volume to only one instance. Use the\u00a0<code>openstack</code>\u00a0client commands to create and manage volumes.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#create-a-volume","title":"Create a volume","text":"<p>This example creates a\u00a0<code>my-new-volume</code>\u00a0volume based on an image.</p> <ol> <li>List images, and note the ID of the image that you want to use for your volume:</li> </ol> <pre><code>$ openstack image list\n+--------------------------------------+---------------------------------+\n| ID                                   | Name                            |\n+--------------------------------------+---------------------------------+\n| 8bf4dc2a-bf78-4dd1-aefa-f3347cf638c8 | cirros-0.3.5-x86_64-uec         |\n| 9ff9bb2e-3a1d-4d98-acb5-b1d3225aca6c | cirros-0.3.5-x86_64-uec-kernel  |\n| 4b227119-68a1-4b28-8505-f94c6ea4c6dc | cirros-0.3.5-x86_64-uec-ramdisk |\n+--------------------------------------+---------------------------------+\n</code></pre> <p>2. List the availability zones, and note the ID of the availability zone in which you want to create your volume:</p> <pre><code>$ openstack availability zone list\n+-----------+-------------+\n| Zone Name | Zone Status |\n+-----------+-------------+\n| nova      | available   |\n+-----------+-------------+\n</code></pre> <p>3. Create a volume with 8 gibibytes (GiB) of space, and specify the availability zone and image:</p> <pre><code>$ openstack volume create --image 8bf4dc2a-bf78-4dd1-aefa-f3347cf638c8 \\\n  --size 8 --availability-zone nova my-new-volume\n\n+------------------------------+--------------------------------------+\n| Property                     | Value                                |\n+------------------------------+--------------------------------------+\n| attachments                  | []                                   |\n| availability_zone            | nova                                 |\n| bootable                     | false                                |\n| consistencygroup_id          | None                                 |\n| created_at                   | 2016-09-23T07:52:42.000000           |\n| description                  | None                                 |\n| encrypted                    | False                                |\n| id                           | bab4b0e0-ce3d-4d57-bf57-3c51319f5202 |\n| metadata                     | {}                                   |\n| multiattach                  | False                                |\n| name                         | my-new-volume                        |\n| os-vol-tenant-attr:tenant_id | 3f670abbe9b34ca5b81db6e7b540b8d8     |\n| replication_status           | disabled                             |\n| size                         | 8                                    |\n| snapshot_id                  | None                                 |\n| source_volid                 | None                                 |\n| status                       | creating                             |\n| updated_at                   | None                                 |\n| user_id                      | fe19e3a9f63f4a14bd4697789247bbc5     |\n| volume_type                  | lvmdriver-1                          |\n+------------------------------+--------------------------------------+\n</code></pre> <p>4. To verify that your volume was created successfully, list the available volumes:</p> <pre><code>$ openstack volume list\n+--------------------------------------+---------------+-----------+------+-------------+\n| ID                                   | Name          |  Status   | Size | Attached to |\n+--------------------------------------+---------------+-----------+------+-------------+\n| bab4b0e0-ce3d-4d57-bf57-3c51319f5202 | my-new-volume | available | 8    |             |\n+--------------------------------------+---------------+-----------+------+-------------+\n</code></pre> <p>If your volume was created successfully, its status is\u00a0<code>available</code>. If its status is\u00a0<code>error</code>, you might have exceeded your quota.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#volume-types","title":"Volume Types","text":"<p>Cinder supports these three ways to specify\u00a0<code>volume\u00a0type</code>\u00a0during volume creation.</p> <ol> <li> <p>volume_type</p> </li> <li> <p>cinder_img_volume_type (via glance image metadata)</p> </li> <li> <p>default volume type (via project defaults or cinder.conf)</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#volume-type","title":"volume-type","text":"<p>User can specify\u00a0volume type\u00a0when creating a volume.</p> <pre><code>$ openstack volume create -h -f {json,shell,table,value,yaml}\n                         -c COLUMN --max-width &lt;integer&gt;\n                         --noindent --prefix PREFIX --size &lt;size&gt;\n                         --type &lt;volume-type&gt; --image &lt;image&gt;\n                         --snapshot &lt;snapshot&gt; --source &lt;volume&gt;\n                         --description &lt;description&gt; --user &lt;user&gt;\n                         --project &lt;project&gt;\n                         --availability-zone &lt;availability-zone&gt;\n                         --property &lt;key=value&gt;\n                         &lt;name&gt;\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#cinder_img_volume_type","title":"cinder_img_volume_type","text":"<p>If glance image has\u00a0<code>cinder_img_volume_type</code>\u00a0property, Cinder uses this parameter to specify\u00a0<code>volume\u00a0type</code>\u00a0when creating a volume.</p> <p>Choose glance image which has\u00a0<code>cinder_img_volume_type</code>\u00a0property and create a volume from the image.</p> <pre><code>$ openstack image list\n+----------------------------------+---------------------------------+--------+\n| ID                               | Name                            | Status |\n+----------------------------------+---------------------------------+--------+\n| 376bd633-c9c9-4c5d-a588-342f4f66 | cirros-0.3.5-x86_64-uec         | active |\n| d086                             |                                 |        |\n| 2c20fce7-2e68-45ee-ba8d-         | cirros-0.3.5-x86_64-uec-ramdisk | active |\n| beba27a91ab5                     |                                 |        |\n| a5752de4-9faf-4c47-acbc-         | cirros-0.3.5-x86_64-uec-kernel  | active |\n| 78a5efa7cc6e                     |                                 |        |\n+----------------------------------+---------------------------------+--------+\n\n\n$ openstack image show 376bd633-c9c9-4c5d-a588-342f4f66d086\n+------------------------+------------------------------------------------------+\n| Field                  | Value                                                |\n+------------------------+------------------------------------------------------+\n| checksum               | eb9139e4942121f22bbc2afc0400b2a                      |\n| cinder_img_volume_type | nfstype                                              |\n| container_format       | ami                                                  |\n| created_at             | 2016-10-13T03:28:55Z                                 |\n| disk_format            | ami                                                  |\n| file                   | /v2/images/376bd633-c9c9-4c5d-a588-342f4f66d086/file |\n| id                     | 376bd633-c9c9-4c5d-a588-342f4f66d086                 |\n| min_disk               | 0                                                    |\n| min_ram                | 0                                                    |\n| name                   | cirros-0.3.5-x86_64-uec                              |\n| owner                  | 88ba456e3a884c318394737765e0ef4d                     |\n| properties             | kernel_id='a5752de4-9faf-4c47-acbc-78a5efa7cc6e',    |\n|                        | ramdisk_id='2c20fce7-2e68-45ee-ba8d-beba27a91ab5'    |\n| protected              | False                                                |\n| schema                 | /v2/schemas/image                                    |\n| size                   | 25165824                                             |\n| status                 | active                                               |\n| tags                   |                                                      |\n| updated_at             | 2016-10-13T03:28:55Z                                 |\n| virtual_size           | None                                                 |\n| visibility             | public                                               |\n+------------------------+------------------------------------------------------+\n\n$ openstack volume create --image 376bd633-c9c9-4c5d-a588-342f4f66d086 \\\n  --size 1 --availability-zone nova test\n+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| attachments         | []                                   |\n| availability_zone   | nova                                 |\n| bootable            | false                                |\n| consistencygroup_id | None                                 |\n| created_at          | 2016-10-13T06:29:53.688599           |\n| description         | None                                 |\n| encrypted           | False                                |\n| id                  | e6e6a72d-cda7-442c-830f-f306ea6a03d5 |\n| multiattach         | False                                |\n| name                | test                                 |\n| properties          |                                      |\n| replication_status  | disabled                             |\n| size                | 1                                    |\n| snapshot_id         | None                                 |\n| source_volid        | None                                 |\n| status              | creating                             |\n| type                | nfstype                              |\n| updated_at          | None                                 |\n| user_id             | 33fdc37314914796883706b33e587d51     |\n+---------------------+--------------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#default-volume-type","title":"default volume type","text":"<p>If above parameters are not set, cinder uses default volume type during volume creation.</p> <p>The effective default volume type (whether it be project default or default_volume_type) can be checked with the following command:</p> <pre><code>$ cinder type-default\n</code></pre> <p>There are 2 ways to set the default volume type:</p> <ol> <li> <p>Project specific defaults</p> </li> <li> <p>default_volume_type defined in cinder.conf</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#project-specific-defaults-available-since-mv-362-or-higher","title":"Project specific defaults (available since mv 3.62 or higher)","text":"<p>Project specific defaults can be managed using the\u00a0Default Volume Types API\u00a0It is set on a per project basis and has a higher priority over default_volume_type defined in cinder.conf</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#default_volume_type","title":"default_volume_type","text":"<p>If the project specific default is not set then default_volume_type configured in cinder.conf is used to create volumes.</p> <p>Example cinder.conf file configuration</p> <pre><code>[default]\ndefault_volume_type = lvmdriver-1\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#attach-a-volume-to-an-instance","title":"Attach a volume to an instance","text":"<ol> <li>Attach your volume to a server, specifying the server ID and the volume ID:</li> </ol> <pre><code>$ openstack server add volume 84c6e57d-a6b1-44b6-81eb-fcb36afd31b5 \\\n  573e024d-5235-49ce-8332-be1576d323f8 --device /dev/vdb\n</code></pre> <p>2. Show information for your volume:</p> <pre><code>$ openstack volume show 573e024d-5235-49ce-8332-be1576d323f8\n</code></pre> <p>The output shows that the volume is attached to the server with ID\u00a0<code>84c6e57d-a6b1-44b6-81eb-fcb36afd31b5</code>, is in the nova availability zone, and is bootable.</p> <pre><code>+------------------------------+-----------------------------------------------+\n| Field                        | Value                                         |\n+------------------------------+-----------------------------------------------+\n| attachments                  | [{u'device': u'/dev/vdb',                     |\n|                              |        u'server_id': u'84c6e57d-a             |\n|                              |           u'id': u'573e024d-...               |\n|                              |        u'volume_id': u'573e024d...            |\n| availability_zone            | nova                                          |\n| bootable                     | true                                          |\n| consistencygroup_id          | None                                          |\n| created_at                   | 2016-10-13T06:08:07.000000                    |\n| description                  | None                                          |\n| encrypted                    | False                                         |\n| id                           | 573e024d-5235-49ce-8332-be1576d323f8          |\n| multiattach                  | False                                         |\n| name                         | my-new-volume                                 |\n| properties                   |                                               |\n| replication_status           | disabled                                      |\n| size                         | 8                                             |\n| snapshot_id                  | None                                          |\n| source_volid                 | None                                          |\n| status                       | in-use                                        |\n| type                         | lvmdriver-1                                   |\n| updated_at                   | 2016-10-13T06:08:11.000000                    |\n| user_id                      | 33fdc37314914796883706b33e587d51              |\n+------------------------------+-----------------------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#detach-a-volume-from-an-instance","title":"Detach a volume from an instance","text":"<ol> <li>Detach your volume from a server, specifying the server ID and the volume ID:</li> </ol> <pre><code>$ openstack server remove volume 84c6e57d-a6b1-44b6-81eb-fcb36afd31b5 \\\n  573e024d-5235-49ce-8332-be1576d323f8\n</code></pre> <p>2. Show information for your volume:</p> <pre><code>$ openstack volume show 573e024d-5235-49ce-8332-be1576d323f8\n</code></pre> <p>The output shows that the volume is no longer attached to the server:</p> <pre><code>+------------------------------+-----------------------------------------------+\n| Field                        | Value                                         |\n+------------------------------+-----------------------------------------------+\n| attachments                  | []                                            |\n| availability_zone            | nova                                          |\n| bootable                     | true                                          |\n| consistencygroup_id          | None                                          |\n| created_at                   | 2016-10-13T06:08:07.000000                    |\n| description                  | None                                          |\n| encrypted                    | False                                         |\n| id                           | 573e024d-5235-49ce-8332-be1576d323f8          |\n| multiattach                  | False                                         |\n| name                         | my-new-volume                                 |\n| properties                   |                                               |\n| replication_status           | disabled                                      |\n| size                         | 8                                             |\n| snapshot_id                  | None                                          |\n| source_volid                 | None                                          |\n| status                       | in-use                                        |\n| type                         | lvmdriver-1                                   |\n| updated_at                   | 2016-10-13T06:08:11.000000                    |\n| user_id                      | 33fdc37314914796883706b33e587d51              |\n+------------------------------+-----------------------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#delete-a-volume","title":"Delete a volume","text":"<ol> <li>To delete your volume, you must first detach it from the server. To detach the volume from your server and check for the list of existing volumes, see steps 1 and 2 in\u00a0Resize_a_volume.Delete the volume using either the volume name or ID:</li> </ol> <pre><code>$ openstack volume delete my-new-volume\n</code></pre> <p>This command does not provide any output.</p> <p>2. List the volumes again, and note that the status of your volume is\u00a0<code>deleting</code>:</p> <pre><code>$ openstack volume list\n+----------------+-----------------+-----------+------+-------------+\n|       ID       |   Name          |  Status   | Size | Attached to |\n+----------------+-----------------+-----------+------+-------------+\n| 573e024d-52... |  my-new-volume  |  deleting |  8   |             |\n| bd7cf584-45... | my-bootable-vol | available |  8   |             |\n+----------------+-----------------+-----------+------+-------------+\n</code></pre> <p>When the volume is fully deleted, it disappears from the list of volumes:</p> <pre><code>$ openstack volume list\n+----------------+-----------------+-----------+------+-------------+\n|       ID       |   Name          |  Status   | Size | Attached to |\n+----------------+-----------------+-----------+------+-------------+\n| bd7cf584-45... | my-bootable-vol | available |  8   |             |\n+----------------+-----------------+-----------+------+-------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#resize-a-volume","title":"Resize a volume","text":"<ol> <li>To resize your volume, you must first detach it from the server if the volume driver does not support in-use extend. (See\u00a0Extend_attached_volume.) To detach the volume from your server, pass the server ID and volume ID to the following command:</li> </ol> <pre><code>$ openstack server remove volume 84c6e57d-a6b1-44b6-81eb-fcb36afd31b5 573e024d-5235-49ce-8332-be1576d323f8\n</code></pre> <p>This command does not provide any output.</p> <p>2. List volumes:</p> <pre><code>$ openstack volume list\n+----------------+-----------------+-----------+------+-------------+\n|       ID       |   Name          |  Status   | Size | Attached to |\n+----------------+-----------------+-----------+------+-------------+\n| 573e024d-52... |  my-new-volume  | available |  8   |             |\n| bd7cf584-45... | my-bootable-vol | available |  8   |             |\n+----------------+-----------------+-----------+------+-------------+\n</code></pre> <p>Note  that the volume is now available.</p> <p>3. Resize the volume by passing the volume ID and the new size (a value greater than the old one) as parameters:</p> <pre><code>$ openstack volume set 573e024d-5235-49ce-8332-be1576d323f8 --size 10\n</code></pre> <p>This command does not provide any output. Note: The volume status\u00a0<code>reserved</code>\u00a0is not a valid state for an extend operation.</p> <p>Note When extending an LVM volume with a snapshot, the volume will be deactivated. The reactivation is automatic unless\u00a0<code>auto_activation_volume_list</code>\u00a0is defined in\u00a0<code>lvm.conf</code>. See\u00a0<code>lvm.conf</code>\u00a0for more information.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#extend-attached-volume","title":"Extend attached volume","text":"<p>Starting from microversion 3.42, it is also possible to extend an attached volume with status\u00a0<code>in-use</code>, depending upon policy settings and the capabilities of the backend storage. Sufficient amount of storage must exist to extend the volume.</p> <ol> <li>Resize the volume by passing the microversion,the volume ID, and the new size (a value greater than the old one) as parameters:</li> </ol> <pre><code>$ openstack --os-volume-api-version 3.42 volume set 573e024d-5235-49ce-8332-be1576d323f8 --size 10\n</code></pre> <p>This command does not provide any output.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#migrate-a-volume","title":"Migrate a volume","text":"<p>As an administrator, you can migrate a volume with its data from one location to another in a manner that is transparent to users and workloads. You can migrate only detached volumes with no snapshots.</p> <p>Possible use cases for data migration include:</p> <ul> <li> <p>Bring down a physical storage device for maintenance without disrupting workloads.</p> </li> <li> <p>Modify the properties of a volume.</p> </li> <li> <p>Free up space in a thinly-provisioned back end.</p> </li> </ul> <p>Migrate a volume with the\u00a0openstack volume migrate\u00a0command, as shown in the following example:</p> <pre><code>$ openstack volume migrate [-h] --host &lt;host&gt; [--force-host-copy]\n                                  [--lock-volume] &lt;volume&gt;\n</code></pre> <p>The arguments for this command are:host</p> <p>The destination host in the format\u00a0host@backend-name#pool.volume</p> <p>The ID of the volume to migrate.force-host-copy</p> <p>Disables any driver optimizations and forces the data to be copied by the host.lock-volume</p> <p>Prevents other processes from aborting the migration.</p> <p>Note If the volume has snapshots, the specified host destination cannot accept the volume. If the user is not an administrator, the migration fails.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#transfer-a-volume","title":"Transfer a volume","text":"<p>You can transfer a volume from one owner to another by using the\u00a0openstack volume transfer request create\u00a0command. The volume donor, or original owner, creates a transfer request and sends the created transfer ID and authorization key to the volume recipient. The volume recipient, or new owner, accepts the transfer by using the ID and key.</p> <p>Starting with the Rocky release, Cinder changes the API behavior for the v2 and v3 API up to microversion 3.55. Snapshots will be transferred with the volume by default. That means if the volume has some snapshots, when a user transfers a volume from one owner to another, then those snapshots will be transferred with the volume as well.</p> <p>Starting with microversion 3.55 and later, Cinder supports the ability to transfer volume without snapshots. If users don\u2019t want to transfer snapshots, they need to specify the new optional argument\u00a0\u2013no-snapshots.</p> <p>Starting with microversion 3.70 and later, Cinder supports the ability to transfer encrypted volumes. Snapshots must be transferred with the volume.</p> <p>Note The procedure for volume transfer is intended for projects (both the volume donor and recipient) within the same cloud.</p> <p>Use cases include:</p> <ul> <li> <p>Create a custom bootable volume or a volume with a large data set and transfer it to a customer.</p> </li> <li> <p>For bulk import of data to the cloud, the data ingress system creates a new Block Storage volume, copies data from the physical device, and transfers device ownership to the end user.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#create-a-volume-transfer-request","title":"Create a volume transfer request","text":"<ol> <li>While logged in as the volume donor, list the available volumes:</li> </ol> <pre><code>$ openstack volume list\n+-----------------+-----------------+-----------+------+-------------+\n|       ID        |   Name          |  Status   | Size | Attached to |\n+-----------------+-----------------+-----------+------+-------------+\n| 72bfce9f-cac... |       None      |   error   |  1   |             |\n| a1cdace0-08e... |       None      | available |  1   |             |\n+-----------------+-----------------+-----------+------+-------------+\n</code></pre> <p>2. As the volume donor, request a volume transfer authorization code for a specific volume:</p> <pre><code>$ openstack volume transfer request create [--no-snapshots] &lt;volume&gt;\n</code></pre> <p>The arguments to be passed are:</p> <p><code>&lt;volume&gt;</code>\u00a0Name or ID of volume to transfer.</p> <p><code>--no-snapshots</code>\u00a0Transfer the volume without snapshots.</p> <p>The volume must be in an\u00a0<code>available</code>\u00a0state or the request will be denied. If the transfer request is valid in the database (that is, it has not expired or been deleted), the volume is placed in an\u00a0<code>awaiting-transfer</code>\u00a0state. For example:</p> <pre><code>$ openstack volume transfer request create a1cdace0-08e4-4dc7-b9dc-457e9bcfe25f\n</code></pre> <p>The output shows the volume transfer ID in the\u00a0<code>id</code>\u00a0row and the authorization key.</p> <pre><code>+------------+--------------------------------------+\n| Field      | Value                                |\n+------------+--------------------------------------+\n| auth_key   | 0a59e53630f051e2                     |\n| created_at | 2016-11-03T11:49:40.346181           |\n| id         | 34e29364-142b-4c7b-8d98-88f765bf176f |\n| name       | None                                 |\n| volume_id  | a1cdace0-08e4-4dc7-b9dc-457e9bcfe25f |\n+------------+--------------------------------------+\n</code></pre> <p>Note Optionally, you can specify a name for the transfer by using the\u00a0<code>--name\u00a0transferName</code>\u00a0parameter.</p> <p>Note While the\u00a0<code>auth_key</code>\u00a0property is visible in the output of\u00a0<code>openstack\u00a0volume\u00a0transfer\u00a0request\u00a0create\u00a0VOLUME_ID</code>, it will not be available in subsequent\u00a0<code>openstack\u00a0volume\u00a0transfer\u00a0request\u00a0show\u00a0TRANSFER_ID</code>\u00a0command.</p> <ol> <li> <p>Send the volume transfer ID and authorization key to the new owner (for example, by email).</p> </li> <li> <p>View pending transfers:</p> </li> </ol> <pre><code>$ openstack volume transfer request list\n+--------------------------------------+--------------------------------------+------+\n|               ID                     |             Volume                   | Name |\n+--------------------------------------+--------------------------------------+------+\n| 6e4e9aa4-bed5-4f94-8f76-df43232f44dc | a1cdace0-08e4-4dc7-b9dc-457e9bcfe25f | None |\n+--------------------------------------+--------------------------------------+------+\n</code></pre> <p>3. After the volume recipient, or new owner, accepts the transfer, you can see that the transfer is no longer available:</p> <pre><code>$ openstack volume transfer request list\n+----+-----------+------+\n| ID | Volume ID | Name |\n+----+-----------+------+\n+----+-----------+------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#accept-a-volume-transfer-request","title":"Accept a volume transfer request","text":"<ol> <li> <p>As the volume recipient, you must first obtain the transfer ID and authorization key from the original owner.</p> </li> <li> <p>Accept the request:</p> </li> </ol> <pre><code>$ openstack volume transfer request accept transferID authKey\n</code></pre> <p>For example:</p> <pre><code>$ openstack volume transfer request accept 6e4e9aa4-bed5-4f94-8f76-df43232f44dc b2c8e585cbc68a80\n+-----------+--------------------------------------+\n|  Property |                Value                 |\n+-----------+--------------------------------------+\n|     id    | 6e4e9aa4-bed5-4f94-8f76-df43232f44dc |\n|    name   |                 None                 |\n| volume_id | a1cdace0-08e4-4dc7-b9dc-457e9bcfe25f |\n+-----------+--------------------------------------+\n</code></pre> <p>Note If you do not have a sufficient quota for the transfer, the transfer is refused.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#delete-a-volume-transfer","title":"Delete a volume transfer","text":"<ol> <li>List available volumes and their statuses:</li> </ol> <pre><code>$ openstack volume list\n+-----------------+-----------------+-----------------+------+-------------+\n|       ID        |   Name          |      Status     | Size | Attached to |\n+-----------------+-----------------+-----------------+------+-------------+\n| 72bfce9f-cac... |       None      |      error      |  1   |             |\n| a1cdace0-08e... |       None      |awaiting-transfer|  1   |             |\n+-----------------+-----------------+-----------------+------+-------------+\n</code></pre> <p>2. Find the matching transfer ID:</p> <pre><code>$ openstack volume transfer request list\n+--------------------------------------+--------------------------------------+------+\n|               ID                     |             VolumeID                 | Name |\n+--------------------------------------+--------------------------------------+------+\n| a6da6888-7cdf-4291-9c08-8c1f22426b8a | a1cdace0-08e4-4dc7-b9dc-457e9bcfe25f | None |\n+--------------------------------------+--------------------------------------+------+\n</code></pre> <p>3. Delete the volume:</p> <pre><code>$ openstack volume transfer request delete &lt;transfer&gt;\n</code></pre> <p>Name or ID of transfer to delete.</p> <p>For example:</p> <pre><code>$ openstack volume transfer request delete a6da6888-7cdf-4291-9c08-8c1f22426b8a\n</code></pre> <p>4. Verify that transfer list is now empty and that the volume is again available for transfer:</p> <pre><code>$ openstack volume transfer request list\n+----+-----------+------+\n| ID | Volume ID | Name |\n+----+-----------+------+\n+----+-----------+------+\n</code></pre> <pre><code>$ openstack volume list\n+-----------------+-----------------+-----------------+------+-------------+\n|       ID        |   Name          |      Status     | Size | Attached to |\n+-----------------+-----------------+-----------------+------+-------------+\n| 72bfce9f-cac... |       None      |      error      |  1   |             |\n| a1cdace0-08e... |       None      |    available    |  1   |             |\n+-----------------+-----------------+-----------------+------+-------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#manage-and-unmanage-a-snapshot","title":"Manage and unmanage a snapshot","text":"<p>A snapshot is a point in time version of a volume. As an administrator, you can manage and unmanage snapshots.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#manage-a-snapshot","title":"Manage a snapshot","text":"<p>Manage a snapshot with the\u00a0openstack volume snapshot set\u00a0command:</p> <pre><code>$ openstack volume snapshot set [-h]\n                                [--name &lt;name&gt;]\n                                [--description &lt;description&gt;]\n                                [--no-property]\n                                [--property &lt;key=value&gt;]\n                                [--state &lt;state&gt;]\n                                &lt;snapshot&gt;\n</code></pre> <p>The arguments to be passed are:<code>--name\u00a0&lt;name&gt;</code></p> <p>New snapshot name<code>--description\u00a0&lt;description&gt;</code></p> <p>New snapshot description<code>--no-property</code></p> <p>Remove all properties from \\ (specify both \u2013no-property and \u2013property to remove the current properties before setting new properties.)<code>--property\u00a0&lt;key=value&gt;</code> <p>Property to add or modify for this snapshot (repeat option to set multiple properties)<code>--state\u00a0&lt;state&gt;</code></p> <p>New snapshot state. (\u201cavailable\u201d, \u201cerror\u201d, \u201ccreating\u201d, \u201cdeleting\u201d, or \u201cerror_deleting\u201d) (admin only) (This option simply changes the state of the snapshot in the database with no regard to actual status, exercise caution when using)<code>&lt;snapshot&gt;</code></p> <p>Snapshot to modify (name or ID)</p> <pre><code>$ openstack volume snapshot set my-snapshot-id\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#unmanage-a-snapshot","title":"Unmanage a snapshot","text":"<p>Unmanage a snapshot with the\u00a0openstack volume snapshot unset\u00a0command:</p> <pre><code>$ openstack volume snapshot unset [-h]\n                                  [--property &lt;key&gt;]\n                                  &lt;snapshot&gt;\n</code></pre> <p>The arguments to be passed are:<code>--property\u00a0&lt;key&gt;</code></p> <p>Property to remove from snapshot (repeat option to remove multiple properties)<code>&lt;snapshot&gt;</code></p> <p>Snapshot to modify (name or ID).</p> <p>The following example unmanages the\u00a0<code>my-snapshot-id</code>\u00a0image:</p> <pre><code>$ openstack volume snapshot unset my-snapshot-id\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Manage_Volumes_via_CLI/#report-backend-state-in-service-list","title":"Report backend state in service list","text":"<p>Each of the Cinder services report a Status and a State. These are the administrative state and the runtime state, respectively.</p> <p>To get a listing of all Cinder services and their states, run the command:</p> <pre><code>$ openstack volume service list\n+------------------+-------------------+------+---------+-------+----------------------------+\n| Binary           | Host              | Zone | Status  | State | Updated At                 |\n+------------------+-------------------+------+---------+-------+----------------------------+\n| cinder-scheduler | tower             | nova | enabled | up    | 2018-03-30T21:16:11.000000 |\n| cinder-volume    | tower@lvmdriver-1 | nova | enabled | up    | 2018-03-30T21:16:15.000000 |\n| cinder-backup    | tower             | nova | enabled | up    | 2018-03-30T21:16:14.000000 |\n+------------------+-------------------+------+---------+-------+----------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Volume_Backups/","title":"Volume Backups","text":"<p>The\u00a0<code>openstack</code>\u00a0command-line interface provides the tools for creating a volume backup. You can restore a volume from a backup as long as the backup\u2019s associated database information (or backup metadata) is intact in the Block Storage database.</p> <p>Run this command to create a backup of a volume:</p> <pre><code>$ openstack volume backup create [--incremental] [--force] VOLUME\n</code></pre> <p>Where\u00a0<code>VOLUME</code>\u00a0is the name or ID of the volume,\u00a0<code>incremental</code>\u00a0is a flag that indicates whether an incremental backup should be performed, and\u00a0<code>force</code>\u00a0is a flag that allows or disallows backup of a volume when the volume is attached to an instance.</p> <p>Without the\u00a0<code>incremental</code>\u00a0flag, a full backup is created by default. With the\u00a0<code>incremental</code>\u00a0flag, an incremental backup is created.</p> <p>Without the\u00a0<code>force</code>\u00a0flag, the volume will be backed up only if its status is\u00a0<code>available</code>. With the\u00a0<code>force</code>\u00a0flag, the volume will be backed up whether its status is\u00a0<code>available</code>\u00a0or\u00a0<code>in-use</code>. A volume is\u00a0<code>in-use</code>\u00a0when it is attached to an instance. The backup of an\u00a0<code>in-use</code>\u00a0volume means your data is crash consistent. The\u00a0<code>force</code>\u00a0flag is False by default.</p> <p>Note The\u00a0<code>force</code>\u00a0flag is new in OpenStack Liberty.</p> <p>The incremental backup is based on a parent backup which is an existing backup with the latest timestamp. The parent backup can be a full backup or an incremental backup depending on the timestamp.</p> <p>Note The first backup of a volume has to be a full backup. Attempting to do an incremental backup without any existing backups will fail. There is an\u00a0<code>is_incremental</code>\u00a0flag that indicates whether a backup is incremental when showing details on the backup. Another flag,\u00a0<code>has_dependent_backups</code>, returned when showing backup details, will indicate whether the backup has dependent backups. If it is\u00a0<code>true</code>, attempting to delete this backup will fail.</p> <p>A new configure option\u00a0<code>backup_swift_block_size</code>\u00a0is introduced into\u00a0<code>cinder.conf</code>\u00a0for the default Swift backup driver. This is the size in bytes that changes are tracked for incremental backups. The existing\u00a0<code>backup_swift_object_size</code>\u00a0option, the size in bytes of Swift backup objects, has to be a multiple of\u00a0<code>backup_swift_block_size</code>. The default is 32768 for\u00a0<code>backup_swift_block_size</code>, and the default is 52428800 for\u00a0<code>backup_swift_object_size</code>.</p> <p>The configuration option\u00a0<code>backup_swift_enable_progress_timer</code>\u00a0in\u00a0<code>cinder.conf</code>\u00a0is used when backing up the volume to Object Storage back end. This option enables or disables the timer. It is enabled by default to send the periodic progress notifications to the Telemetry service.</p> <p>This command also returns a backup ID. Use this backup ID when restoring the volume:</p> <pre><code>$ openstack volume backup restore BACKUP_ID VOLUME_ID\n</code></pre> <p>When restoring from a full backup, it is a full restore.</p> <p>When restoring from an incremental backup, a list of backups is built based on the IDs of the parent backups. A full restore is performed based on the full backup first, then restore is done based on the incremental backup, laying on top of it in order.</p> <p>You can view a backup list with the\u00a0openstack volume backup list\u00a0command. Optional arguments to clarify the status of your backups include: running\u00a0<code>--name</code>,\u00a0<code>--status</code>, and\u00a0<code>--volume</code>\u00a0to filter through backups by the specified name, status, or volume-id. Search with\u00a0<code>--all-projects</code>\u00a0for details of the projects associated with the listed backups.</p> <p>Because volume backups are dependent on the Block Storage database, you must also back up your Block Storage database regularly to ensure data recovery.</p> <p>Note Alternatively, you can export and save the metadata of selected volume backups. Doing so precludes the need to back up the entire Block Storage database. This is useful if you need only a small subset of volumes to survive a catastrophic database failure. If you specify a UUID encryption key when setting up the volume specifications, the backup metadata ensures that the key will remain valid when you back up and restore the volume. For more information about how to export and import volume backup metadata, see the section called\u00a0Export and import backup metadata.</p> <p>By default, the swift object store is used for the backup repository.</p> <p>If instead you want to use an NFS export as the backup repository, add the following configuration options to the\u00a0<code>[DEFAULT]</code>\u00a0section of the\u00a0<code>cinder.conf</code>\u00a0file and restart the Block Storage services:</p> <pre><code>backup_driver = cinder.backup.drivers.nfs\nbackup_share = HOST:EXPORT_PATH\n</code></pre> <p>For the\u00a0<code>backup_share</code>\u00a0option, replace\u00a0<code>HOST</code>\u00a0with the DNS resolvable host name or the IP address of the storage server for the NFS share, and\u00a0<code>EXPORT_PATH</code>\u00a0with the path to that share. If your environment requires that non-default mount options be specified for the share, set these as follows:</p> <pre><code>backup_mount_options = MOUNT_OPTIONS\n</code></pre> <p><code>MOUNT_OPTIONS</code>\u00a0is a comma-separated string of NFS mount options as detailed in the NFS man page.</p> <p>There are several other options whose default values may be overridden as appropriate for your environment:</p> <pre><code>backup_compression_algorithm = zlib\nbackup_sha_block_size_bytes = 32768\nbackup_file_size = 1999994880\n</code></pre> <p>The option\u00a0<code>backup_compression_algorithm</code>\u00a0can be set to\u00a0<code>zlib</code>,\u00a0<code>bz2</code>,\u00a0<code>zstd</code>\u00a0or\u00a0<code>none</code>. The value\u00a0<code>none</code>\u00a0can be a useful setting when the server providing the share for the backup repository itself performs deduplication or compression on the backup data.</p> <p>The option\u00a0<code>backup_file_size</code>\u00a0must be a multiple of\u00a0<code>backup_sha_block_size_bytes</code>. It is effectively the maximum file size to be used, given your environment, to hold backup data. Volumes larger than this will be stored in multiple files in the backup repository.\u00a0<code>backup_file_size</code>\u00a0also determines the buffer size used to produce backup files; on smaller hosts it may need to be scaled down to avoid OOM issues. The\u00a0<code>backup_sha_block_size_bytes</code>\u00a0option determines the size of blocks from the cinder volume being backed up on which digital signatures are calculated in order to enable incremental backup capability.</p> <p>You also have the option of resetting the state of a backup. When creating or restoring a backup, sometimes it may get stuck in the creating or restoring states due to problems like the database or rabbitmq being down. In situations like these resetting the state of the backup can restore it to a functional status.</p> <p>Run this command to restore the state of a backup:</p> <pre><code>$ cinder backup-reset-state [--state STATE] BACKUP_ID-1 BACKUP_ID-2 ...\n</code></pre> <p>Run this command to create a backup of a snapshot:</p> <pre><code>$ openstack volume backup create [--incremental] [--force] \\\n  [--snapshot SNAPSHOT_ID] VOLUME\n</code></pre> <p>Where\u00a0<code>VOLUME</code>\u00a0is the name or ID of the volume,\u00a0<code>SNAPSHOT_ID</code>\u00a0is the ID of the volume\u2019s snapshot.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Volume_Backups/#cancelling","title":"Cancelling","text":"<p>Since Liberty it is possible to cancel an ongoing backup operation on any of the Chunked Backup type of drivers such as Swift, NFS, Google, GlusterFS, and Posix.</p> <p>To issue a backup cancellation on a backup we must request a force delete on the backup.</p> <pre><code>$ openstack volume backup delete --force BACKUP_ID\n</code></pre> <p>Note The policy on force delete defaults to admin only.</p> <p>Even if the backup is immediately deleted, and therefore no longer appears in the listings, the cancellation may take a little bit longer, so please check the status of the source resource to see when it stops being \u201cbacking-up\u201d.</p> <p>Note Before Pike the \u201cbacking-up\u201d status would always be stored in the volume, even when backing up a snapshot, so when backing up a snapshot any delete operation on the snapshot that followed a cancellation could result in an error if the snapshot was still mapped. Polling on the volume to stop being \u201cbacking-up\u201d prior to the deletion is required to ensure success.</p> <p>Since Rocky it is also possible to cancel an ongoing restoring operation on any of the Chunked Backup type of drivers.</p> <p>To issue a backup restoration cancellation we need to alter its status to anything other than\u00a0restoring. We strongly recommend using the \u201cerror\u201d state to avoid any confusion on whether the restore was successful or not.</p> <pre><code>$ openstack volume backup set --state error BACKUP_ID\n</code></pre> <p>After a restore operation has started, if it is then cancelled, the destination volume is useless, as there is no way of knowing how much data, or if any, was actually restored, hence our recommendation of using the \u201cerror\u201d state.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Volume_Backups/#backup_max_operations","title":"backup_max_operations","text":"<p>With this configuration option will let us select the maximum number of operations, backup and restore, that can be performed concurrently.</p> <p>This option has a default value of 15, which means that we can have 15 concurrent backups, or 15 concurrent restores, or any combination of backups and restores as long as the sum of the 2 operations don\u2019t exceed 15.</p> <p>The concurrency limitation of this configuration option is also enforced when we run multiple processes for the same backup service using the\u00a0<code>backup_workers</code>\u00a0configuration option. It is not a per process restriction, but global to the service, so we won\u2019t be able to run\u00a0<code>backup_max_operations</code>\u00a0on each one of the processes, but on all the running processes from the same backup service.</p> <p>Backups and restore operations are both CPU and memory intensive, but thanks to this option we can limit the concurrency and prevent DoS attacks or just service disruptions caused by many concurrent requests that lead to Out of Memory (OOM) kills.</p> <p>The amount of memory (RAM) used during the operation depends on the configured chunk size as well as the compression ratio achieved on the data during the operation.</p> <p>Example:</p> <p>Let\u2019s have a look at how much memory would be needed if we use the default backup chunk size (\\~1.86 GB) while doing a restore to an RBD volume from a non Ceph backend (Swift, NFS etc).In a restore operation the worst case scenario, from the memory point of view, is when the compression ratio is close to 0% (the compressed data chunk is almost the same size as the uncompressed data).In this case the memory usage would be \\~5.58 GB of data for each chunk: \\~5.58 GB = read buffer + decompressed buffer + write buffer used by the librbd library = \\~1.86 GB + 1.86 GB + 1.86 GBFor 15 concurrent restore operations, the cinder-backup service will require \\~83.7 GB of memory.</p> <p>Similar calculations can be done for environment specific scenarios and this config option can be set accordingly.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Volume_encryption_supported_by_the_key_maager/","title":"Volume encryption supported by the key manager","text":"<p>We recommend the Key management service (barbican) for storing encryption keys used by the OpenStack volume encryption feature. It can be enabled by updating\u00a0<code>cinder.conf</code>\u00a0and\u00a0<code>nova.conf</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Volume_encryption_supported_by_the_key_maager/#initial-configuration","title":"Initial configuration","text":"<p>Configuration changes need to be made to any nodes running the\u00a0<code>cinder-api</code>\u00a0or\u00a0<code>nova-compute</code>\u00a0server.</p> <p>Steps to update\u00a0<code>cinder-api</code>\u00a0servers:</p> <p>1. Edit the\u00a0<code>/etc/cinder/cinder.conf</code>\u00a0file to use Key management service as follows:</p> <ul> <li> <p>Look for the\u00a0<code>[key_manager]</code>\u00a0section.</p> </li> <li> <p>Enter a new line directly below\u00a0<code>[key_manager]</code>\u00a0with the following</p> </li> </ul> <pre><code>backend = barbican\n</code></pre> <p>2. Restart\u00a0<code>cinder-api</code>,\u00a0<code>cinder-volume</code>\u00a0and\u00a0<code>cinder-backup</code>.</p> <p>Update\u00a0<code>nova-compute</code>\u00a0servers:</p> <p>1. Install the\u00a0<code>python-barbicanclient</code>\u00a0Python package.</p> <p>2. Set up the Key Manager service by editing\u00a0<code>/etc/nova/nova.conf</code>:</p> <pre><code>[key_manager]\nbackend = barbican\n</code></pre> <p>Note Use a \u2018#\u2019 prefix to comment out the line in this section that begins with \u2018fixed_key\u2019.</p> <p>3. Restart\u00a0<code>nova-compute</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Volume_encryption_supported_by_the_key_maager/#key-management-access-control","title":"Key management access control","text":"<p>Special privileges can be assigned on behalf of an end user to allow them to manage their own encryption keys, which are required when creating the encrypted volumes. The Barbican\u00a0Default Policy\u00a0for access control specifies that only users with an\u00a0<code>admin</code>\u00a0or\u00a0<code>creator</code>\u00a0role can create keys. The policy is very flexible and can be modified.</p> <p>To assign the\u00a0<code>creator</code>\u00a0role, the admin must know the user ID, project ID, and creator role ID. See\u00a0Assign a role\u00a0for more information. An admin can list existing roles and associated IDs using the\u00a0<code>openstack\u00a0role\u00a0list</code>\u00a0command. If the creator role does not exist, the admin can\u00a0create the role.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Volume_encryption_supported_by_the_key_maager/#create-an-encrypted-volume-type","title":"Create an encrypted volume type","text":"<p>Block Storage volume type assignment provides scheduling to a specific back-end, and can be used to specify actionable information for a back-end storage device.</p> <p>This example creates a volume type called LUKS and provides configuration information for the storage system to encrypt or decrypt the volume.</p> <ol> <li>Source your admin credentials:</li> </ol> <pre><code>$ . admin-openrc.sh\n</code></pre> <ol> <li>Create the volume type, marking the volume type as encrypted and providing the necessary details. Use\u00a0<code>--encryption-control-location</code>\u00a0to specify where encryption is performed:\u00a0<code>front-end</code>\u00a0(default) or\u00a0<code>back-end</code>.</li> </ol> <pre><code>$ openstack volume type create --encryption-provider luks \\\n  --encryption-cipher aes-xts-plain64 --encryption-key-size 256 --encryption-control-location front-end LUKS\n\n  +-------------+----------------------------------------------------------------+\n  | Field       | Value                                                          |\n  +-------------+----------------------------------------------------------------+\n  | description | None                                                           |\n  | encryption  | cipher='aes-xts-plain64', control_location='front-end',        |\n  |             | encryption_id='8584c43f-1666-43d1-a348-45cfcef72898',          |\n  |             | key_size='256',                                                |\n  |             | provider='luks'                                                |\n  | id          | b9a8cff5-2f60-40d1-8562-d33f3bf18312                           |\n  | is_public   | True                                                           |\n  | name        | LUKS                                                           |\n  +-------------+----------------------------------------------------------------+\n</code></pre> <p>The OpenStack dashboard (horizon) supports creating the encrypted volume type as of the Kilo release. For instructions, see\u00a0Create an encrypted volume type.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Volume_encryption_supported_by_the_key_maager/#create-an-encrypted-volume","title":"Create an encrypted volume","text":"<p>Use the OpenStack dashboard (horizon), or\u00a0openstack volume create\u00a0command to create volumes just as you normally would. For an encrypted volume, pass the\u00a0<code>--type\u00a0LUKS</code>\u00a0flag, which specifies that the volume type will be\u00a0<code>LUKS</code>\u00a0(Linux Unified Key Setup). If that argument is left out, the default volume type,\u00a0<code>unencrypted</code>, is used.</p> <ol> <li>Source your admin credentials:</li> </ol> <pre><code>$ . admin-openrc.sh\n</code></pre> <p>2. Create an unencrypted 1GB test volume:</p> <pre><code>$ openstack volume create --size 1 'unencrypted volume'\n</code></pre> <p>3. Create an encrypted 1GB test volume:</p> <pre><code>$ openstack volume create --size 1 --type LUKS 'encrypted volume'\n</code></pre> <p>Notice the encrypted parameter; it will show\u00a0<code>True</code>\u00a0or\u00a0<code>False</code>. The option\u00a0<code>volume_type</code>\u00a0is also shown for easy review.</p> <p>Non-admin users need the\u00a0<code>creator</code>\u00a0role to store secrets in Barbican and to create encrypted volumes. As an administrator, you can give a user the creator role in the following way:</p> <pre><code>$ openstack role add --project PROJECT --user USER creator\n</code></pre> <p>For details, see the\u00a0Barbican Access Control page.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Cinder/Volume_encryption_supported_by_the_key_maager/#testing-volume-encryption","title":"Testing volume encryption","text":"<p>This is a simple test scenario to help validate your encryption. It assumes an LVM based Block Storage server.</p> <p>Perform these steps after completing the volume encryption setup and creating the volume-type for LUKS as described in the preceding sections.</p> <ol> <li>Create a VM:</li> </ol> <pre><code>$ openstack server create --image cirros-0.3.1-x86_64-disk --flavor m1.tiny TESTVM\n</code></pre> <ol> <li>Create two volumes, one encrypted and one not encrypted then attach them to your VM:</li> </ol> <pre><code>$ openstack volume create --size 1 'unencrypted volume'\n$ openstack volume create --size 1 --type LUKS 'encrypted volume'\n$ openstack volume list\n$ openstack server add volume --device /dev/vdb TESTVM 'unencrypted volume'\n$ openstack server add volume --device /dev/vdc TESTVM 'encrypted volume'\n</code></pre> <p>Note The\u00a0<code>--device</code>\u00a0option to specify the mountpoint for the attached volume may not be where the block device is actually attached in the guest VM, it is used here for illustration purposes.</p> <ol> <li>On the VM, send some text to the newly attached volumes and synchronize them:</li> </ol> <pre><code># echo \"Hello, world (unencrypted /dev/vdb)\" &gt;&gt; /dev/vdb\n# echo \"Hello, world (encrypted /dev/vdc)\" &gt;&gt; /dev/vdc\n# sync &amp;&amp; sleep 2\n# sync &amp;&amp; sleep 2\n</code></pre> <ol> <li>On the system hosting cinder-volume services, synchronize to flush the I/O cache then test to see if your strings can be found:</li> </ol> <pre><code># sync &amp;&amp; sleep 2\n# sync &amp;&amp; sleep 2\n# strings /dev/stack-volumes/volume-* | grep \"Hello\"\nHello, world (unencrypted /dev/vdb)\n</code></pre> <p>In the above example you see that the search returns the string written to the unencrypted volume, but not the encrypted one.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Designate/Introduction_to_Designate/","title":"Introduction to Designate (DNS-as-a-Service)","text":"<p>Designate is an Open Source DNS-as-a-Service implementation and a part of the OpenStack ecosystem of services for running clouds. In order to understand what Designate can do and how it works, it\u2019s necessary to understand some of the basics of DNS.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Designate/Introduction_to_Designate/#what-is-dns","title":"What is DNS?","text":"<p>The Domain Name System (DNS) is a system for naming resources connected to a network, and works by storing various types of\u00a0record, such as an IP address associated with a domain name. In practice, this is implemented by\u00a0authoritative name servers\u00a0which contain these records and\u00a0resolvers\u00a0which query name servers for records. Names are divided up into a hierarchy of zones, allowing different name servers to be responsible for separate groups of zones by delegating responsibility using records.</p> <p>The root zone, which is simply \u201c.\u201d, is comprised entirely of records delegating various top level domains (TLDs) to other nameservers. The TLD name servers will contain records for domains within their TLD, such as the\u00a0.com\u00a0nameserver having an\u00a0example.com\u00a0record, as well as records that delegate zones to other nameservers, for example\u00a0openstack.org\u00a0might have their own nameserver so that they can then create\u00a0cloud.openstack.org.</p> <p></p> <p>Resolvers\u00a0are often formed in two parts: a\u00a0stub\u00a0resolver which is often merely a library on a user\u2019s computer, and a\u00a0recursive resolver\u00a0that will perform queries against nameservers before returning the result to the user. When searching for a domain, the resolver will start at the end of the domain and work its way back to the beginning.</p> <p>For example in the diagram below, when searching for cloud.openstack.org, it will start with the root nameserver \u201c.\u201d, which will reply with the location of the \u201c.org\u201d nameserver. The resolver can then contact the \u201c.org\u201d nameserver to get the \u201copenstack.org\u201d nameserver and from there finally get the \u201ccloud.openstack.org\u201d record and return it to the user.</p> <p></p> <p>In order to make this more efficient, the results are cached on the resolver, so after the first user has requested \u201ccloud.openstack.org\u201d, the resolver can return the cached result for subsequent requests.Further reading on DNS and how it works is available here:</p> <ul> <li>https://en.wikipedia.org/wiki/Domain_Name_System</li> </ul> <p>While the system itself is defined via RFCs such as this:</p> <ul> <li>https://tools.ietf.org/html/rfc1034</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Designate/Introduction_to_Designate/#introducing-designate","title":"Introducing Designate","text":"<p>Designate is an OpenStack service that allows users and operators to manage DNS records, names and zones via a REST API and can configure existing DNS name servers to contain those records. Designate can also be configured by an operator to integrate with both the OpenStack Network Service (Neutron) and the Compute Service (Nova) so that records are automatically created when floating IPs and compute instances are created respectively, and uses the OpenStack Identity Service (Keystone) for user management. Since there are a multitude of software implementations of the DNS name server, Designate has a pluggable backend that can be configured to manage many of them, most notably BIND9 and PowerDNS.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Designate/Introduction_to_Designate/#designate-architecture","title":"Designate Architecture","text":"<p>Designate is comprised of several different services: the API, Producer, Central, Worker and Mini DNS. It uses an oslo.db compatible database to store state and data, and an oslo.messaging compatible message queue to facilitate communication between services. Multiple copies of all Designate services can be run in tandem to facilitate high availability deployments, with the API process often sitting behind load balancers.</p> <p></p> <p>Neutron and other users of Designate only need to be able to access the API server, while administrators should ensure the DNS Nameservers to be configured are able to access Mini DNS from which to request updates.</p> <p>Below we can see a common deployment scenario:</p> <p>A user has created two zones in Designate:\u00a0zone1.cloud.openstack.org\u00a0and\u00a0zone2.cloud.openstack.org. This will result in two new zones being created on the Designate-managed nameserver with SOA records.</p> <p>The user then created two networks in Neutron: one private network with\u00a0zone1.cloud.openstack.org\u00a0assigned to it, and one public network with\u00a0zone2.cloud.openstack.org.</p> <p>They have then created virtual machine\u00a0vm1\u00a0in Nova, connected to the private network in Neutron and attached to a floating IP, and the virtual machine\u00a0vm2\u00a0attached directly to the public network. Each of these actions triggers a chain of events that will cause Neutron to request Designate create records on behalf of the user, with the end result being that records are created in the authoritative nameserver mapping the vm names to domains along with PTR records to allow reverse lookups.</p> <p>More information about configuring Neutron to work with Designate can be found in the Neutron documentation at\u00a0https://docs.openstack.org/neutron/latest/admin/config-dns-int-ext-serv.html</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Designate/Introduction_to_Designate/#using-designate","title":"Using Designate","text":"<p>Designate provides a REST API and that is commonly used by one of three methods. The most common is to use the OpenStack client, a python command-line tool with commands for interacting with OpenStack services. The documentation for the OpenStack client is available at\u00a0https://docs.openstack.org/python-openstackclient/. The\u00a0designate plugin https://docs.openstack.org/python-designateclient/latest/\u00a0for the OpenStack client needs to be installed as well:</p> <pre><code>pip install python-openstackclient\npip install python-designateclient\n</code></pre> <p>Another popular way to use Designate is via the OpenStack Dashboard, Horizon. Administrators will need to add the\u00a0Designate Horizon plugin https://opendev.org/openstack/designate-dashboard\u00a0to the dashboard in order to enable Designate features.</p> <p>Finally, for python developers the aforementioned Designate plugin for the OpenStack client which can be used as a python library. Other languages may have bindings available from one of the third party\u00a0SDKs https://wiki.openstack.org/wiki/SDKs\u00a0for OpenStack.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Glance/Image_Service_Overview_glance/","title":"Welcome to Glance\u2019s documentation!","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Glance/Image_Service_Overview_glance/#about-glance","title":"About Glance","text":"<p>The Image service (glance) project provides a service where users can upload and discover data assets that are meant to be used with other services. This currently includes\u00a0images\u00a0and\u00a0metadata definitions.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Glance/Image_Service_Overview_glance/#images","title":"Images","text":"<p>Glance image services include discovering, registering, and retrieving virtual machine (VM) images. Glance has a RESTful API that allows querying of VM image metadata as well as retrieval of the actual image.</p> <p>Note The Images API v1, DEPRECATED in the Newton release, has been removed.</p> <p>VM images made available through Glance can be stored in a variety of locations from simple filesystems to object-storage systems like the OpenStack Swift project.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Glance/Image_Service_Overview_glance/#metadata-definitions","title":"Metadata Definitions","text":"<p>Glance hosts a\u00a0metadefs\u00a0catalog. This provides the OpenStack community with a way to programmatically determine various metadata key names and valid values that can be applied to OpenStack resources.</p> <p>Note that what we\u2019re talking about here is simply a\u00a0catalog; the keys and values don\u2019t actually do anything unless they are applied to individual OpenStack resources using the APIs or client tools provided by the services responsible for those resources.</p> <p>It\u2019s also worth noting that there is no special relationship between the Image Service and the Metadefs Service. If you want to apply the keys and values defined in the Metadefs Service to images, you must use the Image Service API or client tools just as you would for any other OpenStack service.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Glance/Image_Service_Overview_glance/#design-principles","title":"Design Principles","text":"<p>Glance, as with all OpenStack projects, is written with the following design guidelines in mind:</p> <ul> <li> <p>Component based architecture: Quickly add new behaviors</p> </li> <li> <p>Highly available: Scale to very serious workloads</p> </li> <li> <p>Fault tolerant: Isolated processes avoid cascading failures</p> </li> <li> <p>Recoverable: Failures should be easy to diagnose, debug, and rectify</p> </li> <li> <p>Open standards: Be a reference implementation for a community-driven api</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Glance/Image_Service_Overview_glance/#glance-documentation","title":"Glance Documentation","text":"<p>The Glance Project Team has put together the following documentation for you. Pick the documents that best match your user profile.</p> <p>      * Glance Contribution Guidelines <p>     * Glance Administration Guide <pre><code>* [&lt;u&gt;Glance Utility Programs&lt;/u&gt;](https://docs.openstack.org/glance/zed/cli/index.html)\n\n* [&lt;u&gt;Glance Release Notes&lt;/u&gt;](https://docs.openstack.org/releasenotes/glance/index.html)\n</code></pre> <p></p> <p>     * Glance Installation <pre><code>* [&lt;u&gt;Glance Configuration Options&lt;/u&gt;](https://docs.openstack.org/glance/zed/configuration/index.html)\n</code></pre> <p></p> <p>     * Image Service API Reference <pre><code>* [I&lt;u&gt;mage Service API Guide&lt;/u&gt;](https://specs.openstack.org/openstack/glance-specs/specs/api/v2/image-api-v2.html)\n\n* [&lt;u&gt;Glance User Guide&lt;/u&gt;](https://docs.openstack.org/glance/zed/user/index.html)\n</code></pre> <p></p> <p>     Here\u2019s a handy Glossary of terms related to Glance    </p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Glance/Manage_images/","title":"Manage images","text":"<p>The cloud operator assigns roles to users. Roles determine who can upload and manage images. The operator might restrict image upload and management to only cloud administrators or operators.</p> <p>You can upload images through the\u00a0glance image-create\u00a0or\u00a0glance image-create-via-import\u00a0command or the Image service API. You can use the\u00a0<code>glance</code>\u00a0client for the image management. It provides mechanisms to do all operations supported by the Images API v2.</p> <p>After you upload an image, you cannot change the content, but you can update the metadata.</p> <p>For details about image creation, see the\u00a0Virtual Machine Image Guide.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Glance/Manage_images/#list-or-get-details-for-images-glance","title":"List or get details for images (glance)","text":"<p>To get a list of images and to get further details about a single image, use\u00a0glance image-list\u00a0and\u00a0glance image-show\u00a0commands.</p> <pre><code>glance image-list`\n+--------------------------------------+---------------------------------+\n| ID                                   | Name                            |\n+--------------------------------------+---------------------------------+\n| dfc1dfb0-d7bf-4fff-8994-319dd6f703d7 | cirros-0.3.5-x86_64-uec         |\n| a3867e29-c7a1-44b0-9e7f-10db587cad20 | cirros-0.3.5-x86_64-uec-kernel  |\n| 4b916fba-6775-4092-92df-f41df7246a6b | cirros-0.3.5-x86_64-uec-ramdisk |\n| d07831df-edc3-4817-9881-89141f9134c3 | myCirrosImage                   |\n+--------------------------------------+---------------------------------+\n</code></pre> <pre><code>$ glance image-show d07831df-edc3-4817-9881-89141f9134c3\n+------------------+------------------------------------------------------+\n| Field            | Value                                                |\n+------------------+------------------------------------------------------+\n| checksum         | 443b7623e27ecf03dc9e01ee93f67afe                     |\n| container_format | ami                                                  |\n| created_at       | 2016-08-11T15:07:26Z                                 |\n| disk_format      | ami                                                  |\n| file             | /v2/images/d07831df-edc3-4817-9881-89141f9134c3/file |\n| id               | d07831df-edc3-4817-9881-89141f9134c3                 |\n| min_disk         | 0                                                    |\n| min_ram          | 0                                                    |\n| name             | myCirrosImage                                        |\n| os_hash_algo     | sha512                                               |\n| os_hash_value    | 6513f21e44aa3da349f248188a44bc304a3653a04122d8fb4535 |\n|                  | 423c8e1d14cd6a153f735bb0982e2161b5b5186106570c17a9e5 |\n|                  | 8b64dd39390617cd5a350f78                             |\n| os_hidden        | False                                                |\n| owner            | d88310717a8e4ebcae84ed075f82c51e                     |\n| protected        | False                                                |\n| schema           | /v2/schemas/image                                    |\n| size             | 13287936                                             |\n| status           | active                                               |\n| tags             |                                                      |\n| updated_at       | 2016-08-11T15:20:02Z                                 |\n| virtual_size     | None                                                 |\n| visibility       | private                                              |\n+------------------+------------------------------------------------------+\n</code></pre> <p>When viewing a list of images, you can also use\u00a0<code>grep</code>\u00a0to filter the list, as follows:</p> <pre><code>$ glance image-list | grep 'cirros'\n| dfc1dfb0-d7bf-4fff-8994-319dd6f703d7 | cirros-0.3.5-x86_64-uec         |\n| a3867e29-c7a1-44b0-9e7f-10db587cad20 | cirros-0.3.5-x86_64-uec-kernel  |\n| 4b916fba-6775-4092-92df-f41df7246a6b | cirros-0.3.5-x86_64-uec-ramdisk |\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Glance/Manage_images/#create-or-update-an-image-glance","title":"Create or update an image (glance)","text":"<p>To create an image, use\u00a0glance image-create:</p> <pre><code>$ glance image-create --name imageName\n</code></pre> <p>To update an image, you must specify its ID and use\u00a0glance image-update:</p> <pre><code>$ glance image-update --property x=\"y\" &lt;IMAGE_ID&gt;\n</code></pre> <p>The following list explains the commonly used properties that you can set or modify when using the\u00a0<code>image-create</code>\u00a0and\u00a0<code>image-update</code>\u00a0commands. For more information, refer to the\u00a0OpenStack Useful Image Properties.</p> <p><code>--architecture\u00a0&lt;ARCHITECTURE&gt;</code></p> <p>Operating system architecture as specified in\u00a0https://docs.openstack.org/glance/latest/admin/useful-image-properties.html</p> <p><code>--protected\u00a0[True|False]</code></p> <p>If true, image will not be deletable.</p> <p><code>--name\u00a0&lt;NAME&gt;</code></p> <p>Descriptive name for the image</p> <p><code>--instance-uuid\u00a0&lt;INSTANCE_UUID&gt;</code></p> <p>Metadata which can be used to record which instance this image is associated with. (Informational only, does not create an instance snapshot.)</p> <p><code>--min-disk\u00a0&lt;MIN_DISK&gt;</code></p> <p>Amount of disk space (in GB) required to boot image.</p> <p><code>--visibility\u00a0&lt;VISIBILITY&gt;</code></p> <p>Scope of image accessibility. Valid values:\u00a0<code>public</code>,\u00a0<code>private</code>,\u00a0<code>community</code>,\u00a0<code>shared</code></p> <p><code>--kernel-id\u00a0&lt;KERNEL_ID&gt;</code></p> <p>ID of image stored in Glance that should be used as the kernel when booting an AMI-style image.</p> <p><code>--os-version\u00a0&lt;OS_VERSION&gt;</code></p> <p>Operating system version as specified by the distributor</p> <p><code>--disk-format\u00a0&lt;DISK_FORMAT&gt;</code></p> <p>Format of the disk. May not be modified once an image has gone to\u00a0<code>active</code>\u00a0status. Valid values:\u00a0<code>ami</code>,\u00a0<code>ari</code>,\u00a0<code>aki</code>,\u00a0<code>vhd</code>,\u00a0<code>vhdx</code>,\u00a0<code>vmdk</code>,\u00a0<code>raw</code>,\u00a0<code>qcow2</code>,\u00a0<code>vdi</code>,\u00a0<code>iso</code>,\u00a0<code>ploop</code></p> <p><code>--os-distro\u00a0&lt;OS_DISTRO&gt;</code></p> <p>Common name of operating system distribution as specified in\u00a0https://docs.openstack.org/glance/latest/admin/useful-image-properties.html</p> <p><code>--owner\u00a0&lt;OWNER&gt;</code></p> <p>Owner of the image. Usually, may be set by an admin only.</p> <p><code>--ramdisk-id\u00a0&lt;RAMDISK_ID&gt;</code></p> <p>ID of image stored in Glance that should be used as the ramdisk when booting an AMI-style image.</p> <p><code>--min-ram\u00a0&lt;MIN_RAM&gt;</code></p> <p>Amount of ram (in MB) required to boot image.</p> <p><code>--container-format\u00a0&lt;CONTAINER_FORMAT&gt;</code></p> <p>Format of the container. May not be modified once an image has gone to\u00a0<code>active</code>\u00a0status. Valid values:\u00a0<code>ami</code>,\u00a0<code>ari</code>,\u00a0<code>aki</code>,\u00a0<code>bare</code>,\u00a0<code>ovf</code>,\u00a0<code>ova</code>,\u00a0<code>docker</code>,\u00a0<code>compressed</code></p> <p><code>--hidden\u00a0[True|False]</code></p> <p>If true, image will not appear in default image list response.</p> <p><code>--property\u00a0&lt;key=value&gt;</code></p> <p>Arbitrary property to associate with image. May be used multiple times.</p> <p><code>--remove-property\u00a0key</code></p> <p>Name of arbitrary property to remove from the image.</p> <p>The following example shows the command that you would use to upload a CentOS 6.3 image in qcow2 format and configure it for public access:</p> <pre><code>$ glance image-create --disk-format qcow2 --container-format bare \\\n  --visibility public --file ./centos63.qcow2 --name centos63-image\n</code></pre> <p>The following example shows how to update an existing image with a properties that describe the disk bus, the CD-ROM bus, and the VIF model:</p> <p>Note When you use OpenStack with VMware vCenter Server, you need to specify the\u00a0<code>vmware_disktype</code>\u00a0and\u00a0<code>vmware_adaptertype</code>\u00a0properties with\u00a0glance image-create. Also, we recommend that you set the\u00a0<code>hypervisor_type=\"vmware\"</code>\u00a0property. For more information, see\u00a0Images with VMware vSphere\u00a0in the OpenStack Configuration Reference.</p> <pre><code>$ glance image-update \\\n    --property hw_disk_bus=scsi \\\n    --property hw_cdrom_bus=ide \\\n    --property hw_vif_model=e1000 \\\n    &lt;Image-ID&gt;\n</code></pre> <p>Currently the libvirt virtualization tool determines the disk, CD-ROM, and VIF device models based on the configured hypervisor type (<code>libvirt_type</code>\u00a0in\u00a0<code>/etc/nova/nova.conf</code>\u00a0file). For the sake of optimal performance, libvirt defaults to using virtio for both disk and VIF (NIC) models. The disadvantage of this approach is that it is not possible to run operating systems that lack virtio drivers, for example, BSD, Solaris, and older versions of Linux and Windows.</p> <p>If you specify a disk or CD-ROM bus model that is not supported, see the\u00a0Disk_and_CD-ROM_bus_model_values_table. If you specify a VIF model that is not supported, the instance fails to launch. See the\u00a0VIF_model_values_table.</p> <p>The valid model values depend on the\u00a0<code>libvirt_type</code>\u00a0setting, as shown in the following tables.</p> <p>Disk and CD-ROM bus model values</p> libvirt_type setting Supported model values qemu or kvm - fdc - ide - scsi - sata - virtio - usb xen - ide - xen <p>VIF model values</p> libvirt_type setting Supported model values qemu or kvm - e1000 - ne2k_pci - pcnet - rtl8139 - virtio xen - e1000 - netfront - ne2k_pci - pcnet - rtl8139 vmware - VirtualE1000 - VirtualPCNet32 - VirtualVmxnet <p>Note By default, hardware properties are retrieved from the image properties. However, if this information is not available, the\u00a0<code>libosinfo</code>\u00a0database provides an alternative source for these values. If the guest operating system is not in the database, or if the use of\u00a0<code>libosinfo</code>\u00a0is disabled, the default system values are used. Users can set the operating system ID or a\u00a0<code>short-id</code>\u00a0in image properties. For example:</p> <pre><code>$ glance image-update --property short-id=fedora23 \\\n  &lt;Image-ID&gt;\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Glance/Manage_images/#create-an-image-from-iso-image","title":"Create an image from ISO image","text":"<p>You can upload ISO images to the Image service (glance). You can subsequently boot an ISO image using Compute.</p> <p>In the Image service, run the following command:</p> <pre><code>$ glance image-create --name ISO_IMAGE --file IMAGE.iso \\\n  --disk-format iso --container-format bare\n</code></pre> <p>Optionally, to confirm the upload in Image service, run:</p> <pre><code>$ glance image-list\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Glance/Manage_images/#troubleshoot-image-creation","title":"Troubleshoot image creation","text":"<p>If you encounter problems in creating an image in the Image service or Compute, the following information may help you troubleshoot the creation process.</p> <ul> <li> <p>Ensure that the version of qemu you are using is version 0.14 or later. Earlier versions of qemu result in an\u00a0<code>unknown\u00a0option\u00a0-s</code>\u00a0error message in the\u00a0<code>/var/log/nova/nova-compute.log</code>\u00a0file.</p> </li> <li> <p>Examine the\u00a0<code>/var/log/nova/nova-api.log</code>\u00a0and\u00a0<code>/var/log/nova/nova-compute.log</code>\u00a0log files for error messages.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Heat/Orchestration_Service_Overview/","title":"Orchestration Service Overview (heat)","text":"<p>Heat is a service to orchestrate composite cloud applications using a declarative template format through an OpenStack-native REST API.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Heat/Orchestration_Service_Overview/#heats-purpose-and-vision","title":"Heat\u2019s purpose and vision","text":"<ul> <li> <p>Heat provides a template based orchestration for describing a cloud application by executing appropriate\u00a0OpenStack\u00a0API calls to generate running cloud applications.</p> </li> <li> <p>A Heat template describes the infrastructure for a cloud application in text files which are readable and writable by humans, and can be managed by version control tools.</p> </li> <li> <p>Templates specify the relationships between resources (e.g. this volume is connected to this server). This enables Heat to call out to the OpenStack APIs to create all of your infrastructure in the correct order to completely launch your application.</p> </li> <li> <p>The software integrates other components of OpenStack. The templates allow creation of most OpenStack resource types (such as instances, floating ips, volumes, security groups, users, etc), as well as some more advanced functionality such as instance high availability, instance autoscaling, and nested stacks.</p> </li> <li> <p>Heat primarily manages infrastructure, but the templates integrate well with software configuration management tools such as Puppet and Ansible.</p> </li> <li> <p>Operators can customise the capabilities of Heat by installing plugins.</p> </li> </ul> <p>This documentation offers information aimed at end-users, operators and developers of Heat.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Heat/Orchestration_Service_Overview/#operating-heat","title":"Operating Heat","text":"<ul> <li> <p>Installing Heat</p> </li> <li> <p>Running Heat API services in HTTP Server</p> </li> <li> <p>Configuring Heat</p> </li> <li> <p>Administering Heat</p> </li> <li> <p>Scaling a Deployment</p> </li> <li> <p>Upgrades Guideline</p> </li> <li> <p>Man pages for services and utilities</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Heat/Orchestration_Service_Overview/#using-heat","title":"Using Heat","text":"<ul> <li> <p>Creating your first stack</p> </li> <li> <p>Glossary</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Heat/Orchestration_Service_Overview/#working-with-templates","title":"Working with Templates","text":"<ul> <li> <p>Template Guide</p> </li> <li> <p>Example Templates</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Heat/Orchestration_Service_Overview/#using-the-heat-service","title":"Using the Heat Service","text":"<ul> <li> <p>OpenStack Orchestration API v1 Reference</p> </li> <li> <p>Python and CLI client</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Heat/Orchestration_Service_Overview/#developing-heat","title":"Developing Heat","text":"<ul> <li> <p>Heat Developer Guidelines</p> </li> <li> <p>Heat and DevStack</p> </li> <li> <p>Blueprints and Specs</p> </li> <li> <p>Heat architecture</p> </li> <li> <p>Heat Resource Plug-in Development Guide</p> </li> <li> <p>Heat Stack Lifecycle Scheduler Hints</p> </li> <li> <p>Guru Meditation Reports</p> </li> <li> <p>Heat Support Status usage Guide</p> </li> <li> <p>Using Rally on Heat gates</p> </li> <li> <p>Source Code Index</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Heat/Orchestration_Service_Overview/#for-contributors","title":"For Contributors","text":"<ul> <li> <p>If you are a new contributor to Heat please refer:\u00a0So You Want to Contribute\u2026</p> </li> <li> <p>Heat Contributor Guidelines</p> <ul> <li>So You Want to Contribute\u2026</li> </ul> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Heat/Orchestration_Service_Overview/#indices-and-tables","title":"Indices and tables","text":"<ul> <li> <p>Index</p> </li> <li> <p>Module Index</p> </li> <li> <p>Search Page</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Launch_Virtual_Machines/","title":"Launch Virtual Machines (Instances)","text":"<p>Instances are virtual machines that run inside the cloud. You can launch an instance from the following sources:</p> <ul> <li> <p>Images uploaded to the Image service.</p> </li> <li> <p>Image that you have copied to a persistent volume. The instance launches from the volume, which is provided by the\u00a0<code>cinder-volume</code>\u00a0API through iSCSI.</p> </li> <li> <p>Instance snapshot that you took.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Launch_Virtual_Machines/#launch-an-instance","title":"Launch an instance","text":"<ol> <li> <p>Log in to the dashboard.</p> </li> <li> <p>Select the appropriate project from the drop down menu at the top left.</p> </li> <li> <p>On the\u00a0Project\u00a0tab, open the\u00a0Compute\u00a0tab and click\u00a0Instances\u00a0category.The dashboard shows the instances with its name, its private and floating IP addresses, size, status, task, power state, and so on.</p> </li> <li> <p>Click\u00a0Launch Instance.</p> </li> <li> <p>In the\u00a0Launch Instance\u00a0dialog box, specify the following values:Details\u00a0tabInstance NameAssign a name to the virtual machine.</p> </li> </ol> <p>Note- The name you assign here becomes the initial host name of the server. If the name is longer than 63 characters, the Compute service truncates it automatically to ensure dnsmasq works correctly.</p> <ul> <li> <p>After the server is built, if you change the server name in the API or change the host name directly, the names are not updated in the dashboard.</p> </li> <li> <p>Server names are not guaranteed to be unique when created so you could have two instances with the same host name.</p> </li> </ul> <p>Description</p> <p>You can assign a brief description of the virtual machine.Availability Zone</p> <p>By default, this value is set to the availability zone given by the cloud provider (for example,\u00a0<code>us-west</code>\u00a0or\u00a0<code>apac-south</code>). For some cases, it could be\u00a0<code>nova</code>.Count</p> <p>To launch multiple instances, enter a value greater than\u00a0<code>1</code>. The default is\u00a0<code>1</code>.</p> <p>Source\u00a0tabInstance Boot Source</p> <p>Your options are:Boot from image</p> <p>If you choose this option, a new field for\u00a0Image Name\u00a0displays. You can select the image from the list.Boot from snapshot</p> <p>If you choose this option, a new field for\u00a0Instance Snapshot\u00a0displays. You can select the snapshot from the list.Boot from volume</p> <p>If you choose this option, a new field for\u00a0Volume\u00a0displays. You can select the volume from the list.Boot from image (creates a new volume)</p> <p>With this option, you can boot from an image and create a volume by entering the\u00a0Device Size\u00a0and\u00a0Device Name\u00a0for your volume. Click the\u00a0Delete Volume on Instance Delete\u00a0option to delete the volume on deleting the instance.Boot from volume snapshot (creates a new volume)</p> <p>Using this option, you can boot from a volume snapshot and create a new volume by choosing\u00a0Volume Snapshot\u00a0from a list and adding a\u00a0Device Name\u00a0for your volume. Click the\u00a0Delete Volume on Instance Delete\u00a0option to delete the volume on deleting the instance.Image Name</p> <p>This field changes based on your previous selection. If you have chosen to launch an instance using an image, the\u00a0Image Name\u00a0field displays. Select the image name from the dropdown list.Instance Snapshot</p> <p>This field changes based on your previous selection. If you have chosen to launch an instance using a snapshot, the\u00a0Instance Snapshot\u00a0field displays. Select the snapshot name from the dropdown list.Volume</p> <p>This field changes based on your previous selection. If you have chosen to launch an instance using a volume, the\u00a0Volume\u00a0field displays. Select the volume name from the dropdown list. If you want to delete the volume on instance delete, check the\u00a0Delete Volume on Instance Delete\u00a0option.</p> <p>Flavor\u00a0tabFlavor</p> <p>Specify the size of the instance to launch.</p> <p>Note The flavor is selected based on the size of the image selected for launching an instance. For example, while creating an image, if you have entered the value in the\u00a0Minimum RAM (MB)\u00a0field as 2048, then on selecting the image, the default flavor is\u00a0<code>m1.small</code>. 1) Networks\u00a0tabSelected NetworksTo add a network to the instance, click the\u00a0+\u00a0in the\u00a0Available\u00a0field.Network Ports\u00a0tabPortsActivate the ports that you want to assign to the instance.Security Groups\u00a0tabSecurity GroupsActivate the security groups that you want to assign to the instance.Security groups are a kind of cloud firewall that define which incoming network traffic is forwarded to instances.If you have not created any security groups, you can assign only the default security group to the instance.Key Pair\u00a0tabKey PairSpecify a key pair.If the image uses a static root password or a static key set (neither is recommended), you do not need to provide a key pair to launch the instance.Configuration\u00a0tabCustomization Script SourceSpecify a customization script that runs after your instance launches.Metadata\u00a0tabAvailable MetadataAdd Metadata items to your instance. 2) Click\u00a0Launch Instance.The instance starts on a compute node in the cloud.</p> <p>Note If you did not provide a key pair, security groups, or rules, users can access the instance only from inside the cloud through VNC. Even pinging the instance is not possible without an ICMP rule configured.</p> <p>You can also launch an instance from the\u00a0Images\u00a0or\u00a0Volumes\u00a0category when you launch an instance from an image or a volume respectively.</p> <p>When you launch an instance from an image, OpenStack creates a local copy of the image on the compute node where the instance starts.</p> <p>For details on creating images, see\u00a0Creating images manually\u00a0in the\u00a0OpenStack Virtual Machine Image Guide.</p> <p>When you launch an instance from a volume, note the following steps:</p> <ul> <li> <p>To select the volume from which to launch, launch an instance from an arbitrary image on the volume. The arbitrary image that you select does not boot. Instead, it is replaced by the image on the volume that you choose in the next steps.To boot a Xen image from a volume, the image you launch in must be the same type, fully virtualized or paravirtualized, as the one on the volume.</p> </li> <li> <p>Select the volume or volume snapshot from which to boot. Enter a device name. Enter\u00a0<code>vda</code>\u00a0for KVM images or\u00a0<code>xvda</code>\u00a0for Xen images.</p> </li> </ul> <p>Note When running QEMU without support for the hardware virtualization, set\u00a0<code>cpu_mode=\"none\"</code>\u00a0alongside\u00a0<code>virt_type=qemu</code>\u00a0in\u00a0<code>/etc/nova/nova-compute.conf</code>\u00a0to solve the following error:</p> <pre><code>libvirtError: unsupported configuration: CPU mode 'host-model'\nfor ``x86_64`` qemu domain on ``x86_64`` host is not supported by hypervisor\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Launch_Virtual_Machines/#connect-to-your-instance-by-using-ssh","title":"Connect to your instance by using SSH","text":"<p>To use SSH to connect to your instance, use the downloaded keypair file.</p> <p>Note The user name is\u00a0<code>ubuntu</code>\u00a0for the Ubuntu cloud images on TryStack.</p> <ol> <li> <p>Copy the IP address for your instance.</p> </li> <li> <p>Use the\u00a0ssh\u00a0command to make a secure connection to the instance. For example:</p> </li> </ol> <pre><code>$ ssh -i MyKey.pem ubuntu@10.0.0.2\n</code></pre> <ol> <li>At the prompt, type\u00a0<code>yes</code>.</li> </ol> <p>It is also possible to SSH into an instance without an SSH keypair, if the administrator has enabled root password injection. For more information about root password injection, see\u00a0Injecting the administrator password\u00a0in the\u00a0OpenStack Administrator Guide.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Launch_Virtual_Machines/#track-usage-for-instances","title":"Track usage for instances","text":"<p>You can track usage for instances for each project. You can track costs per month by showing meters like number of vCPUs, disks, RAM, and uptime for all your instances.</p> <ol> <li> <p>Log in to the dashboard.</p> </li> <li> <p>Select the appropriate project from the drop down menu at the top left.</p> </li> <li> <p>On the\u00a0Project\u00a0tab, open the\u00a0Compute\u00a0tab and click\u00a0Overview\u00a0category.</p> </li> <li> <p>To query the instance usage for a month, select a month and click\u00a0Submit.</p> </li> <li> <p>To download a summary, click\u00a0Download CSV Summary.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Launch_Virtual_Machines/#create-an-instance-snapshot","title":"Create an instance snapshot","text":"<ol> <li> <p>Log in to the dashboard.</p> </li> <li> <p>Select the appropriate project from the drop down menu at the top left.</p> </li> <li> <p>On the\u00a0Project\u00a0tab, open the\u00a0Compute\u00a0tab and click the\u00a0Instances\u00a0category.</p> </li> <li> <p>Select the instance from which to create a snapshot.</p> </li> <li> <p>In the actions column, click\u00a0Create Snapshot.</p> </li> <li> <p>In the\u00a0Create Snapshot\u00a0dialog box, enter a name for the snapshot, and click\u00a0Create Snapshot.The\u00a0Images\u00a0category shows the instance snapshot.</p> </li> </ol> <p>To launch an instance from the snapshot, select the snapshot and click\u00a0Launch. Proceed with launching an instance.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Launch_Virtual_Machines/#manage-an-instance","title":"Manage an instance","text":"<ol> <li> <p>Log in to the dashboard.</p> </li> <li> <p>Select the appropriate project from the drop down menu at the top left.</p> </li> <li> <p>On the\u00a0Project\u00a0tab, open the\u00a0Compute\u00a0tab and click\u00a0Instances\u00a0category.</p> </li> <li> <p>Select an instance.</p> </li> <li> <p>In the menu list in the actions column, select the state.You can resize or rebuild an instance. You can also choose to view the instance console log, edit instance or the security groups. Depending on the current state of the instance, you can pause, resume, suspend, soft or hard reboot, or terminate it.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Manage_Volumes/","title":"Manage volumes and volume types","text":"<p>Volumes are the Block Storage devices that you attach to instances to enable persistent storage. Users can attach a volume to a running instance or detach a volume and attach it to another instance at any time. For information about using the dashboard to create and manage volumes as an end user, see the\u00a0OpenStack End User Guide.</p> <p>As an administrative user, you can manage volumes and volume types for users in various projects. You can create and delete volume types, and you can view and delete volumes. Note that a volume can be encrypted by using the steps outlined below.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Manage_Volumes/#create-a-volume-type","title":"Create a volume type","text":"<ol> <li> <p>Log in to the dashboard and select the\u00a0admin\u00a0project from the drop-down list.</p> </li> <li> <p>On the\u00a0Admin\u00a0tab, open the\u00a0Volume\u00a0tab.</p> </li> <li> <p>Click the\u00a0Volume Types\u00a0tab, and click\u00a0Create Volume Type\u00a0button. In the\u00a0Create Volume Type\u00a0window, enter a name for the volume type.</p> </li> <li> <p>Click\u00a0Create Volume Type\u00a0button to confirm your changes.</p> </li> </ol> <p>Note A message indicates whether the action succeeded.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Manage_Volumes/#create-an-encrypted-volume-type","title":"Create an encrypted volume type","text":"<ol> <li> <p>Create a volume type using the steps above for\u00a0Create a volume type.</p> </li> <li> <p>Click\u00a0Create Encryption\u00a0in the Actions column of the newly created volume type.</p> </li> <li> <p>Configure the encrypted volume by setting the parameters below from available options (see table):</p> </li> </ol> <p>Provider</p> <ul> <li>Specifies the encryption provider format.</li> </ul> <p>Control Location</p> <ul> <li>Specifies whether the encryption is from the front end (nova) or the back end   (cinder).</li> </ul> <p>Cipher</p> <ul> <li>Specifies the encryption algorithm.</li> </ul> <p>Key Size (bits)</p> <ul> <li> <p>Specifies the encryption key size.</p> </li> <li> <p>Click\u00a0Create Volume Type Encryption.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Manage_Volumes/#encryption-options","title":"Encryption Options","text":"<p>The table below provides a few alternatives available for creating encrypted volumes.</p> Encryption Parameters Parameter Options Comments Provider luks (Recommended) Allows easier import and migration of imported encrypted volumes, and allows access key to be changed without re-encrypting the volume plain Less disk overhead than LUKS Control Location front-end (Recommended) The encryption occurs within nova so that the data transmitted over the network is encrypted back-end This could be selected if a cinder plug-in supporting an encrypted back-end block storage device becomes available in the future. TLS or other network encryption would also be needed to protect data as it traverses the network Cipher aes-xts-plain64 (Recommended) See NIST reference below to see advantages* aes-cbc-essiv Note: On the command line, type <code>cryptsetup benchmark</code> for additional options Key Size (bits) 256 (Recommended for aes-xts-plain64 and aes-cbc-essiv) Using this selection for aes-xts, the underlying key size would only be 128-bits* <p>*\u00a0Source\u00a0NIST SP 800-38E</p> <p>Note To see further information and CLI instructions, see\u00a0Create an encrypted volume type\u00a0in the OpenStack Block Storage Configuration Guide.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Manage_Volumes/#delete-volume-types","title":"Delete volume types","text":"<p>When you delete a volume type, volumes of that type are not deleted.</p> <ol> <li> <p>Log in to the dashboard and select the\u00a0admin\u00a0project from the drop-down list.</p> </li> <li> <p>On the\u00a0Admin\u00a0tab, open the\u00a0Volume\u00a0tab.</p> </li> <li> <p>Click the\u00a0Volume Types\u00a0tab, select the volume type or types that you want to delete.</p> </li> <li> <p>Click\u00a0Delete Volume Types\u00a0button.</p> </li> <li> <p>In the\u00a0Confirm Delete Volume Types\u00a0window, click the\u00a0Delete Volume Types\u00a0button to confirm the action.</p> </li> </ol> <p>Note A message indicates whether the action succeeded.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Manage_Volumes/#delete-volumes","title":"Delete volumes","text":"<p>When you delete an instance, the data of its attached volumes is not destroyed.</p> <ol> <li> <p>Log in to the dashboard and select the\u00a0admin\u00a0project from the drop-down list.</p> </li> <li> <p>On the\u00a0Admin\u00a0tab, open the\u00a0Volume\u00a0tab.</p> </li> <li> <p>Click the\u00a0Volumes\u00a0tab, Select the volume or volumes that you want to delete.</p> </li> <li> <p>Click\u00a0Delete Volumes\u00a0button.</p> </li> <li> <p>In the\u00a0Confirm Delete Volumes\u00a0window, click the\u00a0Delete Volumes\u00a0button to confirm the action.</p> </li> </ol> <p>Note A message indicates whether the action succeeded.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Taikun_OCP_Dashboard/","title":"Introduction","text":"<p>Horizon is the canonical implementation of\u00a0OpenStack\u2019s Dashboard, which provides a web based user interface to OpenStack services including Nova, Swift, Keystone, etc.</p> <p>For a more in-depth look at Horizon and its architecture, see the\u00a0Horizon Basics.</p> <p>To learn what you need to know to get going, see the\u00a0Quickstart.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Taikun_OCP_Dashboard/#using-horizon","title":"Using Horizon","text":"<p>How to use Horizon in your own projects.</p> <ul> <li> <p>Installation Guide</p> </li> <li> <p>System Requirements</p> </li> <li> <p>Installing from Packages</p> </li> <li> <p>Installing from Source</p> </li> <li> <p>Horizon plugins</p> </li> <li> <p>Configuration Guide</p> </li> <li> <p>Settings Reference</p> </li> <li> <p>Pluggable Panels and Groups</p> </li> <li> <p>Customizing Horizon</p> </li> <li> <p>Themes</p> </li> <li> <p>Branding Horizon</p> </li> <li> <p>User Documentation</p> </li> <li> <p>Log in to the dashboard</p> </li> <li> <p>Upload and manage images</p> </li> <li> <p>Configure access and security for instances</p> </li> <li> <p>Launch and manage instances</p> </li> <li> <p>Create and manage networks</p> </li> <li> <p>Create and manage object containers</p> </li> <li> <p>Create and manage volumes</p> </li> <li> <p>Supported Browsers</p> </li> <li> <p>Administration Guide</p> </li> <li> <p>Customize and configure the Dashboard</p> </li> <li> <p>Set up session storage for the Dashboard</p> </li> <li> <p>Create and manage images</p> </li> <li> <p>Create and manage roles</p> </li> <li> <p>Manage projects and users</p> </li> <li> <p>Manage instances</p> </li> <li> <p>Manage flavors</p> </li> <li> <p>Manage volumes and volume types</p> </li> <li> <p>View and manage quotas</p> </li> <li> <p>View services information</p> </li> <li> <p>Create and manage host aggregates</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Taikun_OCP_Dashboard/#contributor-docs","title":"Contributor Docs","text":"<p>For those wishing to develop Horizon itself, or go in-depth with building your own\u00a0<code>Dashboard</code>\u00a0or\u00a0<code>Panel</code>\u00a0classes, the following documentation is provided.</p> <ul> <li> <p>Contributor Documentation</p> </li> <li> <p>So You Want to Contribute\u2026</p> </li> <li> <p>Horizon Basics</p> </li> <li> <p>Project Policies</p> </li> <li> <p>Quickstart</p> </li> <li> <p>Horizon\u2019s tests and you</p> </li> <li> <p>Tutorials</p> </li> <li> <p>Topic Guides</p> </li> <li> <p>Module Reference</p> </li> <li> <p>Frequently Asked Questions</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Taikun_OCP_Dashboard/#release-notes","title":"Release Notes","text":"<p>See\u00a0https://docs.openstack.org/releasenotes/horizon/.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/Taikun_OCP_Dashboard/#information","title":"Information","text":"<ul> <li> <p>Glossary</p> </li> <li> <p>Index</p> </li> <li> <p>Module Index</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/View_and_Manage_Quotas/","title":"View and manage quotas","text":"<p>To prevent system capacities from being exhausted without notification, you can set up quotas. Quotas are operational limits. For example, the number of gigabytes allowed for each project can be controlled so that cloud resources are optimized. Quotas can be enforced at both the project and the project-user level.</p> <p>Typically, you change quotas when a project needs more than ten volumes or 1\u00a0TB on a compute node.</p> <p>Using the Dashboard, you can view default Compute and Block Storage quotas for new projects, as well as update quotas for existing projects.</p> <p>Note Using the command-line interface, you can manage quotas for\u00a0the OpenStack Compute service,\u00a0the OpenStack Block Storage service, and the OpenStack Networking service (For CLI details, see\u00a0OpenStackClient CLI reference). Additionally, you can update Compute service quotas for project users.</p> <p>The following table describes the Compute and Block Storage service quotas:</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/View_and_Manage_Quotas/#quota-descriptions","title":"Quota Descriptions","text":"Quota Name Defines the number of Service Gigabytes Volume gigabytes allowed for each project. Block Storage Instances Instances allowed for each project. Compute Injected Files Injected files allowed for each project. Compute Injected File Content Bytes Content bytes allowed for each injected file. Compute Keypairs Number of keypairs. Compute Metadata Items Metadata items allowed for each instance. Compute RAM (MB) RAM megabytes allowed for each instance. Compute Security Groups Security groups allowed for each project. Compute Security Group Rules Security group rules allowed for each project. Compute Snapshots Volume snapshots allowed for each project. Block Storage VCPUs Instance cores allowed for each project. Compute Volumes Volumes allowed for each project. Block Storage"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/View_and_Manage_Quotas/#view-default-project-quotas","title":"View default project quotas","text":"<ol> <li> <p>Log in to the dashboard and select the\u00a0admin\u00a0project from the drop-down list.</p> </li> <li> <p>On the\u00a0Admin\u00a0tab, open the\u00a0System\u00a0tab and click the\u00a0Defaults\u00a0category.</p> </li> <li> <p>The default quota values are displayed.</p> </li> </ol> <p>Note You can sort the table by clicking on either the\u00a0Quota Name\u00a0or\u00a0Limit\u00a0column headers.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Horizon/View_and_Manage_Quotas/#update-project-quotas","title":"Update project quotas","text":"<ol> <li> <p>Log in to the dashboard and select the\u00a0admin\u00a0project from the drop-down list.</p> </li> <li> <p>On the\u00a0Admin\u00a0tab, open the\u00a0System\u00a0tab and click the\u00a0Defaults\u00a0category.</p> </li> <li> <p>Click the\u00a0Update Defaults\u00a0button.</p> </li> <li> <p>In the\u00a0Update Default Quotas\u00a0window, you can edit the default quota values.</p> </li> <li> <p>Click the\u00a0Update Defaults\u00a0button.</p> </li> </ol> <p>Note The dashboard does not show all possible project quotas. To view and update the quotas for a service, use its command-line client. See\u00a0OpenStack Administrator Guide.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/","title":"Ansible Deploy Interface","text":"<p>Ansible\u00a0is a mature and popular automation tool, written in Python and requiring no agents running on the node being configured. All communications with the node are by default performed over secure SSH transport.</p> <p>The\u00a0<code>ansible</code>\u00a0deploy interface uses Ansible playbooks to define the deployment logic. It is not based on\u00a0Ironic Python Agent (IPA)\u00a0and does not generally need IPA to be running in the deploy ramdisk.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#overview","title":"Overview","text":"<p>The main advantage of this deploy interface is extended flexibility in regards to changing and adapting node deployment logic for specific use cases, via Ansible tooling that is already familiar to operators.</p> <p>It can be used to shorten the usual feature development cycle of</p> <ul> <li> <p>implementing logic in ironic,</p> </li> <li> <p>implementing logic in IPA,</p> </li> <li> <p>rebuilding deploy ramdisk,</p> </li> <li> <p>uploading deploy ramdisk to Glance/HTTP storage,</p> </li> <li> <p>reassigning deploy ramdisk to nodes,</p> </li> <li> <p>restarting ironic-conductor service(s) and</p> </li> <li> <p>running a test deployment</p> </li> </ul> <p>by using a \u201cstable\u201d deploy ramdisk and not requiring ironic-conductor restarts (see\u00a0Extending playbooks).</p> <p>The main disadvantage of this deploy interface is the synchronous manner of performing deployment/cleaning tasks. A separate\u00a0<code>ansible-playbook</code>\u00a0process is spawned for each node being provisioned or cleaned, which consumes one thread from the thread pool available to the\u00a0<code>ironic-conductor</code>\u00a0process and blocks this thread until the node provisioning or cleaning step is finished or fails. This has to be taken into account when planning an ironic deployment that enables this deploy interface.</p> <p>Each action (deploy, clean) is described by a single playbook with roles, which is run whole during deployment, or tag-wise during cleaning. Control of cleaning steps is through tags and auxiliary clean steps file. The playbooks for actions can be set per-node, as can the clean steps file.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#features","title":"Features","text":"<p>Similar to deploy interfaces relying on\u00a0Ironic Python Agent (IPA), this deploy interface also depends on the deploy ramdisk calling back to ironic API\u2019s\u00a0<code>heartbeat</code>\u00a0endpoint.</p> <p>However, the driver is currently synchronous, so only the first heartbeat is processed and is used as a signal to start\u00a0<code>ansible-playbook</code>\u00a0process.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#user-images","title":"User images","text":"<p>Supports whole-disk images and partition images:</p> <ul> <li> <p>compressed images are downloaded to RAM and converted to disk device;</p> </li> <li> <p>raw images are streamed to disk directly.</p> </li> </ul> <p>For partition images the driver will create root partition, and, if requested, ephemeral and swap partitions as set in node\u2019s\u00a0<code>instance_info</code>\u00a0by the Compute service or operator. The create partition table will be of\u00a0<code>msdos</code>\u00a0type by default, the node\u2019s\u00a0<code>disk_label</code>\u00a0capability is honored if set in node\u2019s\u00a0<code>instance_info</code>\u00a0(see also\u00a0Choosing the disk label).</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#configdrive-partition","title":"Configdrive partition","text":"<p>Creating a configdrive partition is supported for both whole disk and partition images, on both\u00a0<code>msdos</code>\u00a0and\u00a0<code>GPT</code>\u00a0labeled disks.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#root-device-hints","title":"Root device hints","text":"<p>Root device hints are currently supported in their basic form only, with exact matches (see\u00a0Specifying the disk for deployment (root device hints)\u00a0for more details). If no root device hint is provided for the node, the first device returned as part of\u00a0<code>ansible_devices</code>\u00a0fact is used as root device to create partitions on or write the whole disk image to.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#node-cleaning","title":"Node cleaning","text":"<p>Cleaning is supported, both automated and manual. The driver has two default clean steps:</p> <ul> <li> <p>wiping device metadata</p> </li> <li> <p>disk shredding</p> </li> </ul> <p>Their priority can be overridden via\u00a0<code>[deploy]\\erase_devices_metadata_priority</code>\u00a0and\u00a0<code>[deploy]\\erase_devices_priority</code>\u00a0options, respectively, in the ironic configuration file.</p> <p>As in the case of this driver all cleaning steps are known to the ironic-conductor service, booting the deploy ramdisk is completely skipped when there are no cleaning steps to perform.</p> <p>Note Aborting cleaning steps is not supported.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#logging","title":"Logging","text":"<p>Logging is implemented as custom Ansible callback module, that makes use of\u00a0<code>oslo.log</code>\u00a0and\u00a0<code>oslo.config</code>\u00a0libraries and can re-use logging configuration defined in the main ironic configuration file to set logging for Ansible events, or use a separate file for this purpose.</p> <p>It works best when\u00a0<code>journald</code>\u00a0support for logging is enabled.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#requirements","title":"Requirements","text":"<p>Ansible</p> <p>Tested with, and targets, Ansible 2.5.x</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#bootstrap-image-requirements","title":"Bootstrap image requirements","text":"<ul> <li> <p>password-less sudo permissions for the user used by Ansible</p> </li> <li> <p>python 2.7.x</p> </li> <li> <p>openssh-server</p> </li> <li> <p>GNU coreutils</p> </li> <li> <p>utils-linux</p> </li> <li> <p>parted</p> </li> <li> <p>gdisk</p> </li> <li> <p>qemu-utils</p> </li> <li> <p>python-requests (for ironic callback and streaming image download)</p> </li> <li> <p>python-netifaces (for ironic callback)</p> </li> </ul> <p>A set of scripts to build a suitable deploy ramdisk based on TinyCore Linux and\u00a0<code>tinyipa</code>\u00a0ramdisk, and an element for\u00a0<code>diskimage-builder</code>\u00a0can be found in\u00a0ironic-staging-drivers\u00a0project but will be eventually migrated to the new\u00a0ironic-python-agent-builder\u00a0project.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#setting-up-your-environment","title":"Setting up your environment","text":"<ol> <li> <p>Install ironic (either as part of OpenStack or standalone)</p> </li> <li> <p>If using ironic as part of OpenStack, ensure that the Image service is configured to use the Object Storage service as backend, and the Bare Metal service is configured accordingly, see\u00a0Configure the Image service for temporary URLs.</p> </li> <li> <p>Install Ansible version as specified in\u00a0<code>ironic/driver-requirements.txt</code>\u00a0file</p> </li> <li> <p>Edit ironic configuration file</p> </li> <li> <p>Add\u00a0<code>ansible</code>\u00a0to the list of deploy interfaces defined in\u00a0<code>[DEFAULT]\\enabled_deploy_interfaces</code>\u00a0option.</p> </li> <li> <p>Ensure that a hardware type supporting\u00a0<code>ansible</code>\u00a0deploy interface is enabled in\u00a0<code>[DEFAULT]\\enabled_hardware_types</code>\u00a0option.</p> </li> <li> <p>Modify options in the\u00a0<code>[ansible]</code>\u00a0section of ironic\u2019s configuration file if needed (see\u00a0Configuration file).</p> </li> <li> <p>(Re)start ironic-conductor service</p> </li> <li> <p>Build suitable deploy kernel and ramdisk images</p> </li> <li> <p>Upload them to Glance or put in your HTTP storage</p> </li> <li> <p>Create new or update existing nodes to use the enabled driver of your choice and populate\u00a0Driver properties for the Node\u00a0when different from defaults.</p> </li> <li> <p>Deploy the node as usual.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#ansible-deploy-options","title":"Ansible-deploy options","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#configuration-file","title":"Configuration file","text":"<p>Driver options are configured in\u00a0<code>[ansible]</code>\u00a0section of ironic configuration file, for their descriptions and default values please see\u00a0configuration file sample.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#driver-properties-for-the-node","title":"Driver properties for the Node","text":"<p>Set them per-node via\u00a0<code>baremetal\u00a0node\u00a0set</code>\u00a0command, for example:</p> <pre><code>baremetal node set &lt;node&gt; \\\n    --deploy-interface ansible \\\n    --driver-info ansible_username=stack \\\n    --driver-info ansible_key_file=/etc/ironic/id_rsa\n</code></pre> <p>ansible_username</p> <p>User name to use for Ansible to access the node. Default is taken from\u00a0<code>[ansible]/default_username</code>\u00a0option of the ironic configuration file (defaults to\u00a0<code>ansible</code>).</p> <p>ansible_key_file</p> <p>Private SSH key used to access the node. Default is taken from\u00a0<code>[ansible]/default_key_file</code>\u00a0option of the ironic configuration file. If neither is set, the default private SSH keys of the user running the\u00a0<code>ironic-conductor</code>\u00a0process will be used.</p> <p>ansible_deploy_playbook</p> <p>Playbook to use when deploying this node. Default is taken from\u00a0<code>[ansible]/default_deploy_playbook</code>\u00a0option of the ironic configuration file (defaults to\u00a0<code>deploy.yaml</code>).</p> <p>ansible_shutdown_playbook</p> <p>Playbook to use to gracefully shutdown the node in-band. Default is taken from\u00a0<code>[ansible]/default_shutdown_playbook</code>\u00a0option of the ironic configuration file (defaults to\u00a0<code>shutdown.yaml</code>).</p> <p>ansible_clean_playbook</p> <p>Playbook to use when cleaning the node. Default is taken from\u00a0<code>[ansible]/default_clean_playbook</code>\u00a0option of the ironic configuration file (defaults to\u00a0<code>clean.yaml</code>).</p> <p>ansible_clean_steps_config</p> <p>Auxiliary YAML file that holds description of cleaning steps used by this node, and defines playbook tags in\u00a0<code>ansible_clean_playbook</code>\u00a0file corresponding to each cleaning step. Default is taken from\u00a0<code>[ansible]/default_clean_steps_config</code>\u00a0option of the ironic configuration file (defaults to\u00a0<code>clean_steps.yaml</code>).</p> <p>ansible_python_interpreter</p> <p>Absolute path to the python interpreter on the managed machine. Default is taken from\u00a0<code>[ansible]/default_python_interpreter</code>\u00a0option of the ironic configuration file. Ansible uses\u00a0<code>/usr/bin/python</code>\u00a0by default.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#customizing-the-deployment-logic","title":"Customizing the deployment logic","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#expected-playbooks-directory-layout","title":"Expected playbooks directory layout","text":"<p>The\u00a0<code>[ansible]\\playbooks_path</code>\u00a0option in the ironic configuration file is expected to have a standard layout for an Ansible project with some additions:</p> <pre><code>&lt;playbooks_path&gt;\n|\n\\_ inventory\n\\_ add-ironic-nodes.yaml\n\\_ roles\n \\_ role1\n \\_ role2\n \\_ ...\n|\n\\_callback_plugins\n \\_ ...\n|\n\\_ library\n \\_ ...\n</code></pre> <p>The extra files relied by this driver are:</p> <p>inventory</p> <p>Ansible inventory file containing a single entry of\u00a0<code>conductor\u00a0ansible_connection=local</code>. This basically defines an alias to\u00a0<code>localhost</code>. Its purpose is to make logging for tasks performed by Ansible locally and referencing the localhost in playbooks more intuitive. This also suppresses warnings produced by Ansible about\u00a0<code>hosts</code>\u00a0file being empty.</p> <p>add-ironic-nodes.yaml</p> <p>This file contains an Ansible play that populates in-memory Ansible inventory with access information received from the ansible-deploy interface, as well as some per-node variables. Include it in all your custom playbooks as the first play.</p> <p>The default\u00a0<code>deploy.yaml</code>\u00a0playbook is using several smaller roles that correspond to particular stages of deployment process:</p> <ul> <li> <p><code>discover</code>\u00a0\u2013 e.g. set root device and image target</p> </li> <li> <p><code>prepare</code>\u00a0\u2013 if needed, prepare system, for example create partitions</p> </li> <li> <p><code>deploy</code>\u00a0\u2013 download/convert/write user image and configdrive</p> </li> <li> <p><code>configure</code>\u00a0\u2013 post-deployment steps, e.g. installing the bootloader</p> </li> </ul> <p>Some more included roles are:</p> <ul> <li> <p><code>shutdown</code>\u00a0\u2013 used to gracefully power the node off in-band</p> </li> <li> <p><code>clean</code>\u00a0\u2013 defines cleaning procedure, with each clean step defined as separate playbook tag.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#extending-playbooks","title":"Extending playbooks","text":"<p>Most probably you\u2019d start experimenting like this:</p> <ol> <li> <p>Create a copy of\u00a0<code>deploy.yaml</code>\u00a0playbook\u00a0in the same folder, name it distinctively.</p> </li> <li> <p>Create Ansible roles with your customized logic in\u00a0<code>roles</code>\u00a0folder.</p> </li> <li> <p>In your custom deploy playbook, replace the\u00a0<code>prepare</code>\u00a0role with your own one that defines steps to be run\u00a0before\u00a0image download/writing. This is a good place to set facts overriding those provided/omitted by the driver, like\u00a0<code>ironic_partitions</code>\u00a0or\u00a0<code>ironic_root_device</code>, and create custom partitions or (software) RAIDs.</p> </li> <li> <p>In your custom deploy playbook, replace the\u00a0<code>configure</code>\u00a0role with your own one that defines steps to be run\u00a0after\u00a0image is written to disk. This is a good place for example to configure the bootloader and add kernel options to avoid additional reboots.</p> </li> <li> <p>Use those new roles in your new playbook.</p> </li> <li> <p>Assign the custom deploy playbook you\u2019ve created to the node\u2019s\u00a0<code>driver_info/ansible_deploy_playbook</code>\u00a0field.</p> </li> <li> <p>Run deployment.</p> </li> <li> <p>No ironic-conductor restart is necessary.</p> </li> <li> <p>A new deploy ramdisk must be built and assigned to nodes only when you want to use a command/script/package not present in the current deploy ramdisk and you can not or do not want to install those at runtime.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#variables-you-have-access-to","title":"Variables you have access to","text":"<p>This driver will pass the single JSON-ified extra var argument to Ansible (as in\u00a0<code>ansible-playbook\u00a0-e\u00a0..</code>). Those values are then accessible in your plays as well (some of them are optional and might not be defined):</p> <pre><code>ironic:\n  nodes:\n  - ip: \"&lt;IPADDRESS&gt;\"\n    name: \"&lt;NODE_UUID&gt;\"\n    user: \"&lt;USER ANSIBLE WILL USE&gt;\"\n    extra: \"&lt;COPY OF NODE's EXTRA FIELD&gt;\"\n  image:\n    url: \"&lt;URL TO FETCH THE USER IMAGE FROM&gt;\"\n    disk_format: \"&lt;qcow2|raw|...&gt;\"\n    container_format: \"&lt;bare|...&gt;\"\n    checksum: \"&lt;hash-algo:hashstring&gt;\"\n    mem_req: \"&lt;REQUIRED FREE MEMORY TO DOWNLOAD IMAGE TO RAM&gt;\"\n    tags: \"&lt;LIST OF IMAGE TAGS AS DEFINED IN GLANCE&gt;\"\n    properties: \"&lt;DICT OF IMAGE PROPERTIES AS DEFINED IN GLANCE&gt;\"\n  configdrive:\n    type: \"&lt;url|file&gt;\"\n    location: \"&lt;URL OR PATH ON CONDUCTOR&gt;\"\n  partition_info:\n    label: \"&lt;msdos|gpt&gt;\"\n    preserve_ephemeral: \"&lt;bool&gt;\"\n    ephemeral_format: \"&lt;FILESYSTEM TO CREATE ON EPHEMERAL PARTITION&gt;\"\n    partitions: \"&lt;LIST OF PARTITIONS IN FORMAT EXPECTED BY PARTED MODULE&gt;\"\n  raid_config: \"&lt;COPY OF NODE's TARGET_RAID_CONFIG FIELD&gt;\"\n</code></pre> <p><code>ironic.nodes</code></p> <p>List of dictionaries (currently of only one element) that will be used by\u00a0<code>add-ironic-nodes.yaml</code>\u00a0play to populate in-memory inventory. It also contains a copy of node\u2019s\u00a0<code>extra</code>\u00a0field so you can access it in the playbooks. The Ansible\u2019s host is set to node\u2019s UUID.</p> <p><code>ironic.image</code></p> <p>All fields of node\u2019s\u00a0<code>instance_info</code>\u00a0that start with\u00a0<code>image_</code>\u00a0are passed inside this variable. Some extra notes and fields:</p> <ul> <li> <p><code>mem_req</code>\u00a0is calculated from image size (if available) and config option\u00a0<code>[ansible]extra_memory</code>.</p> </li> <li> <p>if\u00a0<code>checksum</code>\u00a0is not in the form\u00a0<code>&lt;hash-algo&gt;:&lt;hash-sum&gt;</code>, hashing algorithm is assumed to be\u00a0<code>md5</code>\u00a0(default in Glance).</p> </li> <li> <p><code>validate_certs</code>\u00a0\u2013 boolean (<code>yes/no</code>) flag that turns validating image store SSL certificate on or off (default is \u2018yes\u2019). Governed by\u00a0<code>[ansible]image_store_insecure</code>\u00a0option in ironic configuration file.</p> </li> <li> <p><code>cafile</code>\u00a0\u2013 custom CA bundle to use for validating image store SSL certificate. Takes value of\u00a0<code>[ansible]image_store_cafile</code>\u00a0if that is defined. Currently is not used by default playbooks, as Ansible has no way to specify the custom CA bundle to use for single HTTPS actions, however you can use this value in your custom playbooks to for example upload and register this CA in the ramdisk at deploy time.</p> </li> <li> <p><code>client_cert</code>\u00a0\u2013 cert file for client-side SSL authentication. Takes value of\u00a0<code>[ansible]image_store_certfile</code>\u00a0option if defined. Currently is not used by default playbooks, however you can use this value in your custom playbooks.</p> </li> <li> <p><code>client_key</code>\u00a0\u2013 private key file for client-side SSL authentication. Takes value of\u00a0<code>[ansible]image_store_keyfile</code>\u00a0option if defined. Currently is not used by default playbooks, however you can use this value in your custom playbooks.</p> </li> </ul> <p><code>ironic.partition_info.partitions</code></p> <p>Optional. List of dictionaries defining partitions to create on the node in the form:</p> <pre><code>partitions:\n- name: \"&lt;NAME OF PARTITION&gt;\"\n  unit: \"&lt;UNITS FOR SIZE&gt;\"\n  size: \"&lt;SIZE OF THE PARTITION&gt;\"\n  type: \"&lt;primary|extended|logical&gt;\"\n  align: \"&lt;ONE OF PARTED_SUPPORTED OPTIONS&gt;\"\n  format: \"&lt;PARTITION TYPE TO SET&gt;\"\n  flags:\n    flag_name: \"&lt;bool&gt;\"\n</code></pre> <p>The driver will populate this list from\u00a0<code>root_gb</code>,\u00a0<code>swap_mb</code>\u00a0and\u00a0<code>ephemeral_gb</code>\u00a0fields of\u00a0<code>instance_info</code>. The driver will also prepend the\u00a0<code>bios_grub</code>-labeled partition when deploying on GPT-labeled disk, and pre-create a 64 MiB partition for configdrive if it is set in\u00a0<code>instance_info</code>.</p> <p>Please read the documentation included in the\u00a0<code>ironic_parted</code>\u00a0module\u2019s source for more info on the module and its arguments.</p> <p><code>ironic.partition_info.ephemeral_format</code></p> <p>Optional. Taken from\u00a0<code>instance_info</code>, it defines file system to be created on the ephemeral partition. Defaults to the value of\u00a0<code>[pxe]\\default_ephemeral_format</code>\u00a0option in ironic configuration file.</p> <p><code>ironic.partition_info.preserve_ephemeral</code></p> <p>Optional. Taken from the\u00a0<code>instance_info</code>, it specifies if the ephemeral partition must be preserved or rebuilt. Defaults to\u00a0<code>no</code>.<code>ironic.raid_config</code></p> <p>Taken from the\u00a0<code>target_raid_config</code>\u00a0if not empty, it specifies the RAID configuration to apply.</p> <p>As usual for Ansible playbooks, you also have access to standard Ansible facts discovered by\u00a0<code>setup</code>\u00a0module.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ansible_Deploy_Interface/#included-custom-ansible-modules","title":"Included custom Ansible modules","text":"<p>The provided\u00a0<code>playbooks_path/library</code>\u00a0folder includes several custom Ansible modules used by default implementation of\u00a0<code>deploy</code>\u00a0and\u00a0<code>prepare</code>\u00a0roles. You can use these modules in your playbooks as well.</p> <p><code>stream_url</code></p> <p>Streaming download from HTTP(S) source to the disk device directly, tries to be compatible with Ansible\u2019s\u00a0<code>get_url</code>\u00a0module in terms of module arguments. Due to the low level of such operation it is not idempotent.</p> <p><code>ironic_parted</code></p> <p>creates partition tables and partitions with\u00a0<code>parted</code>\u00a0utility. Due to the low level of such operation it is not idempotent. Please read the documentation included in the module\u2019s source for more information about this module and its arguments. The name is chosen so that the\u00a0<code>parted</code>\u00a0module included in Ansible is not shadowed.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Bare_Metal_service_overview/","title":"Bare Metal service overview","text":"<p>The Bare Metal service, codenamed\u00a0<code>ironic</code>, is a collection of components that provides support to manage and provision physical machines.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Bare_Metal_service_overview/#bare-metal-service-components","title":"Bare Metal service components","text":"<p>The Bare Metal service includes the following components:</p> <p>ironic-api</p> <p>A RESTful API that processes application requests by sending them to the ironic-conductor over\u00a0remote procedure call (RPC). Can be run through\u00a0WSGI\u00a0or as a separate process.</p> <p>ironic-conductor</p> <p>Adds/edits/deletes nodes; powers on/off nodes with IPMI or other vendor-specific protocol; provisions/deploys/cleans bare metal nodes.</p> <p>ironic-conductor uses\u00a0drivers\u00a0to execute operations on hardware.</p> <p>ironic-python-agent</p> <p>A python service which is run in a temporary ramdisk to provide ironic-conductor and ironic-inspector services with remote access, in-band hardware control, and hardware introspection.</p> <p>Additionally, the Bare Metal service has certain external dependencies, which are very similar to other OpenStack services:</p> <ul> <li> <p>A database to store hardware information and state. You can set the database back-end type and location. A simple approach is to use the same database back end as the Compute service. Another approach is to use a separate database back-end to further isolate bare metal resources (and associated metadata) from users.</p> </li> <li> <p>An\u00a0oslo.messaging\u00a0compatible queue, such as RabbitMQ. It may use the same implementation as that of the Compute service, but that is not a requirement. Used to implement RPC between ironic-api and ironic-conductor.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Bare_Metal_service_overview/#deployment-architecture","title":"Deployment architecture","text":"<p>The Bare Metal RESTful API service is used to enroll hardware that the Bare Metal service will manage. A cloud administrator usually registers it, specifying their attributes such as MAC addresses and IPMI credentials. There can be multiple instances of the API service.</p> <p>The\u00a0ironic-conductor\u00a0process does the bulk of the work. For security reasons, it is advisable to place it on an isolated host, since it is the only service that requires access to both the data plane and IPMI control plane.</p> <p>There can be multiple instances of the conductor service to support various class of drivers and also to manage fail over. Instances of the conductor service should be on separate nodes. Each conductor can itself run many drivers to operate heterogeneous hardware. This is depicted in the following figure.</p> <p></p> <p>The API exposes a list of supported drivers and the names of conductor hosts servicing them.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Bare_Metal_service_overview/#interaction-with-openstack-components","title":"Interaction with OpenStack components","text":"<p>The Bare Metal service may, depending upon configuration, interact with several other OpenStack services. This includes:</p> <ul> <li> <p>the OpenStack Telemetry module (<code>ceilometer</code>) for consuming the IPMI metrics</p> </li> <li> <p>the OpenStack Identity service (<code>keystone</code>) for request authentication and to locate other OpenStack services</p> </li> <li> <p>the OpenStack Image service (<code>glance</code>) from which to retrieve images and image meta-data</p> </li> <li> <p>the OpenStack Networking service (<code>neutron</code>) for DHCP and network configuration</p> </li> <li> <p>the OpenStack Compute service (<code>nova</code>) works with the Bare Metal service and acts as a user-facing API for instance management, while the Bare Metal service provides the admin/operator API for hardware management. The OpenStack Compute service also provides scheduling facilities (matching flavors \\&lt;-&gt; images \\&lt;-&gt; hardware), tenant quotas, IP assignment, and other services which the Bare Metal service does not, in and of itself, provide.</p> </li> <li> <p>the OpenStack Object Storage (<code>swift</code>) provides temporary storage for the configdrive, user images, deployment logs and inspection data.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Bare_Metal_service_overview/#logical-architecture","title":"Logical architecture","text":"<p>The diagram below shows the logical architecture. It shows the basic components that form the Bare Metal service, the relation of the Bare Metal service with other OpenStack services and the logical flow of a boot instance request resulting in the provisioning of a physical server.</p> <p></p> <p>A user\u2019s request to boot an instance is passed to the Compute service via the Compute API and the Compute Scheduler. The Compute service uses the\u00a0ironic virt driver\u00a0to hand over this request to the Bare Metal service, where the request passes from the Bare Metal API, to the Conductor, to a Driver to successfully provision a physical server for the user.</p> <p>Just as the Compute service talks to various OpenStack services like Image, Network, Object Store etc to provision a virtual machine instance, here the Bare Metal service talks to the same OpenStack services for image, network and other resource needs to provision a bare metal instance.</p> <p>See\u00a0Understanding Bare Metal Deployment\u00a0for a more detailed breakdown of a typical deployment process.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Bare_Metal_service_overview/#associated-projects","title":"Associated projects","text":"<p>Optionally, one may wish to utilize the following associated projects for additional functionality:</p> <p>python-ironicclient</p> <p>A command-line interface (CLI) and python bindings for interacting with the Bare Metal service.</p> <p>ironic-ui</p> <p>Horizon dashboard, providing graphical interface (GUI) for the Bare Metal API.</p> <p>ironic-inspector</p> <p>An associated service which performs in-band hardware introspection by PXE booting unregistered hardware into the ironic-python-agent ramdisk.</p> <p>diskimage-builder</p> <p>A related project to help facilitate the creation of ramdisks and machine images, such as those running the ironic-python-agent.</p> <p>bifrost</p> <p>A set of Ansible playbooks that automates the task of deploying a base image onto a set of known hardware using ironic in a standalone mode.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Drivers%2C_Hardware_Types_and_Hardware_Interfaces/","title":"Generic Interfaces","text":"<ul> <li> <p>Boot interfaces</p> </li> <li> <p>PXE boot</p> </li> <li> <p>Common options</p> </li> <li> <p>Deploy Interfaces</p> </li> <li> <p>Direct deploy</p> </li> <li> <p>Ansible deploy</p> </li> <li> <p>Anaconda deploy</p> </li> <li> <p>Ramdisk deploy</p> </li> <li> <p>Custom agent deploy</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Drivers%2C_Hardware_Types_and_Hardware_Interfaces/#hardware-types","title":"Hardware Types","text":"<ul> <li> <p>iBMC driver</p> </li> <li> <p>iDRAC driver</p> </li> <li> <p>iLO driver</p> </li> <li> <p>Intel IPMI driver</p> </li> <li> <p>IPMI driver</p> </li> <li> <p>iRMC driver</p> </li> <li> <p>Redfish driver</p> </li> <li> <p>SNMP driver</p> </li> <li> <p>XClarity driver</p> </li> <li> <p>Fake driver</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Drivers%2C_Hardware_Types_and_Hardware_Interfaces/#changing-hardware-types-and-interfaces","title":"Changing Hardware Types and Interfaces","text":"<p>Hardware types and interfaces are enabled in the configuration as described in\u00a0Enabling drivers and hardware types. Usually, a hardware type is configured on enrolling as described in\u00a0Enrollment:</p> <p><code>baremetal node create --driver &lt;hardware type&gt;</code></p> <p>Any hardware interfaces can be specified on enrollment as well:</p> <p>baremetal node create --driver \\ <code>\\ --deploy-interface direct --&lt;other&gt;-interface &lt;other implementation&gt;</code> <p>For the remaining interfaces the default value is assigned as described in\u00a0Defaults for hardware interfaces. Both the hardware type and the hardware interfaces can be changed later via the node update API.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Drivers%2C_Hardware_Types_and_Hardware_Interfaces/#changing-hardware-interfaces","title":"Changing Hardware Interfaces","text":"<p>Hardware interfaces can be changed by the following command:</p> <p>baremetal node set \\ \\     --deploy-interface direct \\ <code>--&lt;other&gt;-interface &lt;other implementation&gt;</code> <p>The modified interfaces must be enabled and compatible with the current node\u2019s hardware type.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Drivers%2C_Hardware_Types_and_Hardware_Interfaces/#changing-hardware-type","title":"Changing Hardware Type","text":"<p>Changing the node\u2019s hardware type can pose a problem. When the\u00a0<code>driver</code>\u00a0field is updated, the final result must be consistent, that is, the resulting hardware interfaces must be compatible with the new hardware type. This will not work:</p> <pre><code>baremetal node create --name test --driver fake-hardware\nbaremetal node set test --driver ipmi\n</code></pre> <p>This is because the\u00a0<code>fake-hardware</code>\u00a0hardware type defaults to\u00a0<code>fake</code>\u00a0implementations for some or all interfaces, but the\u00a0<code>ipmi</code>\u00a0hardware type is not compatible with them. There are three ways to deal with this situation:</p> <p>1. Provide new values for all incompatible interfaces, for example:</p> <pre><code>baremetal node set test --driver ipmi \\\n    --boot-interface pxe \\\n    --deploy-interface direct \\\n    --management-interface ipmitool \\\n    --power-interface ipmitool\n</code></pre> <p>2. Request resetting some of the interfaces to their new defaults by using the\u00a0<code>--reset-&lt;IFACE&gt;-interface</code>\u00a0family of arguments, for example:</p> <pre><code>baremetal node set test --driver ipmi \\\n    --reset-boot-interface \\\n    --reset-deploy-interface \\\n    --reset-management-interface \\\n    --reset-power-interface\n</code></pre> <p>Note This feature is available starting with ironic 11.1.0 (Rocky series, API version 1.45).</p> <p>3. Request resetting all interfaces to their new defaults:</p> <pre><code>baremetal node set test --driver ipmi --reset-interfaces\n</code></pre> <p>You can still specify explicit values for some interfaces:</p> <pre><code>baremetal node set test --driver ipmi --reset-interfaces \\\n    --deploy-interface direct\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Drivers%2C_Hardware_Types_and_Hardware_Interfaces/#static-boot-order-configuration","title":"Static boot order configuration","text":"<p>Some hardware is known to misbehave when changing the boot device through the BMC. To work around it you can use the\u00a0<code>noop</code>\u00a0management interface implementation with the\u00a0<code>ipmi</code>\u00a0and\u00a0<code>redfish</code>\u00a0hardware types. In this case the Bare Metal service will not change the boot device for you, leaving the pre-configured boot order.</p> <p>For example, in case of the\u00a0PXE boot:</p> <ol> <li> <p>Via any available means configure the boot order on the node as follows:</p> </li> <li> <p>Boot from PXE/iPXE on the provisioning NIC.</p> <ul> <li>If it is not possible to limit network boot to only provisioning NIC, make sure that no other DHCP/PXE servers are accessible by the node.</li> </ul> </li> <li> <p>Boot from hard drive.</p> </li> <li> <p>Make sure the\u00a0<code>noop</code>\u00a0management interface is enabled, for example:</p> </li> </ol> <pre><code>[DEFAULT]\nenabled_hardware_types = ipmi,redfish\nenabled_management_interfaces = ipmitool,redfish,noop\n</code></pre> <p>3. Change the node to use the\u00a0<code>noop</code>\u00a0management interface:</p> <pre><code>baremetal node set &lt;NODE&gt; --management-interface noop\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Drivers%2C_Hardware_Types_and_Hardware_Interfaces/#unsupported-drivers","title":"Unsupported drivers","text":"<p>The following drivers were declared as unsupported in ironic Newton release and as of Ocata release they are removed from ironic:</p> <ul> <li> <p>AMT driver \u2013 available as part of\u00a0ironic-staging-drivers</p> </li> <li> <p>iBoot driver \u2013 available as part of\u00a0ironic-staging-drivers</p> </li> <li> <p>Wake-On-Lan driver \u2013 available as part of\u00a0ironic-staging-drivers</p> </li> <li> <p>Virtualbox drivers</p> </li> <li> <p>SeaMicro drivers</p> </li> <li> <p>MSFT OCS drivers</p> </li> </ul> <p>The SSH drivers were removed in the Pike release. Similar functionality can be achieved either with\u00a0VirtualBMC\u00a0or using libvirt drivers from\u00a0ironic-staging-drivers.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/","title":"Enrollment of Hardware","text":"<p>After all the services have been properly configured, you should enroll your hardware with the Bare Metal service, and confirm that the Compute service sees the available hardware. The nodes will be visible to the Compute service once they are in the\u00a0<code>available</code>\u00a0provision state.</p> <p>Note After enrolling nodes with the Bare Metal service, the Compute service will not be immediately notified of the new resources. The Compute service\u2019s resource tracker syncs periodically, and so any changes made directly to the Bare Metal service\u2019s resources will become visible in the Compute service only after the next run of that periodic task. More information is in the\u00a0Troubleshooting\u00a0section.</p> <p>Note Any bare metal node that is visible to the Compute service may have a workload scheduled to it, if both the\u00a0<code>power</code>\u00a0and\u00a0<code>management</code>\u00a0interfaces pass the\u00a0<code>validate</code>\u00a0check. If you wish to exclude a node from the Compute service\u2019s scheduler, for instance so that you can perform maintenance on it, you can set the node to \u201cmaintenance\u201d mode. For more information see the\u00a0Maintenance mode\u00a0section.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#choosing-a-driver","title":"Choosing a driver","text":"<p>When enrolling a node, the most important information to supply is\u00a0driver. See\u00a0Enabling drivers and hardware types\u00a0for a detailed explanation of bare metal drivers, hardware types and interfaces. The\u00a0<code>driver\u00a0list</code>\u00a0command can be used to list all drivers enabled on all hosts:</p> <pre><code>baremetal driver list\n+---------------------+-----------------------+\n| Supported driver(s) | Active host(s)        |\n+---------------------+-----------------------+\n| ipmi                | localhost.localdomain |\n+---------------------+-----------------------+\n</code></pre> <p>The specific driver to use should be picked based on actual hardware capabilities and expected features. See\u00a0Drivers, Hardware Types and Hardware Interfaces\u00a0for more hints on that.</p> <p>Each driver has a list of\u00a0driver properties\u00a0that need to be specified via the node\u2019s\u00a0<code>driver_info</code>\u00a0field, in order for the driver to operate on node. This list consists of the properties of the hardware interfaces that the driver uses. These driver properties are available with the\u00a0<code>driver\u00a0property\u00a0list</code>\u00a0command:</p> <pre><code>$ baremetal driver property list ipmi\n+----------------------+-------------------------------------------------------------------------------------------------------------+\n| Property             | Description                                                                                                 |\n+----------------------+-------------------------------------------------------------------------------------------------------------+\n| ipmi_address         | IP address or hostname of the node. Required.                                                               |\n| ipmi_password        | password. Optional.                                                                                         |\n| ipmi_username        | username; default is NULL user. Optional.                                                                   |\n| ...                  | ...                                                                                                         |\n| deploy_kernel        | UUID (from Glance) of the deployment kernel. Required.                                                      |\n| deploy_ramdisk       | UUID (from Glance) of the ramdisk that is mounted at boot time. Required.                                   |\n+----------------------+-------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>The properties marked as required must be supplied either during node creation or shortly after. Some properties may only be required for certain features.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#note-on-api-versions","title":"Note on API versions","text":"<p>Starting with API version 1.11, the Bare Metal service added a new initial provision state of\u00a0<code>enroll</code>\u00a0to its state machine. When this or later API version is used, new nodes get this state instead of\u00a0<code>available</code>.</p> <p>Existing automation tooling that use an API version lower than 1.11 are not affected, since the initial provision state is still\u00a0<code>available</code>. However, using API version 1.11 or above may break existing automation tooling with respect to node creation.</p> <p>The default API version used by (the most recent) python-ironicclient is 1.9, but it may change in the future and should not be relied on.</p> <p>In the examples below we will use version 1.11 of the Bare metal API. This gives us the following advantages:</p> <ul> <li> <p>Explicit power credentials validation before leaving the\u00a0<code>enroll</code>\u00a0state.</p> </li> <li> <p>Running node cleaning before entering the\u00a0<code>available</code>\u00a0state.</p> </li> <li> <p>Not exposing half-configured nodes to the scheduler.</p> </li> </ul> <p>To set the API version for all commands, you can set the environment variable\u00a0<code>IRONIC_API_VERSION</code>. For the OpenStackClient baremetal plugin, set the\u00a0<code>OS_BAREMETAL_API_VERSION</code>\u00a0variable to the same value. For example:</p> <pre><code>$ export IRONIC_API_VERSION=1.11\n$ export OS_BAREMETAL_API_VERSION=1.11\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#enrollment-process","title":"Enrollment process","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#creating-a-node","title":"Creating a node","text":"<p>This section describes the main steps to enroll a node and make it available for provisioning. Some steps are shown separately for illustration purposes, and may be combined if desired.</p> <p>1. Create a node in the Bare Metal service with the\u00a0<code>node\u00a0create</code>\u00a0command. At a minimum, you must specify the driver name (for example,\u00a0<code>ipmi</code>).This command returns the node UUID along with other information about the node. The node\u2019s provision state will be\u00a0<code>enroll</code>:</p> <pre><code>$ export OS_BAREMETAL_API_VERSION=1.11\n$ baremetal node create --driver ipmi\n+--------------+--------------------------------------+\n| Property     | Value                                |\n+--------------+--------------------------------------+\n| uuid         | dfc6189f-ad83-4261-9bda-b27258eb1987 |\n| driver_info  | {}                                   |\n| extra        | {}                                   |\n| driver       | ipmi                                 |\n| chassis_uuid |                                      |\n| properties   | {}                                   |\n| name         | None                                 |\n+--------------+--------------------------------------+\n\n$ baremetal node show dfc6189f-ad83-4261-9bda-b27258eb1987\n+------------------------+--------------------------------------+\n| Property               | Value                                |\n+------------------------+--------------------------------------+\n| target_power_state     | None                                 |\n| extra                  | {}                                   |\n| last_error             | None                                 |\n| maintenance_reason     | None                                 |\n| provision_state        | enroll                               |\n| uuid                   | dfc6189f-ad83-4261-9bda-b27258eb1987 |\n| console_enabled        | False                                |\n| target_provision_state | None                                 |\n| provision_updated_at   | None                                 |\n| maintenance            | False                                |\n| power_state            | None                                 |\n| driver                 | ipmi                                 |\n| properties             | {}                                   |\n| instance_uuid          | None                                 |\n| name                   | None                                 |\n| driver_info            | {}                                   |\n| ...                    | ...                                  |\n+------------------------+--------------------------------------+\n</code></pre> <p>A node may also be referred to by a logical name as well as its UUID. A name can be assigned to the node during its creation by adding the\u00a0<code>-n</code>\u00a0option to the\u00a0<code>node\u00a0create</code>\u00a0command or by updating an existing node with the\u00a0<code>node\u00a0set</code>\u00a0command. See\u00a0Logical Names\u00a0for examples.</p> <p>2. Starting with API version 1.31 (and\u00a0<code>python-ironicclient</code>\u00a01.13), you can pick which hardware interface to use with nodes that use hardware types. Each interface is represented by a node field called\u00a0<code>&lt;IFACE&gt;_interface</code>\u00a0where\u00a0<code>&lt;IFACE&gt;</code>\u00a0in the interface type, e.g.\u00a0<code>boot</code>. See\u00a0Enabling drivers and hardware types\u00a0for details on hardware interfaces.An interface can be set either separately:</p> <pre><code>$ baremetal node set $NODE_UUID --deploy-interface direct --raid-interface agent\n</code></pre> <p>Or set during node creation:</p> <pre><code>$ baremetal node create --driver ipmi \\\n    --deploy-interface direct \\\n    --raid-interface agent\n</code></pre> <p>If no value is provided for some interfaces,\u00a0Defaults for hardware interfaces\u00a0are used instead.</p> <p>3. Update the node\u00a0<code>driver_info</code>\u00a0with the required driver properties, so that the Bare Metal service can manage the node:</p> <pre><code>$ baremetal node set $NODE_UUID \\\n    --driver-info ipmi_username=$USER \\\n    --driver-info ipmi_password=$PASS \\\n    --driver-info ipmi_address=$ADDRESS\n</code></pre> <p>Note If IPMI is running on a port other than 623 (the default). The port must be added to\u00a0<code>driver_info</code>\u00a0by specifying the\u00a0<code>ipmi_port</code>\u00a0value. Example:</p> <pre><code>$ baremetal node set $NODE_UUID --driver-info ipmi_port=$PORT_NUMBER\n</code></pre> <p>You may also specify all\u00a0<code>driver_info</code>\u00a0parameters during node creation by passing the\u00a0\u2013driver-info\u00a0option multiple times:</p> <pre><code>$ baremetal node create --driver ipmi \\\n    --driver-info ipmi_username=$USER \\\n    --driver-info ipmi_password=$PASS \\\n    --driver-info ipmi_address=$ADDRESS\n</code></pre> <p>See\u00a0Choosing a driver\u00a0above for details on driver properties.</p> <p>4. Specify a deploy kernel and ramdisk compatible with the node\u2019s driver, for example:</p> <pre><code>$ baremetal node set $NODE_UUID \\\n    --driver-info deploy_kernel=$DEPLOY_VMLINUZ_UUID \\\n    --driver-info deploy_ramdisk=$DEPLOY_INITRD_UUID\n</code></pre> <p>See\u00a0Add images to the Image service\u00a0for details.</p> <p>5. Optionally you can specify the provisioning and/or cleaning network UUID or name in the node\u2019s\u00a0<code>driver_info</code>. The\u00a0<code>neutron</code>\u00a0network interface requires both\u00a0<code>provisioning_network</code>\u00a0and\u00a0<code>cleaning_network</code>, while the\u00a0<code>flat</code>\u00a0network interface requires the\u00a0<code>cleaning_network</code>\u00a0to be set either in the configuration or on the nodes. For example:</p> <pre><code>$ baremetal node set $NODE_UUID \\\n    --driver-info cleaning_network=$CLEAN_UUID_OR_NAME \\\n    --driver-info provisioning_network=$PROVISION_UUID_OR_NAME\n</code></pre> <p>See\u00a0Configure tenant networks\u00a0for details.</p> <p>6. You must also inform the Bare Metal service of the network interface cards which are part of the node by creating a port with each NIC\u2019s MAC address. These MAC addresses are passed to the Networking service during instance provisioning and used to configure the network appropriately:</p> <pre><code>$ baremetal port create $MAC_ADDRESS --node $NODE_UUID\n</code></pre> <p>Note When it is time to remove the node from the Bare Metal service, the command used to remove the port is\u00a0<code>baremetal\u00a0port\u00a0delete\u00a0&lt;port\u00a0uuid&gt;</code>. When doing so, it is important to ensure that the baremetal node is not in\u00a0<code>maintenance</code>\u00a0as guarding logic to prevent orphaning Neutron Virtual Interfaces (VIFs) will be overridden.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#adding-scheduling-information","title":"Adding scheduling information","text":"<p>1. Assign a\u00a0resource class\u00a0to the node. A\u00a0resource class\u00a0should represent a class of hardware in your data center, that corresponds to a Compute flavor.For example, let\u2019s split hardware into these three groups: We can define three resource classes to reflect these hardware groups, named\u00a0<code>large-cpu</code>,\u00a0<code>large-gpu</code>\u00a0and\u00a0<code>small</code>\u00a0respectively. Then, for each node in each of the hardware groups, we\u2019ll set their\u00a0<code>resource_class</code>\u00a0appropriately via:</p> <p>1. nodes with a lot of RAM and powerful CPU for computational tasks,nodes with powerful GPU for OpenCL computing,smaller nodes for development and testing.</p> <pre><code>$ baremetal node set $NODE_UUID --resource-class $CLASS_NAME\n</code></pre> <p>The\u00a0<code>--resource-class</code>\u00a0argument can also be used when creating a node:</p> <pre><code>$ baremetal node create --driver $DRIVER --resource-class $CLASS_NAME\n</code></pre> <p>To use resource classes for scheduling you need to update your flavors as described in\u00a0Create flavors for use with the Bare Metal service.</p> <p>Note This is not required for standalone deployments, only for those using the Compute service for provisioning bare metal instances.</p> <p>2. Update the node\u2019s properties to match the actual hardware of the node:</p> <pre><code>$ baremetal node set $NODE_UUID \\\n    --property cpus=$CPU_COUNT \\\n    --property memory_mb=$RAM_MB \\\n    --property local_gb=$DISK_GB\n</code></pre> <p>As above, these can also be specified at node creation by passing the\u00a0\u2013property\u00a0option to\u00a0<code>node\u00a0create</code>\u00a0multiple times:</p> <pre><code>$ baremetal node create --driver ipmi \\\n    --driver-info ipmi_username=$USER \\\n    --driver-info ipmi_password=$PASS \\\n    --driver-info ipmi_address=$ADDRESS \\\n    --property cpus=$CPU_COUNT \\\n    --property memory_mb=$RAM_MB \\\n    --property local_gb=$DISK_GB\n</code></pre> <p>These values can also be discovered during\u00a0Hardware Inspection.</p> <p>Warning The value provided for the\u00a0<code>local_gb</code>\u00a0property must match the size of the root device you\u2019re going to deploy on. By default\u00a0ironic-python-agent\u00a0picks the smallest disk which is not smaller than 4 GiB.If you override this logic by using root device hints (see\u00a0Specifying the disk for deployment (root device hints)), the\u00a0<code>local_gb</code>\u00a0value should match the size of picked target disk.</p> <p>3. If you wish to perform more advanced scheduling of the instances based on hardware capabilities, you may add metadata to each node that will be exposed to the Compute scheduler (see:\u00a0ComputeCapabilitiesFilter). A full explanation of this is outside of the scope of this document. It can be done through the special\u00a0<code>capabilities</code>\u00a0member of node properties:</p> <pre><code>$ baremetal node set $NODE_UUID \\\n    --property capabilities=key1:val1,key2:val2\n</code></pre> <p>4. If you wish to perform advanced scheduling of instances based on qualitative attributes of bare metal nodes, you may add traits to each bare metal node that will be exposed to the Compute scheduler (see:\u00a0Scheduling based on traits\u00a0for a more in-depth discussion of traits in the Bare Metal service). For example, to add the standard trait\u00a0<code>HW_CPU_X86_VMX</code>\u00a0and a custom trait\u00a0<code>CUSTOM_TRAIT1</code>\u00a0to a node:</p> <pre><code>$ baremetal node add trait $NODE_UUID \\\n    CUSTOM_TRAIT1 HW_CPU_X86_VMX\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#validating-node-information","title":"Validating node information","text":"<ol> <li>To check if Bare Metal service has the minimum information necessary for a node\u2019s driver to be functional, you may\u00a0<code>validate</code>\u00a0it:</li> </ol> <pre><code>$ baremetal node validate $NODE_UUID\n+------------+--------+--------+\n| Interface  | Result | Reason |\n+------------+--------+--------+\n| boot       | True   |        |\n| console    | True   |        |\n| deploy     | True   |        |\n| inspect    | True   |        |\n| management | True   |        |\n| network    | True   |        |\n| power      | True   |        |\n| raid       | True   |        |\n| storage    | True   |        |\n+------------+--------+--------+\n</code></pre> <p>If the node fails validation, each driver interface will return information as to why it failed:</p> <pre><code>$ baremetal node validate $NODE_UUID\n+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------+\n| Interface  | Result | Reason                                                                                                                              |\n+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------+\n| boot       | True   |                                                                                                                                     |\n| console    | None   | not supported                                                                                                                       |\n| deploy     | False  | Cannot validate iSCSI deploy. Some parameters were missing in node's instance_info. Missing are: ['root_gb', 'image_source']        |\n| inspect    | True   |                                                                                                                                     |\n| management | False  | Missing the following IPMI credentials in node's driver_info: ['ipmi_address'].                                                     |\n| network    | True   |                                                                                                                                     |\n| power      | False  | Missing the following IPMI credentials in node's driver_info: ['ipmi_address'].                                                     |\n| raid       | None   | not supported                                                                                                                       |\n| storage    | True   |                                                                                                                                     |\n+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>When using the Compute Service with the Bare Metal service, it is safe to ignore the deploy interface\u2019s validation error due to lack of image information. You may continue the enrollment process. This information will be set by the Compute Service just before deploying, when an instance is requested:</p> <pre><code>$ baremetal node validate $NODE_UUID\n+------------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Interface  | Result | Reason                                                                                                                                                           |\n+------------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| boot       | False  | Cannot validate image information for node because one or more parameters are missing from its instance_info. Missing are: ['ramdisk', 'kernel', 'image_source'] |\n| console    | True   |                                                                                                                                                                  |\n| deploy     | False  | Cannot validate image information for node because one or more parameters are missing from its instance_info. Missing are: ['ramdisk', 'kernel', 'image_source'] |\n| inspect    | True   |                                                                                                                                                                  |\n| management | True   |                                                                                                                                                                  |\n| network    | True   |                                                                                                                                                                  |\n| power      | True   |                                                                                                                                                                  |\n| raid       | None   | not supported                                                                                                                                                    |\n| storage    | True   |                                                                                                                                                                  |\n+------------+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#making-node-available-for-deployment","title":"Making node available for deployment","text":"<p>In order for nodes to be available for deploying workloads on them, nodes must be in the\u00a0<code>available</code>\u00a0provision state. To do this, nodes created with API version 1.11 and above must be moved from the\u00a0<code>enroll</code>\u00a0state to the\u00a0<code>manageable</code>\u00a0state and then to the\u00a0<code>available</code>\u00a0state. This section can be safely skipped, if API version 1.10 or earlier is used (which is the case by default).</p> <p>After creating a node and before moving it from its initial provision state of\u00a0<code>enroll</code>, basic power and port information needs to be configured on the node. The Bare Metal service needs this information because it verifies that it is capable of controlling the node when transitioning the node from\u00a0<code>enroll</code>\u00a0to\u00a0<code>manageable</code>\u00a0state.</p> <p>To move a node from\u00a0<code>enroll</code>\u00a0to\u00a0<code>manageable</code>\u00a0provision state:</p> <pre><code>$ baremetal node manage $NODE_UUID\n$ baremetal node show $NODE_UUID\n+------------------------+--------------------------------------------------------------------+\n| Property               | Value                                                              |\n+------------------------+--------------------------------------------------------------------+\n| ...                    | ...                                                                |\n| provision_state        | manageable                                                         | &lt;- verify correct state\n| uuid                   | 0eb013bb-1e4b-4f4c-94b5-2e7468242611                               |\n| ...                    | ...                                                                |\n+------------------------+--------------------------------------------------------------------+\n</code></pre> <p>Note Since it is an asynchronous call, the response for\u00a0<code>baremetal\u00a0node\u00a0manage</code>\u00a0will not indicate whether the transition succeeded or not. You can check the status of the operation via\u00a0<code>baremetal\u00a0node\u00a0show</code>. If it was successful,\u00a0<code>provision_state</code>\u00a0will be in the desired state. If it failed, there will be information in the node\u2019s\u00a0<code>last_error</code>.</p> <p>When a node is moved from the\u00a0<code>manageable</code>\u00a0to\u00a0<code>available</code>\u00a0provision state, the node will go through automated cleaning if configured to do so (see\u00a0Configure the Bare Metal service for cleaning).</p> <p>To move a node from\u00a0<code>manageable</code>\u00a0to\u00a0<code>available</code>\u00a0provision state:</p> <pre><code>$ baremetal node provide $NODE_UUID\n$ baremetal node show $NODE_UUID\n+------------------------+--------------------------------------------------------------------+\n| Property               | Value                                                              |\n+------------------------+--------------------------------------------------------------------+\n| ...                    | ...                                                                |\n| provision_state        | available                                                          | &lt; - verify correct state\n| uuid                   | 0eb013bb-1e4b-4f4c-94b5-2e7468242611                               |\n| ...                    | ...                                                                |\n+------------------------+--------------------------------------------------------------------+\n</code></pre> <p>For more details on the Bare Metal service\u2019s state machine, see the\u00a0Bare Metal State Machine\u00a0documentation.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#mapping-nodes-to-compute-cells","title":"Mapping nodes to Compute cells","text":"<p>If the Compute service is used for scheduling, and the\u00a0<code>discover_hosts_in_cells_interval</code>\u00a0was not set as described in\u00a0Configure the Compute service to use the Bare Metal service, then log into any controller node and run the following command to map the new node(s) to Compute cells:</p> <pre><code>nova-manage cell_v2 discover_hosts\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#logical-names","title":"Logical names","text":"<p>A node may also be referred to by a logical name as well as its UUID. Names can be assigned either during its creation by adding the\u00a0<code>-n</code>\u00a0option to the\u00a0<code>node\u00a0create</code>\u00a0command or by updating an existing node with the\u00a0<code>node\u00a0set</code>\u00a0command.</p> <p>Node names must be unique, and conform to:</p> <ul> <li> <p>rfc952</p> </li> <li> <p>rfc1123</p> </li> <li> <p>wiki_hostname</p> </li> </ul> <p>The node is named \u2018example\u2019 in the following examples:</p> <pre><code>`$ baremetal node create --driver ipmi --name example`\n</code></pre> <p>or</p> <pre><code>`$ baremetal node set $NODE_UUID --name example`\n</code></pre> <p>Once assigned a logical name, a node can then be referred to by name or UUID interchangeably:</p> <pre><code>$ baremetal node create --driver ipmi --name example\n+--------------+--------------------------------------+\n| Property     | Value                                |\n+--------------+--------------------------------------+\n| uuid         | 71e01002-8662-434d-aafd-f068f69bb85e |\n| driver_info  | {}                                   |\n| extra        | {}                                   |\n| driver       | ipmi                                 |\n| chassis_uuid |                                      |\n| properties   | {}                                   |\n| name         | example                              |\n+--------------+--------------------------------------+\n\n$ baremetal node show example\n+------------------------+--------------------------------------+\n| Property               | Value                                |\n+------------------------+--------------------------------------+\n| target_power_state     | None                                 |\n| extra                  | {}                                   |\n| last_error             | None                                 |\n| updated_at             | 2015-04-24T16:23:46+00:00            |\n| ...                    | ...                                  |\n| instance_info          | {}                                   |\n+------------------------+--------------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#defaults-for-hardware-interfaces","title":"Defaults for hardware interfaces","text":"<p>For\u00a0hardware types, users can request one of enabled implementations when creating or updating a node as explained in\u00a0Creating a node.</p> <p>When no value is provided for a certain interface when creating a node, or changing a node\u2019s hardware type, the default value is used. You can use the driver details command to list the current enabled and default interfaces for a hardware type (for your deployment):</p> <pre><code>$ baremetal driver show ipmi\n+-------------------------------+----------------+\n| Field                         | Value          |\n+-------------------------------+----------------+\n| default_boot_interface        | pxe            |\n| default_console_interface     | no-console     |\n| default_deploy_interface      | direct         |\n| default_inspect_interface     | no-inspect     |\n| default_management_interface  | ipmitool       |\n| default_network_interface     | flat           |\n| default_power_interface       | ipmitool       |\n| default_raid_interface        | no-raid        |\n| default_vendor_interface      | no-vendor      |\n| enabled_boot_interfaces       | pxe            |\n| enabled_console_interfaces    | no-console     |\n| enabled_deploy_interfaces     | direct         |\n| enabled_inspect_interfaces    | no-inspect     |\n| enabled_management_interfaces | ipmitool       |\n| enabled_network_interfaces    | flat, noop     |\n| enabled_power_interfaces      | ipmitool       |\n| enabled_raid_interfaces       | no-raid, agent |\n| enabled_vendor_interfaces     | no-vendor      |\n| hosts                         | ironic-host-1  |\n| name                          | ipmi           |\n| type                          | dynamic        |\n+-------------------------------+----------------+\n</code></pre> <p>The defaults are calculated as follows:</p> <ol> <li> <p>If the\u00a0<code>default_&lt;IFACE&gt;_interface</code>\u00a0configuration option (where\u00a0<code>&lt;IFACE&gt;</code>\u00a0is the interface name) is set, its value is used as the default.If this implementation is not compatible with the node\u2019s hardware type, an error is returned to a user. An explicit value has to be provided for the node\u2019s\u00a0<code>&lt;IFACE&gt;_interface</code>\u00a0field in this case.</p> </li> <li> <p>Otherwise, the first supported implementation that is enabled by an operator is used as the default.A list of supported implementations is calculated by taking the intersection between the implementations supported by the node\u2019s hardware type and implementations enabled by the\u00a0<code>enabled_&lt;IFACE&gt;_interfaces</code>\u00a0option (where\u00a0<code>&lt;IFACE&gt;</code>\u00a0is the interface name). The calculation preserves the order of items, as provided by the hardware type.If the list of supported implementations is not empty, the first one is used. Otherwise, an error is returned to a user. In this case, an explicit value has to be provided for the\u00a0<code>&lt;IFACE&gt;_interface</code>\u00a0field.</p> </li> </ol> <p>See\u00a0Enabling drivers and hardware types\u00a0for more details on configuration.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#example","title":"Example","text":"<p>Consider the following configuration (shortened for simplicity):</p> <pre><code>[DEFAULT]\nenabled_hardware_types = ipmi,redfish\nenabled_console_interfaces = no-console,ipmitool-shellinabox\nenabled_deploy_interfaces = direct\nenabled_management_interfaces = ipmitool,redfish\nenabled_power_interfaces = ipmitool,redfish\ndefault_deploy_interface = ansible\n</code></pre> <p>A new node is created with the\u00a0<code>ipmi</code>\u00a0driver and no interfaces specified:</p> <pre><code>$ export OS_BAREMETAL_API_VERSION=1.31\n$ baremetal node create --driver ipmi\n+--------------+--------------------------------------+\n| Property     | Value                                |\n+--------------+--------------------------------------+\n| uuid         | dfc6189f-ad83-4261-9bda-b27258eb1987 |\n| driver_info  | {}                                   |\n| extra        | {}                                   |\n| driver       | ipmi                                 |\n| chassis_uuid |                                      |\n| properties   | {}                                   |\n| name         | None                                 |\n+--------------+--------------------------------------+\n</code></pre> <p>Then the defaults for the interfaces that will be used by the node in this example are calculated as follows:</p> <p>deploy</p> <p>An explicit value of\u00a0<code>ansible</code>\u00a0is provided for\u00a0<code>default_deploy_interface</code>, so it is used.</p> <p>power</p> <p>No default is configured. The\u00a0<code>ipmi</code>\u00a0hardware type supports only\u00a0<code>ipmitool</code>\u00a0power. The intersection between supported power interfaces and values provided in the\u00a0<code>enabled_power_interfaces</code>\u00a0option has only one item:\u00a0<code>ipmitool</code>. It is used.</p> <p>console</p> <p>No default is configured. The\u00a0<code>ipmi</code>\u00a0hardware type supports the following console interfaces:\u00a0<code>ipmitool-socat</code>,\u00a0<code>ipmitool-shellinabox</code>\u00a0and\u00a0<code>no-console</code>\u00a0(in this order). Of these three, only two are enabled:\u00a0<code>no-console</code>\u00a0and\u00a0<code>ipmitool-shellinabox</code>\u00a0(order does not matter). The intersection contains\u00a0<code>ipmitool-shellinabox</code>\u00a0and\u00a0<code>no-console</code>. The first item is used, and it is\u00a0<code>ipmitool-shellinabox</code>.</p> <p>management</p> <p>Following the same calculation as\u00a0power, the\u00a0<code>ipmitool</code>\u00a0management interface is used.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#hardware-inspection","title":"Hardware Inspection","text":"<p>The Bare Metal service supports hardware inspection that simplifies enrolling nodes \u2013 please see\u00a0Hardware Inspection\u00a0for details.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Enrollment_of_Hardware/#tenant-networks-and-port-groups","title":"Tenant Networks and Port Groups","text":"<p>See\u00a0Multi-tenancy in the Bare Metal service\u00a0and\u00a0Port groups support.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Hardware_Inspection/","title":"Overview","text":"<p>Inspection allows Bare Metal service to discover required node properties once required\u00a0<code>driver_info</code>\u00a0fields (for example, IPMI credentials) are set by an operator. Inspection will also create the Bare Metal service ports for the discovered ethernet MACs. Operators will have to manually delete the Bare Metal service ports for which physical media is not connected.</p> <p>There are two kinds of inspection supported by Bare Metal service:</p> <ol> <li> <p>Out-of-band inspection is currently implemented by several hardware types, including\u00a0<code>ilo</code>,\u00a0<code>idrac</code>\u00a0and\u00a0<code>irmc</code>.</p> </li> <li> <p>In-band inspection\u00a0by utilizing the\u00a0ironic-inspector\u00a0project.</p> </li> </ol> <p>The node should be in the\u00a0<code>manageable</code>\u00a0state before inspection is initiated. If it is in the\u00a0<code>enroll</code>\u00a0or\u00a0<code>available</code>\u00a0state, move it to\u00a0<code>manageable</code>\u00a0first:</p> <pre><code>baremetal node manage &lt;node_UUID&gt;\n</code></pre> <p>Then inspection can be initiated using the following command:</p> <pre><code>baremetal node inspect &lt;node_UUID&gt;\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Hardware_Inspection/#capabilities-discovery","title":"Capabilities discovery","text":"<p>This is an incomplete list of capabilities we want to discover during inspection. The exact support is hardware and hardware type specific though, the most complete list is provided by the iLO\u00a0Hardware Inspection Support.</p> <p><code>secure_boot</code>\u00a0(<code>true</code>\u00a0or\u00a0<code>false</code>)</p> <p>whether secure boot is supported for the node</p> <p><code>boot_mode</code>\u00a0(<code>bios</code>\u00a0or\u00a0<code>uefi</code>)</p> <p>the boot mode the node is using</p> <p><code>cpu_vt</code>\u00a0(<code>true</code>\u00a0or\u00a0<code>false</code>)</p> <p>whether the CPU virtualization is enabled</p> <p><code>cpu_aes</code>\u00a0(<code>true</code>\u00a0or\u00a0<code>false</code>)</p> <p>whether the AES CPU extensions are enabled</p> <p><code>max_raid_level</code>\u00a0(integer, 0-10)</p> <p>maximum RAID level supported by the node</p> <p><code>pci_gpu_devices</code>\u00a0(non-negative integer)</p> <p>number of GPU devices on the node</p> <p>The operator can specify these capabilities in nova flavor for node to be selected for scheduling:</p> <pre><code>openstack flavor set my-baremetal-flavor --property capabilities:pci_gpu_devices=\"&gt; 0\"\n\nopenstack flavor set my-baremetal-flavor --property capabilities:secure_boot=\"true\"\n</code></pre> <p>Please see a specific\u00a0hardware type page\u00a0for the exact list of capabilities this hardware type can discover.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Hardware_Inspection/#in-band-inspection","title":"In-band inspection","text":"<p>In-band inspection involves booting a ramdisk on the target node and fetching information directly from it. This process is more fragile and time-consuming than the out-of-band inspection, but it is not vendor-specific and works across a wide range of hardware. In-band inspection is using the\u00a0ironic-inspector\u00a0project.</p> <p>It is supported by all hardware types, and used by default, if enabled, by the\u00a0<code>ipmi</code>\u00a0hardware type. The\u00a0<code>inspector</code> inspect\u00a0interface has to be enabled to use it:</p> <pre><code>[DEFAULT]\nenabled_inspect_interfaces = inspector,no-inspect\n</code></pre> <p>If the ironic-inspector service is not registered in the service catalog, set the following option:</p> <pre><code>[inspector]\nendpoint_override = `[`http://inspector.example.com:5050`](http://inspector.example.com:5050)\n</code></pre> <p>In order to ensure that ports in Bare Metal service are synchronized with NIC ports on the node, the following settings in the ironic-inspector configuration file must be set:</p> <pre><code>[processing]\nadd_ports = all\nkeep_ports = present\n</code></pre> <p>There are two modes of in-band inspection:\u00a0managed inspection\u00a0and\u00a0unmanaged inspection.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Hardware_Inspection/#managed-inspection","title":"Managed inspection","text":"<p>Inspection is\u00a0managed\u00a0when the Bare Metal conductor fully configures the node for inspection, including setting boot device, boot mode and power state. This is the only way to conduct inspection using\u00a0Virtual media boot\u00a0or with\u00a0Layer 3 or DHCP-less ramdisk booting. This mode is engaged automatically when the node has sufficient information to configure boot (e.g. ports in case of iPXE).</p> <p>There are a few configuration options that tune managed inspection, the most important is\u00a0<code>extra_kernel_params</code>, which allows adding kernel parameters for inspection specifically. This is where you can configure\u00a0inspection collectors and other parameters, for example:</p> <pre><code>[inspector]\nextra_kernel_params = ipa-inspection-collectors=default,logs ipa-collect-lldp=1\n</code></pre> <p>For the callback URL the ironic-inspector endpoint from the service catalog is used. If you want to override the endpoint for callback only, set the following option:</p> <pre><code>[inspector]\ncallback_endpoint_override = `[`https://example.com/baremetal-introspection/v1/continue`](https://example.com/baremetal-introspection/v1/continue)\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Hardware_Inspection/#unmanaged-inspection","title":"Unmanaged inspection","text":"<p>Under\u00a0unmanaged\u00a0inspection we understand in-band inspection orchestrated by ironic-inspector or a third party. This was the only inspection mode before the Ussuri release, and it is still used when the node\u2019s boot cannot be configured by the conductor. The options described above do not affect unmanaged inspection. See\u00a0ironic-inspector installation guide\u00a0for more information.</p> <p>If you want to\u00a0prevent\u00a0unmanaged inspection from working, set this option:</p> <pre><code>[inspector]\nrequire_managed_boot = True\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/High_Availability_and_Scalability/","title":"HA and Scalability","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/High_Availability_and_Scalability/#ironic-api","title":"ironic-api","text":"<p>The Bare Metal API service is stateless, and thus can be easily scaled horizontally. It is recommended to deploy it as a WSGI application behind e.g. Apache or another WSGI container.</p> <p>Note This service accesses the ironic database for reading entities (e.g. in response to\u00a0<code>GET\u00a0/v1/nodes</code>\u00a0request) and in rare cases for writing.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/High_Availability_and_Scalability/#ironic-conductor","title":"ironic-conductor","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/High_Availability_and_Scalability/#high-availability","title":"High availability","text":"<p>The Bare Metal conductor service utilizes the active/active HA model. Every conductor manages a certain subset of nodes. The nodes are organized in a hash ring that tries to keep the load spread more or less uniformly across the conductors. When a conductor is considered offline, its nodes are taken over by other conductors. As a result of this, you need at least 2 conductor hosts for an HA deployment.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/High_Availability_and_Scalability/#performance","title":"Performance","text":"<p>Conductors can be resource intensive, so it is recommended (but not required) to keep all conductors separate from other services in the cloud. The minimum required number of conductors in a deployment depends on several factors:</p> <ul> <li> <p>the performance of the hardware where the conductors will be running,</p> </li> <li> <p>the speed and reliability of the\u00a0management controller\u00a0of the bare metal nodes (for example, handling slower controllers may require having less nodes per conductor),</p> </li> <li> <p>the frequency, at which the management controllers are polled by the Bare Metal service (see the\u00a0<code>sync_power_state_interval</code>\u00a0option),</p> </li> <li> <p>the bare metal driver used for nodes (see\u00a0Hardware and drivers\u00a0above),</p> </li> <li> <p>the network performance,</p> </li> <li> <p>the maximum number of bare metal nodes that are provisioned simultaneously (see the\u00a0<code>max_concurrent_builds</code>\u00a0option for the Compute service).</p> </li> </ul> <p>We recommend a target of\u00a0100\u00a0bare metal nodes per conductor for maximum reliability and performance. There is some tolerance for a larger number per conductor. However, it was reported\u00a0[1] [2]\u00a0that reliability degrades when handling approximately 300 bare metal nodes per conductor.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/High_Availability_and_Scalability/#disk-space","title":"Disk space","text":"<p>Each conductor needs enough free disk space to cache images it uses. Depending on the combination of the deploy interface and the boot option, the space requirements are different:</p> <ul> <li> <p>The deployment kernel and ramdisk are always cached during the deployment.</p> </li> <li> <p>When\u00a0<code>[agent]image_download_source</code>\u00a0is set to\u00a0<code>http</code>\u00a0and Glance is used, the conductor will download instances images locally to serve them from its HTTP server. Use\u00a0<code>swift</code>\u00a0to publish images using temporary URLs and convert them on the node\u2019s side.When\u00a0<code>[agent]image_download_source</code>\u00a0is set to\u00a0<code>local</code>, it will happen even for HTTP(s) URLs. For standalone case use\u00a0<code>http</code>\u00a0to avoid unnecessary caching of images.In both cases a cached image is converted to raw if\u00a0<code>force_raw_images</code>\u00a0is\u00a0<code>True</code>\u00a0(the default).See\u00a0Deploy with custom HTTP servers\u00a0and\u00a0Streaming raw images\u00a0for more details.</p> </li> <li> <p>When network boot is used, the instance image kernel and ramdisk are cached locally while the instance is active.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ironic/","title":"Ironic","text":"<p>The Bare Metal service, codenamed\u00a0<code>ironic</code>, is a collection of components that provides support to manage and provision physical machines.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ironic/#bare-metal-service-components","title":"Bare Metal service components","text":"<p>The Bare Metal service includes the following components:</p> <p>ironic-api</p> <p>A RESTful API that processes application requests by sending them to the ironic-conductor over\u00a0remote procedure call (RPC). Can be run through\u00a0WSGI\u00a0or as a separate process.</p> <p>ironic-conductor</p> <p>Adds/edits/deletes nodes; powers on/off nodes with IPMI or other vendor-specific protocol; provisions/deploys/cleans bare metal nodes.</p> <p>ironic-conductor uses\u00a0drivers\u00a0to execute operations on hardware.</p> <p>ironic-python-agent</p> <p>A python service which is run in a temporary ramdisk to provide ironic-conductor and ironic-inspector services with remote access, in-band hardware control, and hardware introspection.</p> <p>Additionally, the Bare Metal service has certain external dependencies, which are very similar to other OpenStack services:</p> <ul> <li> <p>A database to store hardware information and state. You can set the database back-end type and location. A simple approach is to use the same database back end as the Compute service. Another approach is to use a separate database back-end to further isolate bare metal resources (and associated metadata) from users.</p> </li> <li> <p>An\u00a0oslo.messaging\u00a0compatible queue, such as RabbitMQ. It may use the same implementation as that of the Compute service, but that is not a requirement. Used to implement RPC between ironic-api and ironic-conductor.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ironic/#deployment-architecture","title":"Deployment architecture","text":"<p>The Bare Metal RESTful API service is used to enroll hardware that the Bare Metal service will manage. A cloud administrator usually registers it, specifying their attributes such as MAC addresses and IPMI credentials. There can be multiple instances of the API service.</p> <p>The\u00a0ironic-conductor\u00a0process does the bulk of the work. For security reasons, it is advisable to place it on an isolated host, since it is the only service that requires access to both the data plane and IPMI control plane.</p> <p>There can be multiple instances of the conductor service to support various class of drivers and also to manage fail over. Instances of the conductor service should be on separate nodes. Each conductor can itself run many drivers to operate heterogeneous hardware. This is depicted in the following figure.</p> <p></p> <p>The API exposes a list of supported drivers and the names of conductor hosts servicing them.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ironic/#interaction-with-openstack-components","title":"Interaction with OpenStack components","text":"<p>The Bare Metal service may, depending upon configuration, interact with several other OpenStack services. This includes:</p> <ul> <li> <p>the OpenStack Telemetry module (<code>ceilometer</code>) for consuming the IPMI metrics</p> </li> <li> <p>the OpenStack Identity service (<code>keystone</code>) for request authentication and to locate other OpenStack services</p> </li> <li> <p>the OpenStack Image service (<code>glance</code>) from which to retrieve images and image meta-data</p> </li> <li> <p>the OpenStack Networking service (<code>neutron</code>) for DHCP and network configuration</p> </li> <li> <p>the OpenStack Compute service (<code>nova</code>) works with the Bare Metal service and acts as a user-facing API for instance management, while the Bare Metal service provides the admin/operator API for hardware management. The OpenStack Compute service also provides scheduling facilities (matching flavors \\&lt;-&gt; images \\&lt;-&gt; hardware), tenant quotas, IP assignment, and other services which the Bare Metal service does not, in and of itself, provide.</p> </li> <li> <p>the OpenStack Object Storage (<code>swift</code>) provides temporary storage for the configdrive, user images, deployment logs and inspection data.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ironic/#logical-architecture","title":"Logical architecture","text":"<p>The diagram below shows the logical architecture. It shows the basic components that form the Bare Metal service, the relation of the Bare Metal service with other OpenStack services and the logical flow of a boot instance request resulting in the provisioning of a physical server.</p> <p></p> <p>A user\u2019s request to boot an instance is passed to the Compute service via the Compute API and the Compute Scheduler. The Compute service uses the\u00a0ironic virt driver\u00a0to hand over this request to the Bare Metal service, where the request passes from the Bare Metal API, to the Conductor, to a Driver to successfully provision a physical server for the user.</p> <p>Just as the Compute service talks to various OpenStack services like Image, Network, Object Store etc to provision a virtual machine instance, here the Bare Metal service talks to the same OpenStack services for image, network and other resource needs to provision a bare metal instance.</p> <p>See\u00a0Understanding Bare Metal Deployment\u00a0for a more detailed breakdown of a typical deployment process.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ironic_Console/","title":"Overview","text":"<p>There are two types of console which are available in Bare Metal service, one is web console (Node web console) which is available directly from web browser, another is serial console (Node serial console).</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ironic_Console/#node-web-console","title":"Node web console","text":"<p>The web console can be configured in Bare Metal service in the following way:</p> <ul> <li>Install shellinabox in ironic conductor node. For RHEL/CentOS, shellinabox package is not present in base repositories, user must enable EPEL repository, you can find more from\u00a0FedoraProject page.</li> </ul> <p>Installation example:</p> <p>Ubuntu:</p> <pre><code>sudo apt-get install shellinabox\n</code></pre> <p>RHEL8/CentOS8/Fedora:</p> <pre><code>sudo dnf install shellinabox\n</code></pre> <p>You can find more about shellinabox on the\u00a0shellinabox page.You can optionally use the SSL certificate in shellinabox. If you want to use the SSL certificate in shellinabox, you should install openssl and generate the SSL certificate.</p> <p>1. Install openssl, for example:</p> <pre><code>#Ubuntu\nsudo apt-get install openssl\n\n#RHEL8/CentOS8/Fedora\nsudo dnf install openssl\n</code></pre> <p>2. Generate the SSL certificate, here is an example, you can find more about openssl on the\u00a0openssl page:</p> <pre><code>cd /tmp/ca\nopenssl genrsa -des3 -out my.key 1024\nopenssl req -new -key my.key  -out my.csr\ncp my.key my.key.org\nopenssl rsa -in my.key.org -out my.key\nopenssl x509 -req -days 3650 -in my.csr -signkey my.key -out my.crt\ncat my.crt my.key &gt; certificate.pem\n</code></pre> <ul> <li>Customize the console section in the Bare Metal service configuration file (/etc/ironic/ironic.conf), if you want to use SSL certificate in shellinabox, you should specify\u00a0<code>terminal_cert_dir</code>. for example:</li> </ul> <pre><code>[console]\n\n#\n# Options defined in ironic.drivers.modules.console_utils\n#\n\n# Path to serial console terminal program. Used only by Shell\n# In A Box console. (string value)\n#terminal=shellinaboxd\n\n# Directory containing the terminal SSL cert (PEM) for serial\n# console access. Used only by Shell In A Box console. (string\n# value)\nterminal_cert_dir=/tmp/ca\n\n# Directory for holding terminal pid files. If not specified,\n# the temporary directory will be used. (string value)\n#terminal_pid_dir=&lt;None&gt;\n\n# Time interval (in seconds) for checking the status of\n# console subprocess. (integer value)\n#subprocess_checking_interval=1\n\n# Time (in seconds) to wait for the console subprocess to\n# start. (integer value)\n#subprocess_timeout=10\n</code></pre> <ul> <li> <p>Append console parameters for bare metal PXE boot in the Bare Metal service configuration file (/etc/ironic/ironic.conf). See the reference for configuration in\u00a0Appending kernel parameters to boot instances.</p> </li> <li> <p>Enable the\u00a0<code>ipmitool-shellinabox</code>\u00a0console interface, for example:</p> </li> </ul> <pre><code>[DEFAULT]\nenabled_console_interfaces = ipmitool-shellinabox,no-console\n</code></pre> <ul> <li>Configure node web console.If the node uses a hardware type, for example\u00a0<code>ipmi</code>, set the node\u2019s console interface to\u00a0<code>ipmitool-shellinabox</code>:</li> </ul> <pre><code>baremetal node set &lt;node&gt; --console-interface ipmitool-shellinabox\n</code></pre> <p>Enable the web console, for example:</p> <pre><code>baremetal node set &lt;node&gt; \\\n    --driver-info &lt;terminal_port&gt;=&lt;customized_port&gt;\nbaremetal node console enable &lt;node&gt;\n</code></pre> <p>Check whether the console is enabled, for example:</p> <pre><code>baremetal node validate &lt;node&gt;\n</code></pre> <p>Disable the web console, for example:</p> <pre><code>baremetal node console disable  &lt;node&gt;\nbaremetal node unset &lt;node&gt; --driver-info &lt;terminal_port&gt;\n</code></pre> <p>The\u00a0<code>&lt;terminal_port&gt;</code>\u00a0is driver dependent. The actual name of this field can be checked in driver properties, for example:</p> <pre><code>baremetal driver property list &lt;driver&gt;\n</code></pre> <p>For the\u00a0<code>ipmi</code>\u00a0hardware type, this option is\u00a0<code>ipmi_terminal_port</code>. Give a customized port number to\u00a0<code>&lt;customized_port&gt;</code>, for example\u00a0<code>8023</code>, this customized port is used in web console url.</p> <p>Get web console information for a node as follows:</p> <pre><code>baremetal node console show &lt;node&gt;\n+-----------------+----------------------------------------------------------------------+\n| Property        | Value                                                                |\n+-----------------+----------------------------------------------------------------------+\n| console_enabled | True                                                                 |\n| console_info    | {u'url': u'http://&lt;url&gt;:&lt;customized_port&gt;', u'type': u'shellinabox'} |\n+-----------------+----------------------------------------------------------------------+\n</code></pre> <p>You can open web console using above\u00a0<code>url</code>\u00a0through web browser.</p> <p>If\u00a0<code>console_enabled</code>\u00a0is\u00a0<code>false</code>,\u00a0<code>console_info</code>\u00a0is\u00a0<code>None</code>, web console is disabled. If you want to launch web console, see the\u00a0<code>Configure\u00a0node\u00a0web\u00a0console</code>\u00a0part.</p> <p>Note An error message you may encounter when enabling the console can read\u00a0<code>Console\u00a0subprocess\u00a0failed\u00a0to\u00a0start.\u00a0Timeout\u00a0or\u00a0error\u00a0while\u00a0waiting\u00a0for\u00a0console\u00a0subprocess\u00a0to\u00a0start\u00a0for\u00a0node</code>\u00a0along with\u00a0<code>[server]\u00a0Failed\u00a0to\u00a0find\u00a0any\u00a0available\u00a0port!</code>. This error is coming from shellinabox itself, not from the communication with the BMC. One potential cause for this issue is that there are already shellinabox daemons running which block the configured port (remove them if appropriate and retry to enable the console).</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ironic_Console/#node-serial-console","title":"Node serial console","text":"<p>Serial consoles for nodes are implemented using\u00a0socat. It is supported by the\u00a0<code>ipmi</code>\u00a0and\u00a0<code>irmc</code>\u00a0hardware types.</p> <p>Serial consoles can be configured in the Bare Metal service as follows:</p> <ul> <li>Install socat on the ironic conductor node. Also,\u00a0<code>socat</code>\u00a0needs to be in the $PATH environment variable that the ironic-conductor service uses.Installation example:</li> </ul> <pre><code>#Ubuntu\nsudo apt-get install socat\n\n#RHEL8/CentOS8/Fedora\nsudo dnf install socat\n</code></pre> <ul> <li> <p>Append console parameters for bare metal PXE boot in the Bare Metal service configuration file. See the reference on how to configure them in\u00a0Appending kernel parameters to boot instances.</p> </li> <li> <p>Enable the\u00a0<code>ipmitool-socat</code>\u00a0console interface, for example:</p> </li> </ul> <pre><code>[DEFAULT]\nenabled_console_interfaces = ipmitool-socat,no-console\n</code></pre> <ul> <li>Configure node console.If the node uses a hardware type, for example\u00a0<code>ipmi</code>, set the node\u2019s console interface to\u00a0<code>ipmitool-socat</code>:</li> </ul> <pre><code>baremetal node set &lt;node&gt; --console-interface ipmitool-socat\n</code></pre> <p>Enable the serial console, for example:</p> <pre><code>baremetal node set &lt;node&gt; --driver-info ipmi_terminal_port=&lt;port&gt;\nbaremetal node console enable &lt;node&gt;\n</code></pre> <p>Check whether the serial console is enabled, for example:</p> <pre><code>baremetal node validate &lt;node&gt;\n</code></pre> <p>Disable the serial console, for example:</p> <pre><code>baremetal node console disable  &lt;node&gt;\nbaremetal node unset &lt;node&gt; --driver-info &lt;ipmi_terminal_port&gt;\n</code></pre> <p>Serial console information is available from the Bare Metal service. Get serial console information for a node from the Bare Metal service as follows:</p> <pre><code>baremetal node console show &lt;node&gt;\n+-----------------+----------------------------------------------------------------------+\n| Property        | Value                                                                |\n+-----------------+----------------------------------------------------------------------+\n| console_enabled | True                                                                 |\n| console_info    | {u'url': u'tcp://&lt;host&gt;:&lt;port&gt;', u'type': u'socat'}                  |\n+-----------------+----------------------------------------------------------------------+\n</code></pre> <p>If\u00a0<code>console_enabled</code>\u00a0is\u00a0<code>false</code>\u00a0or\u00a0<code>console_info</code>\u00a0is\u00a0<code>None</code>\u00a0then the serial console is disabled. If you want to launch serial console, see the\u00a0<code>Configure\u00a0node\u00a0console</code>.</p> <p>Node serial console of the Bare Metal service is compatible with the serial console of the Compute service. Hence, serial consoles to Bare Metal nodes can be seen and interacted with via the Dashboard service. In order to achieve that, you need to follow the documentation for\u00a0Serial Console\u00a0from the Compute service.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/Ironic_Console/#configuring-ha","title":"Configuring HA","text":"<p>When using Bare Metal serial console under High Availability (HA) configuration, you may consider some settings below.</p> <ul> <li>If you use HAProxy, you may need to set the timeout for both client and server sides with appropriate values. Here is an example of the configuration for the timeout parameter.</li> </ul> <pre><code>frontend nova_serial_console\n  bind 192.168.20.30:6083\n  timeout client 10m  # This parameter is necessary\n  use_backend nova_serial_console if &lt;...&gt;\n\nbackend nova_serial_console\n  balance source\n  timeout server 10m  # This parameter is necessary\n  option  tcpka\n  option  tcplog\n  server  controller01 192.168.30.11:6083 check inter 2000 rise 2 fall 5\n  server  controller02 192.168.30.12:6083 check inter 2000 rise 2 fall 5\n</code></pre> <ul> <li>The Compute service\u2019s caching feature may need to be enabled in order to make the Bare Metal serial console work under a HA configuration. Here is an example of caching configuration in\u00a0<code>nova.conf</code>.</li> </ul> <pre><code>[cache]\nenabled = true\nbackend = dogpile.cache.memcached\nmemcache_servers = memcache01:11211,memcache02:11211,memcache03:11211\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/","title":"Overview","text":"<p>The integrated Dell Remote Access Controller (iDRAC) is an out-of-band management platform on Dell EMC servers, and is supported directly by the\u00a0<code>idrac</code>\u00a0hardware type. This driver uses the Dell Web Services for Management (WSMAN) protocol and the standard Distributed Management Task Force (DMTF) Redfish protocol to perform all of its functions.</p> <p>iDRAC\u00a0hardware is also supported by the generic\u00a0<code>ipmi</code>\u00a0and\u00a0<code>redfish</code>\u00a0hardware types, though with smaller feature sets.</p> <p>Key features of the Dell iDRAC driver include:</p> <ul> <li> <p>Out-of-band node inspection</p> </li> <li> <p>Boot device management and firmware management</p> </li> <li> <p>Power management</p> </li> <li> <p>RAID controller management and RAID volume configuration</p> </li> <li> <p>BIOS settings configuration</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#ironic-features","title":"Ironic Features","text":"<p>The\u00a0<code>idrac</code>\u00a0hardware type supports the following Ironic interfaces:</p> <ul> <li> <p>BIOS Interface: BIOS management</p> </li> <li> <p>Inspect Interface: Hardware inspection</p> </li> <li> <p>Management Interface: Boot device and firmware management</p> </li> <li> <p>Power Interface: Power management</p> </li> <li> <p>RAID Interface: RAID controller and disk management</p> </li> <li> <p>Vendor Interface: BIOS management (WSMAN) and eject virtual media (Redfish)</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#prerequisites","title":"Prerequisites","text":"<p>The\u00a0<code>idrac</code>\u00a0hardware type requires the\u00a0<code>python-dracclient</code>\u00a0library to be installed on the ironic conductor node(s) if an Ironic node is configured to use an\u00a0<code>idrac-wsman</code>\u00a0interface implementation, for example:</p> <pre><code>sudo pip install 'python-dracclient&gt;=3.1.0'\n</code></pre> <p>Additionally, the\u00a0<code>idrac</code>\u00a0hardware type requires the\u00a0<code>sushy</code>\u00a0library to be installed on the ironic conductor node(s) if an Ironic node is configured to use an\u00a0<code>idrac-redfish</code>\u00a0interface implementation, for example:</p> <pre><code>sudo pip install 'python-dracclient&gt;=3.1.0' 'sushy&gt;=2.0.0'\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#enabling","title":"Enabling","text":"<p>The iDRAC driver supports WSMAN for the bios, inspect, management, power, raid, and vendor interfaces. In addition, it supports Redfish for the bios, inspect, management, power, and raid interfaces. The iDRAC driver allows you to mix and match WSMAN and Redfish interfaces.</p> <p>The\u00a0<code>idrac-wsman</code>\u00a0implementation must be enabled to use WSMAN for an interface. The\u00a0<code>idrac-redfish</code>\u00a0implementation must be enabled to use Redfish for an interface.</p> <p>To enable the\u00a0<code>idrac</code>\u00a0hardware type with the minimum interfaces, all using WSMAN, add the following to your\u00a0<code>/etc/ironic/ironic.conf</code>:</p> <pre><code>[DEFAULT]\nenabled_hardware_types=idrac\nenabled_management_interfaces=idrac-wsman\nenabled_power_interfaces=idrac-wsman\n</code></pre> <p>To enable all optional features (BIOS, inspection, RAID, and vendor passthru) using Redfish where it is supported and WSMAN where not, use the following configuration:</p> <pre><code>[DEFAULT]\nenabled_hardware_types=idrac\nenabled_bios_interfaces=idrac-redfish\nenabled_inspect_interfaces=idrac-redfish\nenabled_management_interfaces=idrac-redfish\nenabled_power_interfaces=idrac-redfish\nenabled_raid_interfaces=idrac-redfish\nenabled_vendor_interfaces=idrac-redfish\n</code></pre> Interface Supported Implementations bios idrac-wsman, idrac-redfish, no-bios boot ipxe, pxe, idrac-redfish-virtual-media console no-console deploy direct, ansible, ramdisk inspect idrac-wsman, idrac, idrac-redfish, inspector, no-inspect management idrac-wsman, idrac, idrac-redfish network flat, neutron, noop power idrac-wsman, idrac-redfish raid idrac-wsman, idrac-redfish, no-raid rescue no-rescue, agent storage noop, cinder, external vendor idrac-wsman, idrac-redfish, no-vendor <p>Note <code>idrac</code>\u00a0is the legacy name of the WSMAN interface. It has been deprecated in favor of\u00a0<code>idrac-wsman</code>\u00a0and may be removed in a future release.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#protocol-specific-properties","title":"Protocol-specific Properties","text":"<p>The WSMAN and Redfish protocols require different properties to be specified in the Ironic node\u2019s\u00a0<code>driver_info</code>\u00a0field to communicate with the bare metal system\u2019s iDRAC.</p> <p>The WSMAN protocol requires the following properties:</p> <ul> <li> <p><code>drac_username</code>: The WSMAN user name to use when communicating with the iDRAC. Usually\u00a0<code>root</code>.</p> </li> <li> <p><code>drac_password</code>: The password for the WSMAN user to use when communicating with the iDRAC.</p> </li> <li> <p><code>drac_address</code>: The IP address of the iDRAC.</p> </li> </ul> <p>The Redfish protocol requires the following properties:</p> <ul> <li> <p><code>redfish_username</code>: The Redfish user name to use when communicating with the iDRAC. Usually\u00a0<code>root</code>.</p> </li> <li> <p><code>redfish_password</code>: The password for the Redfish user to use when communicating with the iDRAC.</p> </li> <li> <p><code>redfish_address</code>: The URL address of the iDRAC. It must include the authority portion of the URL, and can optionally include the scheme. If the scheme is missing, https is assumed.</p> </li> <li> <p><code>redfish_system_id</code>: The Redfish ID of the server to be managed. This should always be:\u00a0<code>/redfish/v1/Systems/System.Embedded.1</code>.</p> </li> </ul> <p>For other Redfish protocol parameters see\u00a0Redfish driver.</p> <p>If using only interfaces which use WSMAN (<code>idrac-wsman</code>), then only the WSMAN properties must be supplied. If using only interfaces which use Redfish (<code>idrac-redfish</code>), then only the Redfish properties must be supplied. If using a mix of interfaces, where some use WSMAN and others use Redfish, both the WSMAN and Redfish properties must be supplied.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#enrolling","title":"Enrolling","text":"<p>The following command enrolls a bare metal node with the\u00a0<code>idrac</code>\u00a0hardware type using WSMAN for all interfaces:</p> <pre><code>baremetal node create --driver idrac \\\n    --driver-info drac_username=user \\\n    --driver-info drac_password=pa$$w0rd \\\n    --driver-info drac_address=drac.host\n</code></pre> <p>The following command enrolls a bare metal node with the\u00a0<code>idrac</code>\u00a0hardware type using Redfish for all interfaces:</p> <pre><code>baremetal node create --driver idrac \\\n    --driver-info redfish_username=user \\\n    --driver-info redfish_password=pa$$w0rd \\\n    --driver-info redfish_address=drac.host \\\n    --driver-info redfish_system_id=/redfish/v1/Systems/System.Embedded.1 \\\n    --bios-interface idrac-redfish \\\n    --inspect-interface idrac-redfish \\\n    --management-interface idrac-redfish \\\n    --power-interface idrac-redfish \\\n    --raid-interface idrac-redfish \\\n    --vendor-interface idrac-redfish\n</code></pre> <p>The following command enrolls a bare metal node with the\u00a0<code>idrac</code>\u00a0hardware type assuming a mix of Redfish and WSMAN interfaces are used:</p> <pre><code>baremetal node create --driver idrac \\\n    --driver-info drac_username=user \\\n    --driver-info drac_password=pa$$w0rd\n    --driver-info drac_address=drac.host \\\n    --driver-info redfish_username=user \\\n    --driver-info redfish_password=pa$$w0rd \\\n    --driver-info redfish_address=drac.host \\\n    --driver-info redfish_system_id=/redfish/v1/Systems/System.Embedded.1 \\\n    --bios-interface idrac-redfish \\\n    --inspect-interface idrac-redfish \\\n    --management-interface idrac-redfish \\\n    --power-interface idrac-redfish\n</code></pre> <p>Note If using WSMAN for the management interface, then WSMAN must be used for the power interface. The same applies to Redfish. It is currently not possible to use Redfish for one and WSMAN for the other.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#bios-interface","title":"BIOS Interface","text":"<p>The BIOS interface implementations supported by the\u00a0<code>idrac</code>\u00a0hardware type allows BIOS to be configured with the standard clean/deploy step approach.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#example","title":"Example","text":"<p>A clean step to enable\u00a0<code>Virtualization</code>\u00a0and\u00a0<code>SRIOV</code>\u00a0in BIOS of an iDRAC BMC would be as follows:</p> <pre><code>{\n  \"target\":\"clean\",\n  \"clean_steps\": [\n    {\n      \"interface\": \"bios\",\n      \"step\": \"apply_configuration\",\n      \"args\": {\n        \"settings\": [\n          {\n            \"name\": \"ProcVirtualization\",\n            \"value\": \"Enabled\"\n          },\n          {\n            \"name\": \"SriovGlobalEnable\",\n            \"value\": \"Enabled\"\n          }\n        ]\n      }\n    }\n  ]\n}\n</code></pre> <p>See the\u00a0Known Issues\u00a0for a known issue with\u00a0<code>factory_reset</code>\u00a0clean step. For additional details of BIOS configuration, see\u00a0BIOS Configuration.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#inspect-interface","title":"Inspect Interface","text":"<p>The Dell iDRAC out-of-band inspection process catalogs all the same attributes of the server as the IPMI driver. Unlike IPMI, it does this without requiring the system to be rebooted, or even to be powered on. Inspection is performed using the Dell WSMAN or Redfish protocol directly without affecting the operation of the system being inspected.</p> <p>The inspection discovers the following properties:</p> <ul> <li> <p><code>cpu_arch</code>: cpu architecture</p> </li> <li> <p><code>cpus</code>: number of cpus</p> </li> <li> <p><code>local_gb</code>: disk size in gigabytes</p> </li> <li> <p><code>memory_mb</code>: memory size in megabytes</p> </li> </ul> <p>Extra capabilities:</p> <ul> <li> <p><code>boot_mode</code>: UEFI or BIOS boot mode.</p> </li> <li> <p><code>pci_gpu_devices</code>: number of GPU devices connected to the bare metal.</p> </li> </ul> <p>It also creates baremetal ports for each NIC port detected in the system. The\u00a0<code>idrac-wsman</code>\u00a0inspect interface discovers which NIC ports are configured to PXE boot and sets\u00a0<code>pxe_enabled</code>\u00a0to\u00a0<code>True</code>\u00a0on those ports. The\u00a0<code>idrac-redfish</code>\u00a0inspect interface does not currently set\u00a0<code>pxe_enabled</code>\u00a0on the ports. The user should ensure that\u00a0<code>pxe_enabled</code>\u00a0is set correctly on the ports following inspection with the\u00a0<code>idrac-redfish</code>\u00a0inspect interface.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#management-interface","title":"Management Interface","text":"<p>The management interface for\u00a0<code>idrac-redfish</code>\u00a0supports:</p> <ul> <li> <p>updating firmware on nodes using a manual cleaning step. See\u00a0Redfish driver\u00a0for more information on firmware update support.</p> </li> <li> <p>updating system and getting its inventory using configuration molds. For more information see\u00a0Import and export configuration.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#import-and-export-configuration","title":"Import and export configuration","text":"<p>The clean and deploy steps provided in this section allow to configure the system and collect the system inventory using configuration mold files.</p> <p>The introduction of this feature in the Wallaby release is experimental.</p> <p>These steps are:</p> <ul> <li> <p><code>export_configuration</code>\u00a0with the\u00a0<code>export_configuration_location</code>\u00a0input parameter to export the configuration from the existing system.</p> </li> <li> <p><code>import_configuration</code>\u00a0with the\u00a0<code>import_configuration_location</code>\u00a0input parameter to import the existing configuration mold into the system.</p> </li> <li> <p><code>import_export_configuration</code>\u00a0with the\u00a0<code>export_configuration_location</code>\u00a0and\u00a0<code>import_configuration_location</code>\u00a0input parameters. This step combines the previous two steps into one step that first imports existing configuration mold into system, then exports the resulting configuration.</p> </li> </ul> <p>The input parameters provided include the URL where the configuration mold is to be stored after the export, or the reference location for an import. For more information on setting up storage and available options see\u00a0Storage setup.</p> <p>Configuration molds are JSON files that contain three top-level sections:\u00a0<code>bios</code>,\u00a0<code>raid</code>\u00a0and\u00a0<code>oem</code>. The following is an example of a configuration mold:</p> <pre><code>{\n  \"bios\": {\n    \"reset\": false,\n    \"settings\": [\n      {\n        \"name\": \"ProcVirtualization\",\n        \"value\": \"Enabled\"\n      },\n      {\n        \"name\": \"MemTest\",\n        \"value\": \"Disabled\"\n      }\n    ]\n  }\n  \"raid\": {\n    \"create_nonroot_volumes\": true,\n    \"create_root_volume\": true,\n    \"delete_existing\": false,\n    \"target_raid_config\": {\n      \"logical_disks\": [\n        {\n          \"size_gb\": 50,\n          \"raid_level\": \"1+0\",\n          \"controller\": \"RAID.Integrated.1-1\",\n          \"volume_name\": \"root_volume\",\n          \"is_root_volume\": true,\n          \"physical_disks\": [\n            \"Disk.Bay.0:Encl.Int.0-1:RAID.Integrated.1-1\",\n            \"Disk.Bay.1:Encl.Int.0-1:RAID.Integrated.1-1\"\n          ]\n        },\n        {\n          \"size_gb\": 100,\n          \"raid_level\": \"5\",\n          \"controller\": \"RAID.Integrated.1-1\",\n          \"volume_name\": \"data_volume\",\n          \"physical_disks\": [\n            \"Disk.Bay.2:Encl.Int.0-1:RAID.Integrated.1-1\",\n            \"Disk.Bay.3:Encl.Int.0-1:RAID.Integrated.1-1\",\n            \"Disk.Bay.4:Encl.Int.0-1:RAID.Integrated.1-1\"\n          ]\n        }\n      ]\n    }\n  }\n  \"oem\": {\n    \"interface\": \"idrac-redfish\",\n    \"data\": {\n      \"SystemConfiguration\": {\n        \"Model\": \"PowerEdge R640\",\n        \"ServiceTag\": \"8CY9Z99\",\n        \"TimeStamp\": \"Fri Jun 26 08:43:15 2020\",\n        \"Components\": [\n          {\n            [...]\n            \"FQDD\": \"NIC.Slot.1-1-1\",\n            \"Attributes\": [\n              {\n              \"Name\": \"BlnkLeds\",\n              \"Value\": \"15\",\n              \"Set On Import\": \"True\",\n              \"Comment\": \"Read and Write\"\n              },\n              {\n              \"Name\": \"VirtMacAddr\",\n              \"Value\": \"00:00:00:00:00:00\",\n              \"Set On Import\": \"False\",\n              \"Comment\": \"Read and Write\"\n              },\n              {\n              \"Name\": \"VirtualizationMode\",\n              \"Value\": \"NONE\",\n              \"Set On Import\": \"True\",\n              \"Comment\": \"Read and Write\"\n              },\n            [...]\n            ]\n          }\n        ]\n      }\n  }\n}\n</code></pre> <p>Currently, the OEM section is the only section that is supported. The OEM section uses the iDRAC Server Configuration Profile (SCP) and can be edited as necessary if it complies with the SCP. For more information about SCP and its capabilities, see\u00a0SCP_Reference_Guide.</p> <p>Note iDRAC BMC connection settings are not exported to avoid overwriting these in another system when using unmodified exported configuration mold in import step. If need to replicate iDRAC BMC connection settings, then add these settings manually to configuration mold for import step.</p> <p>To replicate the system configuration to that of a similar system, perform the following steps:</p> <ol> <li> <p>Configure a golden, or one to many, system.</p> </li> <li> <p>Use the\u00a0<code>export_configuration</code>\u00a0step to export the configuration to the wanted location.</p> </li> <li> <p>Adjust the exported configuration mold for other systems to replicate. For example, remove sections that do not need to be replicated such as iDRAC connection settings. The configuration mold can be accessed directly from the storage location.</p> </li> <li> <p>Import the selected configuration mold into the other systems using the\u00a0<code>import_configuration</code>\u00a0step.</p> </li> </ol> <p>It is not mandatory to use\u00a0<code>export_configuration</code>\u00a0step to create a configuration mold. Upload the file to a designated storage location without using Ironic if it has been created manually or by other means.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#storage-setup","title":"Storage setup","text":"<p>To start using these steps, configure the storage location. The settings can be found in the\u00a0<code>[molds]</code>\u00a0section. Configure the storage type from the\u00a0<code>[molds]storage</code>\u00a0setting. Currently,\u00a0<code>swift</code>, which is enabled by default, and\u00a0<code>http</code>\u00a0are supported.</p> <p>In the setup input parameters, the complete HTTP URL is used. This requires that the containers (for\u00a0<code>swift</code>) and the directories (for\u00a0<code>http</code>) are created beforehand, and that read/write access is configured accordingly.</p> <p>Note Use of TLS is strongly advised.</p> <p>This setup configuration allows a user to access these locations outside of Ironic to list, create, update, and delete the configuration molds.</p> <p>For more information see\u00a0Swift configuration\u00a0and\u00a0HTTP configuration.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#swift-configuration","title":"Swift configuration","text":"<p>To use Swift with configuration molds,</p> <ol> <li> <p>Create the containers to be used for configuration mold storage.</p> </li> <li> <p>For Ironic Swift user that is configured in the\u00a0<code>[swift]</code>\u00a0section add read/write access to these containers.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#http-configuration","title":"HTTP configuration","text":"<p>To use HTTP server with configuration molds,</p> <ol> <li> <p>Enable HTTP PUT support.</p> </li> <li> <p>Create the directory to be used for the configuration mold storage.</p> </li> <li> <p>Configure read/write access for HTTP Basic access authentication and provide user credentials in\u00a0<code>[molds]user</code>\u00a0and\u00a0<code>[molds]password</code>\u00a0fields.</p> </li> </ol> <p>The HTTP web server does not support multitenancy and is intended to be used in a stand-alone Ironic, or single-tenant OpenStack environment.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#raid-interface","title":"RAID Interface","text":"<p>See\u00a0RAID Configuration\u00a0for more information on Ironic RAID support.</p> <p>RAID interface of\u00a0<code>redfish</code>\u00a0hardware type can be used on iDRAC systems. Compared to\u00a0<code>redfish</code>\u00a0RAID interface, using\u00a0<code>idrac-redfish</code>\u00a0adds:</p> <ul> <li> <p>Waiting for real-time operations to be available on RAID controllers. When using\u00a0<code>redfish</code>\u00a0this is not guaranteed and reboots might be intermittently required to complete,</p> </li> <li> <p>Converting non-RAID disks to RAID mode if there are any,</p> </li> <li> <p>Clearing foreign configuration, if any, after deleting virtual disks.</p> </li> </ul> <p>The following properties are supported by the iDRAC WSMAN and Redfish RAID interface implementation:</p> <p>Note When using\u00a0<code>idrac-redfish</code>\u00a0for RAID interface iDRAC firmware greater than 4.40.00.00 is required.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#mandatory-properties","title":"Mandatory properties","text":"<ul> <li> <p><code>size_gb</code>: Size in gigabytes (integer) for the logical disk. Use\u00a0<code>MAX</code>\u00a0as\u00a0<code>size_gb</code>\u00a0if this logical disk is supposed to use the rest of the space available.</p> </li> <li> <p><code>raid_level</code>: RAID level for the logical disk. Valid values are\u00a0<code>0</code>,\u00a0<code>1</code>,\u00a0<code>5</code>,\u00a0<code>6</code>,\u00a0<code>1+0</code>,\u00a0<code>5+0</code>\u00a0and\u00a0<code>6+0</code>.</p> </li> </ul> <p>Note <code>JBOD</code>\u00a0and\u00a0<code>2</code>\u00a0are not supported, and will fail with reason: \u2018Cannot calculate spans for RAID level.\u2019</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#optional-properties","title":"Optional properties","text":"<ul> <li> <p><code>is_root_volume</code>: Optional. Specifies whether this disk is a root volume. By default, this is\u00a0<code>False</code>.</p> </li> <li> <p><code>volume_name</code>: Optional. Name of the volume to be created. If this is not specified, it will be auto-generated.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#backing-physical-disk-hints","title":"Backing physical disk hints","text":"<p>See\u00a0RAID Configuration\u00a0for more information on backing disk hints.</p> <p>These are machine-independent information. The hints are specified for each logical disk to help Ironic find the desired disks for RAID configuration.</p> <ul> <li> <p><code>disk_type</code></p> </li> <li> <p><code>interface_type</code></p> </li> <li> <p><code>share_physical_disks</code></p> </li> <li> <p><code>number_of_physical_disks</code></p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#backing-physical-disks","title":"Backing physical disks","text":"<p>These are Dell RAID controller-specific values and must match the names provided by the iDRAC.</p> <ul> <li> <p><code>controller</code>: Mandatory. The name of the controller to use.</p> </li> <li> <p><code>physical_disks</code>: Optional. The names of the physical disks to use.</p> </li> </ul> <p>Note <code>physical_disks</code>\u00a0is a mandatory parameter if the property\u00a0<code>size_gb</code>\u00a0is set to\u00a0<code>MAX</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#examples","title":"Examples","text":"<p>Creation of RAID\u00a0<code>1+0</code>\u00a0logical disk with six disks on one controller:</p> <pre><code>{ \"logical_disks\":\n  [ { \"controller\": \"RAID.Integrated.1-1\",\n      \"is_root_volume\": \"True\",\n      \"physical_disks\": [\n        \"Disk.Bay.0:Enclosure.Internal.0-1:RAID.Integrated.1-1\",\n        \"Disk.Bay.1:Enclosure.Internal.0-1:RAID.Integrated.1-1\",\n        \"Disk.Bay.2:Enclosure.Internal.0-1:RAID.Integrated.1-1\",\n        \"Disk.Bay.3:Enclosure.Internal.0-1:RAID.Integrated.1-1\",\n        \"Disk.Bay.4:Enclosure.Internal.0-1:RAID.Integrated.1-1\",\n        \"Disk.Bay.5:Enclosure.Internal.0-1:RAID.Integrated.1-1\"],\n      \"raid_level\": \"1+0\",\n      \"size_gb\": \"MAX\"}]}\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#manual-raid-invocation","title":"Manual RAID Invocation","text":"<p>The following command can be used to delete any existing RAID configuration. It deletes all virtual disks/RAID volumes, unassigns all global and dedicated hot spare physical disks, and clears foreign configuration:</p> <pre><code>baremetal node clean --clean-steps \\\n  '[{\"interface\": \"raid\", \"step\": \"delete_configuration\"}]' ${node_uuid}\n</code></pre> <p>The following command shows an example of how to set the target RAID configuration:</p> <pre><code>baremetal node set --target-raid-config '{ \"logical_disks\":\n  [ { \"controller\": \"RAID.Integrated.1-1\",\n      \"is_root_volume\": true,\n      \"physical_disks\": [\n        \"Disk.Bay.0:Enclosure.Internal.0-1:RAID.Integrated.1-1\",\n        \"Disk.Bay.1:Enclosure.Internal.0-1:RAID.Integrated.1-1\"],\n      \"raid_level\": \"0\",\n      \"size_gb\": \"MAX\"}]}' ${node_uuid}\n</code></pre> <p>The following command can be used to create a RAID configuration:</p> <pre><code>baremetal node clean --clean-steps \\\n  '[{\"interface\": \"raid\", \"step\": \"create_configuration\"}]' &lt;node&gt;\n</code></pre> <p>When the physical disk names or controller names are not known, the following Python code example shows how the\u00a0<code>python-dracclient</code>\u00a0can be used to fetch the information directly from the Dell bare metal:</p> <pre><code>import dracclient.client\n\n\nclient = dracclient.client.DRACClient(\n    host=\"192.168.1.1\",\n    username=\"root\",\n    password=\"calvin\")\ncontrollers = client.list_raid_controllers()\nprint(controllers)\n\nphysical_disks = client.list_physical_disks()\nprint(physical_disks)\n</code></pre> <p>Or using\u00a0<code>sushy</code>\u00a0with Redfish:</p> <pre><code>import sushy\n\nclient = sushy.Sushy('https://192.168.1.1', username='root', password='calvin', verify=False)\nfor s in client.get_system_collection().get_members():\n  print(\"System: %(id)s\" % {'id': s.identity})\n  for c in system1.storage.get_members():\n      print(\"\\tController: %(id)s\" % {'id': c.identity})\n      for d in c.drives:\n        print(\"\\t\\tDrive: %(id)s\" % {'id': d.identity})\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#vendor-interface","title":"Vendor Interface","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#idrac-wsman","title":"idrac-wsman","text":"<p>Dell iDRAC BIOS management is available through the Ironic WSMAN vendor passthru interface.</p> Method Name HTTP Method Description abandon_bios_config DELETE Abandon a BIOS configuration job. commit_bios_config POST Commit a BIOS configuration job submitted through\u00a0<code>set_bios_config</code>. Required argument:\u00a0<code>reboot</code>\u00a0\u2013 indicates whether a reboot job should be automatically created with the config job. Returns a dictionary containing the\u00a0<code>job_id</code>\u00a0key with the ID of the newly created config job, and the\u00a0<code>reboot_required</code>\u00a0key indicating whether the node needs to be rebooted to execute the config job. get_bios_config GET Returns a dictionary containing the node\u2019s BIOS settings. list_unfinished_jobs GET Returns a dictionary containing the key unfinished_jobs; its value is a list of dictionaries. Each dictionary represents an unfinished config job object. set_bios_config POST Change the BIOS configuration on a node. Required argument: a dictionary of {<code>AttributeName</code>:\u00a0<code>NewValue</code>}. Returns a dictionary containing the\u00a0<code>is_commit_required</code>\u00a0key indicating whether\u00a0<code>commit_bios_config</code>\u00a0needs to be called to apply the changes and the\u00a0<code>is_reboot_required</code>\u00a0value indicating whether the server must also be rebooted. Possible values are\u00a0<code>true</code>\u00a0and\u00a0<code>false</code>."},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#examples_1","title":"Examples","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#get-bios-config","title":"Get BIOS Config","text":"<pre><code>baremetal node passthru call --http-method GET &lt;node&gt; get_bios_config\n</code></pre> <p>Snippet of output showing virtualization enabled:</p> <pre><code>{\"ProcVirtualization\": {\n      \"current_value\": \"Enabled\",\n      \"instance_id\": \"BIOS.Setup.1-1:ProcVirtualization\",\n      \"name\": \"ProcVirtualization\",\n      \"pending_value\": null,\n      \"possible_values\": [\n          \"Enabled\",\n          \"Disabled\"],\n      \"read_only\": false }}\n</code></pre> <p>There are a number of items to note from the above snippet:</p> <ul> <li> <p><code>name</code>: this is the name to use in a call to\u00a0<code>set_bios_config</code>.</p> </li> <li> <p><code>current_value</code>: the current state of the setting.</p> </li> <li> <p><code>pending_value</code>: if the value has been set, but not yet committed, the new value is shown here. The change can either be committed or abandoned.</p> </li> <li> <p><code>possible_values</code>: shows a list of valid values which can be used in a call to\u00a0<code>set_bios_config</code>.</p> </li> <li> <p><code>read_only</code>: indicates if the value is capable of being changed.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#set-bios-config","title":"Set BIOS Config","text":"<pre><code>baremetal node passthru call &lt;node&gt; set_bios_config --arg \"name=value\"\n</code></pre> <p>Walkthrough of perfoming a BIOS configuration change:</p> <p>The following section demonstrates how to change BIOS configuration settings, detect that a commit and reboot are required, and act on them accordingly. The two properties that are being changed are:</p> <ul> <li> <p>Enable virtualization technology of the processor</p> </li> <li> <p>Globally enable SR-IOV</p> </li> </ul> <pre><code>baremetal node passthru call &lt;node&gt; set_bios_config \\\n  --arg \"ProcVirtualization=Enabled\" \\\n  --arg \"SriovGlobalEnable=Enabled\"\n</code></pre> <p>This returns a dictionary indicating what actions are required next:</p> <pre><code>{\n  \"is_reboot_required\": true,\n  \"is_commit_required\": true\n}\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#commit-bios-changes","title":"Commit BIOS Changes","text":"<p>The next step is to commit the pending change to the BIOS. Note that in this example, the\u00a0<code>reboot</code>\u00a0argument is set to\u00a0<code>true</code>. The response indicates that a reboot is no longer required as it has been scheduled automatically by the\u00a0<code>commit_bios_config</code>\u00a0call. If the reboot argument is not supplied, the job is still created, however it remains in the\u00a0<code>scheduled</code>\u00a0state until a reboot is performed. The reboot can be initiated through the Ironic power API.</p> <pre><code>baremetal node passthru call &lt;node&gt; commit_bios_config \\\n  --arg \"reboot=true\"\n</code></pre> <pre><code>{\n  \"job_id\": \"JID_499377293428\",\n  \"reboot_required\": false\n}\n</code></pre> <p>The state of any executing job can be queried:</p> <pre><code>baremetal node passthru call --http-method GET &lt;node&gt; list_unfinished_jobs\n</code></pre> <pre><code>{\"unfinished_jobs\":\n    [{\"status\": \"Scheduled\",\n      \"name\": \"ConfigBIOS:BIOS.Setup.1-1\",\n      \"until_time\": \"TIME_NA\",\n      \"start_time\": \"TIME_NOW\",\n      \"message\": \"Task successfully scheduled.\",\n      \"percent_complete\": \"0\",\n      \"id\": \"JID_499377293428\"}]}\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#abandon-bios-changes","title":"Abandon BIOS Changes","text":"<p>Instead of committing, a pending change can be abandoned:</p> <pre><code>baremetal node passthru call --http-method DELETE &lt;node&gt; abandon_bios_config\n</code></pre> <p>The abandon command does not provide a response body.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#change-boot-mode","title":"Change Boot Mode","text":"<p>The boot mode of the iDRAC can be changed to:</p> <ul> <li> <p>BIOS \u2013 Also called legacy or traditional boot mode. The BIOS initializes the system\u2019s processors, memory, bus controllers, and I/O devices. After initialization is complete, the BIOS passes control to operating system (OS) software. The OS loader uses basic services provided by the system BIOS to locate and load OS modules into system memory. After booting the system, the BIOS and embedded management controllers execute system management algorithms, which monitor and optimize the condition of the underlying hardware. BIOS configuration settings enable fine-tuning of the performance, power management, and reliability features of the system.</p> </li> <li> <p>UEFI \u2013 The Unified Extensible Firmware Interface does not change the traditional purposes of the system BIOS. To a large extent, a UEFI-compliant BIOS performs the same initialization, boot, configuration, and management tasks as a traditional BIOS. However, UEFI does change the interfaces and data structures the BIOS uses to interact with I/O device firmware and operating system software. The primary intent of UEFI is to eliminate shortcomings in the traditional BIOS environment, enabling system firmware to continue scaling with industry trends.</p> </li> </ul> <p>The UEFI boot mode offers:</p> <ul> <li> <p>Improved partitioning scheme for boot media</p> </li> <li> <p>Support for media larger than 2 TB</p> </li> <li> <p>Redundant partition tables</p> </li> <li> <p>Flexible handoff from BIOS to OS</p> </li> <li> <p>Consolidated firmware user interface</p> </li> <li> <p>Enhanced resource allocation for boot device firmware</p> </li> </ul> <p>The boot mode can be changed via the WSMAN vendor passthru interface as follows:</p> <pre><code>baremetal node passthru call &lt;node&gt; set_bios_config \\\n  --arg \"BootMode=Uefi\"\n\nbaremetal node passthru call &lt;node&gt; commit_bios_config \\\n  --arg \"reboot=true\"\n</code></pre> <pre><code>baremetal node passthru call &lt;node&gt; set_bios_config \\\n  --arg \"BootMode=Bios\"\n\nbaremetal node passthru call &lt;node&gt; commit_bios_config \\\n  --arg \"reboot=true\"\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Ironic/iDRAC_Driver/#idrac-redfish","title":"idrac-redfish","text":"<p>Through the\u00a0<code>idrac-redfish</code>\u00a0vendor passthru interface these methods are available:</p> Method Name HTTP Method Description eject_media POST Eject a virtual media device. If no device is provided, all attached devices will be ejected. Optional argument: <code>boot_device</code> \u2013 cd, dvd, usb, floppy."},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/","title":"Prerequisites","text":"<p>If you are not familiar with the idea of federated identity, see the\u00a0Introduction to Keystone Federation\u00a0first.</p> <p>In this section, we will configure keystone as a Service Provider, consuming identity properties issued by an external Identity Provider, such as SAML assertions or OpenID Connect claims. For testing purposes, we recommend using\u00a0samltest.id\u00a0as a SAML Identity Provider, or Google as an OpenID Connect Identity Provider, and the examples here will references those providers. If you plan to set up\u00a0Keystone as an Identity Provider (IdP), it is easiest to set up keystone with a dummy SAML provider first and then reconfigure it to point to the keystone Identity Provider later.</p> <p>The following configuration steps were performed on a machine running Ubuntu 16.04 and Apache 2.4.18.</p> <p>To enable federation, you\u2019ll need to run keystone behind a web server such as Apache rather than running the WSGI application directly with uWSGI or Gunicorn. See the installation guide for\u00a0SUSE,\u00a0RedHat\u00a0or\u00a0Ubuntu\u00a0to configure the Apache web server for keystone.</p> <p>Throughout the rest of the guide, you will need to decide on three pieces of information and use them consistently throughout your configuration:</p> <ol> <li> <p>The protocol name. This must be a valid keystone auth method and must match one of:\u00a0<code>saml2</code>,\u00a0<code>openid</code>,\u00a0<code>mapped</code>\u00a0or a\u00a0custom auth method\u00a0for which you must\u00a0register as an external driver.</p> </li> <li> <p>The identity provider name. This can be arbitrary.</p> </li> <li> <p>The entity ID of the service provider. This should be a URN but need not resolve to anything.</p> </li> </ol> <p>You will also need to decide what HTTPD module to use as a Service Provider. This guide provides examples for\u00a0<code>mod_shib</code>\u00a0and\u00a0<code>mod_auth_mellon</code>\u00a0as SAML service providers, and\u00a0<code>mod_auth_openidc</code>\u00a0as an OpenID Connect Service Provider.</p> <p>Note In this guide, the keystone Service Provider is configured on a host called sp.keystone.example.org listening on the standard HTTPS port. All keystone paths will start with the keystone version prefix,\u00a0<code>/v3</code>. If you have configured keystone to listen on port 5000, or to respond on the path\u00a0<code>/identity</code>\u00a0(for example), take this into account in your own configuration.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#creating-federation-resources-in-keystone","title":"Creating federation resources in keystone","text":"<p>You need to create three resources via the keystone API to identify the Identity Provider to keystone and align remote user attributes with keystone objects:</p> <ul> <li> <p>Create an Identity Provider</p> </li> <li> <p>Create a Mapping</p> </li> <li> <p>Create a Protocol</p> </li> </ul> <p>See also the\u00a0keystone federation API reference.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#create-an-identity-provider","title":"Create an Identity Provider","text":"<p>Create an Identity Provider object in keystone, which represents the Identity Provider we will use to authenticate end users:</p> <pre><code>$ openstack identity provider create --remote-id https://samltest.id/saml/idp samltest\n</code></pre> <p>The value for the\u00a0<code>remote-id</code>\u00a0option is the unique identifier provided by the Identity Provider, called the\u00a0entity ID\u00a0or the\u00a0remote ID. For a SAML Identity Provider, it can found by querying its metadata endpoint:</p> <pre><code>$ curl -s https://samltest.id/saml/idp | grep -o 'entityID=\".*\"'`\nentityID=\"https://samltest.id/saml/idp\"\n</code></pre> <p>For an OpenID Connect IdP, it is the Identity Provider\u2019s Issuer Identifier. A remote ID must be globally unique: two identity providers cannot be associated with the same remote ID. The remote ID will usually appear as a URN but need not be a resolvable URL.</p> <p>The local name, called\u00a0<code>samltest</code>\u00a0in our example, is decided by you and will be used by the mapping and protocol, and later for authentication.</p> <p>Note An identity provider keystone object may have multiple\u00a0<code>remote-ids</code>\u00a0specified, this allows the same\u00a0keystone\u00a0identity provider resource to be used with multiple external identity providers. For example, an identity provider resource\u00a0<code>university-idp</code>, may have the following\u00a0<code>remote_ids</code>:\u00a0<code>['university-x',\u00a0'university-y',\u00a0'university-z']</code>. This removes the need to configure N identity providers in keystone.</p> <p>See also the\u00a0API reference on identity providers.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#create-a-mapping","title":"Create a Mapping","text":"<p>Next, create a mapping. A mapping is a set of rules that link the attributes of a remote user to user properties that keystone understands. It is especially useful for granting remote users authorization to keystone resources, either by associating them with a local keystone group and inheriting its role assignments, or dynamically provisioning projects within keystone based on these rules.</p> <p>Note By default, group memberships that a user gets from a mapping are only valid for the duration of the token. It is possible to persist these groups memberships for a limited period of time. To enable this, either set the\u00a0<code>authorization_ttl`\u00a0attribute\u00a0of\u00a0the\u00a0identity\u00a0provider,\u00a0or\u00a0the\u00a0``[federation]\u00a0default_authorization_ttl</code>\u00a0in the keystone.conf file. This value is in minutes, and will result in a lag from when a user is removed from a group in the identity provider, and when that will happen in keystone. Please consider your security requirements carefully.</p> <p>An Identity Provider has exactly one mapping specified per protocol. Mapping objects can be used multiple times by different combinations of Identity Provider and Protocol.</p> <p>As a simple example, create a mapping with a single rule to map all remote users to a local user in a single group in keystone:</p> <pre><code>$ cat &gt; rules.json &lt;&lt;EOF\n[\n    {\n        \"local\": [\n            {\n                \"user\": {\n                    \"name\": \"{0}\"\n                },\n                \"group\": {\n                    \"domain\": {\n                        \"name\": \"Default\"\n                    },\n                    \"name\": \"federated_users\"\n                }\n            }\n        ],\n        \"remote\": [\n            {\n                \"type\": \"REMOTE_USER\"\n            }\n        ]\n    }\n]\nEOF\n$ openstack mapping create --rules rules.json samltest_mapping\n</code></pre> <p>This mapping rule evaluates the\u00a0<code>REMOTE_USER</code>\u00a0variable set by the HTTPD auth module and uses it to fill in the name of the local user in keystone. It also ensures all remote users become effective members of the\u00a0<code>federated_users</code>\u00a0group, thereby inheriting the group\u2019s role assignments.</p> <p>In this example, the\u00a0<code>federated_users</code>\u00a0group must exist in the keystone Identity backend and must have a role assignment on some project, domain, or system in order for federated users to have an authorization in keystone. For example, to create the group:</p> <pre><code>$ openstack group create federated_users\n</code></pre> <p>Create a project these users should be assigned to:</p> <pre><code>$ openstack project create federated_project\n</code></pre> <p>Assign the group a\u00a0<code>member</code>\u00a0role in the project:</p> <pre><code>$ openstack role add --group federated_users --project federated_project member\n</code></pre> <p>Mappings can be quite complex. A detailed guide can be found on the\u00a0Mapping Combinations\u00a0page.</p> <p>See also the\u00a0API reference on mapping rules.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#create-a-protocol","title":"Create a Protocol","text":"<p>Now create a federation protocol. A federation protocol object links the Identity Provider to a mapping.</p> <p>You can create a protocol like this:</p> <pre><code>$ openstack federation protocol create saml2 \\\n--mapping samltest_mapping --identity-provider samltest\n</code></pre> <p>As mentioned in\u00a0Prerequisites, the name you give the protocol is not arbitrary, it must be a valid auth method.</p> <p>See also the\u00a0API reference for federation protocols.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configuring-an-httpd-auth-module","title":"Configuring an HTTPD auth module","text":"<p>This guide currently only includes examples for the Apache web server, but it possible to use SAML, OpenIDC, and other auth modules in other web servers. See the installation guides for running keystone behind Apache for\u00a0SUSE,\u00a0RedHat\u00a0or\u00a0Ubuntu.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configure-protected-endpoints","title":"Configure protected endpoints","text":"<p>There is a minimum of one endpoint that must be protected in the VirtualHost configuration for the keystone service:</p> <pre><code>&lt;Location /v3/OS-FEDERATION/identity_providers/IDENTITYPROVIDER/protocols/PROTOCOL/auth&gt;\n  Require valid-user\n  AuthType [...]\n  ...\n&lt;/Location&gt;\n</code></pre> <p>This is the endpoint for federated users to request an unscoped token.</p> <p>If configuring WebSSO, you should also protect one or both of the following endpoints:</p> <pre><code>&lt;Location /v3/auth/OS-FEDERATION/websso/PROTOCOL&gt;\n  Require valid-user\n  AuthType [...]\n  ...\n&lt;/Location&gt;\n&lt;Location /v3/auth/OS-FEDERATION/identity_providers/IDENTITYPROVIDER/protocols/PROTOCOL/websso&gt;\n  Require valid-user\n  AuthType [...]\n  ...\n&lt;/Location&gt;\n</code></pre> <p>The first example only specifies a protocol, and keystone will use the incoming remote ID to determine the Identity Provider. The second specifies the Identity Provider directly, which must then be supplied to horizon when configuring\u00a0horizon for WebSSO.</p> <p>The path must exactly match the path that will be used to access the keystone service. For example, if the identity provider you created in\u00a0Create an Identity Provider\u00a0is\u00a0<code>samltest</code>\u00a0and the protocol you created in\u00a0Create a Protocol\u00a0is\u00a0<code>saml2</code>, then the Locations will be:</p> <pre><code>&lt;Location /v3/OS-FEDERATION/identity_providers/samltest/protocols/saml2/auth&gt;\n  Require valid-user\n  AuthType [...]\n  ...\n&lt;/Location&gt;\n&lt;Location /v3/auth/OS-FEDERATION/websso/saml2&gt;\n  Require valid-user\n  AuthType [...]\n  ...\n&lt;/Location&gt;\n&lt;Location /v3/auth/OS-FEDERATION/identity_providers/samltest/protocols/saml2/websso&gt;\n  Require valid-user\n  AuthType [...]\n  ...\n&lt;/Location&gt;\n</code></pre> <p>However, if you have configured the keystone service to use a virtual path such as\u00a0<code>/identity</code>, that part of the path should be included:</p> <pre><code>&lt;Location /identity/v3/OS-FEDERATION/identity_providers/samltest/protocols/saml2/auth&gt;\n  Require valid-user\n  AuthType [...]\n  ...\n&lt;/Location&gt;\n...\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configure-the-auth-module","title":"Configure the auth module","text":"<p>If your Identity Provider is a SAML IdP, there are two main Apache modules that can be used as a SAML Service Provider:\u00a0mod_shib\u00a0and\u00a0mod_auth_mellon. For an OpenID Connect Identity Provider,\u00a0mod_auth_openidc\u00a0is used. You can also use other auth modules such as kerberos, X.509, or others. Check the documentation for the provider you choose for detailed installation and configuration guidance.</p> <p>Depending on the Service Provider module you\u2019ve chosen, you will need to install the applicable Apache module package and follow additional configuration steps. This guide contains examples for two major federation protocols:</p> <ul> <li> <p>SAML2.0 \u2013 see guides for the following implementations:</p> </li> <li> <p>Set up mod_shib.</p> </li> <li> <p>Set up mod_auth_mellon.</p> </li> <li> <p>OpenID Connect:\u00a0Set up mod_auth_openidc.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configuring-keystone","title":"Configuring Keystone","text":"<p>While the Apache module does the majority of the heavy lifting, minor changes are needed to allow keystone to allow and understand federated authentication.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#add-the-auth-method","title":"Add the Auth Method","text":"<p>Add the authentication methods to the\u00a0<code>[auth]</code>\u00a0section in\u00a0<code>keystone.conf</code>. The auth method here must have the same name as the protocol you created in\u00a0Create a Protocol. You should also remove\u00a0<code>external</code>\u00a0as an allowable method.</p> <pre><code>[auth]\nmethods = password,token,saml2,openid\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configure-the-remote-id-attribute","title":"Configure the Remote ID Attribute","text":"<p>Keystone is mostly apathetic about what HTTPD auth module you choose to configure for your Service Provider, but must know what header key to look for from the auth module to determine the Identity Provider\u2019s remote ID so it can associate the incoming request with the Identity Provider resource. The key name is decided by the auth module choice:</p> <ul> <li> <p>For\u00a0<code>mod_shib</code>: use\u00a0<code>Shib-Identity-Provider</code></p> </li> <li> <p>For\u00a0<code>mod_auth_mellon</code>: the attribute name is configured with the\u00a0<code>MellonIdP</code>\u00a0parameter in the VirtualHost configuration, if set to e.g.\u00a0<code>IDP</code>\u00a0then use\u00a0<code>MELLON_IDP</code></p> </li> <li> <p>For\u00a0<code>mod_auth_openidc</code>: the attribute name is related to the\u00a0<code>OIDCClaimPrefix</code>\u00a0parameter in the Apache configuration, if set to e.g.\u00a0<code>OIDC-</code>\u00a0use\u00a0<code>HTTP_OIDC_ISS</code></p> </li> </ul> <p>It is recommended that this option be set on a per-protocol basis by creating a new section named after the protocol:</p> <pre><code>[saml2]\nremote_id_attribute = Shib-Identity-Provider\n[openid]\nremote_id_attribute = HTTP_OIDC_ISS\n</code></pre> <p>Alternatively, a generic option may be set at the\u00a0<code>[federation]</code>\u00a0level.</p> <pre><code>[federation]\nremote_id_attribute = HTTP_OIDC_ISS\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#add-a-trusted-dashboard-websso","title":"Add a Trusted Dashboard (WebSSO)","text":"<p>If you intend to configure horizon as a WebSSO frontend, you must specify the URLs of trusted horizon servers. This value may be repeated multiple times. This setting ensures that keystone only sends token data back to trusted servers. This is performed as a precaution, specifically to prevent man-in-the-middle (MITM) attacks. The value must exactly match the origin address sent by the horizon server, including any trailing slashes.</p> <pre><code>[federation]\ntrusted_dashboard = https://horizon1.example.org/auth/websso/\ntrusted_dashboard = https://horizon2.example.org/auth/websso/\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#add-the-callback-template-websso","title":"Add the Callback Template (WebSSO)","text":"<p>If you intend to configure horizon as a WebSSO frontend, and if not already done for you by your distribution\u2019s keystone package, copy the\u00a0sso_callback_template.html\u00a0template into the location specified by the\u00a0<code>[federation]/sso_callback_template</code>\u00a0option in\u00a0<code>keystone.conf</code>. You can also use this template as an example to create your own custom HTML redirect page.</p> <p>Restart the keystone WSGI service or the Apache frontend service after making changes to your keystone configuration.</p> <pre><code># systemctl restart apache2\n</code></pre> <p>Configuring Horizon as a WebSSO Frontend</p> <p>Note Consult\u00a0horizon\u2019s official documentation\u00a0for details on configuring horizon.</p> <p>Keystone on its own is not capable of supporting a browser-based Single Sign-on authentication flow such as the SAML2.0 WebSSO profile, therefore we must enlist horizon\u2019s assistance. Horizon can be configured to support SSO by enabling it in horizon\u2019s\u00a0<code>local_settings.py</code>\u00a0configuration file and adding the possible authentication choices that will be presented to the user on the login screen.</p> <p>Ensure the\u00a0WEBSSO_ENABLED\u00a0option is set to\u00a0True\u00a0in horizon\u2019s local_settings.py file, this will provide users with an updated login screen for horizon.</p> <pre><code>WEBSSO_ENABLED = True\n</code></pre> <p>Configure the options for authenticating that a user may choose from at the login screen. The pairs configured in this list map a user-friendly string to an authentication option, which may be one of:</p> <ul> <li> <p>The string\u00a0<code>credentials</code>\u00a0which forces horizon to present its own username and password fields that the user will use to authenticate as a local keystone user</p> </li> <li> <p>The name of a protocol that you created in\u00a0Create a Protocol, such as\u00a0<code>saml2</code>\u00a0or\u00a0<code>openid</code>, which will cause horizon to call keystone\u2019s\u00a0WebSSO API without an Identity Provider\u00a0to authenticate the user</p> </li> <li> <p>A string that maps to an Identity Provider and Protocol combination configured in\u00a0<code>WEBSSO_IDP_MAPPING</code>\u00a0which will cause horizon to call keystone\u2019s\u00a0WebSSO API specific to the given Identity Provider.</p> </li> </ul> <pre><code>WEBSSO_CHOICES = (\n    (\"credentials\", _(\"Keystone Credentials\")),\n    (\"openid\", _(\"OpenID Connect\")),\n    (\"saml2\", _(\"Security Assertion Markup Language\")),\n    (\"myidp_openid\", \"Acme Corporation - OpenID Connect\"),\n    (\"myidp_saml2\", \"Acme Corporation - SAML2\")\n)\n\nWEBSSO_IDP_MAPPING = {\n    \"myidp_openid\": (\"myidp\", \"openid\"),\n    \"myidp_saml2\": (\"myidp\", \"saml2\")\n}\n</code></pre> <p>The initial selection of the dropdown menu can also be configured:</p> <pre><code>WEBSSO_INITIAL_CHOICE = \"credentials\"\n</code></pre> <p>Remember to restart the web server when finished configuring horizon:</p> <pre><code># systemctl restart apache2\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#authenticating","title":"Authenticating","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#use-the-cli-to-authenticate-with-a-saml20-identity-provider","title":"Use the CLI to authenticate with a SAML2.0 Identity Provider","text":"<p>The\u00a0<code>python-openstackclient</code>\u00a0can be used to authenticate a federated user in a SAML Identity Provider to keystone.</p> <p>Note The SAML Identity Provider must be configured to support the ECP authentication profile.</p> <p>To use the CLI tool, you must have the name of the Identity Provider resource in keystone, the name of the federation protocol configured in keystone, and the ECP endpoint for the Identity Provider. If you are the cloud administrator, the name of the Identity Provider and protocol was configured in\u00a0Create an Identity Provider\u00a0and\u00a0Create a Protocol\u00a0respectively. If you are not the administrator, you must obtain this information from the administrator.</p> <p>The ECP endpoint for the Identity Provider can be obtained from its metadata without involving an administrator. This endpoint is the\u00a0<code>urn:oasis:names:tc:SAML:2.0:bindings:SOAP</code>\u00a0binding in the metadata document:</p> <pre><code>$ curl -s https://samltest.id/saml/idp | grep urn:oasis:names:tc:SAML:2.0:bindings:SOAP\n     &lt;SingleSignOnService Binding=\"urn:oasis:names:tc:SAML:2.0:bindings:SOAP\" Location=\"https://samltest.id/idp/profile/SAML2/SOAP/ECP\"/&gt;\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#find-available-scopes","title":"Find available scopes","text":"<p>If you are a new user and are not aware of what resources you have access to, you can use an unscoped query to list the projects or domains you have been granted a role assignment on:</p> <pre><code>export OS_AUTH_TYPE=v3samlpassword\nexport OS_IDENTITY_PROVIDER=samltest\nexport OS_IDENTITY_PROVIDER_URL=https://samltest.id/idp/profile/SAML2/SOAP/ECP\nexport OS_PROTOCOL=saml2\nexport OS_USERNAME=morty\nexport OS_PASSWORD=panic\nexport OS_AUTH_URL=https://sp.keystone.example.org/v3\nexport OS_IDENTITY_API_VERSION=3\nopenstack federation project list\nopenstack federation domain list\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#get-a-scoped-token","title":"Get a scoped token","text":"<p>If you already know the project, domain or system you wish to scope to, you can directly request a scoped token:</p> <pre><code>export OS_AUTH_TYPE=v3samlpassword\nexport OS_IDENTITY_PROVIDER=samltest\nexport OS_IDENTITY_PROVIDER_URL=https://samltest.id/idp/profile/SAML2/SOAP/ECP\nexport OS_PROTOCOL=saml2\nexport OS_USERNAME=morty\nexport OS_PASSWORD=panic\nexport OS_AUTH_URL=https://sp.keystone.example.org/v3\nexport OS_IDENTITY_API_VERSION=3\nexport OS_PROJECT_NAME=federated_project\nexport OS_PROJECT_DOMAIN_NAME=Default\nopenstack token issue\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#use-horizon-to-authenticate-with-an-external-identity-provider","title":"Use horizon to authenticate with an external Identity Provider","text":"<p>When horizon is configured to enable WebSSO, a dropdown menu will appear on the login screen before the user has authenticated. Select an authentication method from the menu to be redirected to your Identity Provider for authentication.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#keystone-as-an-identity-provider-idp","title":"Keystone as an Identity Provider (IdP)","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#prerequisites_1","title":"Prerequisites","text":"<p>When keystone is configured as an Identity Provider, it is often referred to as\u00a0Keystone to Keystone, because it enables federation between multiple OpenStack clouds using the SAML2.0 protocol.</p> <p>If you are not familiar with the idea of federated identity, see the\u00a0introduction\u00a0first.</p> <p>When setting up\u00a0Keystone to Keystone, it is easiest to\u00a0configure a keystone Service Provider\u00a0first with a sandbox Identity Provider such as\u00a0samltest.id.</p> <p>This feature requires installation of the xmlsec1 tool via your distribution packaging system (for instance apt or yum)</p> <pre><code># apt-get install xmlsec1\n</code></pre> <p>Note In this guide, the keystone Identity Provider is configured on a host called idp.keystone.example.org listening on the standard HTTPS port. All keystone paths will start with the keystone version prefix,\u00a0<code>/v3</code>. If you have configured keystone to listen on port 5000, or to respond on the path\u00a0<code>/identity</code>\u00a0(for example), take this into account in your own configuration.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configuring-metadata","title":"Configuring Metadata","text":"<p>Since keystone is acting as a SAML Identity Provider, its metadata must be configured in the\u00a0<code>[saml]</code>\u00a0section (not to be confused with an optional\u00a0<code>[saml2]</code>\u00a0section which you may have configured in\u00a0Configure the Remote Id Attribute\u00a0while setting up keystone as Service Provider) of\u00a0<code>keystone.conf</code>\u00a0so that it can served by the\u00a0metadata API.</p> <p>The two parameters that\u00a0must\u00a0be set in order for keystone to generate metadata are\u00a0<code>idp_entity_id</code>\u00a0and\u00a0<code>idp_sso_endpoint</code>:</p> <pre><code>[saml]\nidp_entity_id=https://idp.keystone.example.org/v3/OS-FEDERATION/saml2/idp\nidp_sso_endpoint=https://idp.keystone.example.org/v3/OS-FEDERATION/saml2/sso\n</code></pre> <p><code>idp_entity_id</code>\u00a0sets the Identity Provider entity ID, which is a string of your choosing that uniquely identifies the Identity Provider to any Service Provider.</p> <p><code>idp_sso_endpoint</code>\u00a0is required to generate valid metadata, but its value is currently not used because keystone as an Identity Provider does not support the SAML2.0 WebSSO auth profile. This may change in the future which is why there is no default value provided and must be set by the operator.</p> <p>For completeness, the following Organization and Contact configuration options should also be updated to reflect your organization and administrator contact details.</p> <pre><code>idp_organization_name=example_company\nidp_organization_display_name=Example Corp.\nidp_organization_url=example.com\nidp_contact_company=example_company\nidp_contact_name=John\nidp_contact_surname=Smith\nidp_contact_email=jsmith@example.com\nidp_contact_telephone=555-555-5555\nidp_contact_type=technical\n</code></pre> <p>It is important to take note of the default\u00a0<code>certfile</code>\u00a0and\u00a0<code>keyfile</code>\u00a0options, and adjust them if necessary:</p> <pre><code>certfile=/etc/keystone/ssl/certs/signing_cert.pem\nkeyfile=/etc/keystone/ssl/private/signing_key.pem\n</code></pre> <p>You must generate a PKI key pair and copy the files to these paths. You can use the\u00a0<code>openssl</code>\u00a0tool to do so. Keystone does not provide a utility for this.</p> <p>Check the\u00a0<code>idp_metadata_path</code>\u00a0setting and adjust it if necessary:</p> <pre><code>idp_metadata_path=/etc/keystone/saml2_idp_metadata.xml\n</code></pre> <p>To create metadata for your keystone IdP, run the\u00a0<code>keystone-manage</code>\u00a0command and redirect the output to a file. For example:</p> <pre><code># keystone-manage saml_idp_metadata &gt; /etc/keystone/saml2_idp_metadata.xml\n</code></pre> <p>Finally, restart the keystone WSGI service or the web server frontend:</p> <pre><code># systemctl restart apache2\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#creating-a-service-provider-resource","title":"Creating a Service Provider Resource","text":"<p>Create a Service Provider resource to represent your Service Provider as an object in keystone:</p> <pre><code>$ openstack service provider create keystonesp \\\n--service-provider-url https://sp.keystone.example.org/Shibboleth.sso/SAML2/ECP\n--auth-url https://sp.keystone.example.org/v3/OS-FEDERATION/identity_providers/keystoneidp/protocols/saml2/auth\n</code></pre> <p>The\u00a0<code>--auth-url</code>\u00a0is the\u00a0federated auth endpoint\u00a0for a specific Identity Provider and protocol name, here named\u00a0<code>keystoneidp</code>\u00a0and\u00a0<code>saml2</code>.</p> <p>The\u00a0<code>--service-provider-url</code>\u00a0is the\u00a0<code>urn:oasis:names:tc:SAML:2.0:bindings:PAOS</code>\u00a0binding for the Assertion Consumer Service of the Service Provider. It can be obtained from the Service Provider metadata:</p> <pre><code>$ curl -s https://sp.keystone.example.org/Shibboleth.sso/Metadata | grep urn:oasis:names:tc:SAML:2.0:bindings:PAOS\n&lt;md:AssertionConsumerService Binding=\"urn:oasis:names:tc:SAML:2.0:bindings:PAOS\" Location=\"https://sp.keystone.example.org/Shibboleth.sso/SAML2/ECP\" index=\"4\"/&gt;\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#authenticating_1","title":"Authenticating","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#use-the-cli-to-authenticate-with-keystone-to-keystone","title":"Use the CLI to authenticate with Keystone-to-Keystone","text":"<p>Use\u00a0<code>python-openstackclient</code>\u00a0to authenticate with the IdP and then get a scoped token from the SP.</p> <pre><code>export OS_USERNAME=demo\nexport OS_PASSWORD=nomoresecret\nexport OS_AUTH_URL=https://idp.keystone.example.org/v3\nexport OS_IDENTITY_API_VERSION=3\nexport OS_PROJECT_NAME=federated_project\nexport OS_PROJECT_DOMAIN_NAME=Default\nexport OS_SERVICE_PROVIDER=keystonesp\nexport OS_REMOTE_PROJECT_NAME=federated_project\nexport OS_REMOTE_PROJECT_DOMAIN_NAME=Default\nopenstack token issue\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#use-horizon-to-switch-clouds","title":"Use Horizon to switch clouds","text":"<p>No additional configuration is necessary to enable horizon for Keystone to Keystone. Log into the horizon instance for the Identity Provider using your regular local keystone credentials. Once logged in, you will see a Service Provider dropdown menu which you can use to switch your dashboard view to another cloud.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#setting-up-openid-connect","title":"Setting Up OpenID Connect","text":"<p>See\u00a0Keystone as a Service Provider (SP)\u00a0before proceeding with these OpenIDC-specific instructions.</p> <p>These examples use Google as an OpenID Connect Identity Provider. The Service Provider must be added to the Identity Provider in the\u00a0Google API console.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configuring-apache-httpd-for-mod_auth_openidc","title":"Configuring Apache HTTPD for mod_auth_openidc","text":"<p>Note You are advised to carefully examine the\u00a0mod_auth_openidc documentation.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#install-the-module","title":"Install the Module","text":"<p>Install the Apache module package. For example, on Ubuntu:</p> <pre><code># apt-get install libapache2-mod-auth-openidc\n</code></pre> <p>The package and module name will differ between distributions.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configure-mod_auth_openidc","title":"Configure mod_auth_openidc","text":"<p>In the Apache configuration for the keystone VirtualHost, set the following OIDC options:</p> <pre><code>OIDCClaimPrefix \"OIDC-\"\nOIDCResponseType \"id_token\"\nOIDCScope \"openid email profile\"\nOIDCProviderMetadataURL https://accounts.google.com/.well-known/openid-configuration\nOIDCOAuthVerifyJwksUri https://www.googleapis.com/oauth2/v3/certs\nOIDCClientID &lt;openid_client_id&gt;\nOIDCClientSecret &lt;openid_client_secret&gt;\nOIDCCryptoPassphrase &lt;random string&gt;\nOIDCRedirectURI https://sp.keystone.example.org/v3/OS-FEDERATION/identity_providers/google/protocols/openid/auth\n</code></pre> <p><code>OIDCScope</code>\u00a0is the list of attributes that the user will authorize the Identity Provider to send to the Service Provider.\u00a0<code>OIDCClientID</code>\u00a0and\u00a0<code>OIDCClientSecret</code>\u00a0must be generated and obtained from the Identity Provider.\u00a0<code>OIDCProviderMetadataURL</code>\u00a0is a URL from which the Service Provider will fetch the Identity Provider\u2019s metadata.\u00a0<code>OIDCOAuthVerifyJwksUri</code>\u00a0is a URL from which the Service Provider will download the public key from the Identity Provider to check if the user\u2019s access token is valid or not, this configuration must be used while using the AuthType\u00a0<code>auth-openidc</code>, when using the AuthType\u00a0<code>openid-connect</code>\u00a0and the OIDCProviderMetadataURL is configured, this property will not be necessary.\u00a0<code>OIDCRedirectURI</code>\u00a0is a vanity URL that must point to a protected path that does not have any content, such as an extension of the protected federated auth path.</p> <p>Note If using a mod_wsgi version less than 4.3.0, then the\u00a0OIDCClaimPrefix\u00a0must be specified to have only alphanumerics or a dash (\u201c-\u201c). This is because\u00a0mod_wsgi blocks headers that do not fit this criteria.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configure-protected-endpoints_1","title":"Configure Protected Endpoints","text":"<p>Configure each protected path to use the\u00a0<code>openid-connect</code>\u00a0AuthType:</p> <pre><code>&lt;Location /v3/OS-FEDERATION/identity_providers/google/protocols/openid/auth&gt;\n    Require valid-user\n    AuthType openid-connect\n&lt;/Location&gt;\n</code></pre> <p>Note To add support to Bearer Access Token authentication flow that is used by applications that do not adopt the browser flow, such the OpenStack CLI, you will need to change the AuthType from\u00a0<code>openid-connect</code>\u00a0to\u00a0<code>auth-openidc</code>.</p> <p>Do the same for the WebSSO auth paths if using horizon:</p> <pre><code>&lt;Location /v3/auth/OS-FEDERATION/websso/openid&gt;\n    Require valid-user\n    AuthType openid-connect\n&lt;/Location&gt;\n&lt;Location /v3/auth/OS-FEDERATION/identity_providers/google/protocols/openid/websso&gt;\n    Require valid-user\n    AuthType openid-connect\n&lt;/Location&gt;\n</code></pre> <p>Remember to reload Apache after altering the VirtualHost:</p> <pre><code># systemctl reload apache2\n</code></pre> <p>Note When creating\u00a0mapping rules, in keystone, note that the \u2018remote\u2019 attributes will be prefixed, with\u00a0<code>HTTP_</code>, so for instance, if you set\u00a0<code>OIDCClaimPrefix</code>\u00a0to\u00a0<code>OIDC-</code>, then a typical remote value to check for is:\u00a0<code>HTTP_OIDC_ISS</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configuring-multiple-identity-providers","title":"Configuring Multiple Identity Providers\u00b6","text":"<p>To configure multiples Identity Providers in your environment you will need to set your OIDC options like the following options:</p> <pre><code>OIDCClaimPrefix \"OIDC-\"\nOIDCResponseType \"id_token\"\nOIDCScope \"openid email profile\"\nOIDCMetadataDir &lt;IDP metadata directory&gt;\nOIDCCryptoPassphrase &lt;random string&gt;\nOIDCRedirectURI https://sp.keystone.example.org/redirect_uri\nOIDCOAuthVerifyCertFiles &lt;kid&gt;#&lt;/path/to-cert.pem&gt; &lt;kid2&gt;#&lt;/path/to-cert2.pem&gt; &lt;kidN&gt;#&lt;/path/to-certN.pem&gt;`\n</code></pre> <p>The\u00a0<code>OIDCOAuthVerifyCertFiles</code>\u00a0is a tuple separated with\u00a0space\u00a0containing the key-id (kid) of the Issuer\u2019s public key and a path to the Issuer certificate. The separator\u00a0<code>#</code>\u00a0is used to split the (<code>kid</code>) and the public certificate address</p> <p>The metadata folder configured in the option\u00a0<code>OIDCMetadataDir</code>\u00a0must have all your Identity Providers configurations, the name of the files will be the name (with path) of the Issuers like:</p> <pre><code>- &lt;IDP metadata directory&gt;\n  |\n  - accounts.google.com.client\n  |\n  - accounts.google.com.conf\n  |\n  - accounts.google.com.provider\n  |\n  - keycloak.example.org%2Fauth%2Frealms%2Fidp.client\n  |\n  - keycloak.example.org%2Fauth%2Frealms%2Fidp.conf\n  |\n  - keycloak.example.org%2Fauth%2Frealms%2Fidp.provider\n</code></pre> <p>Note The name of the file must be url-encoded if needed, as the Apache2 mod_auth_openidc will get the raw value from the query parameter\u00a0<code>iss</code>\u00a0from the http request and check if there is a metadata with this name, as the query parameter is url-encoded, so the metadata file name need to be encoded too. For example, if you have an Issuer with\u00a0<code>/</code>\u00a0in the URL, then you need to escape it to\u00a0<code>%2F</code>\u00a0by applying a URL escape in the file name.</p> <p>The content of these files must be a JSON like</p> <p><code>accounts.google.com.client</code>:</p> <pre><code>{\n  \"client_id\":\"&lt;openid_client_id&gt;\",\n  \"client_secret\":\"&lt;openid_client_secret&gt;\"\n}\n</code></pre> <p>The\u00a0<code>.client</code>\u00a0file handles the SP credentials in the Issuer.</p> <p><code>accounts.google.com.conf</code>:</p> <p>This file will be a JSON that overrides some of OIDC options. The options that are able to be overridden are listed in the\u00a0OpenID Connect Apache2 plugin documentation.</p> <p>If you do not want to override the config values, you can leave this file as an empty JSON like\u00a0<code>{}</code>.</p> <p><code>accounts.google.com.provider</code>:</p> <p>This file will contain all specifications about the IdentityProvider. To simplify, you can just use the JSON returned in the\u00a0<code>.well-known</code>\u00a0endpoint:</p> <pre><code>{\n  \"issuer\": \"https://accounts.google.com\",\n  \"authorization_endpoint\": \"https://accounts.google.com/o/oauth2/v2/auth\",\n  \"token_endpoint\": \"https://oauth2.googleapis.com/token\",\n  \"userinfo_endpoint\": \"https://openidconnect.googleapis.com/v1/userinfo\",\n  \"revocation_endpoint\": \"https://oauth2.googleapis.com/revoke\",\n  \"jwks_uri\": \"https://www.googleapis.com/oauth2/v3/certs\",\n  \"response_types_supported\": [\n   \"code\",\n   \"token\",\n   \"id_token\",\n   \"code token\",\n   \"code id_token\",\n   \"token id_token\",\n   \"code token id_token\",\n   \"none\"\n  ],\n  \"subject_types_supported\": [\n   \"public\"\n  ],\n  \"id_token_signing_alg_values_supported\": [\n   \"RS256\"\n  ],\n  \"scopes_supported\": [\n   \"openid\",\n   \"email\",\n   \"profile\"\n  ],\n  \"token_endpoint_auth_methods_supported\": [\n   \"client_secret_post\",\n   \"client_secret_basic\"\n  ],\n  \"claims_supported\": [\n   \"aud\",\n   \"email\",\n   \"email_verified\",\n   \"exp\",\n   \"family_name\",\n   \"given_name\",\n   \"iat\",\n   \"iss\",\n   \"locale\",\n   \"name\",\n   \"picture\",\n   \"sub\"\n  ],\n  \"code_challenge_methods_supported\": [\n   \"plain\",\n   \"S256\"\n  ]\n}\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#continue-configuring-keystone","title":"Continue configuring keystone","text":"<p>Continue configuring keystone</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#setting-up-mellon","title":"Setting Up Mellon","text":"<p>See\u00a0Keystone as a Service Provider (SP)\u00a0before proceeding with these Mellon-specific instructions.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configuring-apache-httpd-for-mod_auth_mellon","title":"Configuring Apache HTTPD for mod_auth_mellon","text":"<p>Note You are advised to carefully examine the\u00a0mod_auth_mellon documentation.</p> <p>Follow the steps outlined at: Keystone install guide for\u00a0SUSE,\u00a0RedHat\u00a0or\u00a0Ubuntu.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#install-the-module_1","title":"Install the Module","text":"<p>Install the Apache module package. For example, on Ubuntu:</p> <pre><code># apt-get install libapache2-mod-auth-mellon\n</code></pre> <p>The package and module name will differ between distributions.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configure-mod_auth_mellon","title":"Configure mod_auth_mellon","text":"<p>Unlike\u00a0<code>mod_shib</code>, all of\u00a0<code>mod_auth_mellon</code>\u2019s configuration is done in Apache, not in a separate config file. Set up the shared settings in a single\u00a0<code>&lt;Location&gt;</code>\u00a0directive near the top in your keystone VirtualHost file, before your protected endpoints:</p> <pre><code>&lt;Location /v3&gt;\n    MellonEnable \"info\"\n    MellonSPPrivateKeyFile /etc/apache2/mellon/sp.keystone.example.org.key\n    MellonSPCertFile /etc/apache2/mellon/sp.keystone.example.org.cert\n    MellonSPMetadataFile /etc/apache2/mellon/sp-metadata.xml\n    MellonIdPMetadataFile /etc/apache2/mellon/idp-metadata.xml\n    MellonEndpointPath /v3/mellon\n    MellonIdP \"IDP\"\n&lt;/Location&gt;\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configure-protected-endpoints_2","title":"Configure Protected Endpoints","text":"<p>Configure each protected path to use the\u00a0<code>Mellon</code>\u00a0AuthType:</p> <pre><code>&lt;Location /v3/OS-FEDERATION/identity_providers/samltest/protocols/saml2/auth&gt;\n   Require valid-user\n   AuthType Mellon\n   MellonEnable auth\n&lt;/Location&gt;\n</code></pre> <p>Do the same for the WebSSO auth paths if using horizon as a single sign-on frontend:</p> <pre><code>&lt;Location /v3/auth/OS-FEDERATION/websso/saml2&gt;\n   Require valid-user\n   AuthType Mellon\n   MellonEnable auth\n&lt;/Location&gt;\n&lt;Location /v3/auth/OS-FEDERATION/identity_providers/samltest/protocols/saml2/websso&gt;\n   Require valid-user\n   AuthType Mellon\n   MellonEnable auth\n&lt;/Location&gt;\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configure-the-mellon-service-provider-metadata","title":"Configure the Mellon Service Provider Metadata","text":"<p>Mellon provides a script called\u00a0<code>mellon_create_metadata.sh``_\u00a0which\u00a0generates\u00a0the\u00a0values\u00a0for\u00a0the\u00a0config\u00a0directives\u00a0``MellonSPPrivateKeyFile</code>,\u00a0<code>MellonSPCertFile</code>, and\u00a0<code>MellonSPMetadataFile</code>. Run the script:</p> <pre><code>$ ./mellon_create_metadata.sh \\\nhttps://sp.keystone.example.org/mellon \\\nhttp://sp.keystone.example.org/v3/OS-FEDERATION/identity_providers/samltest/protocols/saml2/auth/mellon\n</code></pre> <p>The first parameter is used as the entity ID, a URN of your choosing that must uniquely identify the Service Provider to the Identity Provider. The second parameter is the full URL for the endpoint path corresponding to the parameter\u00a0<code>MellonEndpointPath</code>.</p> <p>After generating the keypair and metadata, copy the files to the locations given by the\u00a0<code>MellonSPPrivateKeyFile</code>\u00a0and\u00a0<code>MellonSPCertFile</code>\u00a0settings in your Apache configuration.</p> <p>Upload the Service Provider\u2019s Metadata file which you just generated to your Identity Provider. This is the file used as the value of the\u00a0MellonSPMetadataFile\u00a0in the config. The IdP may provide a webpage where you can upload the file, or you may be required to submit the file using\u00a0wget\u00a0or\u00a0curl. Please check your IdP documentation for details.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#exchange-metadata","title":"Exchange Metadata","text":"<p>Fetch your Identity Provider\u2019s Metadata file and copy it to the path specified by the\u00a0<code>MellonIdPMetadataFile</code>\u00a0setting in your Apache configuration.</p> <pre><code>$ wget -O /etc/apache2/mellon/idp-metadata.xml https://samltest.id/saml/idp\n</code></pre> <p>Remember to reload Apache after finishing configuring Mellon:</p> <pre><code># systemctl reload apache2\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#continue-configuring-keystone_1","title":"Continue configuring keystone","text":"<p>Continue configuring keystone</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#setting-up-shibboleth","title":"Setting up Shibboleth","text":"<p>See\u00a0Keystone as a Service Provider (SP)\u00a0before proceeding with these Shibboleth-specific instructions.</p> <p>Note The examples below are for Ubuntu 16.04, for which only version 2 of the Shibboleth Service Provider is available. Version 3 is available for other distributions and the configuration should be identical to version 2.</p> <p>Configuring Apache HTTPD for mod_shib</p> <p>Note You are advised to carefully examine the\u00a0mod_shib Apache configuration documentation.</p> <p>Configure keystone under Apache, following the steps in the install guide for\u00a0SUSE,\u00a0RedHat\u00a0or\u00a0Ubuntu.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#install-the-module_2","title":"Install the Module","text":"<p>Install the Apache module package. For example, on Ubuntu:</p> <pre><code># apt-get install libapache2-mod-shib2\n</code></pre> <p>The package and module name will differ between distributions.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configure-protected-endpoints_3","title":"Configure Protected Endpoints","text":"<p>In the Apache configuration for the keystone VirtualHost, set an additional\u00a0<code>&lt;Location&gt;</code>\u00a0which is not part of keystone\u2019s API:</p> <pre><code>&lt;Location /Shibboleth.sso&gt;\n    SetHandler shib\n&lt;/Location&gt;\n</code></pre> <p>If you are using\u00a0<code>mod_proxy</code>, for example to proxy requests to the\u00a0<code>/identity</code>\u00a0path to keystone\u2019s UWSGI service, you must exempt this Shibboleth endpoint from it:</p> <pre><code>Proxypass Shibboleth.sso !\n</code></pre> <p>Configure each protected path to use the\u00a0<code>shibboleth</code>\u00a0AuthType:</p> <pre><code>&lt;Location /v3/OS-FEDERATION/identity_providers/samltest/protocols/saml2/auth&gt;\n    Require valid-user\n    AuthType shibboleth\n    ShibRequestSetting requireSession 1\n    ShibExportAssertion off\n    &lt;IfVersion &lt; 2.4&gt;\n        ShibRequireSession On\n        ShibRequireAll On\n    &lt;/IfVersion&gt;\n&lt;/Location&gt;\n</code></pre> <p>Do the same for the WebSSO auth paths if using horizon as a single sign-on frontend:</p> <pre><code>&lt;Location /v3/auth/OS-FEDERATION/websso/saml2&gt;\n    Require valid-user\n    AuthType shibboleth\n    ShibRequestSetting requireSession 1\n    ShibExportAssertion off\n    &lt;IfVersion &lt; 2.4&gt;\n        ShibRequireSession On\n        ShibRequireAll On\n    &lt;/IfVersion&gt;\n&lt;/Location&gt;\n&lt;Location /v3/auth/OS-FEDERATION/identity_providers/samltest/protocols/saml2/websso&gt;\n    Require valid-user\n    AuthType shibboleth\n    ShibRequestSetting requireSession 1\n    ShibExportAssertion off\n    &lt;IfVersion &lt; 2.4&gt;\n        ShibRequireSession On\n        ShibRequireAll On\n    &lt;/IfVersion&gt;\n&lt;/Location&gt;\n</code></pre> <p>Remember to reload Apache after altering the VirtualHost:</p> <pre><code># systemctl reload apache2\n</code></pre> <p>Configuring mod_shib</p> <p>Note You are advised to examine\u00a0Shibboleth Service Provider Configuration documentation</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#generate-a-keypair","title":"Generate a keypair","text":"<p>For all SAML Service Providers, a PKI key pair must be generated and exchanged with the Identity Provider. The\u00a0<code>mod_shib</code>\u00a0package on the Ubuntu distribution provides a utility to generate the key pair:</p> <pre><code># shib-keygen -y &lt;number of years&gt;\n</code></pre> <p>which will generate a key pair under\u00a0<code>/etc/shibboleth</code>. In other cases, the package might generate the key pair automatically upon installation.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configure-metadata","title":"Configure metadata","text":"<p><code>mod_shib</code>\u00a0also has its own configuration file at\u00a0<code>/etc/shibboleth/shibboleth2.xml</code>\u00a0that must be altered, as well as its own daemon. First, give the Service Provider an entity ID. This is a URN that you choose that must be globally unique to the Identity Provider:</p> <pre><code>&lt;ApplicationDefaults entityID=\"https://sp.keystone.example.org/shibboleth\"\n    REMOTE_USER=\"eppn persistent-id targeted-id\"&gt;\n</code></pre> <p>Depending on your Identity Provider, you may also want to change the REMOTE_USER setting, more on that in a moment.</p> <p>Set the entity ID of the Identity Provider (this is the same as the value you provided for\u00a0<code>--remote-id</code>\u00a0in\u00a0Identity Provider):</p> <pre><code>&lt;SSO entityID=\"https://samltest.id/saml/idp\"&gt;\n</code></pre> <p>Additionally, if you want to enable ECP (required for Keystone-to-Keystone), the SSO tag for this entity must also have the ECP flag set:</p> <pre><code>&lt;SSO entityID=\"https://samltest.id/saml/idp\" ECP=\"true\"&gt;\n</code></pre> <p>Tell Shibboleth where to find the metadata of the Identity Provider. You could either tell it to fetch it from a URI or point it to a local file. For example, pointing to a local file:</p> <pre><code>`&lt;MetadataProvider type=\"XML\" file=\"/etc/shibboleth/samltest-metadata.xml\" /&gt;`\n</code></pre> <p>or pointing to a remote location:</p> <pre><code>&lt;MetadataProvider type=\"XML\" url=\"https://samltest.id/saml/idp\"\n    backingFile=\"samltest-metadata.xml\" /&gt;\n</code></pre> <p>When you are finished configuring\u00a0<code>shibboleth2.xml</code>, restart the\u00a0<code>shibd</code>\u00a0daemon:</p> <pre><code># systemctl restart shibd\n</code></pre> <p>Check the\u00a0<code>shibd</code>\u00a0logs in\u00a0<code>/var/log/shibboleth/shibd.log</code>\u00a0and\u00a0<code>/var/log/shibboleth/shibd_warn.log</code>\u00a0for errors or warnings.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#configure-allowed-attributes","title":"Configure allowed attributes","text":"<p>Note For more information see the\u00a0attributes documentation</p> <p>By default,\u00a0<code>mod_shib</code>\u00a0does not pass all attributes received from the Identity Provider to keystone. If your Identity Provider does not use attributes known to\u00a0<code>shibd</code>, you must configure them. For example,\u00a0samltest.id\u00a0uses a custom UID attribute. It is not discoverable in the Identity Provider metadata, but the attribute name and type is logged in the\u00a0<code>mod_shib</code>\u00a0logs when an authentication attempt is made. To allow the attribute, add it to\u00a0<code>/etc/shibboleth/attribute-map.xml</code>:</p> <pre><code>&lt;Attribute name=\"urn:oid:0.9.2342.19200300.100.1.1\" id=\"uid\" /&gt;\n</code></pre> <p>You may also want to use that attribute as a value for the\u00a0<code>REMOTE_USER</code>\u00a0variable, which will make the\u00a0<code>REMOTE_USER</code>\u00a0variable usable as a parameter to your mapping rules. To do so, add it to\u00a0<code>/etc/shibboleth/shibboleth2.xml</code>:</p> <pre><code>&lt;ApplicationDefaults entityID=\"https://sp.keystone.example.org/shibboleth\"\n    REMOTE_USER=\"uid\"&gt;\n</code></pre> <p>Similarly, if using keystone as your Identity Provider, several custom attributes will be needed in\u00a0<code>/etc/shibboleth/attribute-map.xml</code>:</p> <pre><code>&lt;Attribute name=\"openstack_user\" id=\"openstack_user\"/&gt;\n&lt;Attribute name=\"openstack_roles\" id=\"openstack_roles\"/&gt;\n&lt;Attribute name=\"openstack_project\" id=\"openstack_project\"/&gt;\n&lt;Attribute name=\"openstack_user_domain\" id=\"openstack_user_domain\"/&gt;\n&lt;Attribute name=\"openstack_project_domain\" id=\"openstack_project_domain\"/&gt;\n&lt;Attribute name=\"openstack_groups\" id=\"openstack_groups\"/&gt;\n</code></pre> <p>And update the\u00a0<code>REMOTE_USER</code>\u00a0variable in\u00a0<code>/etc/shibboleth/shibboleth2.xml</code>\u00a0if desired:</p> <pre><code>&lt;ApplicationDefaults entityID=\"https://sp.keystone.example.org/shibboleth\"\n    REMOTE_USER=\"openstack_user\"&gt;\n</code></pre> <p>Restart the\u00a0<code>shibd</code>\u00a0daemon after making these changes:</p> <pre><code># systemctl restart shibd\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Configuring_Keystone_for_Federation/#exchange-metadata_1","title":"Exchange Metadata","text":"<p>Once configured, the Service Provider metadata is available to download:</p> <pre><code># wget https://sp.keystone.example.org/Shibboleth.sso/Metadata\n</code></pre> <p>Upload your Service Provider\u2019s metadata to your Identity Provider. This step depends on your Identity Provider choice and is not covered here. If keystone is your Identity Provider you do not need to upload this file.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Create_a_Domain%2C_Projects%2C_Users%2C_and_Roles/","title":"Create a Domain, Projects, Users, and Roles","text":"<p>The Identity service provides authentication services for each OpenStack service. The authentication service uses a combination of domains, projects, users, and roles.</p> <ol> <li>Although the \u201cdefault\u201d domain already exists from the\u00a0keystone-manage bootstrap\u00a0step in this guide, a formal way to create a new domain would be:</li> </ol> <pre><code>$ openstack domain create --description \"An Example Domain\" example\n\n+-------------+----------------------------------+\n| Field       | Value                            |\n+-------------+----------------------------------+\n| description | An Example Domain                |\n| enabled     | True                             |\n| id          | 2f4f80574fd84fe6ba9067228ae0a50c |\n| name        | example                          |\n| tags        | []                               |\n+-------------+----------------------------------+\n</code></pre> <p>2. This guide uses a service project that contains a unique user for each service that you add to your environment. Create the\u00a0<code>service</code>\u00a0project:</p> <pre><code>$ openstack project create --domain default \\\n  --description \"Service Project\" service\n\n+-------------+----------------------------------+\n| Field       | Value                            |\n+-------------+----------------------------------+\n| description | Service Project                  |\n| domain_id   | default                          |\n| enabled     | True                             |\n| id          | 24ac7f19cd944f4cba1d77469b2a73ed |\n| is_domain   | False                            |\n| name        | service                          |\n| parent_id   | default                          |\n| tags        | []                               |\n+-------------+----------------------------------+\n</code></pre> <p>3.</p> <p>Regular (non-admin) tasks should use an unprivileged project and user. As an example, this guide creates the\u00a0<code>myproject</code>\u00a0project and\u00a0<code>myuser</code>\u00a0user.</p> <ul> <li>Create the\u00a0<code>myproject</code>\u00a0project:</li> </ul> <pre><code>$ openstack project create --domain default \\\n  --description \"Demo Project\" myproject\n\n+-------------+----------------------------------+\n| Field       | Value                            |\n+-------------+----------------------------------+\n| description | Demo Project                     |\n| domain_id   | default                          |\n| enabled     | True                             |\n| id          | 231ad6e7ebba47d6a1e57e1cc07ae446 |\n| is_domain   | False                            |\n| name        | myproject                        |\n| parent_id   | default                          |\n| tags        | []                               |\n+-------------+----------------------------------+\n</code></pre> <p>Note Do not repeat this step when creating additional users for this project.</p> <p>Create the\u00a0<code>myuser</code>\u00a0user:</p> <pre><code>$ openstack user create --domain default \\\n  --password-prompt myuser\n\nUser Password:\nRepeat User Password:\n+---------------------+----------------------------------+\n| Field               | Value                            |\n+---------------------+----------------------------------+\n| domain_id           | default                          |\n| enabled             | True                             |\n| id                  | aeda23aa78f44e859900e22c24817832 |\n| name                | myuser                           |\n| options             | {}                               |\n| password_expires_at | None                             |\n+---------------------+----------------------------------+\n</code></pre> <p>Create the\u00a0<code>myrole</code>\u00a0role:</p> <pre><code>$ openstack role create myrole\n\n+-----------+----------------------------------+\n| Field     | Value                            |\n+-----------+----------------------------------+\n| domain_id | None                             |\n| id        | 997ce8d05fc143ac97d83fdfb5998552 |\n| name      | myrole                           |\n+-----------+----------------------------------+\n</code></pre> <p>Add the\u00a0<code>myrole</code>\u00a0role to the\u00a0<code>myproject</code>\u00a0project and\u00a0<code>myuser</code>\u00a0user:</p> <pre><code>$ openstack role add --project myproject --user myuser myrole\n</code></pre> <p>Note This command provides no output.</p> <p>Note You can repeat this procedure to create additional projects and users.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/","title":"Services","text":"<p>Keystone is organized as a group of internal services exposed on one or many endpoints. Many of these services are used in a combined fashion by the frontend. For example, an authenticate call will validate user/project credentials with the Identity service and, upon success, create and return a token with the Token service.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#identity","title":"Identity","text":"<p>The Identity service provides auth credential validation and data about\u00a0users\u00a0and\u00a0groups. In the basic case, this data is managed by the Identity service, allowing it to also handle all CRUD operations associated with this data. In more complex cases, the data is instead managed by an authoritative backend service. An example of this would be when the Identity service acts as a frontend for LDAP. In that case the LDAP server is the source of truth and the role of the Identity service is to relay that information accurately.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#users","title":"Users","text":"<p><code>Users</code>\u00a0represent an individual API consumer. A user itself must be owned by a specific domain, and hence all user names are\u00a0not\u00a0globally unique, but only unique to their domain.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#groups","title":"Groups","text":"<p><code>Groups</code>\u00a0are a container representing a collection of users. A group itself must be owned by a specific domain, and hence all group names are\u00a0not\u00a0globally unique, but only unique to their domain.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#resource","title":"Resource","text":"<p>The Resource service provides data about\u00a0projects\u00a0and\u00a0domains.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#projects","title":"Projects","text":"<p><code>Projects</code>\u00a0represent the base unit of\u00a0<code>ownership</code>\u00a0in OpenStack, in that all resources in OpenStack should be owned by a specific project. A project itself must be owned by a specific domain, and hence all project names are\u00a0not\u00a0globally unique, but unique to their domain. If the domain for a project is not specified, then it is added to the default domain.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#domains","title":"Domains","text":"<p><code>Domains</code>\u00a0are a high-level container for projects, users and groups. Each is owned by exactly one domain. Each domain defines a namespace where an API-visible name attribute exists. Keystone provides a default domain, aptly named \u2018Default\u2019.</p> <p>In the Identity v3 API, the uniqueness of attributes is as follows:</p> <ul> <li> <p>Domain Name. Globally unique across all domains.</p> </li> <li> <p>Role Name. Unique within the owning domain.</p> </li> <li> <p>User Name. Unique within the owning domain.</p> </li> <li> <p>Project Name. Unique within the owning domain.</p> </li> <li> <p>Group Name. Unique within the owning domain.</p> </li> </ul> <p>Due to their container architecture, domains may be used as a way to delegate management of OpenStack resources. A user in a domain may still access resources in another domain, if an appropriate assignment is granted.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#assignment","title":"Assignment","text":"<p>The Assignment service provides data about\u00a0roles\u00a0and\u00a0role assignments.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#roles","title":"Roles","text":"<p><code>Roles</code>\u00a0dictate the level of authorization the end user can obtain. Roles can be granted at either the domain or project level. A role can be assigned at the individual user or group level. Role names are unique within the owning domain.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#role-assignments","title":"Role Assignments","text":"<p>A 3-tuple that has a\u00a0<code>Role</code>, a\u00a0<code>Resource</code>\u00a0and an\u00a0<code>Identity</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#token","title":"Token","text":"<p>The Token service validates and manages tokens used for authenticating requests once a user\u2019s credentials have already been verified.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#catalog","title":"Catalog","text":"<p>The Catalog service provides an endpoint registry used for endpoint discovery.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#application-construction","title":"Application Construction","text":"<p>Keystone is an HTTP front-end to several services. Since the Rocky release Keystone uses the\u00a0Flask-RESTful\u00a0library to provide a REST API interface to these services.</p> <p>Keystone defines functions related to\u00a0Flask-RESTful\u00a0in\u00a0<code>keystone.server.flask.common</code>. Keystone creates API resources which inherit from class\u00a0<code>keystone.server.flask.common.ResourceBase</code>\u00a0and exposes methods for each supported HTTP methods GET, PUT , POST, PATCH and DELETE. For example, the User resource will look like:</p> <pre><code>class UserResource(ks_flask.ResourceBase): \n  collection_key = 'users' \n  member_key = 'user' \n  get_member_from_driver = PROVIDERS.deferred_provider_lookup( api='identity_api', method='get_user') \n\n  def get(self, user_id=None): \n  \"\"\"Get a user resource or list users. GET/HEAD /v3/users GET/HEAD /v3/users/{user_id} \"\"\" \n  ... \n\n  def post(self): \n  \"\"\"Create a user. POST /v3/users \"\"\"\n  ...\n  class UserChangePasswordResource(ks_flask.ResourceBase): \n  @ks_flask.unenforced_api \n   def post(self, user_id): \n   ...\n</code></pre> <p>Routes for each API resource are defined by classes which inherit from\u00a0<code>keystone.server.flask.common.APIBase</code>. For example, the UserAPI will look like:</p> <pre><code>class UserAPI(ks_flask.APIBase):\n    _name = 'users'\n    _import_name = __name__\n    resources = [UserResource]\n    resource_mapping = [\n        ks_flask.construct_resource_map(\n            resource=UserChangePasswordResource,\n            url='/users/&lt;string:user_id&gt;/password',\n            resource_kwargs={},\n            rel='user_change_password',\n            path_vars={'user_id': json_home.Parameters.USER_ID}\n        ),\n        # Add more resources as needed\n    ]\n\n</code></pre> <p>The methods\u00a0<code>_add_resources()</code>\u00a0or\u00a0<code>_add_mapped_resources()</code>\u00a0in\u00a0<code>keystone.server.flask.common.APIBase</code>\u00a0bind the resources with the APIs. Within each API, one or more managers are loaded (for example, see\u00a0<code>keystone.catalog.core.Manager</code>), which are thin wrapper classes which load the appropriate service driver based on the keystone configuration.</p> <ul> <li> <p>Assignment</p> </li> <li> <p><code>keystone.api.role_assignments</code></p> </li> <li> <p><code>keystone.api.role_inferences</code></p> </li> <li> <p><code>keystone.api.roles</code></p> </li> <li> <p><code>keystone.api.os_inherit</code></p> </li> <li> <p><code>keystone.api.system</code></p> </li> <li> <p>Authentication</p> </li> <li> <p><code>keystone.api.auth</code></p> </li> <li> <p><code>keystone.api.ec2tokens</code></p> </li> <li> <p><code>keystone.api.s3tokens</code></p> </li> <li> <p>Catalog</p> </li> <li> <p><code>keystone.api.endpoints</code></p> </li> <li> <p><code>keystone.api.os_ep_filter</code></p> </li> <li> <p><code>keystone.api.regions</code></p> </li> <li> <p><code>keystone.api.services</code></p> </li> <li> <p>Credentials</p> </li> <li> <p><code>keystone.api.credentials</code></p> </li> <li> <p>Federation</p> </li> <li> <p><code>keystone.api.os_federation</code></p> </li> <li> <p>Identity</p> </li> <li> <p><code>keystone.api.groups</code></p> </li> <li> <p><code>keystone.api.users</code></p> </li> <li> <p>Limits</p> </li> <li> <p><code>keystone.api.registered_limits</code></p> </li> <li> <p><code>keystone.api.limits</code></p> </li> <li> <p>Oauth1</p> </li> <li> <p><code>keystone.api.os_oauth1</code></p> </li> <li> <p>Policy</p> </li> <li> <p><code>keystone.api.policy</code></p> </li> <li> <p>Resource</p> </li> <li> <p><code>keystone.api.domains</code></p> </li> <li> <p><code>keystone.api.projects</code></p> </li> <li> <p>Revoke</p> </li> <li> <p><code>keystone.api.os_revoke</code></p> </li> <li> <p>Trust</p> </li> <li> <p><code>keystone.api.trusts</code></p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#service-backends","title":"Service Backends","text":"<p>Each of the services can be configured to use a backend to allow keystone to fit a variety of environments and needs. The backend for each service is defined in the keystone.conf file with the key\u00a0<code>driver</code>\u00a0under a group associated with each service.</p> <p>A general class exists under each backend to provide an abstract base class for any implementations, identifying the expected service implementations. The abstract base classes are stored in the service\u2019s backends directory as\u00a0<code>base.py</code>. The corresponding drivers for the services are:</p> <ul> <li> <p><code>keystone.assignment.backends.base.AssignmentDriverBase</code></p> </li> <li> <p><code>keystone.assignment.role_backends.base.RoleDriverBase</code></p> </li> <li> <p><code>keystone.auth.plugins.base.AuthMethodHandler</code></p> </li> <li> <p><code>keystone.catalog.backends.base.CatalogDriverBase</code></p> </li> <li> <p><code>keystone.credential.backends.base.CredentialDriverBase</code></p> </li> <li> <p><code>keystone.endpoint_policy.backends.base.EndpointPolicyDriverBase</code></p> </li> <li> <p><code>keystone.federation.backends.base.FederationDriverBase</code></p> </li> <li> <p><code>keystone.identity.backends.base.IdentityDriverBase</code></p> </li> <li> <p><code>keystone.identity.mapping_backends.base.MappingDriverBase</code></p> </li> <li> <p><code>keystone.identity.shadow_backends.base.ShadowUsersDriverBase</code></p> </li> <li> <p><code>keystone.oauth1.backends.base.Oauth1DriverBase</code></p> </li> <li> <p><code>keystone.policy.backends.base.PolicyDriverBase</code></p> </li> <li> <p><code>keystone.resource.backends.base.ResourceDriverBase</code></p> </li> <li> <p><code>keystone.resource.config_backends.base.DomainConfigDriverBase</code></p> </li> <li> <p><code>keystone.revoke.backends.base.RevokeDriverBase</code></p> </li> <li> <p><code>keystone.token.providers.base.Provider</code></p> </li> <li> <p><code>keystone.trust.backends.base.TrustDriverBase</code></p> </li> </ul> <p>If you implement a backend driver for one of the keystone services, you\u2019re expected to subclass from these classes.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#templated-backend","title":"Templated Backend","text":"<p>Largely designed for a common use case around service catalogs in the keystone project, a templated backend is a catalog backend that simply expands pre-configured templates to provide catalog data.</p> <p>Example paste.deploy config (uses $ instead of % to avoid ConfigParser\u2019s interpolation)[DEFAULT]</p> <pre><code>catalog.RegionOne.identity.publicURL = http://localhost:$(public_port)s/v3 catalog.RegionOne.identity.adminURL = http://localhost:$(public_port)s/v3 catalog.RegionOne.identity.internalURL = http://localhost:$(public_port)s/v3 catalog.RegionOne.identity.name = 'Identity Service'\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#data-model","title":"Data Model","text":"<p>Keystone was designed from the ground up to be amenable to multiple styles of backends. As such, many of the methods and data types will happily accept more data than they know what to do with and pass them on to a backend.</p> <p>There are a few main data types:</p> <ul> <li> <p>User: has account credentials, is associated with one or more projects or domains</p> </li> <li> <p>Group: a collection of users, is associated with one or more projects or domains</p> </li> <li> <p>Project: unit of ownership in OpenStack, contains one or more users</p> </li> <li> <p>Domain: unit of ownership in OpenStack, contains users, groups and projects</p> </li> <li> <p>Role: a first-class piece of metadata associated with many user-project pairs.</p> </li> <li> <p>Token: identifying credential associated with a user or user and project</p> </li> <li> <p>Extras: bucket of key-value metadata associated with a user-project pair.</p> </li> <li> <p>Rule: describes a set of requirements for performing an action.</p> </li> </ul> <p>While the general data model allows a many-to-many relationship between users and groups to projects and domains; the actual backend implementations take varying levels of advantage of that functionality.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#approach-to-crud","title":"Approach to CRUD","text":"<p>While it is expected that any \u201creal\u201d deployment at a large company will manage their users and groups in their existing user systems, a variety of CRUD operations are provided for the sake of development and testing.</p> <p>CRUD is treated as an extension or additional feature to the core feature set, in that a backend is not required to support it. It is expected that backends for services that don\u2019t support the CRUD operations will raise a\u00a0<code>keystone.exception.NotImplemented</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#approach-to-authorization-policy","title":"Approach to Authorization (Policy)","text":"<p>Various components in the system require that different actions are allowed based on whether the user is authorized to perform that action.</p> <p>For the purposes of keystone there are only a couple levels of authorization being checked for:</p> <ul> <li> <p>Require that the performing user is considered an admin.</p> </li> <li> <p>Require that the performing user matches the user being referenced.</p> </li> </ul> <p>Other systems wishing to use the policy engine will require additional styles of checks and will possibly write completely custom backends. By default, keystone leverages policy enforcement that is maintained in\u00a0oslo.policy.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#rules","title":"Rules","text":"<p>Given a list of matches to check for, simply verify that the credentials contain the matches. For example:</p> <pre><code>credentials = {'user_id': 'foo', 'is_admin': 1, 'roles': ['nova:netadmin']}\n\n# An admin-only call:\npolicy_api.enforce(('is_admin:1',), credentials)\n\n# An admin or owner call:\npolicy_api.enforce(('is_admin:1', 'user_id:foo'), credentials)\n\n# A netadmin call:\npolicy_api.enforce(('roles:nova:netadmin',), credentials)\n</code></pre> <p>Credentials are generally built from the user metadata in the \u2018extras\u2019 part of the Identity API. So, adding a \u2018role\u2019 to the user just means adding the role to the user metadata.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#capability-rbac","title":"Capability RBAC","text":"<p>(Not yet implemented.)</p> <p>Another approach to authorization can be action-based, with a mapping of roles to which capabilities are allowed for that role. For example:</p> <pre><code>credentials = {'user_id': 'foo', 'is_admin': 1, 'roles': ['nova:netadmin']}\n\n# Add a policy\npolicy_api.add_policy('action:nova:add_network', ('roles:nova:netadmin',))\n\n# Enforce the policy\npolicy_api.enforce(('action:nova:add_network',), credentials)\n</code></pre> <p>In the backend this would look up the policy for \u2018action:nova:add_network\u2019 and then do what is effectively a \u2018Simple Match\u2019 style match against the credentials.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#approach-to-authentication","title":"Approach to Authentication","text":"<p>Keystone provides several authentication plugins that inherit from\u00a0<code>keystone.auth.plugins.base</code>. The following is a list of available plugins.</p> <ul> <li> <p><code>keystone.auth.plugins.external.Base</code></p> </li> <li> <p><code>keystone.auth.plugins.mapped.Mapped</code></p> </li> <li> <p><code>keystone.auth.plugins.oauth1.OAuth</code></p> </li> <li> <p><code>keystone.auth.plugins.password.Password</code></p> </li> <li> <p><code>keystone.auth.plugins.token.Token</code></p> </li> <li> <p><code>keystone.auth.plugins.totp.TOTP</code></p> </li> </ul> <p>In the most basic plugin\u00a0<code>password</code>, two pieces of information are required to authenticate with keystone, a bit of\u00a0<code>Resource</code>\u00a0information and a bit of\u00a0<code>Identity</code>.</p> <p>Take the following call POST data for instance:</p> <pre><code>{\n  \"auth\": {\n    \"identity\": {\n      \"methods\": [\n        \"password\"\n      ],\n      \"password\": {\n        \"user\": {\n          \"id\": \"0ca8f6\",\n          \"password\": \"secretsecret\"\n        }\n      }\n    },\n    \"scope\": {\n      \"project\": {\n        \"id\": \"263fd9\"\n      }\n    }\n  }\n}\n</code></pre> <p>The user (ID of 0ca8f6) is attempting to retrieve a token that is scoped to project (ID of 263fd9).</p> <p>To perform the same call with names instead of IDs, we now need to supply information about the domain. This is because usernames are only unique within a given domain, but user IDs are supposed to be unique across the deployment. Thus, the auth request looks like the following:</p> <pre><code>{\n  \"auth\": {\n    \"identity\": {\n      \"methods\": [\n        \"password\"\n      ],\n      \"password\": {\n        \"user\": {\n          \"domain\": {\n            \"name\": \"acme\"\n          },\n          \"name\": \"userA\",\n          \"password\": \"secretsecret\"\n        }\n      }\n    },\n    \"scope\": {\n      \"project\": {\n        \"domain\": {\n          \"id\": \"1789d1\"\n        },\n        \"name\": \"project-x\"\n      }\n    }\n  }\n}\n</code></pre> <p>For both the user and the project portion, we must supply either a domain ID or a domain name, in order to properly determine the correct user and project.</p> <p>Alternatively, if we wanted to represent this as environment variables for a command line, it would be:</p> <pre><code>export OS_PROJECT_DOMAIN_ID=1789d1\nexport OS_USER_DOMAIN_NAME=acme\nexport OS_USERNAME=userA\nexport OS_PASSWORD=secretsecret\nexport OS_PROJECT_NAME=project-x\n</code></pre> <p>Note that the project the user is attempting to access must be in the same domain as the user.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Architecture/#what-is-scope","title":"What is Scope?","text":"<p>Scope is an overloaded term.</p> <p>In reference to authenticating, as seen above, scope refers to the portion of the POST data that dictates what\u00a0<code>Resource</code>\u00a0(project, domain, or system) the user wants to access.</p> <p>In reference to tokens, scope refers to the effectiveness of a token, i.e.: a\u00a0project-scoped\u00a0token is only useful on the project it was initially granted for. A\u00a0domain-scoped\u00a0token may be used to perform domain-related function. A\u00a0system-scoped\u00a0token is only useful for interacting with APIs that affect the entire deployment.</p> <p>In reference to users, groups, and projects, scope often refers to the domain that the entity is owned by. i.e.: a user in domain X is scoped to domain X.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/","title":"Keystone Configuration","text":"<p>Information and recommendations for general configuration of keystone for keystone administrators. See the main\u00a0Configuration\u00a0section for complete keystone configuration documentation and sample config files.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#troubleshoot-the-identity-service","title":"Troubleshoot the Identity service","text":"<p>To troubleshoot the Identity service, review the logs in the\u00a0<code>/var/log/keystone/keystone.log</code>\u00a0file.</p> <p>Use the\u00a0<code>/etc/keystone/logging.conf</code>\u00a0file to configure the location of log files.</p> <p>Note The\u00a0<code>insecure_debug</code>\u00a0flag is unique to the Identity service. If you enable\u00a0<code>insecure_debug</code>, error messages from the API change to return security-sensitive information. For example, the error message on failed authentication includes information on why your authentication failed.</p> <p>The logs show the components that have come in to the WSGI request, and ideally show an error that explains why an authorization request failed. If you do not see the request in the logs, run keystone with the\u00a0<code>--debug</code>\u00a0parameter. Pass the\u00a0<code>--debug</code>\u00a0parameter before the command parameters.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#logging","title":"Logging","text":"<p>You configure logging externally to the rest of Identity. The name of the file specifying the logging configuration is set using the\u00a0<code>log_config_append</code>\u00a0option in the\u00a0<code>[DEFAULT]</code>\u00a0section of the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file. To route logging through syslog, set\u00a0<code>use_syslog=true</code>\u00a0in the\u00a0<code>[DEFAULT]</code>\u00a0section.</p> <p>A sample logging configuration file is available with the project in\u00a0<code>etc/logging.conf.sample</code>. Like other OpenStack projects, Identity uses the\u00a0Python logging module, which provides extensive configuration options that let you define the output levels and formats.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#domain-specific-configuration","title":"Domain-specific configuration","text":"<p>The Identity service supports domain-specific Identity drivers. The drivers allow a domain to have its own LDAP or SQL back end. By default, domain-specific drivers are disabled.</p> <p>Domain-specific Identity configuration options can be stored in domain-specific configuration files, or in the Identity SQL database using API REST calls.</p> <p>Note Storing and managing configuration options in an SQL database is experimental in Kilo, and added to the Identity service in the Liberty release.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#enable-drivers-for-domain-specific-configuration-files","title":"Enable drivers for domain-specific configuration files","text":"<p>To enable domain-specific drivers, set these options in the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file:</p> <pre><code>[identity]\ndomain_specific_drivers_enabled = True\ndomain_config_dir = /etc/keystone/domains\n</code></pre> <p>When you enable domain-specific drivers, Identity looks in the\u00a0<code>domain_config_dir</code>\u00a0directory for configuration files that are named as\u00a0<code>keystone.DOMAIN_NAME.conf</code>. A domain without a domain-specific configuration file uses options in the primary configuration file.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#enable-drivers-for-storing-configuration-options-in-sql-database","title":"Enable drivers for storing configuration options in SQL database","text":"<p>To enable domain-specific drivers, set these options in the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file:</p> <pre><code>[identity]\ndomain_specific_drivers_enabled = True\ndomain_configurations_from_database = True\n</code></pre> <p>Any domain-specific configuration options specified through the Identity v3 API will override domain-specific configuration files in the\u00a0<code>/etc/keystone/domains</code>\u00a0directory.</p> <p>Unlike the file-based method of specifying domain-specific configurations, options specified via the Identity API will become active without needing to restart the keystone server. For performance reasons, the current state of configuration options for a domain are cached in the keystone server, and in multi-process and multi-threaded keystone configurations, the new configuration options may not become active until the cache has timed out. The cache settings for domain config options can be adjusted in the general keystone configuration file (option\u00a0<code>cache_time</code>\u00a0in the\u00a0<code>domain_config</code>\u00a0group).</p> <p>Note It is important to notice that when using either of these methods of specifying domain-specific configuration options, the main keystone configuration file is still maintained. Only those options that relate to the Identity driver for users and groups (i.e. specifying whether the driver for this domain is SQL or LDAP, and, if LDAP, the options that define that connection) are supported in a domain-specific manner. Further, when using the configuration options via the Identity API, the driver option must be set to an LDAP driver (attempting to set it to an SQL driver will generate an error when it is subsequently used).</p> <p>For existing installations that already use file-based domain-specific configurations who wish to migrate to the SQL-based approach, the\u00a0<code>keystone-manage</code>\u00a0command can be used to upload all configuration files to the SQL database:</p> <pre><code>$ keystone-manage domain_config_upload --all\n</code></pre> <p>Once uploaded, these domain-configuration options will be visible via the Identity API as well as applied to the domain-specific drivers. It is also possible to upload individual domain-specific configuration files by specifying the domain name:</p> <pre><code>$ keystone-manage domain_config_upload --domain-name DOMAINA\n</code></pre> <p>Note It is important to notice that by enabling either of the domain-specific configuration methods, the operations of listing all users and listing all groups are not supported, those calls will need either a domain filter to be specified or usage of a domain scoped token.</p> <p>Note Keystone does not support moving the contents of a domain (i.e. \u201cits\u201d users and groups) from one backend to another, nor group membership across backend boundaries.</p> <p>Note When using the file-based domain-specific configuration method, to delete a domain that uses a domain specific backend, it\u2019s necessary to first disable it, remove its specific configuration file (i.e. its corresponding keystone.\\.conf) and then restart the Identity server. When managing configuration options via the Identity API, the domain can simply be disabled and deleted via the Identity API; since any domain-specific configuration options will automatically be removed. <p>Note Although keystone supports multiple LDAP backends via the above domain-specific configuration methods, it currently only supports one SQL backend. This could be either the default driver or a single domain-specific backend, perhaps for storing service users in a predominantly LDAP installation.</p> <p>Note Keystone has deprecated the\u00a0<code>keystone-manage\u00a0domain_config_upload</code>\u00a0option. The keystone team recommends setting domain config options via the API instead.</p> <p>Due to the need for user and group IDs to be unique across an OpenStack installation and for keystone to be able to deduce which domain and backend to use from just a user or group ID, it dynamically builds a persistent identity mapping table from a public ID to the actual domain, local ID (within that backend) and entity type. The public ID is automatically generated by keystone when it first encounters the entity. If the local ID of the entity is from a backend that does not guarantee to generate UUIDs, a hash algorithm will generate a public ID for that entity, which is what will be exposed by keystone.</p> <p>The use of a hash will ensure that if the public ID needs to be regenerated then the same public ID will be created. This is useful if you are running multiple keystones and want to ensure the same ID would be generated whichever server you hit.</p> <p>Note In case of the LDAP backend, the names of users and groups are not hashed. As a result, these are length limited to 255 characters. Longer names will result in an error.</p> <p>While keystone will dynamically maintain the identity mapping, including removing entries when entities are deleted via the keystone, for those entities in backends that are managed outside of keystone (e.g. a read-only LDAP), keystone will not know if entities have been deleted and hence will continue to carry stale identity mappings in its table. While benign, keystone provides an ability for operators to purge the mapping table of such stale entries using the keystone-manage command, for example:</p> <pre><code>$ keystone-manage mapping_purge --domain-name DOMAINA --local-id abc@de.com\n</code></pre> <p>A typical usage would be for an operator to obtain a list of those entries in an external backend that had been deleted out-of-band to keystone, and then call keystone-manage to purge those entries by specifying the domain and local-id. The type of the entity (i.e. user or group) may also be specified if this is needed to uniquely identify the mapping.</p> <p>Since public IDs can be regenerated\u00a0with the correct generator implementation, if the details of those entries that have been deleted are not available, then it is safe to simply bulk purge identity mappings periodically, for example:</p> <pre><code>$ keystone-manage mapping_purge --domain-name DOMAINA\n</code></pre> <p>will purge all the mappings for DOMAINA. The entire mapping table can be purged with the following command:</p> <pre><code>$ keystone-manage mapping_purge --all\n</code></pre> <p>Generating public IDs in the first run may take a while, and most probably first API requests to fetch user list will fail by timeout. To prevent this,\u00a0<code>mapping_populate</code>\u00a0command should be executed. It should be executed right after LDAP has been configured or after\u00a0<code>mapping_purge</code>.</p> <pre><code>$ keystone-manage mapping_populate --domain DOMAINA\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#public-id-generators","title":"Public ID Generators","text":"<p>Keystone supports a customizable public ID generator and it is specified in the\u00a0<code>[identity_mapping]</code>\u00a0section of the configuration file. Keystone provides a sha256 generator as default, which produces regenerable public IDs. The generator algorithm for public IDs is a balance between key size (i.e. the length of the public ID), the probability of collision and, in some circumstances, the security of the public ID. The maximum length of public ID supported by keystone is 64 characters, and the default generator (sha256) uses this full capability. Since the public ID is what is exposed externally by keystone and potentially stored in external systems, some installations may wish to make use of other generator algorithms that have a different trade-off of attributes. A different generator can be installed by configuring the following property:</p> <ul> <li><code>generator</code>\u00a0\u2013 identity mapping generator. Defaults to\u00a0<code>sha256</code>\u00a0(implemented by\u00a0<code>keystone.identity.id_generators.sha256.Generator</code>)</li> </ul> <p>Warning Changing the generator may cause all existing public IDs to be become invalid, so typically the generator selection should be considered immutable for a given installation.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#migrate-domain-specific-configuration-files-to-the-sql-database","title":"Migrate domain-specific configuration files to the SQL database","text":"<p>You can use the\u00a0<code>keystone-manage</code>\u00a0command to migrate configuration options in domain-specific configuration files to the SQL database:</p> <pre><code># keystone-manage domain_config_upload --all\n</code></pre> <p>To upload options from a specific domain-configuration file, specify the domain name:</p> <pre><code># keystone-manage domain_config_upload --domain-name DOMAIN_NAME\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#integrate-identity-with-ldap","title":"Integrate Identity with LDAP","text":"<p>The OpenStack Identity service supports integration with existing LDAP directories for authentication and authorization services. LDAP back ends require initialization before configuring the OpenStack Identity service to work with it. For more information, see\u00a0Setting up LDAP for use with Keystone.</p> <p>When the OpenStack Identity service is configured to use LDAP back ends, you can split authentication (using the\u00a0identity\u00a0feature) and authorization (using the\u00a0assignment\u00a0feature). OpenStack Identity only supports read-only LDAP integration.</p> <p>The\u00a0identity\u00a0feature enables administrators to manage users and groups by each domain or the OpenStack Identity service entirely.</p> <p>The\u00a0assignment\u00a0feature enables administrators to manage project role authorization using the OpenStack Identity service SQL database, while providing user authentication through the LDAP directory.</p> <p>Note It is possible to isolate identity related information to LDAP in a deployment and keep resource information in a separate datastore. It is not possible to do the opposite, where resource information is stored in LDAP and identity information is stored in SQL. If the resource or assignment back ends are integrated with LDAP, the identity back end must also be integrated with LDAP.</p> <p>Identity LDAP server set up</p> <p>Important If you are using SELinux (enabled by default on RHEL derivatives), then in order for the OpenStack Identity service to access LDAP servers, you must enable the\u00a0<code>authlogin_nsswitch_use_ldap</code>\u00a0boolean value for SELinux on the server running the OpenStack Identity service. To enable and make the option persistent across reboots, set the following boolean value as the root user:</p> <pre><code># setsebool -P authlogin_nsswitch_use_ldap on\n</code></pre> <p>The Identity configuration is split into two separate back ends; identity (back end for users and groups), and assignments (back end for domains, projects, roles, role assignments). To configure Identity, set options in the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file. See\u00a0Integrate Identity back end with LDAP\u00a0for Identity back end configuration examples. Modify these examples as needed.</p> <p>To define the destination LDAP server</p> <p>Define the destination LDAP server in the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file:</p> <pre><code>[ldap]\nurl = ldap://localhost\nuser = dc=Manager,dc=example,dc=org\npassword = samplepassword\nsuffix = dc=example,dc=org\n</code></pre> <p>Although it\u2019s not recommended (see note below), multiple LDAP servers can be supplied to\u00a0<code>url</code>\u00a0to provide high-availability support for a single LDAP backend. By default, these will be tried in order of apperance, but an additional option,\u00a0<code>randomize_urls</code>\u00a0can be set to true, to randomize the list in each process (when it starts). To specify multiple LDAP servers, simply change the\u00a0<code>url</code>\u00a0option in the\u00a0<code>[ldap]</code>\u00a0section to be a list, separated by commas:</p> <pre><code>url = \"ldap://localhost,ldap://backup.localhost\"\nrandomize_urls = true\n</code></pre> <p>Note Failover mechanisms in the LDAP backend can cause delays when switching over to the next working LDAP server. Randomizing the order in which the servers are tried only makes the failure behavior not dependent on which of the ordered servers fail. Individual processes can still be delayed or time out, so this doesn\u2019t fix the issue at hand, but only makes the failure mode more gradual. This behavior cannot be easily fixed inside the service, because keystone would have to monitor the status of each LDAP server, which is in fact a task for a load balancer. Because of this, it is recommended to use a load balancer in front of the LDAP servers, which can monitor the state of the cluster and instantly redirect connections to the working LDAP server.</p> <p>Additional LDAP integration settings</p> <p>Set these options in the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file for a single LDAP server, or\u00a0<code>/etc/keystone/domains/keystone.DOMAIN_NAME.conf</code>\u00a0files for multiple back ends. Example configurations appear below each setting summary:</p> <p>Query option</p> <p>Use\u00a0<code>query_scope</code>\u00a0to control the scope level of data presented (search only the first level or search an entire sub-tree) through LDAP.Use\u00a0<code>page_size</code>\u00a0to control the maximum results per page. A value of zero disables paging.Use\u00a0<code>alias_dereferencing</code>\u00a0to control the LDAP dereferencing option for queries.</p> <pre><code>[ldap]\nquery_scope = sub\npage_size = 0\nalias_dereferencing = default\nchase_referrals =\n</code></pre> <p>Debug</p> <p>Use\u00a0<code>debug_level</code>\u00a0to set the LDAP debugging level for LDAP calls. A value of zero means that debugging is not enabled.</p> <pre><code>[ldap]\ndebug_level = 4095\n</code></pre> <p>This setting sets\u00a0<code>OPT_DEBUG_LEVEL</code>\u00a0in the underlying python library. This field is a bit mask (integer), and the possible flags are documented in the OpenLDAP manpages. Commonly used values include 255 and 4095, with 4095 being more verbose and 0 being disabled. We recommend consulting the documentation for your LDAP back end when using this option.</p> <p>Warning Enabling\u00a0<code>debug_level</code>\u00a0will negatively impact performance.</p> <p>Connection pooling</p> <p>Various LDAP back ends use a common LDAP module to interact with LDAP data. By default, a new connection is established for each LDAP operation. This is expensive when TLS support is enabled, which is a likely configuration in an enterprise setup. Reusing connections from a connection pool drastically reduces overhead of initiating a new connection for every LDAP operation.</p> <p>Use\u00a0<code>use_pool</code>\u00a0to enable LDAP connection pooling. Configure the connection pool size, maximum retry, reconnect trials, timeout (-1 indicates indefinite wait) and lifetime in seconds.</p> <pre><code>[ldap]\nuse_pool = true\npool_size = 10\npool_retry_max = 3\npool_retry_delay = 0.1\npool_connection_timeout = -1\npool_connection_lifetime = 600\n</code></pre> <p>Connection pooling for end user authentication</p> <p>LDAP user authentication is performed via an LDAP bind operation. In large deployments, user authentication can use up all available connections in a connection pool. OpenStack Identity provides a separate connection pool specifically for user authentication.</p> <p>Use\u00a0<code>use_auth_pool</code>\u00a0to enable LDAP connection pooling for end user authentication. Configure the connection pool size and lifetime in seconds. Both\u00a0<code>use_pool</code>\u00a0and\u00a0<code>use_auth_pool</code>\u00a0must be enabled to pool connections for user authentication.</p> <pre><code>[ldap]\nuse_auth_pool = false\nauth_pool_size = 100\nauth_pool_connection_lifetime = 60\n</code></pre> <p>When you have finished the configuration, restart the OpenStack Identity service.</p> <p>Warning During the service restart, authentication and authorization are unavailable.</p> <p>To integrate one Identity back end with LDAP</p> <ol> <li>Enable the LDAP Identity driver in the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file. This allows LDAP as an identity back end:</li> </ol> <pre><code>[identity]\n#driver = sql\ndriver = ldap\n</code></pre> <p>Create the organizational units (OU) in the LDAP directory, and define the corresponding location in the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file:</p> <pre><code>[ldap]\nuser_tree_dn = ou=Users,dc=example,dc=org\nuser_objectclass = inetOrgPerson\n\ngroup_tree_dn = ou=Groups,dc=example,dc=org\ngroup_objectclass = groupOfNames\n</code></pre> <p>Note These schema attributes are extensible for compatibility with various schemas. For example, this entry maps to the person attribute in Active Directory:</p> <pre><code>user_objectclass = person\n</code></pre> <p>Restart the OpenStack Identity service.</p> <p>Warning During service restart, authentication and authorization are unavailable.</p> <p>To integrate multiple Identity back ends with LDAP</p> <p>1. Set the following options in the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file:</p> <p>a. Enable the LDAP driver:</p> <pre><code>[identity]\n#driver = sql\ndriver = ldap\n</code></pre> <p>b. Enable domain-specific drivers:</p> <pre><code>[identity]\ndomain_specific_drivers_enabled = True\ndomain_config_dir = /etc/keystone/domains\n</code></pre> <p>2. Restart the OpenStack Identity service.</p> <p>Warning During service restart, authentication and authorization are unavailable.</p> <p>3. List the domains using the dashboard, or the OpenStackClient CLI. Refer to the\u00a0Command List\u00a0for a list of OpenStackClient commands.</p> <p>4. Create domains using OpenStack dashboard, or the OpenStackClient CLI.</p> <p>5. For each domain, create a domain-specific configuration file in the\u00a0<code>/etc/keystone/domains</code>\u00a0directory. Use the file naming convention\u00a0<code>keystone.DOMAIN_NAME.conf</code>, where DOMAIN_NAME is the domain name assigned in the previous step</p> <p>Note The options set in the\u00a0<code>/etc/keystone/domains/keystone.DOMAIN_NAME.conf</code>\u00a0file will override options in the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file.</p> <p>6. Define the destination LDAP server in the\u00a0<code>/etc/keystone/domains/keystone.DOMAIN_NAME.conf</code>\u00a0file. For example:</p> <pre><code>[ldap]\nurl = ldap://localhost\nuser = dc=Manager,dc=example,dc=org\npassword = samplepassword\nsuffix = dc=example,dc=org\n</code></pre> <p>7. Create the organizational units (OU) in the LDAP directories, and define their corresponding locations in the\u00a0<code>/etc/keystone/domains/keystone.DOMAIN_NAME.conf</code>\u00a0file. For example:</p> <pre><code>[ldap]\nuser_tree_dn = ou=Users,dc=example,dc=org\nuser_objectclass = inetOrgPerson\n\ngroup_tree_dn = ou=Groups,dc=example,dc=org\ngroup_objectclass = groupOfNames\n</code></pre> <p>Note These schema attributes are extensible for compatibility with various schemas. For example, this entry maps to the person attribute in Active Directory:</p> <pre><code>user_objectclass = person\n</code></pre> <p>8. Restart the OpenStack Identity service.</p> <p>Warning During service restart, authentication and authorization are unavailable.</p> <p>Additional LDAP integration settings</p> <p>Set these options in the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file for a single LDAP server, or\u00a0<code>/etc/keystone/domains/keystone.DOMAIN_NAME.conf</code>\u00a0files for multiple back ends. Example configurations appear below each setting summary:Filters</p> <p>Use filters to control the scope of data presented through LDAP.</p> <pre><code>[ldap]\nuser_filter = (memberof=cn=openstack-users,ou=workgroups,dc=example,dc=org)\ngroup_filter =\n</code></pre> <p>Identity attribute mapping</p> <p>Mask account status values (include any additional attribute mappings) for compatibility with various directory services. Superfluous accounts are filtered with\u00a0<code>user_filter</code>.</p> <p>Setting attribute ignore to list of attributes stripped off on update.</p> <p>For example, you can mask Active Directory account status attributes in the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file:</p> <pre><code>[ldap]\nuser_id_attribute      = cn\nuser_name_attribute    = sn\nuser_mail_attribute    = mail\nuser_pass_attribute    = userPassword\nuser_enabled_attribute = userAccountControl\nuser_enabled_mask      = 2\nuser_enabled_invert    = false\nuser_enabled_default   = 512\nuser_default_project_id_attribute =\nuser_additional_attribute_mapping =\n\ngroup_id_attribute     = cn\ngroup_name_attribute   = ou\ngroup_member_attribute = member\ngroup_desc_attribute   = description\ngroup_additional_attribute_mapping =\n</code></pre> <p>It is possible to model more complex LDAP schemas. For example, in the user object, the objectClass posixAccount from\u00a0RFC2307\u00a0is very common. If this is the underlying objectClass, then the\u00a0<code>uid</code>\u00a0field should probably be\u00a0<code>uidNumber</code>\u00a0and the\u00a0<code>username</code>\u00a0field should be either\u00a0<code>uid</code>\u00a0or\u00a0<code>cn</code>. The following illustrates the configuration:</p> <pre><code>[ldap]\nuser_id_attribute = uidNumber\nuser_name_attribute = cn\n</code></pre> <p>Enabled emulation</p> <p>OpenStack Identity supports emulation for integrating with LDAP servers that do not provide an\u00a0<code>enabled</code>\u00a0attribute for users. This allows OpenStack Identity to advertise\u00a0<code>enabled</code>\u00a0attributes when the user entity in LDAP does not. The\u00a0<code>user_enabled_emulation</code>\u00a0option must be enabled and the\u00a0<code>user_enabled_emulation_dn</code>\u00a0option must be a valid LDAP group. Users in the group specified by\u00a0<code>user_enabled_emulation_dn</code>\u00a0will be marked as\u00a0<code>enabled</code>. For example, the following will mark any user who is a member of the\u00a0<code>enabled_users</code>\u00a0group as enabled:</p> <pre><code>[ldap]\nuser_enabled_emulation = True\nuser_enabled_emulation_dn = cn=enabled_users,cn=groups,dc=openstack,dc=org\n</code></pre> <p>If the directory server has an enabled attribute, but it is not a boolean type, a mask can be used to convert it. This is useful when the enabled attribute is an integer value. The following configuration highlights the usage:</p> <pre><code>[ldap]\nuser_enabled_attribute = userAccountControl\nuser_enabled_mask = 2\nuser_enabled_default = 512\n</code></pre> <p>In this case, the attribute is an integer and the enabled attribute is listed in bit 1. If the mask configured\u00a0<code>user_enabled_mask</code>\u00a0is different from 0, it retrieves the attribute from\u00a0<code>user_enabled_attribute</code>\u00a0and performs an add operation with the\u00a0<code>user_enabled_mask</code>. If the sum of the operation matches the mask, then the account is disabled.</p> <p>The value of\u00a0<code>user_enabled_attribute</code>\u00a0is also saved before applying the add operation in\u00a0<code>enabled_nomask</code>. This is done in case the user needs to be enabled or disabled. Lastly, setting\u00a0<code>user_enabled_default</code>\u00a0is needed in order to create a default value on the integer attribute (512 = NORMAL ACCOUNT in Active Directory).</p> <p>When you have finished configuration, restart the OpenStack Identity service.</p> <p>Warning During service restart, authentication and authorization are unavailable.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#secure-the-openstack-identity-service-connection-to-an-ldap-back-end","title":"Secure the OpenStack Identity service connection to an LDAP back end","text":"<p>We recommend securing all connections between OpenStack Identity and LDAP. The Identity service supports the use of TLS to encrypt LDAP traffic. Before configuring this, you must first verify where your certificate authority file is located. For more information, see the\u00a0OpenStack Security Guide SSL introduction.</p> <p>Once you verify the location of your certificate authority file:</p> <p>To configure TLS encryption on LDAP traffic</p> <ol> <li> <p>Open the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0configuration file.</p> </li> <li> <p>Find the\u00a0<code>[ldap]</code>\u00a0section.</p> </li> <li> <p>In the\u00a0<code>[ldap]</code>\u00a0section, set the\u00a0<code>use_tls</code>\u00a0configuration key to\u00a0<code>True</code>. Doing so will enable TLS.</p> </li> <li> <p>Configure the Identity service to use your certificate authorities file. To do so, set the\u00a0<code>tls_cacertfile</code>\u00a0configuration key in the\u00a0<code>ldap</code>\u00a0section to the certificate authorities file\u2019s path.</p> </li> </ol> <p>Note You can also set the\u00a0<code>tls_cacertdir</code>\u00a0(also in the\u00a0<code>ldap</code>\u00a0section) to the directory where all certificate authorities files are kept. If both\u00a0<code>tls_cacertfile</code>\u00a0and\u00a0<code>tls_cacertdir</code>\u00a0are set, then the latter will be ignored.</p> <p>5. Specify what client certificate checks to perform on incoming TLS sessions from the LDAP server. To do so, set the\u00a0<code>tls_req_cert</code>\u00a0configuration key in the\u00a0<code>[ldap]</code>\u00a0section to\u00a0<code>demand</code>,\u00a0<code>allow</code>, or\u00a0<code>never</code>:</p> <ul> <li> <p><code>demand</code>\u00a0\u2013 The LDAP server always receives certificate requests. The session terminates if no certificate is provided, or if the certificate provided cannot be verified against the existing certificate authorities file.</p> </li> <li> <p><code>allow</code>\u00a0\u2013 The LDAP server always receives certificate requests. The session will proceed as normal even if a certificate is not provided. If a certificate is provided but it cannot be verified against the existing certificate authorities file, the certificate will be ignored and the session will proceed as normal.</p> </li> <li> <p><code>never</code>\u00a0\u2013 A certificate will never be requested.</p> </li> </ul> <p>When you have finished configuration, restart the OpenStack Identity service</p> <p>Note If you are unable to connect to LDAP via OpenStack Identity, or observe a\u00a0SERVER DOWN\u00a0error, set the\u00a0<code>TLS_CACERT</code>\u00a0in\u00a0<code>/etc/ldap/ldap.conf</code>\u00a0to the same value specified in the\u00a0<code>[ldap]\u00a0tls_certificate</code>\u00a0section of\u00a0<code>keystone.conf</code>.</p> <p>On distributions that include openstack-config, you can configure TLS encryption on LDAP traffic by running the following commands instead.</p> <pre><code># openstack-config --set /etc/keystone/keystone.conf \\\n  ldap use_tls True\n# openstack-config --set /etc/keystone/keystone.conf \\\n  ldap tls_cacertfile ``CA_FILE``\n# openstack-config --set /etc/keystone/keystone.conf \\\n  ldap tls_req_cert ``CERT_BEHAVIOR``\n</code></pre> <p>Where:</p> <ul> <li> <p><code>CA_FILE</code>\u00a0is the absolute path to the certificate authorities file that should be used to encrypt LDAP traffic.</p> </li> <li> <p><code>CERT_BEHAVIOR</code>\u00a0specifies what client certificate checks to perform on an incoming TLS session from the LDAP server (<code>demand</code>,\u00a0<code>allow</code>, or\u00a0<code>never</code>).</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#caching-layer","title":"Caching layer","text":"<p>OpenStack Identity supports a caching layer that is above the configurable subsystems (for example, token). This gives you the flexibility to setup caching for all or some subsystems. OpenStack Identity uses the\u00a0oslo.cache\u00a0library which allows flexible cache back ends. The majority of the caching configuration options are set in the\u00a0<code>[cache]</code>\u00a0section of the\u00a0<code>/etc/keystone/keystone.conf</code>\u00a0file. The\u00a0<code>enabled</code>\u00a0option of the\u00a0<code>[cache]</code>\u00a0section must be set to\u00a0<code>True</code>\u00a0in order for any subsystem to cache responses. Each section that has the capability to be cached will have a\u00a0<code>caching</code>\u00a0boolean value that toggles caching behavior of that particular subsystem.</p> <p>So to enable only the token back end caching, set the values as follows:</p> <pre><code>[cache]\nenabled=true\n\n[catalog]\ncaching=false\n\n[domain_config]\ncaching=false\n\n[federation]\ncaching=false\n\n[resource]\ncaching=false\n\n[revoke]\ncaching=false\n\n[role]\ncaching=false\n\n[token]\ncaching=true\n</code></pre> <p>Note Each subsystem is configured to cache by default. However, the global toggle for caching defaults to\u00a0<code>False</code>. A subsystem is only able to cache responses if the global toggle is enabled.</p> <p>Current functional back ends are:<code>dogpile.cache.null</code></p> <p>A \u201cnull\u201d backend that effectively disables all cache operations.(Default)<code>dogpile.cache.memcached</code></p> <p>Memcached back end using the standard\u00a0<code>python-memcached</code>\u00a0library.<code>dogpile.cache.pylibmc</code></p> <p>Memcached back end using the\u00a0<code>pylibmc</code>\u00a0library.<code>dogpile.cache.bmemcached</code></p> <p>Memcached using the\u00a0<code>python-binary-memcached</code>\u00a0library.<code>dogpile.cache.redis</code></p> <p>Redis back end.<code>dogpile.cache.dbm</code></p> <p>Local DBM file back end.<code>dogpile.cache.memory</code></p> <p>In-memory cache, not suitable for use outside of testing as it does not cleanup its internal cache on cache expiration and does not share cache between processes. This means that caching and cache invalidation will not be consistent or reliable.<code>dogpile.cache.memory_pickle</code></p> <p>In-memory cache, but serializes objects with pickle lib. It\u2019s not suitable for use outside of testing. The reason is the same with\u00a0<code>dogpile.cache.memoryoslo_cache.mongo</code></p> <p>MongoDB as caching back end.<code>oslo_cache.memcache_pool</code></p> <p>Memcached backend that does connection pooling.<code>oslo_cache.etcd3gw</code></p> <p>Uses etcd 3.x for storage.<code>oslo_cache.dict</code></p> <p>A DictCacheBackend based on dictionary, not suitable for use outside of testing as it does not share cache between processes.This means that caching and cache invalidation will not be consistent or reliable.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#caching-for-tokens-and-tokens-validation","title":"Caching for tokens and tokens validation","text":"<p>The token subsystem is OpenStack Identity\u2019s most heavily used API. As a result, all types of tokens benefit from caching, including Fernet tokens. Although Fernet tokens do not need to be persisted, they should still be cached for optimal token validation performance.</p> <p>The token system has a separate\u00a0<code>cache_time</code>\u00a0configuration option, that can be set to a value above or below the global\u00a0<code>expiration_time</code>\u00a0default, allowing for different caching behavior from the other systems in OpenStack Identity. This option is set in the\u00a0<code>[token]</code>\u00a0section of the configuration file.</p> <p>The token revocation list cache time is handled by the configuration option\u00a0<code>revocation_cache_time</code>\u00a0in the\u00a0<code>[token]</code>\u00a0section. The revocation list is refreshed whenever a token is revoked. It typically sees significantly more requests than specific token retrievals or token validation calls.</p> <p>Here is a list of actions that are affected by the cached time:</p> <ul> <li> <p>getting a new token</p> </li> <li> <p>revoking tokens</p> </li> <li> <p>validating tokens</p> </li> <li> <p>checking v3 tokens</p> </li> </ul> <p>The delete token API calls invalidate the cache for the tokens being acted upon, as well as invalidating the cache for the revoked token list and the validate/check token calls.</p> <p>Token caching is configurable independently of the\u00a0<code>revocation_list</code>\u00a0caching. Lifted expiration checks from the token drivers to the token manager. This ensures that cached tokens will still raise a\u00a0<code>TokenNotFound</code>\u00a0flag when expired.</p> <p>For cache consistency, all token IDs are transformed into the short token hash at the provider and token driver level. Some methods have access to the full ID (PKI Tokens), and some methods do not. Cache invalidation is inconsistent without token ID normalization.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#caching-for-non-token-resources","title":"Caching for non-token resources","text":"<p>Various other keystone components have a separate\u00a0<code>cache_time</code>\u00a0configuration option, that can be set to a value above or below the global\u00a0<code>expiration_time</code>\u00a0default, allowing for different caching behavior from the other systems in Identity service. This option can be set in various sections (for example,\u00a0<code>[role]</code>\u00a0and\u00a0<code>[resource]</code>) of the configuration file. The create, update, and delete actions for domains, projects and roles will perform proper invalidations of the cached methods listed above.</p> <p>For more information about the different back ends (and configuration options), see:</p> <ul> <li> <p>dogpile.cache.memory</p> </li> <li> <p>dogpile.cache.memcached</p> </li> </ul> <p>Note</p> <p>The memory back end is not suitable for use in a production environment.</p> <ul> <li> <p>dogpile.cache.redis</p> </li> <li> <p>dogpile.cache.dbm</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#cache-invalidation","title":"Cache invalidation","text":"<p>A common concern with caching is relaying inaccurate information after updating or deleting a resource. Most subsystems within OpenStack Identity invalidate specific cache entries once they have changed. In cases where a specific cache entry cannot be invalidated from the cache, the cache region will be invalidated instead. This invalidates all entries within the cache to prevent returning stale or misleading data. A subsequent request for the resource will be fully processed and cached.</p> <p>Warning Be aware that if a read-only back end is in use for a particular subsystem, the cache will not immediately reflect changes performed through the back end. Any given change may take up to the\u00a0<code>cache_time</code>\u00a0(if set in the subsystem section of the configuration) or the global\u00a0<code>expiration_time</code>\u00a0(set in the\u00a0<code>[cache]</code>\u00a0section of the configuration) before it is reflected. If this type of delay is an issue, we recommend disabling caching for that particular subsystem.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#configure-the-memcached-back-end-example","title":"Configure the Memcached back end example","text":"<p>The following example shows how to configure the memcached back end:</p> <pre><code>[cache]\n\nenabled = true\nbackend = dogpile.cache.memcached\nbackend_argument = url:127.0.0.1:11211\n</code></pre> <p>You need to specify the URL to reach the\u00a0<code>memcached</code>\u00a0instance with the\u00a0<code>backend_argument</code>\u00a0parameter.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#verbose-cache-logging","title":"Verbose cache logging","text":"<p>We do not recommend using verbose cache logging by default in production systems since it\u2019s extremely noisy. However, you may need to debug cache issues. One way to see how keystone is interacting with a cache backend is to enhance logging. The following configuration will aggregate oslo and dogpile logs into keystone\u2019s log file with increased verbosity:</p> <pre><code>[DEFAULT]\ndefault_log_levels = oslo.cache=DEBUG,dogpile.core.dogpile=DEBUG\n\n[cache]\ndebug_cache_backend = True\n</code></pre> <p>These logs will include cache hits and misses, making it easier to diagnose cache configuration and connectivity issues.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#security-compliance-and-pci-dss","title":"Security compliance and PCI-DSS","text":"<p>As of the Newton release, the Identity service contains additional security compliance features, specifically to satisfy Payment Card Industry \u2013 Data Security Standard (PCI-DSS) v3.1 requirements. See\u00a0Security Hardening PCI-DSS\u00a0for more information on PCI-DSS.</p> <p>Security compliance features are disabled by default and most of the features only apply to the SQL backend for the identity driver. Other identity backends, such as LDAP, should implement their own security controls.</p> <p>Enable these features by changing the configuration settings under the\u00a0<code>[security_compliance]</code>\u00a0section in\u00a0<code>keystone.conf</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#setting-an-account-lockout-threshold","title":"Setting an account lockout threshold","text":"<p>The account lockout feature limits the number of incorrect password attempts. If a user fails to authenticate after the maximum number of attempts, the service disables the user. Users can be re-enabled by explicitly setting the enable user attribute with the update user\u00a0v3\u00a0API call.</p> <p>You set the maximum number of failed authentication attempts by setting the\u00a0<code>lockout_failure_attempts</code>:</p> <pre><code>[security_compliance]\nlockout_failure_attempts = 6\n</code></pre> <p>You set the number of minutes a user would be locked out by setting the\u00a0<code>lockout_duration</code>\u00a0in seconds:</p> <pre><code>[security_compliance]\nlockout_duration = 1800\n</code></pre> <p>If you do not set the\u00a0<code>lockout_duration</code>, users will be locked out indefinitely until the user is explicitly enabled via the API.</p> <p>You can ensure specific users are never locked out. This can be useful for service accounts or administrative users. You can do this by setting the user option for\u00a0ignore_lockout_failure_attempts.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#disabling-inactive-users","title":"Disabling inactive users","text":"<p>PCI-DSS 8.1.4 requires that inactive user accounts be removed or disabled within 90 days. You can achieve this by setting the\u00a0<code>disable_user_account_days_inactive</code>:</p> <pre><code>[security_compliance]\ndisable_user_account_days_inactive = 90\n</code></pre> <p>This above example means that users that have not authenticated (inactive) for the past 90 days are automatically disabled. Users can be re-enabled by explicitly setting the enable user attribute via the API.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#force-users-to-change-password-upon-first-use","title":"Force users to change password upon first use","text":"<p>PCI-DSS 8.2.6 requires users to change their password for first time use and upon an administrative password reset. Within the identity\u00a0user API,\u00a0create user\u00a0and\u00a0update user\u00a0are considered administrative password changes. Whereas,\u00a0change password for user\u00a0is a self-service password change. Once this feature is enabled, new users, and users that have had their password reset, will be required to change their password upon next authentication (first use), before being able to access any services.</p> <p>Prior to enabling this feature, you may want to exempt some users that you do not wish to be required to change their password. You can mark a user as exempt by setting the user options attribute\u00a0ignore_change_password_upon_first_use.</p> <p>Warning Failure to mark service users as exempt from this requirement will result in your service account passwords becoming expired after being reset.</p> <p>When ready, you can configure it so that users are forced to change their password upon first use by setting\u00a0<code>change_password_upon_first_use</code>:</p> <pre><code>[security_compliance]\nchange_password_upon_first_use = True\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#configuring-password-expiration","title":"Configuring password expiration","text":"<p>Passwords can be configured to expire within a certain number of days by setting the\u00a0<code>password_expires_days</code>:</p> <pre><code>[security_compliance]\npassword_expires_days = 90\n</code></pre> <p>Once set, any new password changes have an expiration date based on the date/time of the password change plus the number of days defined here. Existing passwords will not be impacted. If you want existing passwords to have an expiration date, you would need to run a SQL script against the password table in the database to update the expires_at column.</p> <p>If there exists a user whose password you do not want to expire, keystone supports setting that via the user option\u00a0ignore_password_expiry.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#configuring-password-strength-requirements","title":"Configuring password strength requirements","text":"<p>You can set password strength requirements, such as requiring numbers in passwords or setting a minimum password length, by adding a regular expression to the\u00a0<code>password_regex</code>\u00a0setting:</p> <pre><code>[security_compliance]\npassword_regex = ^(?=.*\\d)(?=.*[a-zA-Z]).{7,}$\n</code></pre> <p>The above example is a regular expression that requires a password to have:</p> <ul> <li> <p>One (1) letter</p> </li> <li> <p>One (1) digit</p> </li> <li> <p>Minimum length of seven (7) characters</p> </li> </ul> <p>If you do set the\u00a0<code>password_regex</code>, you should provide text that describes your password strength requirements. You can do this by setting the\u00a0<code>password_regex_description</code>:</p> <pre><code>[security_compliance]`\npassword_regex_description = Passwords must contain at least 1 letter, 1\n                             digit, and be a minimum length of 7\n                             characters.\n</code></pre> <p>It is imperative that the\u00a0<code>password_regex_description</code>\u00a0matches the actual regex. If the\u00a0<code>password_regex</code>\u00a0and the\u00a0<code>password_regex_description</code>\u00a0do not match, it will cause user experience to suffer since this description will be returned to users to explain why their requested password was insufficient.</p> <p>Note You must ensure the\u00a0<code>password_regex_description</code>\u00a0accurately and completely describes the\u00a0<code>password_regex</code>. If the two options are out of sync, the help text could inaccurately describe the password requirements being applied to the password. This would lead to a poor user experience.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#requiring-a-unique-password-history","title":"Requiring a unique password history","text":"<p>The password history requirements controls the number of passwords for a user that must be unique before an old password can be reused. You can enforce this by setting the\u00a0<code>unique_last_password_count</code>:</p> <pre><code>[security_compliance]\nunique_last_password_count= 5\n</code></pre> <p>The above example does not allow a user to create a new password that is the same as any of their last four previous passwords.</p> <p>Similarly, you can set the number of days that a password must be used before the user can change it by setting the\u00a0<code>minimum_password_age</code>:</p> <pre><code>[security_compliance]\nminimum_password_age = 1\n</code></pre> <p>In the above example, once a user changes their password, they would not be able to change it again for one day. This prevents users from changing their passwords immediately in order to wipe out their password history and reuse an old password.</p> <p>Note When you set\u00a0<code>password_expires_days</code>, the value for the\u00a0<code>minimum_password_age</code>\u00a0should be less than the\u00a0<code>password_expires_days</code>. Otherwise, users would not be able to change their passwords before they expire.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#prevent-self-service-password-changes","title":"Prevent Self-Service Password Changes","text":"<p>If there exists a user who should not be able to change her own password via the keystone password change API, keystone supports setting that via the user option\u00a0lock_password.</p> <p>This is typically used in the case where passwords are managed externally to keystone.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#performance-and-scaling","title":"Performance and scaling","text":"<p>Before you begin tuning Keystone for performance and scalability, you should first know that Keystone is just a two tier horizontally-scalable web application, and the most effective methods for scaling it are going to be the same as for any other similarly designed web application: give it more processes, more memory, scale horizontally, and load balance the result.</p> <p>With that said, there are many opportunities for tuning the performance of Keystone, many of which are actually trade-offs between performance and security that you need to judge for yourself, and tune accordingly.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#keystone-configuration-options-that-affect-performance","title":"Keystone configuration options that affect performance","text":"<p>These are all of the options in\u00a0<code>keystone.conf</code>\u00a0that have a direct impact on performance. See the help descriptions for these options for more specific details on how and why you might want to tune these options for yourself.</p> <ul> <li> <p><code>[DEFAULT]\u00a0max_project_tree_depth</code>: Reduce this number to increase performance, increase this number to cater to more complicated hierarchical multitenancy use cases.</p> </li> <li> <p><code>[DEFAULT]\u00a0max_password_length</code>: Reduce this number to increase performance, increase this number to allow for more secure passwords.</p> </li> <li> <p><code>[cache]\u00a0enable</code>: Enable this option to increase performance, but you also need to configure other options in the\u00a0<code>[cache]</code>\u00a0section to actually utilize caching.</p> </li> <li> <p><code>[token]\u00a0provider</code>: All supported token providers have been primarily driven by performance considerations. UUID and Fernet both require online validation (cacheable HTTP calls back to keystone to validate tokens). Fernet has the highest scalability characteristics overall, but requires more work to validate, and therefore enabling caching (<code>[cache]\u00a0enable</code>) is absolutely critical.</p> </li> <li> <p><code>[fernet]\u00a0max_active_keys</code>: If you\u2019re using Fernet tokens, decrease this option to improve performance, increase this option to support more advanced key rotation strategies.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#keystonemiddleware-configuration-options-that-affect-performance","title":"Keystonemiddleware configuration options that affect performance","text":"<p>This configuration actually lives in the Paste pipelines of services consuming token validation from keystone (i.e.: nova, cinder, swift, etc.).</p> <ul> <li> <p><code>cache</code>: When keystone\u2019s\u00a0auth_token\u00a0middleware is deployed with a swift cache, use this option to have\u00a0auth_token\u00a0middleware share a caching backend with swift. Otherwise, use the\u00a0<code>memcached_servers</code>\u00a0option instead.</p> </li> <li> <p><code>memcached_servers</code>: Set this option to share a cache across\u00a0<code>keystonemiddleware.auth_token</code>\u00a0processes.</p> </li> <li> <p><code>token_cache_time</code>: Increase this option to improve performance, decrease this option to respond to token revocation events more quickly (thereby increasing security).</p> </li> <li> <p><code>revocation_cache_time</code>: Increase this option to improve performance, decrease this option to respond to token revocation events more quickly (thereby increasing security).</p> </li> <li> <p><code>memcache_security_strategy</code>: Do not set this option to improve performance, but set it to improve security where you\u2019re sharing memcached with other processes.</p> </li> <li> <p><code>include_service_catalog</code>: Disable this option to improve performance, if the protected service does not require a service catalog.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#url-safe-naming-of-projects-and-domains","title":"URL safe naming of projects and domains","text":"<p>In the future, keystone may offer the ability to identify a project in a hierarchy via a URL style of naming from the root of the hierarchy (for example specifying \u2018projectA/projectB/projectC\u2019 as the project name in an authentication request). In order to prepare for this, keystone supports the optional ability to ensure both projects and domains are named without including any of the reserved characters specified in section 2.2 of\u00a0rfc3986.</p> <p>The safety of the names of projects and domains can be controlled via two configuration options:</p> <pre><code>[resource]\nproject_name_url_safe = off\ndomain_name_url_safe = off\n</code></pre> <p>When set to\u00a0<code>off</code>\u00a0(which is the default), no checking is done on the URL safeness of names. When set to\u00a0<code>new</code>, an attempt to create a new project or domain with an unsafe name (or update the name of a project or domain to be unsafe) will cause a status code of 400 (Bad Request) to be returned. Setting the configuration option to\u00a0<code>strict</code>\u00a0will, in addition to preventing the creation and updating of entities with unsafe names, cause an authentication attempt which specifies a project or domain name that is unsafe to return a status code of 401 (Unauthorized).</p> <p>It is recommended that installations take the steps necessary to where they can run with both options set to\u00a0<code>strict</code>\u00a0as soon as is practical.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#limiting-list-return-size","title":"Limiting list return size","text":"<p>Keystone provides a method of setting a limit to the number of entities returned in a collection, which is useful to prevent overly long response times for list queries that have not specified a sufficiently narrow filter. This limit can be set globally by setting\u00a0<code>list_limit</code>\u00a0in the default section of\u00a0<code>keystone.conf</code>, with no limit set by default. Individual driver sections may override this global value with a specific limit, for example:</p> <pre><code>[resource]\nlist_limit = 100\n</code></pre> <p>If a response to\u00a0<code>list_{entity}</code>\u00a0call has been truncated, then the response status code will still be 200 (OK), but the\u00a0<code>truncated</code>\u00a0attribute in the collection will be set to\u00a0<code>true</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#endpoint-filtering","title":"Endpoint Filtering","text":"<p>Endpoint Filtering enables creation of ad-hoc catalogs for each project-scoped token request.</p> <p>Configure the endpoint filter catalog driver in the\u00a0<code>[catalog]</code>\u00a0section. For example:</p> <pre><code>[catalog]\ndriver = catalog_sql\n</code></pre> <p>In the\u00a0<code>[endpoint_filter]</code>\u00a0section, set\u00a0<code>return_all_endpoints_if_no_filter</code>\u00a0to\u00a0<code>False</code>\u00a0to return an empty catalog if no associations are made. For example:</p> <pre><code>[endpoint_filter]\nreturn_all_endpoints_if_no_filter = False\n</code></pre> <p>See\u00a0API Specification for Endpoint Filtering\u00a0for the details of API definition.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Keystone_Configuration/#endpoint-policy","title":"Endpoint Policy","text":"<p>The Endpoint Policy feature provides associations between service endpoints and policies that are already stored in the Identity server and referenced by a policy ID.</p> <p>Configure the endpoint policy backend driver in the\u00a0<code>[endpoint_policy]</code>\u00a0section. For example:</p> <pre><code>[endpoint_policy]\ndriver = sql\n</code></pre> <p>See\u00a0API Specification for Endpoint Policy\u00a0for the details of API definition.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/","title":"Manage projects, users, and roles","text":"<p>As an administrator, you manage projects, users, and roles. Projects are organizational units in the cloud to which you can assign users. Projects are also known as\u00a0tenants\u00a0or\u00a0accounts. Users can be members of one or more projects. Roles define which actions users can perform. You assign roles to user-project pairs.</p> <p>You can define actions for OpenStack service roles in the\u00a0<code>/etc/PROJECT/policy.yaml</code>\u00a0files. For example, define actions for Compute service roles in the\u00a0<code>/etc/nova/policy.yaml</code>\u00a0file.</p> <p>You can manage projects, users, and roles independently from each other.</p> <p>During cloud set up, the operator defines at least one project, user, and role.</p> <p>You can add, update, and delete projects and users, assign users to one or more projects, and change or remove the assignment. To enable or temporarily disable a project or user, update that project or user. You can also change quotas at the project level.</p> <p>Before you can delete a user account, you must remove the user account from its primary project.</p> <p>Before you can run client commands, you need to have a cloud config file or you can download and source an OpenStack RC file. See the\u00a0Configuration\u00a0documentation from the python-openstackclient project for more details.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#projects","title":"Projects","text":"<p>A project is a group of zero or more users. In Compute, a project owns virtual machines. In Object Storage, a project owns containers. Users can be associated with more than one project. Each project and user pairing can have a role associated with it.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#list-projects","title":"List projects","text":"<p>List all projects with their ID, name, and whether they are enabled or disabled:</p> <pre><code>$ openstack project list\n+----------------------------------+--------------------+\n| ID                               | Name               |\n+----------------------------------+--------------------+\n| f7ac731cc11f40efbc03a9f9e1d1d21f | admin              |\n| c150ab41f0d9443f8874e32e725a4cc8 | alt_demo           |\n| a9debfe41a6d4d09a677da737b907d5e | demo               |\n| 9208739195a34c628c58c95d157917d7 | invisible_to_admin |\n| 3943a53dc92a49b2827fae94363851e1 | service            |\n| 80cab5e1f02045abad92a2864cfd76cb | test_project       |\n+----------------------------------+--------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#create-a-project","title":"Create a project","text":"<p>Create a project named\u00a0<code>new-project</code>:</p> <pre><code>$ openstack project create --description 'my new project' new-project \\\n  --domain default\n+-------------+----------------------------------+\n| Field       | Value                            |\n+-------------+----------------------------------+\n| description | my new project                   |\n| domain_id   | e601210181f54843b51b3edff41d4980 |\n| enabled     | True                             |\n| id          | 1a4a0618b306462c9830f876b0bd6af2 |\n| is_domain   | False                            |\n| name        | new-project                      |\n| parent_id   | e601210181f54843b51b3edff41d4980 |\n| tags        | []                               |\n+-------------+----------------------------------+\n</code></pre> <ul> <li>Creating a project without using a domain scoped token, i.e. using a project scoped token or a system scoped token, and also without specifying a domain or domain_id, the project will automatically be created on the default domain.</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#update-a-project","title":"Update a project","text":"<p>Specify the project ID to update a project. You can update the name, description, and enabled status of a project.</p> <ul> <li>To temporarily disable a project:</li> </ul> <pre><code>$ openstack project set PROJECT\\_ID --disable\n</code></pre> <ul> <li>To enable a disabled project:</li> </ul> <pre><code>$ openstack project set PROJECT\\_ID --enable\n</code></pre> <ul> <li>To update the name of a project:</li> </ul> <pre><code>$ openstack project set PROJECT\\_ID --name project-new\n</code></pre> <ul> <li>To verify your changes, show information for the updated project:</li> </ul> <pre><code>$ openstack project show PROJECT_ID\n+-------------+----------------------------------+\n| Field       | Value                            |\n+-------------+----------------------------------+\n| description | my new project                   |\n| domain_id   | e601210181f54843b51b3edff41d4980 |\n| enabled     | True                             |\n| id          | 0b0b995694234521bf93c792ed44247f |\n| is_domain   | False                            |\n| name        | new-project                      |\n| parent_id   | e601210181f54843b51b3edff41d4980 |\n| tags        | []                               |\n+-------------+----------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#delete-a-project","title":"Delete a project","text":"<p>Specify the project ID to delete a project:</p> <pre><code>$ openstack project delete PROJECT\\_ID\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#users","title":"Users","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#list-users","title":"List users","text":"<p>List all users:</p> <pre><code>$ openstack user list\n+----------------------------------+----------+\n| ID                               | Name     |\n+----------------------------------+----------+\n| 352b37f5c89144d4ad0534139266d51f | admin    |\n| 86c0de739bcb4802b8dc786921355813 | demo     |\n| 32ec34aae8ea432e8af560a1cec0e881 | glance   |\n| 7047fcb7908e420cb36e13bbd72c972c | nova     |\n+----------------------------------+----------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#create-a-user","title":"Create a user","text":"<p>To create a user, you must specify a name. Optionally, you can specify a project ID, password, and email address. It is recommended that you include the project ID and password because the user cannot log in to the dashboard without this information.</p> <p>Create the\u00a0<code>new-user</code>\u00a0user:</p> <pre><code>$ openstack user create --project new-project --password PASSWORD new-user\n+------------+----------------------------------+\n| Field      | Value                            |\n+------------+----------------------------------+\n| email      | None                             |\n| enabled    | True                             |\n| id         | 6322872d9c7e445dbbb49c1f9ca28adc |\n| name       | new-user                         |\n| project_id | 0b0b995694234521bf93c792ed44247f |\n| username   | new-user                         |\n+------------+----------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#update-a-user","title":"Update a user","text":"<p>You can update the name, email address, and enabled status for a user.</p> <ul> <li>To temporarily disable a user account:</li> </ul> <pre><code>$ openstack user set USER\\_NAME --disable\n</code></pre> <p>If you disable a user account, the user cannot log in to the dashboard. However, data for the user account is maintained, so you can enable the user at any time.</p> <ul> <li>To enable a disabled user account:</li> </ul> <pre><code>$ openstack user set USER\\_NAME --enable\n</code></pre> <ul> <li>To change the name and description for a user account:</li> </ul> <pre><code>$ openstack user set USER\\_NAME --name user-new --email new- user@example.com\n</code></pre> <p>User has been updated.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#delete-a-user","title":"Delete a user","text":"<p>Delete a specified user account:</p> <p>$ openstack user delete USER_NAME</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#roles-and-role-assignments","title":"Roles and role assignments","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#list-available-roles","title":"List available roles","text":"<p>List the available roles:</p> <pre><code>$ openstack role list\n+----------------------------------+---------------+\n| ID                               | Name          |\n+----------------------------------+---------------+\n| 71ccc37d41c8491c975ae72676db687f | member        |\n| 149f50a1fe684bfa88dae76a48d26ef7 | ResellerAdmin |\n| 9fe2ff9ee4384b1894a90878d3e92bab | reader        |\n| 6ecf391421604da985db2f141e46a7c8 | admin         |\n| deb4fffd123c4d02a907c2c74559dccf | anotherrole   |\n+----------------------------------+---------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#create-a-role","title":"Create a role","text":"<p>Users can be members of multiple projects. To assign users to multiple projects, define a role and assign that role to a user-project pair.</p> <p>Create the\u00a0<code>new-role</code>\u00a0role:</p> <pre><code>$ openstack role create new-role\n+-------------+----------------------------------+\n| Field       | Value                            |\n+-------------+----------------------------------+\n| description | None                             |\n| domain_id   | None                             |\n| id          | a34425c884c74c8881496dc2c2e84ffc |\n| name        | new-role                         |\n+-------------+----------------------------------+\n</code></pre> <p>Note If you are using identity v3, you may need to use the\u00a0<code>--domain</code>\u00a0option with a specific domain name.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#assign-a-role","title":"Assign a role","text":"<p>To assign a user to a project, you must assign the role to a user-project pair.</p> <ol> <li>Assign a role to a user-project pair:</li> </ol> <pre><code>$ openstack role add --user USER\\_NAME --project PROJECT\\_NAME ROLE\\_NAME\n</code></pre> <p>For example, assign the\u00a0<code>new-role</code>\u00a0role to the\u00a0<code>demo</code>\u00a0user and\u00a0<code>test-project</code>\u00a0project pair:</p> <pre><code>$ openstack role add --user demo --project test-project new-role\n</code></pre> <ol> <li>Verify the role assignment:</li> </ol> <pre><code>$ openstack role assignment list --user USER_NAME \\\n  --project PROJECT_NAME --names\n+-------------+--------------+-------+--------------+--------+--------+-----------+\n| Role        | User         | Group | Project      | Domain | System | Inherited |\n+-------------+--------------+-------+--------------+--------+--------+-----------+\n| new-role    | demo@Default |       | demo@Default |        |        | False     |\n| member      | demo@Default |       | demo@Default |        |        | False     |\n| anotherrole | demo@Default |       | demo@Default |        |        | False     |\n+-------------+--------------+-------+--------------+--------+--------+-----------+\n</code></pre> <p>Note Before the Newton release, users would run the\u00a0openstack role list \u2013user USER_NAME \u2013project TENANT_ID\u00a0command to verify the role assignment.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#view-role-details","title":"View role details","text":"<p>View details for a specified role:</p> <pre><code>$ openstack role show ROLE_NAME\n+-------------+----------------------------------+\n| Field       | Value                            |\n+-------------+----------------------------------+\n| description | None                             |\n| domain_id   | None                             |\n| id          | a34425c884c74c8881496dc2c2e84ffc |\n| name        | new-role                         |\n+-------------+----------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#remove-a-role","title":"Remove a role","text":"<p>Remove a role from a user-project pair:</p> <ol> <li>Run the\u00a0openstack role remove\u00a0command:</li> </ol> <pre><code>$ openstack role remove --user USER\\_NAME --project PROJECT\\_NAME ROLE\\_NAME\n</code></pre> <ol> <li>Verify the role removal:</li> </ol> <pre><code>$ openstack role assignment list --user USER\\_NAME --project PROJECT\\_NAME --names\n</code></pre> <p>If the role was removed, the command output omits the removed role.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#creating-implied-roles","title":"Creating implied roles","text":"<p>It is possible to build role hierarchies by having roles imply other roles. These are called implied roles, or role inference rules.</p> <p>To illustrate the capability, let\u2019s have the\u00a0<code>admin</code>\u00a0role imply the\u00a0<code>member</code>\u00a0role. In this example, if a user was assigned the prior role, which in this case is the\u00a0<code>admin</code>\u00a0role, they would also get the\u00a0<code>member</code>\u00a0role that it implies.</p> <pre><code>$ openstack implied role create admin --implied-role member\n+------------+----------------------------------+\n| Field      | Value                            |\n+------------+----------------------------------+\n| implies    | 71ccc37d41c8491c975ae72676db687f |\n| prior_role | 29c09e68e6f741afa952a837e29c700b |\n+------------+----------------------------------+\n</code></pre> <p>Note Role implications only go one way, from a \u201cprior\u201d role to an \u201cimplied\u201d role. Therefore assigning a user the\u00a0<code>member</code>\u00a0will not grant them the\u00a0<code>admin</code>\u00a0role.</p> <p>This makes it easy to break up large roles into smaller pieces, allowing for fine grained permissions, while still having an easy way to assign all the pieces as if they were a single one. For example, you can have a\u00a0<code>member</code>\u00a0role imply\u00a0<code>compute_member</code>,\u00a0<code>network_member</code>, and\u00a0<code>volume_member</code>, and then assign either the full-blown\u00a0<code>member</code>\u00a0role to users or any one of the subsets.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#listing-implied-roles","title":"Listing implied roles","text":"<p>To list implied roles:</p> <pre><code>$ openstack implied role list\n| Prior Role ID                         | Prior Role Name | Implied Role ID                         | Implied Role Name |\n|---------------------------------------|-----------------|-----------------------------------------|-------------------|\n| 29c09e68e6f741afa952a837e29c700b      | admin           | 71ccc37d41c8491c975ae72676db687f        | member            |\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Manage_Projects%2C_Users%2C_and_Roles/#deleting-implied-roles","title":"Deleting implied roles","text":"<p>To delete a role inference rule:</p> <pre><code>$ openstack implied role delete admin --implied-role member\n</code></pre> <p>Note Deleting an implied role removes the role inference rule. It does not delete the prior or implied role. Therefore if a user was assigned the prior role, they will no longer have the roles that it implied.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Middleware_Audit/","title":"Middleware Audit","text":"<p>The Keystone middleware library provides an optional WSGI middleware filter which allows the ability to audit API requests for each component of OpenStack.</p> <p>The audit middleware filter utilises environment variables to build the CADF event.</p> <p></p> <p>The figure above shows the middleware in Nova\u2019s pipeline.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Middleware_Audit/#enabling-audit-middleware","title":"Enabling audit middleware","text":"<p>To enable auditing,\u00a0oslo.messaging\u00a0should be installed. If not, the middleware will log the audit event instead. Auditing can be enabled for a specific project by editing the project\u2019s api-paste.ini file to include the following filter definition:</p> <pre><code>[filter:audit]\npaste.filter_factory = keystonemiddleware.audit:filter_factory\naudit_map_file = /etc/nova/api_audit_map.conf\n</code></pre> <p>The filter should be included after Keystone middleware\u2019s auth_token middleware so it can utilise environment variables set by auth_token. Below is an example using Nova\u2019s WSGI pipeline:</p> <pre><code>[composite:openstack_compute_api_v2]\nuse = call:nova.api.auth:pipeline_factory\nnoauth = faultwrap sizelimit noauth ratelimit osapi_compute_app_v2\nkeystone = faultwrap sizelimit authtoken keystonecontext ratelimit audit osapi_compute_app_v2\nkeystone_nolimit = faultwrap sizelimit authtoken keystonecontext audit osapi_compute_app_v2\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Middleware_Audit/#configure-audit-middleware","title":"Configure audit middleware","text":"<p>To properly audit api requests, the audit middleware requires an api_audit_map.conf to be defined. The project\u2019s corresponding api_audit_map.conf file is included in the\u00a0pyCADF library.</p> <p>The location of the mapping file should be specified explicitly by adding the path to the \u2018audit_map_file\u2019 option of the filter definition:</p> <pre><code>[filter:audit]\npaste.filter_factory = keystonemiddleware.audit:filter_factory\naudit_map_file = /etc/nova/api_audit_map.conf\n</code></pre> <p>Additional options can be set:</p> <pre><code>[filter:audit]\npaste.filter_factory = pycadf.middleware.audit:filter_factory\naudit_map_file = /etc/nova/api_audit_map.conf\nservice_name = test # opt to set HTTP_X_SERVICE_NAME environ variable\nignore_req_list = GET,POST # opt to ignore specific requests\n</code></pre> <p>Audit middleware can be configured to use its own exclusive notification driver and topic(s) value. This can be useful when the service is already using oslo messaging notifications and wants to use a different driver for auditing e.g. service has existing notifications sent to queue via \u2018messagingv2\u2019 and wants to send audit notifications to a log file via \u2018log\u2019 driver. Example shown below:</p> <pre><code>[audit_middleware_notifications]\ndriver = log\n</code></pre> <p>When audit events are sent via \u2018messagingv2\u2019 or \u2018messaging\u2019, middleware can specify a transport URL if its transport URL needs to be different from the service\u2019s own messaging transport setting. Other Transport related settings are read from oslo messaging sections defined in service configuration e.g. \u2018oslo_messaging_rabbit\u2019. Example shown below:</p> <pre><code>[audit_middleware_notifications]\ndriver = messaging\ntransport_url = rabbit://user2:passwd@host:5672/another_virtual_host\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/","title":"Primer","text":"<p>Like most OpenStack services, keystone protects its API using role-based access control (RBAC).</p> <p>Users can access different APIs depending on the roles they have on a project, domain, or system, which we refer to as scope.</p> <p>As of the Rocky release, keystone provides three roles called\u00a0<code>admin</code>,\u00a0<code>member</code>, and\u00a0<code>reader</code>\u00a0by default. Operators can grant these roles to any actor (e.g., group or user) on any scope (e.g., system, domain, or project). If you need a refresher on authorization scopes and token types, please refer to the\u00a0token guide. The following sections describe how each default role behaves with keystone\u2019s API across different scopes. Additionally, other service developers can use this document as a guide for implementing similar patterns in their services.</p> <p>Default roles and behaviors across scopes allow operators to delegate more functionality to their team, auditors, customers, and users without maintaining custom policies.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#roles-definitions","title":"Roles Definitions","text":"<p>The default roles provided by keystone, via\u00a0<code>keystone-manage\u00a0bootstrap</code>, are related through role implications. The\u00a0<code>admin</code>\u00a0role implies the\u00a0<code>member</code>\u00a0role, and the\u00a0<code>member</code>\u00a0role implies the\u00a0<code>reader</code>\u00a0role. These implications mean users with the\u00a0<code>admin</code>\u00a0role automatically have the\u00a0<code>member</code>\u00a0and\u00a0<code>reader</code>\u00a0roles. Additionally, users with the\u00a0<code>member</code>\u00a0role automatically have the\u00a0<code>reader</code>\u00a0role. Implying roles reduces role assignments and forms a natural hierarchy between the default roles. It also reduces the complexity of default policies by making check strings short. For example, a policy that requires\u00a0<code>reader</code>\u00a0can be expressed as:</p> <pre><code>\"identity:list_foo\": \"role:reader\"\n</code></pre> <p>Instead of:</p> <pre><code>\"identity:list_foo\": \"role:admin or role:member or role:reader\"\n</code></pre> <p>Reader</p> <p>Warning While it\u2019s possible to use the\u00a0<code>reader</code>\u00a0role to perform audits, we highly recommend assessing the viability of using\u00a0<code>reader</code>\u00a0for auditing from the perspective of the compliance target you\u2019re pursuing. The\u00a0<code>reader</code>\u00a0role is the least-privileged role within the role hierarchy described here. As such, OpenStack development teams, by default, do not advocate exposing sensitive information to users with the\u00a0<code>reader</code>\u00a0role, regardless of the scope. We have noted the need for a formal, read-only, role that is useful for inspecting all applicable resources within a particular scope, but it shouldn\u2019t be implemented as the lowest level of authorization. This work will come in a subsequent release where we support an elevated read-only role, that implies\u00a0<code>reader</code>, but also exposes sensitive information, where applicable.</p> <p>This will allow operators to grant third-party auditors a permissive role for viewing sensitive information, specifically for compliance targets that require it.</p> <p>The\u00a0<code>reader</code>\u00a0role provides read-only access to resources within the system, a domain, or a project. Depending on the assignment scope, two users with the\u00a0<code>reader</code>\u00a0role can expect different API behaviors. For example, a user with the\u00a0<code>reader</code>\u00a0role on the system can list all projects within the deployment. A user with the\u00a0<code>reader</code>\u00a0role on a domain can only list projects within their domain.</p> <p>By analyzing the scope of a role assignment, we increase the re-usability of the\u00a0<code>reader</code>\u00a0role and provide greater functionality without introducing more roles. For example, to accomplish this without analyzing assignment scope, you would need\u00a0<code>system-reader</code>,\u00a0<code>domain-reader</code>, and\u00a0<code>project-reader</code>\u00a0roles in addition to custom policies for each service.</p> <p>It\u2019s imperative to note that\u00a0<code>reader</code>\u00a0is the least authoritative role in the hierarchy because assignments using\u00a0<code>admin</code>\u00a0or\u00a0<code>member</code>\u00a0ultimately include the\u00a0<code>reader</code>\u00a0role. We document this explicitly so that\u00a0<code>reader</code>\u00a0roles are not overloaded with read-only access to sensitive information. For example, a deployment pursuing a specific compliance target may want to leverage the\u00a0<code>reader</code>\u00a0role to perform the audit. If the audit requires the auditor to evaluate sensitive information, like license keys or administrative metadata, within a given scope, auditors shouldn\u2019t expect to perform these operations with the\u00a0<code>reader</code>\u00a0role. We justify this design decision because sensitive information should be explicitly protected, and not implicitly exposed.</p> <p>The\u00a0<code>reader</code>\u00a0role should be implemented and used from the perspective of least-privilege, which may or may not fulfill your auditing use case.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#member","title":"Member","text":"<p>Within keystone, there isn\u2019t a distinct advantage to having the\u00a0<code>member</code>\u00a0role instead of the\u00a0<code>reader</code>\u00a0role. The\u00a0<code>member</code>\u00a0role is more applicable to other services. The\u00a0<code>member</code>\u00a0role works nicely for introducing granularity between\u00a0<code>admin</code>\u00a0and\u00a0<code>reader</code>\u00a0roles. Other services might write default policies that require the\u00a0<code>member</code>\u00a0role to create resources, but the\u00a0<code>admin</code>\u00a0role to delete them. For example, users with\u00a0<code>reader</code>\u00a0on a project could list instance, users with\u00a0<code>member</code>\u00a0on a project can list and create instances, and users with\u00a0<code>admin</code>\u00a0on a project can list, create, and delete instances. Service developers can use the\u00a0<code>member</code>\u00a0role to provide more flexibility between\u00a0<code>admin</code>\u00a0and\u00a0<code>reader</code>\u00a0on different scopes.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#admin","title":"Admin","text":"<p>We reserve the\u00a0<code>admin</code>\u00a0role for the most privileged operations within a given scope. It is important to note that having\u00a0<code>admin</code>\u00a0on a project, domain, or the system carries separate authorization and are not transitive. For example, users with\u00a0<code>admin</code>\u00a0on the system should be able to manage every aspect of the deployment because they\u2019re operators. Users with\u00a0<code>admin</code>\u00a0on a project shouldn\u2019t be able to manage things outside the project because it would violate the tenancy of their role assignment (this doesn\u2019t apply consistently since services are addressing this individually at their own pace).</p> <p>Note As of the Train release, keystone applies the following personas consistently across its API.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#system-personas","title":"System Personas","text":"<p>This section describes authorization personas typically used for operators and deployers. You can find all users with system role assignments using the following query:</p> <pre><code>$ openstack role assignment list --names --system all\n+--------+------------------------+------------------------+---------+--------+--------+-----------+\n| Role   | User                   | Group                  | Project | Domain | System | Inherited |\n+--------+------------------------+------------------------+---------+--------+--------+-----------+\n| admin  |                        | system-admins@Default  |         |        | all    | False     |\n| admin  | admin@Default          |                        |         |        | all    | False     |\n| admin  | operator@Default       |                        |         |        | all    | False     |\n| reader |                        | system-support@Default |         |        | all    | False     |\n| admin  | operator@Default       |                        |         |        | all    | False     |\n| member | system-support@Default |                        |         |        | all    | False     |\n+--------+------------------------+------------------------+---------+--------+--------+-----------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#system-administrators","title":"System Administrators","text":"<p>System administrators\u00a0are allowed to manage every resource in keystone. System administrators are typically operators and cloud administrators. They can control resources that ultimately affect the behavior of the deployment. For example, they can add or remove services and endpoints in the catalog, create new domains, add federated mappings, and clean up stale resources, like a user\u2019s application credentials or trusts.</p> <p>You can find\u00a0system administrators\u00a0in your deployment with the following assignments:</p> <pre><code>$ openstack role assignment list --names --system all --role admin\n+-------+------------------+-----------------------+---------+--------+--------+-----------+\n| Role  | User             | Group                 | Project | Domain | System | Inherited |\n+-------+------------------+-----------------------+---------+--------+--------+-----------+\n| admin |                  | system-admins@Default |         |        | all    | False     |\n| admin | admin@Default    |                       |         |        | all    | False     |\n| admin | operator@Default |                       |         |        | all    | False     |\n+-------+------------------+-----------------------+---------+--------+--------+-----------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#system-members-system-readers","title":"System Members &amp; System Readers","text":"<p>In keystone,\u00a0system members\u00a0and\u00a0system readers\u00a0are very similar and have the same authorization. Users with these roles on the system can view all resources within keystone. They can list role assignments, users, projects, and group memberships, among other resources.</p> <p>The\u00a0system reader\u00a0persona is useful for members of a support team or auditors if the audit doesn\u2019t require access to sensitive information. You can find\u00a0system members\u00a0and\u00a0system readers\u00a0in your deployment with the following assignments:</p> <pre><code>$ openstack role assignment list --names --system all --role member --role reader\n+--------+------------------------+------------------------+---------+--------+--------+-----------+\n| Role   | User                   | Group                  | Project | Domain | System | Inherited |\n+--------+------------------------+------------------------+---------+--------+--------+-----------+\n| reader |                        | system-support@Default |         |        | all    | False     |\n| admin  | operator@Default       |                        |         |        | all    | False     |\n| member | system-support@Default |                        |         |        | all    | False     |\n+--------+------------------------+------------------------+---------+--------+--------+-----------+\n</code></pre> <p>Warning Filtering system role assignments is currently broken and is being tracked as a\u00a0bug.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#domain-personas","title":"Domain Personas","text":"<p>This section describes authorization personas for people who manage their own domains, which contain projects, users, and groups. You can find all users with role assignments on a specific domain using the following query:</p> <pre><code>$ openstack role assignment list --names --domain foobar\n+--------+-----------------+----------------------+---------+--------+--------+-----------+\n| Role   | User            | Group                | Project | Domain | System | Inherited |\n+--------+-----------------+----------------------+---------+--------+--------+-----------+\n| reader | support@Default |                      |         | foobar |        | False     |\n| admin  | jsmith@Default  |                      |         | foobar |        | False     |\n| admin  |                 | foobar-admins@foobar |         | foobar |        | False     |\n| member | jdoe@foobar     |                      |         | foobar |        | False     |\n+--------+-----------------+----------------------+---------+--------+--------+-----------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#domain-administrators","title":"Domain Administrators","text":"<p>Domain administrators\u00a0can manage most aspects of the domain or its contents. These users can create new projects and users within their domain. They can inspect the role assignments users have on projects within their domain.</p> <p>Domain administrators\u00a0aren\u2019t allowed to access system-specific resources or resources outside their domain. Users that need control over project, group, and user creation are a great fit for\u00a0domain administrators.</p> <p>You can find\u00a0domain administrators\u00a0in your deployment with the following role assignment:</p> <pre><code>$ openstack role assignment list --names --domain foobar --role admin\n+-------+----------------+----------------------+---------+--------+--------+-----------+\n| Role  | User           | Group                | Project | Domain | System | Inherited |\n+-------+----------------+----------------------+---------+--------+--------+-----------+\n| admin | jsmith@Default |                      |         | foobar |        | False     |\n| admin |                | foobar-admins@foobar |         | foobar |        | False     |\n+-------+----------------+----------------------+---------+--------+--------+-----------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#domain-members-domain-readers","title":"Domain Members &amp; Domain Readers","text":"<p>Domain members and domain readers have the same relationship as system members and system readers. They\u2019re allowed to view resources and information about their domain. They aren\u2019t allowed to access system-specific information or information about projects, groups, and users outside their domain.</p> <p>The domain member and domain reader use-cases are great for support teams, monitoring the details of an account, or auditing resources within a domain assuming the audit doesn\u2019t validate sensitive information. You can find domain members and domain readers with the following role assignments:</p> <pre><code>$ openstack role assignment list --names --role member --domain foobar\n+--------+-------------+-------+---------+--------+--------+-----------+\n| Role   | User        | Group | Project | Domain | System | Inherited |\n+--------+-------------+-------+---------+--------+--------+-----------+\n| member | jdoe@foobar |       |         | foobar |        | False     |\n+--------+-------------+-------+---------+--------+--------+-----------+\n$ openstack role assignment list --names --role reader --domain foobar\n+--------+-----------------+-------+---------+--------+--------+-----------+\n| Role   | User            | Group | Project | Domain | System | Inherited |\n+--------+-----------------+-------+---------+--------+--------+-----------+\n| reader | support@Default |       |         | foobar |        | False     |\n+--------+-----------------+-------+---------+--------+--------+-----------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#project-personas","title":"Project Personas","text":"<p>This section describes authorization personas for users operating within a project. These personas are commonly used by end users. You can find all users with role assignments on a specific project using the following query:</p> <pre><code>$ openstack role assignment list --names --project production\n+--------+----------------+----------------------------+-------------------+--------+--------+-----------+\n| Role   | User           | Group                      | Project           | Domain | System | Inherited |\n+--------+----------------+----------------------------+-------------------+--------+--------+-----------+\n| admin  | jsmith@Default |                            | production@foobar |        |        | False     |\n| admin  |                | production-admins@foobar   | production@foobar |        |        | False     |\n| member |                | foobar-operators@Default   | production@foobar |        |        | False     |\n| reader | alice@Default  |                            | production@foobar |        |        | False     |\n| reader |                | production-support@Default | production@foobar |        |        | False     |\n+--------+----------------+----------------------------+-------------------+--------+--------+-----------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#project-administrators","title":"Project Administrators","text":"<p>Project administrators\u00a0can only view and modify data within the project they have authorization on. They\u2019re able to view information about their projects and set tags on their projects. They\u2019re not allowed to view system or domain resources, as that would violate the tenancy of their role assignment. Since the majority of the resources in keystone\u2019s API are system and domain-specific,\u00a0project administrators\u00a0don\u2019t have much authorization.</p> <p>You can find\u00a0project administrators\u00a0in your deployment with the following role assignment:</p> <pre><code>$ openstack role assignment list --names --project production --role admin\n+-------+----------------+--------------------------+-------------------+--------+--------+-----------+\n| Role  | User           | Group                    | Project           | Domain | System | Inherited |\n+-------+----------------+--------------------------+-------------------+--------+--------+-----------+\n| admin | jsmith@Default |                          | production@foobar |        |        | False     |\n| admin |                | production-admins@foobar | production@foobar |        |        | False     |\n+-------+----------------+--------------------------+-------------------+--------+--------+-----------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#project-members-project-readers","title":"Project Members &amp; Project Readers","text":"<p>Project members\u00a0and\u00a0project readers\u00a0can discover information about their projects. They can access important information like resource limits for their project, but they\u2019re not allowed to view information outside their project or view system-specific information.</p> <p>You can find\u00a0project members\u00a0and\u00a0project readers\u00a0in your deployment with the following role assignments:</p> <pre><code>$ openstack role assignment list --names --project production --role member\n+--------+------+--------------------------+-------------------+--------+--------+-----------+\n| Role   | User | Group                    | Project           | Domain | System | Inherited |\n+--------+------+--------------------------+-------------------+--------+--------+-----------+\n| member |      | foobar-operators@Default | production@foobar |        |        | False     |\n+--------+------+--------------------------+-------------------+--------+--------+-----------+\n$ openstack role assignment list --names --project production --role reader\n+--------+---------------+----------------------------+-------------------+--------+--------+-----------+\n| Role   | User          | Group                      | Project           | Domain | System | Inherited |\n+--------+---------------+----------------------------+-------------------+--------+--------+-----------+\n| reader | alice@Default |                            | production@foobar |        |        | False     |\n| reader |               | production-support@Default | production@foobar |        |        | False     |\n+--------+---------------+----------------------------+-------------------+--------+--------+-----------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Keystone/Role_Types/#writing-policies","title":"Writing Policies","text":"<p>If the granularity provided above doesn\u2019t meet your specific use-case, you can still override policies and maintain them manually. You can read more about how to do that in oslo.policy usage\u00a0documentation.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Manila/Administration_Guide/","title":"Administration Guide","text":"<p>Shared File Systems service provides a set of services for management of shared file systems in a multi-project cloud environment. The service resembles OpenStack block-based storage management from the OpenStack Block Storage service project. With the Shared File Systems service, you can create a remote file system, mount the file system on your instances, and then read and write data from your instances to and from your file system.</p> <p>The Shared File Systems service serves same purpose as the Amazon Elastic File System (EFS) does.</p> <p>The Shared File Systems service can run in a single-node or multiple node configuration. The Shared File Systems service can be configured to provision shares from one or more back ends, so it is required to declare at least one back end. Shared File System service contains several configurable components.</p> <p>It is important to understand these components:</p> <ul> <li> <p>Share networks</p> </li> <li> <p>Shares</p> </li> <li> <p>Multi-tenancy</p> </li> <li> <p>Back ends</p> </li> </ul> <p>The Shared File Systems service consists of four types of services, most of which are similar to those of the Block Storage service:</p> <ul> <li> <p><code>manila-api</code></p> </li> <li> <p><code>manila-data</code></p> </li> <li> <p><code>manila-scheduler</code></p> </li> <li> <p><code>manila-share</code></p> </li> </ul> <p>Installation of first three \u2013\u00a0<code>manila-api</code>,\u00a0<code>manila-data</code>, and\u00a0<code>manila-scheduler</code>\u00a0is common for almost all deployments. But configuration of\u00a0<code>manila-share</code>\u00a0is backend-specific and can differ from deployment to deployment.</p> <ul> <li> <p>Key concepts</p> </li> <li> <p>Share management</p> </li> <li> <p>Share types</p> </li> <li> <p>Share group types</p> </li> <li> <p>Share groups</p> </li> <li> <p>Share snapshots</p> </li> <li> <p>Share servers</p> </li> <li> <p>Share server management</p> </li> <li> <p>Share server limits (Since Wallaby release)</p> </li> <li> <p>Security services</p> </li> <li> <p>Share migration</p> </li> <li> <p>Share replication</p> </li> <li> <p>Multi-storage configuration</p> </li> <li> <p>Networking</p> </li> <li> <p>Troubleshoot Shared File Systems service</p> </li> <li> <p>Profiling the Shared File Systems service</p> </li> <li> <p>Upgrading the Shared File System service</p> </li> <li> <p>Share revert to snapshot</p> </li> <li> <p>Share server migration</p> </li> <li> <p>Manila share features support mapping</p> </li> <li> <p>Capabilities and Extra-Specs</p> </li> <li> <p>Group Capabilities and group-specs</p> </li> <li> <p>Export Location Metadata</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Manila/Administration_Guide/#supported-share-back-ends","title":"Supported share back ends","text":"<p>The manila share service must be configured to use drivers for one or more storage back ends, as described in general terms below. See the drivers section in the\u00a0Configuration Reference\u00a0for detailed configuration options for each back end.</p> <ul> <li> <p>Container Driver</p> </li> <li> <p>Supported operations</p> </li> <li> <p>Restrictions</p> </li> <li> <p>Known problems</p> </li> <li> <p>Setting up container driver with devstack</p> </li> <li> <p>Setting Container Driver Up Manually</p> </li> <li> <p>ZFS (on Linux) Driver</p> </li> <li> <p>Requirements</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Possibilities</p> </li> <li> <p>Restrictions</p> </li> <li> <p>Known problems</p> </li> <li> <p>Backend Configuration</p> <ul> <li> <p>The\u00a0<code>manila.share.drivers.zfsonlinux.driver</code>\u00a0Module</p> </li> <li> <p>The\u00a0<code>manila.share.drivers.zfsonlinux.utils</code>\u00a0Module</p> </li> </ul> </li> <li> <p>NetApp Clustered Data ONTAP</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Supported Operating Modes</p> </li> <li> <p>Network approach</p> </li> <li> <p>Supported shared filesystems</p> </li> <li> <p>Required licenses</p> </li> <li> <p>Known restrictions</p> <ul> <li>The\u00a0<code>manila.share.drivers.netapp.common.py</code>\u00a0Module</li> </ul> </li> <li> <p>Isilon Driver</p> </li> <li> <p>Requirements</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Backend Configuration</p> </li> <li> <p>Restrictions</p> <ul> <li> <p>The\u00a0<code>manila.share.drivers.dell_emc.driver</code>\u00a0Module</p> </li> <li> <p>The\u00a0<code>manila.share.drivers.dell_emc.plugins.isilon.isilon</code>\u00a0Module</p> </li> </ul> </li> <li> <p>VNX Driver</p> </li> <li> <p>Requirements</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Pre-Configurations on VNX</p> </li> <li> <p>Backend Configuration</p> </li> <li> <p>IPv6 support</p> <ul> <li>Pre-Configurations for IPv6 support</li> </ul> </li> <li> <p>Snapshot support</p> <ul> <li> <p>Pre-Configurations for Snapshot support</p> </li> <li> <p>To snapshot a share and create share from the snapshot</p> </li> </ul> </li> <li> <p>Restrictions</p> <ul> <li> <p>The\u00a0<code>manila.share.drivers.dell_emc.driver</code>\u00a0Module</p> </li> <li> <p>The\u00a0<code>manila.share.drivers.dell_emc.plugins.vnx.connection</code>\u00a0Module</p> </li> </ul> </li> <li> <p>Dell EMC Unity driver</p> </li> <li> <p>Requirements</p> </li> <li> <p>Supported shared filesystems and operations</p> </li> <li> <p>Supported Network Topologies</p> </li> <li> <p>Pre-Configurations</p> <ul> <li> <p>On Manila Node</p> </li> <li> <p>On Unity System</p> </li> </ul> </li> <li> <p>Backend configurations</p> </li> <li> <p>Supported MTU size</p> </li> <li> <p>IPv6 support</p> </li> <li> <p>Pre-Configurations for IPv6 support</p> </li> <li> <p>Supported share creation in mode that driver does not create and destroy share servers (DHSS=False)</p> </li> <li> <p>Snapshot support</p> </li> <li> <p>Pre-Configurations for Snapshot support</p> </li> <li> <p>To snapshot a share and create share from the snapshot</p> </li> <li> <p>To manage an existing share server</p> </li> <li> <p>To un-manage a Manila share server</p> </li> <li> <p>To manage an existing share</p> </li> <li> <p>To un-manage a Manila share</p> </li> <li> <p>To manage an existing share snapshot</p> </li> <li> <p>To un-manage a Manila share snapshot</p> </li> <li> <p>Supported security services</p> </li> <li> <p>IO Load balance</p> </li> <li> <p>Default filter function</p> </li> <li> <p>Restrictions</p> </li> <li> <p>API Implementations</p> </li> <li> <p>Driver options</p> </li> <li> <p>Generic approach for share provisioning</p> </li> <li> <p>Network configurations</p> </li> <li> <p>Requirements for service image</p> </li> <li> <p>Supported shared filesystems</p> </li> <li> <p>Known restrictions</p> <ul> <li> <p>Using Windows instances</p> </li> <li> <p>The\u00a0<code>manila.share.drivers.generic</code>\u00a0Module</p> </li> <li> <p>The\u00a0<code>manila.share.drivers.service_instance</code>\u00a0Module</p> </li> </ul> </li> <li> <p>GlusterFS driver</p> </li> <li> <p>Supported shared filesystems</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Requirements</p> </li> <li> <p>Manila driver configuration setting</p> </li> <li> <p>Layouts</p> <ul> <li>Gluster NFS with volume mapped layout</li> </ul> </li> <li> <p>Known Restrictions</p> <ul> <li>The\u00a0<code>manila.share.drivers.glusterfs</code>\u00a0Module</li> </ul> </li> <li> <p>GlusterFS Native driver</p> </li> <li> <p>Network Approach</p> </li> <li> <p>Supported shared filesystems</p> </li> <li> <p>Multi-tenancy model</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Requirements</p> </li> <li> <p>Manila driver configuration setting</p> </li> <li> <p>Host and backend configuration</p> </li> <li> <p>Known Restrictions</p> <ul> <li>The\u00a0<code>manila.share.drivers.glusterfs.glusterfs_native.GlusterfsNativeShareDriver</code>\u00a0Module</li> </ul> </li> <li> <p>CephFS driver</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Prerequisites</p> <ul> <li> <p>Ceph testing matrix</p> </li> <li> <p>Common Prerequisites</p> </li> <li> <p>For CephFS native shares</p> </li> <li> <p>For CephFS NFS shares</p> </li> </ul> </li> <li> <p>Authorizing the driver to communicate with Ceph</p> </li> <li> <p>Enabling snapshot support in Ceph backend</p> </li> <li> <p>Configuring CephFS backend in manila.conf</p> <ul> <li> <p>Configure CephFS native share backend in manila.conf</p> </li> <li> <p>Configure CephFS NFS share backend in manila.conf</p> </li> </ul> </li> <li> <p>Space considerations</p> </li> <li> <p>Creating shares</p> <ul> <li> <p>Create CephFS native share</p> </li> <li> <p>Create CephFS NFS share</p> </li> </ul> </li> <li> <p>Allowing access to shares</p> <ul> <li> <p>Allow access to CephFS native share</p> </li> <li> <p>Allow access to CephFS NFS share</p> </li> </ul> </li> <li> <p>Mounting CephFS shares</p> <ul> <li> <p>Mounting CephFS native share using FUSE client</p> </li> <li> <p>Mounting CephFS native share using Kernel client</p> </li> <li> <p>Mount CephFS NFS share using NFS client</p> </li> </ul> </li> <li> <p>Known restrictions</p> </li> <li> <p>Security</p> <ul> <li>Security with CephFS native share backend</li> </ul> </li> <li> <p>The\u00a0<code>manila.share.drivers.cephfs.driver</code>\u00a0Module</p> </li> <li> <p>GPFS Driver</p> </li> <li> <p>Supported shared filesystems</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Requirements</p> </li> <li> <p>Manila driver configuration setting</p> </li> <li> <p>Known Restrictions</p> <ul> <li>The\u00a0<code>manila.share.drivers.ibm.gpfs</code>\u00a0Module</li> </ul> </li> <li> <p>Huawei Driver</p> </li> <li> <p>Requirements</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Pre-Configurations on Huawei</p> </li> <li> <p>Backend Configuration</p> </li> <li> <p>Share Types</p> </li> <li> <p>Restrictions</p> <ul> <li>The\u00a0<code>manila.share.drivers.huawei.huawei_nas</code>\u00a0Module</li> </ul> </li> <li> <p>HDFS native driver</p> </li> <li> <p>Network configuration</p> </li> <li> <p>Supported shared filesystems</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Requirements</p> </li> <li> <p>Manila driver configuration</p> </li> <li> <p>Known Restrictions</p> <ul> <li>The\u00a0<code>manila.share.drivers.hdfs.hdfs_native</code>\u00a0Module</li> </ul> </li> <li> <p>Hitachi NAS Platform File Services Driver for OpenStack</p> </li> <li> <p>Driver Version 3.0</p> <ul> <li> <p>Hitachi NAS Platform Storage Requirements</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Driver Configuration</p> </li> <li> <p>Manage and Unmanage Shares</p> </li> <li> <p>Additional Notes</p> </li> <li> <p>The\u00a0<code>manila.share.drivers.hitachi.hnas.driver</code>\u00a0Module</p> </li> </ul> </li> <li> <p>HPE 3PAR Driver for OpenStack Manila</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Requirements</p> </li> <li> <p>Pre-Configuration on the HPE 3PAR</p> </li> <li> <p>Backend Configuration</p> </li> <li> <p>Backend Configuration for AD user</p> </li> <li> <p>Example of using AD user to access CIFS share</p> </li> <li> <p>Network Approach</p> </li> <li> <p>Share Types</p> </li> <li> <p>Delete Nested Shares</p> <ul> <li>The\u00a0<code>manila.share.drivers.hpe.hpe_3par_driver</code>\u00a0Module</li> </ul> </li> <li> <p>Infortrend Driver for OpenStack Manila</p> </li> <li> <p>Requirements</p> </li> <li> <p>Supported shared filesystems and operations</p> </li> <li> <p>Backend Configuration</p> </li> <li> <p>Share Types</p> </li> <li> <p>Back-end configuration example</p> </li> <li> <p>Macrosan Driver for OpenStack Manila</p> </li> <li> <p>Requirements</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Backend Configuration</p> </li> <li> <p>Share Types</p> </li> <li> <p>Back-end configuration example</p> </li> <li> <p>Pure Storage FlashBlade Driver for OpenStack Manila</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>General Requirements</p> </li> <li> <p>Network Requirements</p> </li> <li> <p>Driver Configuration</p> <ul> <li> <p>Step 1 \u2013 FlashBlade Parameters configuration</p> </li> <li> <p>Step 2 \u2013 Share Type Configuration</p> </li> <li> <p>Step 3 \u2013 Restart the Services</p> </li> </ul> </li> <li> <p>The\u00a0<code>manila.share.drivers.purestorage.flashblade</code>\u00a0Module</p> </li> <li> <p>Tegile Driver</p> </li> <li> <p>Requirements</p> </li> <li> <p>Supported Operations</p> </li> <li> <p>Backend Configuration</p> </li> <li> <p>Restrictions</p> <ul> <li>The\u00a0<code>manila.share.drivers.tegile.tegile</code>\u00a0Module</li> </ul> </li> <li> <p>NexentaStor5 Driver for OpenStack Manila</p> </li> <li> <p>Requirements</p> </li> <li> <p>Supported shared filesystems and operations</p> </li> <li> <p>Backend Configuration</p> </li> <li> <p>Share Types</p> </li> <li> <p>Restrictions</p> </li> <li> <p>Back-end configuration example</p> </li> <li> <p>Windows SMB driver</p> </li> <li> <p>Limitations</p> </li> <li> <p>Prerequisites</p> </li> <li> <p>Configuring</p> </li> <li> <p>Zadara VPSA Driver for OpenStack Manila</p> </li> <li> <p>Requirements</p> </li> <li> <p>Supported shared filesystems and operations</p> <ul> <li> <p>Share file system supported</p> </li> <li> <p>Supported operations</p> </li> <li> <p>Backend Configuration</p> </li> </ul> </li> <li> <p>Driver options</p> </li> <li> <p>Back-end configuration example</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Manila/Shared_Filesystems_Overview/","title":"What is Manila?","text":"<p>Manila is the OpenStack Shared Filesystems service for providing Shared Filesystems as a service. Some of the goals of Manila are to be/have:</p> <ul> <li> <p>Component based architecture: Quickly add new behaviors</p> </li> <li> <p>Highly available: Scale to very serious workloads</p> </li> <li> <p>Fault-Tolerant: Isolated processes avoid cascading failures</p> </li> <li> <p>Recoverable: Failures should be easy to diagnose, debug, and rectify</p> </li> <li> <p>Open Standards: Be a reference implementation for a community-driven api</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Manila/Shared_Filesystems_Overview/#for-end-users","title":"For end users","text":"<p>As an end user of Manila, you\u2019ll use Manila to create a remote file system with either tools or the API directly:\u00a0python-manilaclient, or by directly using the\u00a0REST API.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Manila/Shared_Filesystems_Overview/#tools-for-using-manila","title":"Tools for using Manila","text":"<ul> <li>User</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Manila/Shared_Filesystems_Overview/#using-the-manila-api","title":"Using the Manila API","text":"<p>All features of Manila are exposed via a REST API that can be used to build more complicated logic or automation with Manila. This can be consumed directly or via various SDKs. The following resources can help you get started consuming the API directly:</p> <ul> <li> <p>Manila API</p> </li> <li> <p>Manila microversion history</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Manila/Shared_Filesystems_Overview/#for-operators","title":"For operators","text":"<p>This section has details for deploying and maintaining Manila services.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Manila/Shared_Filesystems_Overview/#installing-manila","title":"Installing Manila","text":"<p>Manila can be configured standalone using the configuration setting\u00a0<code>auth_strategy\u00a0=\u00a0noauth</code>, but in most cases you will want to at least have the\u00a0Keystone\u00a0Identity service and other\u00a0OpenStack services\u00a0installed.</p> <ul> <li>Installation Tutorial</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Manila/Shared_Filesystems_Overview/#administrating-manila","title":"Administrating Manila","text":"<ul> <li>Admin Guide</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Manila/Shared_Filesystems_Overview/#reference","title":"Reference","text":"<ul> <li> <p>Configuration</p> </li> <li> <p>Command Line Interface</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Add_a_New_OS_Bundle_to_Taikun_OCP_Baremetal/","title":"Adding an OS Bundle","text":"<p>OS Bundles are collections of operating system installation files, checksums, kickstart/preseed/cloud-init files, and metadata that allow Taikun OCP Baremetal to provision servers with a specific operation system.</p> <p>This tutorial walks through how to add a new operating system into Taikun OCP Baremetal so it can be used to provision servers within the Taikun OCP Baremetal Platform.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Add_a_New_OS_Bundle_to_Taikun_OCP_Baremetal/#getting-started","title":"Getting Started","text":"<p>Before you begin, you must have an OS Bundle in mind to install. This can be prepared and distributed by taikun.cloud or created manually by you.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Add_a_New_OS_Bundle_to_Taikun_OCP_Baremetal/#available-bundles","title":"Available Bundles","text":"<p>The following OS Bundles are available from taikun.cloud. Input the values from this table into the\u00a0<code>./mojo-manage --os-image</code>\u00a0command. OS Bundles are organized by\u00a0<code>name</code>\u00a0and\u00a0<code>version</code>. The combination of these two fields is unique within the Taikun OCP Baremetal Platform.</p> Name Version Architecture Ubuntu 22.04.03 x64 RHEL 8.8-net x64 RHEL 9.2-net x64"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Add_a_New_OS_Bundle_to_Taikun_OCP_Baremetal/#running","title":"Running","text":"<p>You can upload a new OS Bundle by executing the following command\u00a0inside of your Taikun OCP Baremetal installation directory\u00a0and following the prompts.</p> <pre><code>sudo ./mojo-manage --os-image\n</code></pre> <p>In order to add a new OS Bundle,\u00a0the Taikun OCP Baremetal platform needs to be running.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Add_a_New_OS_Bundle_to_Taikun_OCP_Baremetal/#enter-the-os-bundle-name","title":"Enter the OS Bundle\u00a0<code>name</code>","text":"<p>The\u00a0<code>name</code>\u00a0field does not need to be equal to the operating system\u2019s distribution name, it can be anything you would like it to be. In these examples they are the same (distro:\u00a0<code>ubuntu</code>, name:\u00a0<code>ubuntu</code>).</p> <pre><code>Name? [ubuntu]: ubuntu\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Add_a_New_OS_Bundle_to_Taikun_OCP_Baremetal/#enter-the-os-bundle-version","title":"Enter the OS Bundle\u00a0<code>version</code>","text":"<p>The\u00a0<code>version</code>\u00a0field does not need to be equal to the operating system\u2019s version, it can be anything you would like it to be. In these examples they are the same (os-version:\u00a0<code>22.04.03</code>, mojo-version:\u00a0<code>22.04.03</code>).</p> <pre><code>Version? [22.04.03]: 22.04.03\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Add_a_New_OS_Bundle_to_Taikun_OCP_Baremetal/#enter-the-architecture","title":"Enter the Architecture","text":"<p>The Architecture field is optional and only has an effect on the default URL to download an OS Bundle at. Taikun OCP Baremetal supports any architecture for its OS Bundles as long as it can PXE boot.</p> <pre><code>Architecture? [x64]: x64\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Add_a_New_OS_Bundle_to_Taikun_OCP_Baremetal/#add-or-edit-the-download-url","title":"Add or Edit the Download URL","text":"<p>By convention, when receiving an OS Bundle from taikun.cloud, the URL will be organized with the\u00a0<code>name</code>,\u00a0<code>version</code>, and maybe the\u00a0<code>architecture</code>. These fields are combine into a URL that is presented as the default for this field. If you are downloading an OS Bundle from taikun.cloud and were given a\u00a0<code>name</code>\u00a0and\u00a0<code>version</code>\u00a0to input, you should not change this field. If you have created your own OS Bundle, you will want to point this field to a URL with your downloadable OS Bundle.</p> <pre><code>Download URL? [https://download.taikun.cloud.io/repository/osimages-internal/images/ubuntu/22.04.03/ubuntu-22.04.03.tar.gz]:\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Add_a_New_OS_Bundle_to_Taikun_OCP_Baremetal/#edit-boot-menu-keyword","title":"Edit Boot Menu Keyword","text":"<p>This field can be\u00a0<code>kernel</code>\u00a0or\u00a0<code>chain</code>. If you don\u2019t know what this should be, leave it set to the default of\u00a0<code>kernel</code></p> <pre><code>Boot Menu Keyword? [kernel]:\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Add_a_New_OS_Bundle_to_Taikun_OCP_Baremetal/#edit-location","title":"Edit Location","text":"<p>Some OS Images (Live CDs) can PXE boot directly from an ISO image. If that is the case for your OS Bundle, enter the filename of the file to boot from in this field. Typically this will not be populated.</p> <pre><code>If your OS boots directly from an ISO and not an installation directory, enter the filename here. Location? []:\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Add_a_New_OS_Bundle_to_Taikun_OCP_Baremetal/#installing-os-bundle","title":"Installing OS Bundle","text":"<p>The script will then download and install your OS Bundle if it was found at the configured URL.</p> <pre><code>Configured to add or update the following OS Image...\nName:         ubuntu\nVersion:      22.04.03\nArch:         x64\nURL:          https://download.taikun.cloud.io/repository/osimages-internal/images/ubuntu/22.04.03/ubuntu-22.04.03.tar.gz\nBoot Keyword: kernel\nLocation:\n\nDownloading https://download.taikun.cloud.io/repository/osimages-internal/images/ubuntu/22.04.03/ubuntu-22.04.03.tar.gz...\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 2141M  100 2141M    0     0  21.4M      0  0:01:40  0:01:40 --:--:-- 28.3M\n\nExtracting to volumes/osimages/ubuntu/22.04.03...\nboot/\nboot/initrd\nboot/boot.ipxe\nboot/bootx64.efi\nboot/vmlinuz\niso/\niso/ubuntu-22.04.03-mojo-amd64.iso\nmojo/\nmojo/default.init\n\nConfiguring Mojo with new OS...\nId 1.\n\nAdding default customization template...\n[+] Copying 1/0\n \u2714 mojo-app-1 copy volumes/osimages/ubuntu/22.04.03/mojo/default.init to mojo-app-1:/tmp/default_mojo_ubuntu_22.04.03.init Copied 0.0s\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Create_and_Manage_Node_Pools_in_Taikun_OCP_Baremetal/","title":"Pool Management","text":"<p>To work with Pools in Taikun OCP Baremetal, a few facts need to be understood:</p> <ul> <li> <p>A\u00a0Node\u00a0is a resource that Taikun OCP Baremetal can manage, it could be a server, a switch, a PDU, etc.</p> </li> <li> <p>A\u00a0Node\u00a0in Taikun OCP Baremetal can belong to a single\u00a0Pool</p> </li> <li> <p>A\u00a0Pool\u00a0is a logical group of\u00a0Nodes\u00a0that have the same user permissions applied to them</p> </li> <li> <p>A\u00a0Group\u00a0is a collection of\u00a0Users\u00a0and\u00a0Roles</p> </li> <li> <p>A single\u00a0Group\u00a0is assigned to each\u00a0Pool\u00a0to control permissions on its\u00a0Nodes.\u201cSupermicro Users\u201d group </p> </li> </ul> <p>By default, Taikun OCP Baremetal comes with a\u00a0Default\u00a0Pool and a\u00a0Default\u00a0group.\u00a0Users\u00a0who are members of the\u00a0Default\u00a0group have access to all Nodes in the\u00a0Default\u00a0Pool. On small Taikun OCP Baremetal installs with only a single active user, the\u00a0Default Group\u00a0and the\u00a0Default Pool\u00a0can be used for everything. In larger deployments with different user permission requirements, the\u00a0Groups\u00a0and\u00a0Pools\u00a0should be constructed to meet said requirements.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Create_and_Manage_Node_Pools_in_Taikun_OCP_Baremetal/#create-a-node-management-pool","title":"Create a Node Management Pool","text":"<p>In this use case, we will create a new\u00a0Pool\u00a0for just one of our\u00a0Nodes\u00a0so a specific user group can manage the\u00a0Node. This is a common use case for Taikun OCP Baremetal, where a specific\u00a0Group\u00a0of\u00a0Users\u00a0needs to be able to manage a specific set of\u00a0Nodes.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Create_and_Manage_Node_Pools_in_Taikun_OCP_Baremetal/#create-user","title":"Create User","text":"<p>Create a new user called \u201csupermicro\u201d. Under \u201cAdministration\u201d in the left navigation, select \u201cUsers\u201d and then click the blue \u201cAdd user\u201d button on the right side of the screen. Fill out the form and click \u201cSave\u201d.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Create_and_Manage_Node_Pools_in_Taikun_OCP_Baremetal/#create-group","title":"Create Group","text":"<p>Create a \u201cSupermicro Users\u201d group for all users that can manage Supermicro servers. Under \u201cAdministration\u201d in the left navigation, select \u201cGroups\u201d and then click the blue \u201cAdd group\u201d button on the right side of the screen. Enter the\u00a0Group\u00a0information and press the \u201cCreate group\u201d button.</p> <p></p> <p>Add the \u201csupermicro\u201d user to the \u201cSupermicro Users\u201d group as someone who can do everything. In Taikun OCP Baremetal this is called the \u201cAllInOne\u201d role. Under \u201cAdministration\u201d in the left navigation, select \u201cGroups\u201d, then click on the \u201cSupermicro Users\u201d row. Click \u201cAdd user to group\u201d in the top right and choose the \u201csupermicro\u201d user and the \u201cAllInOne\u201d role.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Create_and_Manage_Node_Pools_in_Taikun_OCP_Baremetal/#create-pool","title":"Create Pool","text":"<p>Create a\u00a0Pool\u00a0called \u201cSupermicro Pool\u201d and assign the \u201csupermicro\u201d group to the\u00a0Pool. Under \u201cManagement\u201d in the left navigation, select \u201cPools\u201d and then click the blue \u201cCreate pool\u201d button on the right side of the screen. Enter the\u00a0Pool\u00a0information and press the \u201cCreate pool\u201d button.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Create_and_Manage_Node_Pools_in_Taikun_OCP_Baremetal/#add-node-to-pool","title":"Add Node to Pool","text":"<p>There are many wants to add a\u00a0Node\u00a0to a\u00a0Pool.\u00a0You only need to perform one of the following methods to add a\u00a0Node\u00a0to a\u00a0Pool.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Create_and_Manage_Node_Pools_in_Taikun_OCP_Baremetal/#pool-detail-page","title":"Pool Detail Page","text":"<p>You can do so from the Pool detail page by clicking on an individual pool in the Pool list.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Create_and_Manage_Node_Pools_in_Taikun_OCP_Baremetal/#server-detail-page","title":"Server Detail Page","text":"<p>A Server can also be added to the pool directly on its detail page. Click on the Server in the server table and then the blue \u201cAssign to pool\u201d button on the right side of the page. This will open a modal window where you can select the pool to assign the server to.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Installing_Taikun_OCP_Baremetal/","title":"System Requirements","text":"<p>Taikun OCP Baremetal is best installed on a fresh operating system. Debian (Ubuntu, Mint, etc.) and Fedora (RedHat, CentOS, Alma, Rocky, etc) distros have been tested. Taikun OCP Baremetal can also be installed on top of Kubernetes in TaikunCloudWorks. Other operating systems are supported but they have not yet been tested. If you are starting from scratch and don\u2019t have a strong OS preference, the Taikun OCP Baremetal platform has been well-tested on both\u00a0<code>ubuntu-22.04.03</code>\u00a0and\u00a0<code>debian12 (bookworm)</code>. However, any OS that can satisfy the installation requirements should work. We recommend a dedicated machine for the Taikun OCP Baremetal Platform, although it is not required. The Taikun OCP Baremetal installation process will not install any additional packages on your machine and all changes will be contained in the installation directory you choose during setup and the\u00a0<code>/tmp</code>\u00a0folder.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Installing_Taikun_OCP_Baremetal/#software-requirements","title":"Software Requirements","text":"<p>The Taikun OCP Baremetal installation requires:\u00a0<code>git</code>,\u00a0<code>docker</code>,\u00a0<code>jq</code>,\u00a0<code>yq</code>,\u00a0<code>curl</code>,\u00a0<code>sudo</code>, and\u00a0<code>bash</code>.</p> <p>1. Install system packages. This may be different depending on your OS. Generally the names for these highly generic packages are standard across platforms</p> OS Command Debian-based <code>apt install -y curl sudo bash git</code> RedHat-based <code>dnf install -y curl sudo bash git</code> <p>2. Install Docker\u00a01.</p> <p> <p> </p> <p>3. Install jq\u00a01.</p> OS Command Debian-based <code>apt install -y jq</code> RedHat-based <code>dnf install -y jq</code> <p>4.\u00a0Install yq\u00a01.</p> <pre><code>sudo wget [https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64](https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64) -O /usr/bin/yq &amp;&amp; sudo chmod +x /usr/bin/yq\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Installing_Taikun_OCP_Baremetal/#install-taikun-ocp-baremetal","title":"Install Taikun OCP Baremetal","text":"<p>After satisfying the System, Software, and License Requirements, you are ready to install Taikun OCP Baremetal! Run the following command to download and run the installer script. You will be prompted for a location to install Taikun OCP Baremetal.</p> <pre><code>curl -L -s [https://download.taikun.cloud.io/repository/filestore-external/mojo-installer/releases/latest/mojo-install](https://download.taikun.cloud.io/repository/filestore-external/mojo-installer/releases/latest/mojo-install) | sudo bash\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Installing_Taikun_OCP_Baremetal/#setup-taikun-ocp-baremetal","title":"Setup Taikun OCP Baremetal","text":"<p>Setup is run automatically during installation. If for some reason the installation failed or you need to setup Taikun OCP Baremetal again, you can run the setup script manually. To run the setup script manually, change directories into your Taikun OCP Baremetal installation directory (default:\u00a0<code>/opt/mojo</code>) and run the following command:</p> <pre><code>sudo ./mojo-setup\n</code></pre> <p>This will create a series of files in the\u00a0<code>configs</code>\u00a0directory and save any existing configuration in a backup folder for safe keeping.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Installing_Taikun_OCP_Baremetal/#configure-taikun-ocp-baremetal","title":"Configure Taikun OCP Baremetal","text":"<p>Basic configuration is done automatically during setup. The\u00a0<code>configs/mojo.env</code>\u00a0file contains the most recent Taikun OCP Baremetal configuration. You can choose to re-run the\u00a0<code>./mojo-setup</code>\u00a0script to update this file or you can edit it directly. Please be aware that any edits done manually to the file will be over-ridden the next time\u00a0<code>./mojo-setup</code>\u00a0is run and may not be compatible with the future releases of Taikun OCP Baremetal.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Installing_Taikun_OCP_Baremetal/#ssl-certificates","title":"SSL Certificates","text":"<p>Taikun OCP Baremetal will automatically generate self-signed SSL certificates for you. If you would like to use your own certificates replace the files in\u00a0<code>configs/ssl/</code>\u00a0with your own. The files should be named\u00a0<code>cert.crt</code>\u00a0and\u00a0<code>cert.key</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Installing_Taikun_OCP_Baremetal/#run-taikun-ocp-baremetal","title":"Run Taikun OCP Baremetal","text":"<p>When you are ready to run Taikun OCP Baremetal for the first time, execute the following command:</p> <pre><code>sudo ./mojo-launcher start mojo --update\n</code></pre> <p>This may take some time as it downloads all of the supporting Taikun OCP Baremetal Docker images. Once complete, you should be able to access Taikun OCP Baremetal at the hostname you configured Taikun OCP Baremetal with.</p> <p>If Taikun OCP Baremetal is operating as expected with your hardware profile and you do not need any updates, you can start Taikun OCP Baremetal without updating it by executing the same command without the\u00a0<code>--update</code>\u00a0flag. This is the recommended way to re-start Taikun OCP Baremetal after it has been stopped or the hardware Taikun OCP Baremetal was installed to is restarted.</p> <pre><code>sudo ./mojo-launcher start mojo\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Installing_Taikun_OCP_Baremetal/#update-taikun-ocp-baremetal","title":"Update Taikun OCP Baremetal","text":"<p>If you want to update the version of Taikun OCP Baremetal you are running, execute the following command:</p> <pre><code>sudo ./mojo-launcher update mojo\n</code></pre> <p>This will not bring down your Taikun OCP Baremetal instance, it will only update the backing Docker images. You will need to stop and start Taikun OCP Baremetal manually for the updated images to take effect. Sometimes updates can take some time to migrate data or other platform features. Please be patient when updating the Taikun OCP Baremetal Platform.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Installing_Taikun_OCP_Baremetal/#stop-taikun-ocp-baremetal","title":"Stop Taikun OCP Baremetal","text":"<p>If you want to stop the Taikun OCP Baremetal platform, execute the following command:</p> <pre><code>sudo ./mojo-launcher stop mojo\n</code></pre> <p>All persistent data required to re-launch Taikun OCP Baremetal is present in the installation directory and you can relaunch Taikun OCP Baremetal at any time as long as the installation directory remains intact.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Keycloak_integration/","title":"Integration with Keycloak Identity Provider","text":"<p>Introduction:</p> <p>This feature aims to integrate the solution with the Identity Provider (IdP) Keycloak. The integration leverages the SAML 2.0 protocol to enable Taikun OCP Baremetal to utilize external authentication and authorization services provided by Keycloak.</p> <p>Overview:</p> <p>Taikun OCP Baremetal possesses the capability to incorporate external Identity Providers through the industry-standard Security Assertion Markup Language (SAML) 2.0. By integrating with Keycloak as the IdP, the system gains access to robust authentication and authorization mechanisms, enhancing security and user management capabilities.</p> <p>Keycloak Identity Provider Integration:</p> <p>The integration process involves configuring Taikun OCP Baremetal to communicate with Keycloak\u2019s SAML 2.0 endpoints. This encompasses the establishment of trust between the two systems and mapping of attributes to ensure seamless user authentication and authorization.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Keycloak_integration/#steps-for-integration","title":"Steps for Integration:","text":"<p>      \u2013 Access the configuration file of Taikun OCP Baremetal. <pre><code>\u00a0\u00a0\u00a0\u2013 Navigate to the authentication config section.\n\n\u00a0\u00a0\u00a0\u2013 Locate the SAML 2.0 integration option and fill Keycloak details as the Identity Provider.\n</code></pre> <p></p> <p>     \u2013 Access the Keycloak administration console. <pre><code>\u00a0\u00a0\u00a0\u2013 Configure a new SAML client for Taikun OCP Baremetal.\n\n\u00a0\u00a0\u00a0\u2013 Define the necessary endpoints and metadata required for SAML 2.0 communication.\n</code></pre> <p></p> <p>     \u2013 Exchange metadata between Taikun OCP Baremetal and Keycloak to establish trust. <pre><code>\u00a0\u00a0\u00a0\u2013 Verify and validate the trust relationship to ensure secure communication.\n</code></pre> <p></p> <p>     \u2013 Map user attributes between Taikun OCP Baremetal and Keycloak. <pre><code>\u00a0\u00a0\u00a0\u2013 Ensure alignment of attribute names and formats to enable proper user authentication and authorization.\n</code></pre> <p></p> <p>     \u2013 Conduct thorough testing to verify the functionality of SAML 2.0 integration. <pre><code>\u00a0\u00a0\u00a0\u2013 Validate user authentication and authorization processes across both systems.\n</code></pre> <p></p> <p>     \u2013 Implement error handling mechanisms to address any issues encountered during integration. <pre><code>\u00a0\u00a0\u00a0\u2013 Establish troubleshooting procedures to diagnose and resolve integration-related issues efficiently.\n</code></pre> <p> </p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Keycloak_integration/#benefits","title":"Benefits:","text":"<p>\u2013\u00a0Enhanced Security:\u00a0Leveraging Keycloak as the Identity Provider enhances security through robust authentication mechanisms and centralized user management.</p> <p>\u2013\u00a0Simplified User Management:\u00a0Centralized user management in Keycloak streamlines administrative tasks and ensures consistency across applications.</p> <p>\u2013\u00a0Scalability and Flexibility:\u00a0The integration provides scalability and flexibility to adapt to evolving authentication requirements and business needs.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Keycloak_integration/#conclusion","title":"Conclusion:","text":"<p>The integration of Taikun OCP Baremetal with Keycloak as the Identity Provider via SAML 2.0 facilitates secure and seamless user authentication and authorization. By leveraging Keycloak\u2019s capabilities, the solution enhances security, simplifies user management, and provides scalability for future authentication needs.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Overview/","title":"Taikun OCP Baremetal Overview","text":"<p>Taikun OCP is based on OpenStack (certified etc.) and Taikun OCP BareMetal Addon is based on Mojo.\u00a0Taikun.cloud is developing Taikun OCP Baremetal. Taikun OCP Mojo uses our own code, open source code and Metify Mojo code as OEM. Code is built from our repositories, and we release and supply the final product.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Overview/#what-is-taikun-ocp-baremetal","title":"What is Taikun OCP Baremetal?","text":"<p>Taikun OCP Baremetal is a platform to manage servers, storage, network devices, and rack elements via the DMTF Redfish standard 2. Taikun OCP Baremetal\u2019s focus is to remove the need to use multiple proprietary tools and platforms to manage multi-OEM infrastructure.</p> <p>Taikun OCP Baremetal has everything needed to start managing the systems lifecycle:</p> <ul> <li> <p>O/S and app stack provisioning w/ built in iPXE facility.</p> </li> <li> <p>Firmware updates (BIOS, BMC and add-on cards).</p> </li> <li> <p>Bulk BIOS settings assignments.</p> </li> <li> <p>RBAC and logical resource pools for servers.</p> </li> <li> <p>YAML based workflow engine to create repeatable workflows.</p> </li> </ul> <p>Taikun OCP Baremetal stands out as a DICM Platform that aims to simplify and enhance the management of diverse data center elements. Leveraging standards like Redfish, supporting multiple OEMs, offering quick installation, and providing appliance options contribute to its flexibility and usability in different data center environments.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Overview/#who-can-benefit-from-taikun-ocp-baremetal","title":"Who can Benefit from Taikun OCP Baremetal?","text":"<ul> <li> <p>Individuals or businesses seeking relief from the burdens of server infrastructure management, desiring to redirect their focus towards applications and customer-facing deliverables.</p> </li> <li> <p>Operators aiming to enhance server deployments and decommissioning with role-based access control and policy implementation. Taikun OCP Baremetal monitors server usage, ensuring security by locking them upon deployment.</p> </li> <li> <p>Operators interested in creating reusable templates for consistent server deployment, specifying firmware, BIOS settings, operating system footprints, and comprehensive stacks like OpenStack, OpenShift, OpenNebula, etc.</p> </li> <li> <p>Operators in need of robust auditing trails to track provisioning or modifications of assets, providing transparency into actions performed on servers at any given time.</p> </li> <li> <p>Operators committed to ensuring the safety and security of management and provisioning operations for servers, utilizing Redfish and VLANs.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Overview/#who-uses-mojo","title":"Who Uses Mojo?","text":"<p>Mojo is currently is used by fortune 500\u2019s, Telcos and Public Sector customers. Check out Major Leauge Baseball\u2019s Medium\u00a0article\u00a01\u00a0highlighting how Mojo is used to deploy Google Anthos within North American ballparks.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Overview/#how-does-taikun-ocp-baremetal-work","title":"How Does Taikun OCP Baremetal Work?","text":"<p>At its core Taikun OCP Baremetal, is a platform for managing hardware through standards-based protocols. We are focused on fully supporting the\u00a0DMTF Redfish standard\u00a02\u00a0for hardware management. Support for other standards such as vPRO are on the roadmap. Taikun OCP Baremetal works by discovering hardware on your network and then registering that hardware in Taikun OCP Baremetal.</p> <p>For Discovery to happen, Taikun OCP Baremetal must be installed on your network and have access to the management controllers on the hardware you want to manage. For Registration to happen, Taikun OCP Baremetal must have credentials to access the hardware\u2019s Redfish API. Once registered, Taikun OCP Baremetal can manage the hardware through its Redfish API.</p> <ul> <li> <p>Installation Types</p> </li> <li> <p>Virtual Appliance</p> </li> <li> <p>Hardware Appliance</p> </li> <li> <p>Architecture</p> </li> <li> <p>Components</p> </li> <li> <p>Core Functionality</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Overview/#installation-types","title":"Installation Types","text":"<p>There are two types of Taikun OCP Baremetal installations,\u00a0Virtual Appliance\u00a0and\u00a0Hardware Appliance.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Overview/#virtual-appliance","title":"Virtual Appliance","text":"<p>A virtual software appliance that can be self-installed anywhere on your infrastructure. The virtual appliance can be installed on any hardware that supports Docker. See the\u00a0installation documentation\u00a03\u00a0and\u00a0best practices\u00a0section for more details. The virtual appliance is suitable for small infrastructure deployments, for those who want to run Taikun OCP Baremetal on their own hardware or in the cloud, and for evaluation purposes.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Overview/#hardware-appliance","title":"Hardware Appliance","text":"<p>A fully self-contained hardware appliance that is shipped to you and installed in your data center. The hardware appliance is a fully self-contained system that is pre-configured and ready to go.</p> <p>The hardware appliance is suitable for large infrastructure deployments and for those who want to run Taikun OCP Baremetal on hardware but do not want to install it themselves. It is also suitable for those who require a higher level of support as the appliance is certified for Taikun OCP Baremetal.</p> <p></p> <p>The hardware appliance comes out of the box with additional features that are not available in the virtual appliance.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Overview/#architecture","title":"Architecture","text":"<p>Taikun OCP Baremetal is built on a microservices architecture. Each microservice is a containerized software application built to be run by an\u00a0OCI compliant runtime. Depending on the installation pattern (virtual or hardware) the containers are run in different setups and with different configurations. Groups of microservices are packaged together and deployed as \u201cComponents\u201d.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Overview/#components","title":"Components","text":"<p>The main Components that make up Taikun OCP Baremetal are:</p> <ul> <li> <p>Coordinator\u00a0\u2013 The core Taikun OCP Baremetal experience: databases, APIs, message brokers/queues, job workers, and the Taikun OCP Baremetal Web UI.</p> </li> <li> <p>TOR\u00a0\u2013 \u201cTop Of the Rack\u201d \u2013 Supporting microservice that allow Taikun OCP Baremetal to provision servers and help separate network traffic from the Coordinator component. These include web proxies, DNS, DHCP, TFTP, Samba, firmware and OS image catalogs, and more.</p> </li> <li> <p>Gateway\u00a0(hardware-appliance only) \u2013 A traffic gateway for Taikun OCP Baremetal that allows you to monitor and control all traffic in and out of your Taikun OCP Baremetal infrastructure. The Gateway is a FreeBSD-based system and provides additional features on top of the TOR and Coordinator components. The Gateway is not available in the virtual appliance. Gateway features include:</p> </li> <li> <p>True network separation of BMC, Management, Services, and Provisioning networks</p> </li> <li> <p>QOS</p> </li> <li> <p>BMC Firewall \u2013 Isolate your BMCs from the rest of your network and monitor all traffic through the BMC network</p> </li> <li> <p>IPS/IDS \u2013 Intrusion detection and prevention system for your BMC network</p> </li> <li> <p>Monitoring \u2013 Built-in Grafana dashboards for your Taikun OCP Baremetal-managed infrastructure and networks</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Mojo/Taikun_OCP_Baremetal_Overview/#core-functionality","title":"Core Functionality","text":"<p>Taikun OCP Baremetal manages infrastructure by first Discovering hardware on your user-defined networks and then Registering the hardware in Taikun OCP Baremetal. The level of management available to each piece of hardware is dependent on the hardware itself and the features it supports. Taikun OCP Baremetal can manage any hardware that supports the Redfish standard and has abstracted common hardware management tasks into a set of features that can be applied to any hardware that supports them.</p> <p>Redfish implementations vary from vendor to vendor and Taikun OCP Baremetal does its best to support as many features as possible across all vendors. The power of Taikun OCP Baremetal is that we have taken the time to understand and manage different vendor implementations of Redfish so that you don\u2019t have to. We support common functionality across all hardware vendors through a single web interface and API.\u00a0\u00a0Taikun OCP Baremetal is a single pane of glass in which to view and manage your entire infrastructure.</p> <p>For a server, Taikun OCP Baremetal can manage the power state, LEDs, and BIOS settings. Taikun OCP Baremetal can also manage the firmware and OS images on your servers and provision them with the OS of your choice. Deployment profiles can be applied to groups of servers to create application clusters for usage outside of Taikun OCP Baremetal. Taikun OCP Baremetal can work with the storage controllers inside of your servers to manage the RAID configuration and disks. Taikun OCP Baremetal can open a virtual console to the server directly from the Taikun OCP Baremetal Web UI.</p> <p>Taikun OCP Baremetal can also work with other hardware that supports the Redfish standard such as PDUs, Switches, and Chassis. Taikun OCP Baremetal can manage the power state of a PDU, or the port configuration of a switch. Taikun OCP Baremetal can also manage the firmware and OS images on your PDUs and Switches.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Monitoring/Prometheus_Alertmanager/","title":"Prometheus Alertmanager","text":"<p>alert manager</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Monitoring/Prometheus_Service_Overview/","title":"Prometheus Service Overview","text":"<p>service overview</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Floating_IP_Port_Forwarding/","title":"Floating IP Port Forwarding","text":"<p>Floating IP port forwarding enables users to forward traffic from a TCP/UDP/other protocol port of a floating IP to a TCP/UDP/other protocol port associated to one of the fixed IPs of a Neutron port. This is accomplished by associating\u00a0<code>port_forwarding</code>\u00a0sub-resource to a floating IP.</p> <p>CRUD operations for port forwarding are implemented by a Neutron API extension and a service plug-in. Please refer to the Neutron API Reference documentation for details on the CRUD operations.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Floating_IP_Port_Forwarding/#configuring-floating-ip-port-forwarding","title":"Configuring floating IP port forwarding","text":"<p>To configure floating IP port forwarding, take the following steps:</p> <p>Add the\u00a0<code>port_forwarding</code>\u00a0service to the\u00a0<code>service_plugins</code>\u00a0setting in\u00a0<code>/etc/neutron/neutron.conf</code>. For example:</p> <pre><code>service_plugins = router,segments,port_forwarding\n</code></pre> <p>Set the\u00a0<code>extensions</code>\u00a0option in the\u00a0<code>[agent]</code>\u00a0section of\u00a0<code>/etc/neutron/l3_agent.ini</code>\u00a0to include\u00a0<code>port_forwarding</code>. This has to be done in each network and compute node where the L3 agent is running. For example:</p> <pre><code>extensions = port_forwarding\n</code></pre> <p>Note The\u00a0<code>router</code>\u00a0service plug-in manages floating IPs and routers. As a consequence, it has to be configured along with the\u00a0<code>port_forwarding</code>\u00a0service plug-in.</p> <p>Note After updating the options in the configuration files, the neutron-server and every neutron-l3-agent need to be restarted for the new values to take effect.</p> <p>After configuring floating IP port forwarding, the\u00a0<code>floating-ip-port-forwarding</code>\u00a0extension alias will be included in the output of the following command:</p> <pre><code>$ openstack extension list --network\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Network_CLI_Guide/","title":"Network CLI Guide","text":"<p>A\u00a0network\u00a0is an isolated Layer 2 networking segment. There are two types of networks, project and provider networks. Project networks are fully isolated and are not shared with other projects. Provider networks map to existing physical networks in the data center and provide external network access for servers and other resources. Only an OpenStack administrator can create provider networks. Networks can be connected via routers.</p> <p>Compute v2, Network v2</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Network_CLI_Guide/#network-create","title":"network create","text":"<p>Create new network</p> <p>openstack network create</p> <ul> <li>    [--extra-property type=\\,name=\\,value=\\]       [--share | --no-share]       [--enable | --disable]       [--project \\]       [--description \\]       [--mtu \\]       [--project-domain \\]       [--availability-zone-hint \\]       [--enable-port-security | --disable-port-security]       [--external | --internal]       [--default | --no-default]       [--qos-policy \\]       [--transparent-vlan | --no-transparent-vlan]       [--provider-network-type \\]       [--provider-physical-network \\]       [--provider-segment \\]       [--dns-domain \\]       [--tag \\ | --no-tag]       --subnet \\       \\ <p>extra-property\u00a0type=\\,name=\\,value=\\ <ul> <li>Additional parameters can be passed using this property. Default type of the extra property is string (\u2018str\u2019), but other types can be used as well. Available types are: \u2018dict\u2019, \u2018list\u2019, \u2018str\u2019, \u2018bool\u2019, \u2018int\u2019. In case of \u2018list\u2019 type, \u2018value\u2019 can be semicolon-separated list of values. For \u2018dict\u2019 value is semicolon-separated list of the key:value pairs.</li> </ul> <p>share</p> <ul> <li>Share the network between projects</li> </ul> <p>no-share</p> <ul> <li>Do not share the network between projects</li> </ul> <p>enable</p> <ul> <li>Enable network (default)</li> </ul> <p>Network version 2 only</p> <p>disable</p> <ul> <li>Disable network</li> </ul> <p>Network version 2 only</p> <p>project\u00a0\\ <ul> <li>Owner\u2019s project (name or ID)</li> </ul> <p>Network version 2 only</p> <p>description\u00a0\\ <ul> <li>Set network description</li> </ul> <p>Network version 2 only</p> <p>mtu\u00a0\\ <ul> <li>Set network mtu</li> </ul> <p>Network version 2 only</p> <p>project-domain\u00a0\\ <ul> <li>Domain the project belongs to (name or ID). This can be used in case collisions between project names exist.</li> </ul> <p>availability-zone-hint\u00a0\\ <ul> <li>Availability Zone in which to create this network (Network Availability Zone extension required, repeat option to set multiple availability zones)</li> </ul> <p>Network version 2 only</p> <p>enable-port-security</p> <ul> <li>Enable port security by default for ports created on this network (default)</li> </ul> <p>Network version 2 only</p> <p>disable-port-security</p> <ul> <li>Disable port security by default for ports created on this network</li> </ul> <p>Network version 2 only</p> <p>external</p> <ul> <li>Set this network as an external network (external-net extension required)</li> </ul> <p>Network version 2 only</p> <p>internal</p> <ul> <li>Set this network as an internal network (default)</li> </ul> <p>Network version 2 only</p> <p>default</p> <ul> <li>Specify if this network should be used as the default external network</li> </ul> <p>Network version 2 only</p> <p>no-default</p> <ul> <li>Do not use the network as the default external network (default)</li> </ul> <p>Network version 2 only</p> <p>qos-policy\u00a0\\ <ul> <li>QoS policy to attach to this network (name or ID)</li> </ul> <p>Network version 2 only</p> <p>transparent-vlan</p> <ul> <li>Make the network VLAN transparent</li> </ul> <p>Network version 2 only</p> <p>no-transparent-vlan</p> <ul> <li>Do not make the network VLAN transparent</li> </ul> <p>Network version 2 only</p> <p>provider-network-type\u00a0\\ <ul> <li>The physical mechanism by which the virtual network is implemented. For example: flat, geneve, gre, local, vlan, vxlan.</li> </ul> <p>provider-physical-network\u00a0\\ <ul> <li>Name of the physical network over which the virtual network is implemented</li> </ul> <p>provider-segment\u00a0\\ <ul> <li>VLAN ID for VLAN networks or Tunnel ID for GENEVE/GRE/VXLAN networks</li> </ul> <p>dns-domain\u00a0\\ <ul> <li>Set DNS domain for this network (requires DNS integration extension)</li> </ul> <p>tag\u00a0\\ <ul> <li>Tag to be added to the network (repeat option to set multiple tags)</li> </ul> <p>Network version 2 only</p> <p>no-tag</p> <ul> <li>No tags associated with the network</li> </ul> <p>Network version 2 only</p> <p>subnet\u00a0\\ <ul> <li>IPv4 subnet for fixed IPs (in CIDR notation)</li> </ul> <p>Compute version 2 only</p> <p>name</p> <ul> <li>New network name</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Network_CLI_Guide/#network-delete","title":"network delete","text":"<p>Delete network(s)</p> <ul> <li>openstack network delete \\ [\\ ...] <p>network</p> <ul> <li>Network(s) to delete (name or ID)</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Network_CLI_Guide/#network-list","title":"network list","text":"<p>List networks</p> <p>openstack network list</p> <ul> <li>    [--sort-column SORT_COLUMN]       [--sort-ascending | --sort-descending]       [--external | --internal]       [--long]       [--name \\]       [--enable | --disable]       [--project \\]       [--project-domain \\]       [--share | --no-share]       [--status \\]       [--provider-network-type \\]       [--provider-physical-network \\]       [--provider-segment \\]       [--agent \\]       [--tags \\[,\\,...]]       [--any-tags \\[,\\,...]]       [--not-tags \\[,\\,...]]       [--not-any-tags \\[,\\,...]] <p>sort-column\u00a0SORT_COLUMN</p> <ul> <li>specify the column(s) to sort the data (columns specified first have a priority, non-existing columns are ignored), can be repeated</li> </ul> <p>sort-ascending</p> <ul> <li>sort the column(s) in ascending order</li> </ul> <p>sort-descending</p> <ul> <li>sort the column(s) in descending order</li> </ul> <p>external</p> <ul> <li>List external networks</li> </ul> <p>Network version 2 only</p> <p>internal</p> <ul> <li>List internal networks</li> </ul> <p>Network version 2 only</p> <p>long</p> <ul> <li>List additional fields in output</li> </ul> <p>Network version 2 only</p> <p>name\u00a0\\ <ul> <li>List networks according to their name</li> </ul> <p>Network version 2 only</p> <p>enable</p> <ul> <li>List enabled networks</li> </ul> <p>Network version 2 only</p> <p>disable</p> <ul> <li>List disabled networks</li> </ul> <p>Network version 2 only</p> <p>project\u00a0\\ <ul> <li>List networks according to their project (name or ID)</li> </ul> <p>project-domain\u00a0\\ <ul> <li>Domain the project belongs to (name or ID). This can be used in case collisions between project names exist.</li> </ul> <p>Network version 2 only</p> <p>share</p> <ul> <li>List networks shared between projects</li> </ul> <p>Network version 2 only</p> <p>no-share</p> <ul> <li>List networks not shared between projects</li> </ul> <p>Network version 2 only</p> <p>status\u00a0\\ <ul> <li>List networks according to their status (\u2018ACTIVE\u2019, \u2018BUILD\u2019, \u2018DOWN\u2019, \u2018ERROR\u2019)</li> </ul> <p>Network version 2 only</p> <p>provider-network-type\u00a0\\ <ul> <li>List networks according to their physical mechanisms. The supported options are: flat, geneve, gre, local, vlan, vxlan.</li> </ul> <p>Network version 2 only</p> <p>provider-physical-network\u00a0\\\u00b6 <ul> <li>List networks according to name of the physical network</li> </ul> <p>Network version 2 only</p> <p>provider-segment\u00a0\\\u00b6 <ul> <li>List networks according to VLAN ID for VLAN networks or Tunnel ID for GENEVE/GRE/VXLAN networks</li> </ul> <p>Network version 2 only</p> <p>agent\u00a0\\ <ul> <li>List networks hosted by agent (ID only)</li> </ul> <p>Network version 2 only</p> <p>tags\u00a0\\[,\\,\u2026] <ul> <li>List networks which have all given tag(s) (Comma-separated list of tags)</li> </ul> <p>Network version 2 only</p> <p>any-tags\u00a0\\[,\\,\u2026] <ul> <li>List networks which have any given tag(s) (Comma-separated list of tags)</li> </ul> <p>Network version 2 only</p> <p>not-tags\u00a0\\[,\\,\u2026] <ul> <li>Exclude networks which have all given tag(s) (Comma-separated list of tags)</li> </ul> <p>Network version 2 only</p> <p>not-any-tags\u00a0\\[,\\,\u2026] <ul> <li>Exclude networks which have any given tag(s) (Comma-separated list of tags)</li> </ul> <p>Network version 2 only</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Network_CLI_Guide/#network-set","title":"network set","text":"<p>Set network properties</p> <p>openstack network set</p> <ul> <li>    [--extra-property type=\\,name=\\,value=\\]       [--name \\]       [--enable | --disable]       [--share | --no-share]       [--description \\&lt;description]       [--mtu \\&lt;mtu]       [--enable-port-security | --disable-port-security]       [--external | --internal]       [--default | --no-default]       [--qos-policy \\ | --no-qos-policy]       [--tag \\]       [--no-tag]       [--provider-network-type \\]       [--provider-physical-network \\]       [--provider-segment \\]       [--dns-domain \\]       \\ <p>extra-property\u00a0type=\\,name=\\,value=\\ <ul> <li>Additional parameters can be passed using this property. Default type of the extra property is string (\u2018str\u2019), but other types can be used as well. Available types are: \u2018dict\u2019, \u2018list\u2019, \u2018str\u2019, \u2018bool\u2019, \u2018int\u2019. In case of \u2018list\u2019 type, \u2018value\u2019 can be semicolon-separated list of values. For \u2018dict\u2019 value is semicolon-separated list of the key:value pairs.</li> </ul> <p>name\u00a0\\ <ul> <li>Set network name</li> </ul> <p>enable</p> <ul> <li>Enable network</li> </ul> <p>disable</p> <ul> <li>Disable network</li> </ul> <p>share</p> <ul> <li>Share the network between projects</li> </ul> <p>no-share</p> <ul> <li>Do not share the network between projects</li> </ul> <p>description\u00a0\\&lt;description</p> <ul> <li>Set network description</li> </ul> <p>mtu\u00a0\\&lt;mtu</p> <ul> <li>Set network mtu</li> </ul> <p>enable-port-security</p> <ul> <li>Enable port security by default for ports created on this network</li> </ul> <p>disable-port-security</p> <ul> <li>Disable port security by default for ports created on this network</li> </ul> <p>external</p> <ul> <li>Set this network as an external network (external-net extension required)</li> </ul> <p>internal</p> <ul> <li>Set this network as an internal network</li> </ul> <p>default</p> <ul> <li>Set the network as the default external network</li> </ul> <p>no-default</p> <ul> <li>Do not use the network as the default external network</li> </ul> <p>qos-policy\u00a0\\ <ul> <li>QoS policy to attach to this network (name or ID)</li> </ul> <p>no-qos-policy</p> <ul> <li>Remove the QoS policy attached to this network</li> </ul> <p>tag\u00a0\\ <ul> <li>Tag to be added to the network (repeat option to set multiple tags)</li> </ul> <p>no-tag</p> <ul> <li>Clear tags associated with the network. Specify both \u2013tag and \u2013no-tag to overwrite current tags</li> </ul> <p>provider-network-type\u00a0\\ <ul> <li>The physical mechanism by which the virtual network is implemented. For example: flat, geneve, gre, local, vlan, vxlan.</li> </ul> <p>provider-physical-network\u00a0\\ <ul> <li>Name of the physical network over which the virtual network is implemented</li> </ul> <p>provider-segment\u00a0\\ <ul> <li>VLAN ID for VLAN networks or Tunnel ID for GENEVE/GRE/VXLAN networks</li> </ul> <p>dns-domain\u00a0\\ <ul> <li>Set DNS domain for this network (requires DNS integration extension)</li> </ul> <p>network</p> <ul> <li>Network to modify (name or ID)</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Network_CLI_Guide/#network-show","title":"network show","text":"<p>Show network details</p> <ul> <li><code>openstack network show \\&lt;network&gt;</code></li> </ul> <p>network</p> <ul> <li>Network to display (name or ID)</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Network_CLI_Guide/#network-unset","title":"network unset","text":"<p>Unset network properties</p> <p>openstack network unset</p> <ul> <li>[--extra-property type=\\,name=\\,value=\\           [--tag \\ | --all-tag]    \\ <p>extra-property\u00a0type=\\,name=\\,value=\\ <ul> <li>Additional parameters can be passed using this property. Default type of the extra property is string (\u2018str\u2019), but other types can be used as well. Available types are: \u2018dict\u2019, \u2018list\u2019, \u2018str\u2019, \u2018bool\u2019, \u2018int\u2019. In case of \u2018list\u2019 type, \u2018value\u2019 can be semicolon-separated list of values. For \u2018dict\u2019 value is semicolon-separated list of the key:value pairs.</li> </ul> <p>tag\u00a0\\ <ul> <li>Tag to be removed from the network (repeat option to remove multiple tags)</li> </ul> <p>all-tag</p> <ul> <li>Clear all tags associated with the network</li> </ul> <p>network</p> <ul> <li>Network to modify (name or ID)</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Quality_of_Service/","title":"Quality of Service (QoS)","text":"<p>QoS is defined as the ability to guarantee certain network requirements like bandwidth, latency, jitter, and reliability in order to satisfy a Service Level Agreement (SLA) between an application provider and end users.</p> <p>Network devices such as switches and routers can mark traffic so that it is handled with a higher priority to fulfill the QoS conditions agreed under the SLA. In other cases, certain network traffic such as Voice over IP (VoIP) and video streaming needs to be transmitted with minimal bandwidth constraints. On a system without network QoS management, all traffic will be transmitted in a \u201cbest-effort\u201d manner making it impossible to guarantee service delivery to customers.</p> <p>QoS is an advanced service plug-in. QoS is decoupled from the rest of the OpenStack Networking code on multiple levels and it is available through the ml2 extension driver.</p> <p>Details about the DB models, API extension, and use cases are out of the scope of this guide but can be found in the\u00a0Neutron QoS specification.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Quality_of_Service/#supported-qos-rule-types","title":"Supported QoS rule types","text":"<p>QoS supported rule types are now available as\u00a0<code>VALID_RULE_TYPES</code>\u00a0in\u00a0QoS rule types:</p> <ul> <li> <p>bandwidth_limit: Bandwidth limitations on networks, ports or floating IPs.</p> </li> <li> <p>packet_rate_limit: Packet rate limitations on certain types of traffic.</p> </li> <li> <p>dscp_marking: Marking network traffic with a DSCP value.</p> </li> <li> <p>minimum_bandwidth: Minimum bandwidth constraints on certain types of traffic.</p> </li> <li> <p>minimum_packet_rate: Minimum packet rate constraints on certain types of traffic.</p> </li> </ul> <p>Any QoS driver can claim support for some QoS rule types by providing a driver property called\u00a0<code>supported_rules</code>, the QoS driver manager will recalculate rule types dynamically that the QoS driver supports. In the most simple case, the property can be represented by a simple Python list defined on the class.</p> <p>The following table shows the Networking back ends, QoS supported rules, and traffic directions (from the VM point of view).</p> <p>Networking back ends, supported rules, and traffic direction</p> Rule \\ Back End Open vSwitch SR-IOV Linux Bridge OVN Bandwidth limit Egress \\ Ingress Egress (1) Egress \\ Ingress Egress \\ Ingress Packet rate limit Egress \\ Ingress \u2022 \u2022 \u2022 Minimum bandwidth Egress \\ Ingress (2) Egress \\ Ingress (2) \u2022 \u2022 Minimum packet rate \u2022 \u2022 \u2022 \u2022 DSCP marking Egress \u2022 Egress Egress <p>Note 1. Max burst parameter is skipped because it is not supported by the IP tool. 2. Placement based enforcement works for both egress and ingress directions, but dataplane enforcement depends on the backend.</p> <p>Neutron backends, supported directions and enforcement types for Minimum Bandwidth rule</p> Enforcement Type Backend Open vSwitch SR-IOV Linux Bridge OVN Dataplane Egress (3) Egress (1) \u2022 \u2022 Placement Egress/Ingress (2) Egress/Ingress (2) \u2022 \u2022 <p>Note 1. Since Newton 2. Since Stein 3. Open vSwitch minimum bandwidth support is only implemented for egress direction and only for networks without tunneled traffic (only VLAN and flat network types).</p> <p>Note The SR-IOV agent does not support dataplane enforcement for ports with\u00a0<code>direct-physical</code>\u00a0vnic_type. However since Yoga the Placement enforcement is supported for this vnic_type too.</p> <p>Neutron backends, supported directions and enforcement types for Minimum Packet Rate rule</p> Enforcement Type Backend Open vSwitch SR-IOV Linux Bridge OVN Dataplane \u2022 \u2022 \u2022 \u2022 Placement Any(1)/Engress/Ingress(2) \u2022 \u2022 \u2022 <p>Note 1. Minimum packet rate rule supports\u00a0<code>any</code>\u00a0direction that can be used with non-hardware-offloaded OVS deployments, where packets processed from both ingress and egress directions are handled by the same set of CPU cores. 2. Since Yoga.</p> <p>For an ml2 plug-in, the list of supported QoS rule types and parameters is defined as a common subset of rules supported by all active mechanism drivers. A QoS rule is always attached to a QoS policy. When a rule is created or updated:</p> <ul> <li> <p>The QoS plug-in will check if this rule and parameters are supported by any active mechanism driver if the QoS policy is not attached to any port or network.</p> </li> <li> <p>The QoS plug-in will check if this rule and parameters are supported by the mechanism drivers managing those ports if the QoS policy is attached to any port or network.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Quality_of_Service/#valid-dscp-marks","title":"Valid DSCP Marks","text":"<p>Valid DSCP mark values are even numbers between 0 and 56, except 2-6, 42, 44, and 50-54. The full list of valid DSCP marks is:</p> <p>0, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 46, 48, 56</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Quality_of_Service/#l3-qos-support","title":"L3 QoS support","text":"<p>The Neutron L3 services have implemented their own QoS extensions. Currently only bandwidth limit QoS is provided. This is the L3 QoS extension list:</p> <ul> <li> <p>Floating IP bandwidth limit: the rate limit is applied per floating IP address independently.</p> </li> <li> <p>Gateway IP bandwidth limit: the rate limit is applied in the router namespace gateway port (or in the SNAT namespace in case of DVR edge router). The rate limit applies to the gateway IP; that means all traffic using this gateway IP will be limited. This rate limit does not apply to the floating IP traffic.</p> </li> </ul> <p>L3 services that provide QoS extensions:</p> <ul> <li> <p>L3 router: implements the rate limit using\u00a0Linux TC.</p> </li> <li> <p>OVN L3: implements the rate limit using the\u00a0OVN QoS metering rules.</p> </li> </ul> <p>The following table shows the L3 service, the QoS supported extension, and traffic directions (from the VM point of view) for\u00a0bandwidth limiting.</p> <p>L3 service, supported extension, and traffic direction</p> Rule\\L3 service L3 Router OVN L3 Floating IP Egress\\Ingress Egress\\Ingress Gateway IP Egress\\Ingress \u2022"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Quality_of_Service/#configuration","title":"Configuration","text":"<p>To enable the service on a cloud with the architecture described in\u00a0Networking architecture, follow the steps below:</p> <p>On the controller nodes:</p> <p>1. Add the QoS service to the\u00a0<code>service_plugins</code>\u00a0setting in\u00a0<code>/etc/neutron/neutron.conf</code>. For example:</p> <pre><code>service_plugins = router,metering,qos\n</code></pre> <p>2. Optionally, set the needed\u00a0<code>notification_drivers</code>\u00a0in the\u00a0<code>[qos]</code>\u00a0section in\u00a0<code>/etc/neutron/neutron.conf</code>\u00a0(<code>message_queue</code>\u00a0is the default).</p> <p>3. Optionally, in order to enable the floating IP QoS extension\u00a0<code>qos-fip</code>, set the\u00a0<code>service_plugins</code>\u00a0option in\u00a0<code>/etc/neutron/neutron.conf</code>\u00a0to include both\u00a0<code>router</code>\u00a0and\u00a0<code>qos</code>. For example:</p> <pre><code>service_plugins = router,qos\n</code></pre> <p>4. In\u00a0<code>/etc/neutron/plugins/ml2/ml2_conf.ini</code>, add\u00a0<code>qos</code>\u00a0to\u00a0<code>extension_drivers</code>\u00a0in the\u00a0<code>[ml2]</code>\u00a0section. For example:</p> <pre><code>[ml2]\nextension_drivers = port_security,qos\n</code></pre> <p>5. Edit the configuration file for the agent you are using and set the\u00a0<code>extensions</code>\u00a0to include\u00a0<code>qos</code>\u00a0in the\u00a0<code>[agent]</code>\u00a0section of the configuration file. The agent configuration file will reside in\u00a0<code>/etc/neutron/plugins/ml2/&lt;agent_name&gt;_agent.ini</code>\u00a0where\u00a0<code>agent_name</code>\u00a0is the name of the agent being used (for example\u00a0<code>openvswitch</code>). For example:</p> <pre><code>[agent]\nextensions = qos\n</code></pre> <p>On the network and compute nodes:</p> <p>1. Edit the configuration file for the agent you are using and set the\u00a0<code>extensions</code>\u00a0to include\u00a0<code>qos</code>\u00a0in the\u00a0<code>[agent]</code>\u00a0section of the configuration file. The agent configuration file will reside in\u00a0<code>/etc/neutron/plugins/ml2/&lt;agent_name&gt;_agent.ini</code>\u00a0where\u00a0<code>agent_name</code>\u00a0is the name of the agent being used (for example\u00a0<code>openvswitch</code>). For example:</p> <pre><code>[agent]\nextensions = qos\n</code></pre> <p>2. Optionally, in order to enable QoS for floating IPs, set the\u00a0<code>extensions</code>\u00a0option in the\u00a0<code>[agent]</code>\u00a0section of\u00a0<code>/etc/neutron/l3_agent.ini</code>\u00a0to include\u00a0<code>fip_qos</code>. If\u00a0<code>dvr</code>\u00a0is enabled, this has to be done for all the L3 agents. For example:</p> <pre><code>[agent]\nextensions = fip_qos\n</code></pre> <p>Note Floating IP associated to neutron port or to port forwarding can all have bandwidth limit since Stein release. These neutron server side and agent side extension configs will enable it once for all.</p> <p>Optionally, in order to enable QoS for router gateway IPs, set the\u00a0<code>extensions</code>\u00a0option in the\u00a0<code>[agent]</code>\u00a0section of\u00a0<code>/etc/neutron/l3_agent.ini</code>\u00a0to include\u00a0<code>gateway_ip_qos</code>. Set this to all the\u00a0<code>dvr_snat</code>\u00a0or\u00a0<code>legacy</code>\u00a0L3 agents. For example:</p> <pre><code>[agent]\nextensions = gateway_ip_qos\n</code></pre> <p>And\u00a0<code>gateway_ip_qos</code>\u00a0should work together with the\u00a0<code>fip_qos</code>\u00a0in L3 agent for centralized routers, then all L3 IPs with binding QoS policy can be limited under the QoS bandwidth limit rules:</p> <pre><code>[agent]\nextensions = fip_qos, gateway_ip_qos\n</code></pre> <p>2. As rate limit doesn\u2019t work on Open vSwitch\u2019s\u00a0<code>internal</code>\u00a0ports, optionally, as a workaround, to make QoS bandwidth limit work on router\u2019s gateway ports, set\u00a0<code>ovs_use_veth</code>\u00a0to\u00a0<code>True</code>\u00a0in\u00a0<code>DEFAULT</code>\u00a0section in\u00a0<code>/etc/neutron/l3_agent.ini</code></p> <pre><code>[DEFAULT]\novs_use_veth = True\n</code></pre> <p>Note QoS currently works with ml2 only (SR-IOV, Open vSwitch, and linuxbridge are drivers enabled for QoS).</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Quality_of_Service/#dscp-marking-on-outer-header-for-overlay-networks","title":"DSCP marking on outer header for overlay networks","text":"<p>When using overlay networks (e.g., VxLAN), the DSCP marking rule only applies to the inner header, and during encapsulation, the DSCP mark is not automatically copied to the outer header.</p> <p>1. In order to set the DSCP value of the outer header, modify the\u00a0<code>dscp</code>\u00a0configuration option in\u00a0<code>/etc/neutron/plugins/ml2/&lt;agent_name&gt;_agent.ini</code>\u00a0where\u00a0<code>&lt;agent_name&gt;</code>\u00a0is the name of the agent being used (e.g.,\u00a0<code>openvswitch</code>):</p> <pre><code>[agent]\ndscp = 8\n</code></pre> <p>2. In order to copy the DSCP field of the inner header to the outer header, change the\u00a0<code>dscp_inherit</code>\u00a0configuration option to true in\u00a0<code>/etc/neutron/plugins/ml2/&lt;agent_name&gt;_agent.ini</code>\u00a0where\u00a0<code>&lt;agent_name&gt;</code>\u00a0is the name of the agent being used (e.g.,\u00a0<code>openvswitch</code>):</p> <pre><code>[agent]\ndscp_inherit = true\n</code></pre> <p>1. If the\u00a0<code>dscp_inherit</code>\u00a0option is set to true, the previous\u00a0<code>dscp</code>\u00a0option is overwritten.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Quality_of_Service/#trusted-projects-policyyaml-configuration","title":"Trusted projects policy.yaml configuration","text":"<p>If projects are trusted to administrate their own QoS policies in your cloud, neutron\u2019s file\u00a0<code>policy.yaml</code>\u00a0can be modified to allow this.</p> <p>Modify\u00a0<code>/etc/neutron/policy.yaml</code>\u00a0policy entries as follows:</p> <pre><code>\"get_policy\": \"rule:regular_user\",\n\"create_policy\": \"rule:regular_user\",\n\"update_policy\": \"rule:regular_user\",\n\"delete_policy\": \"rule:regular_user\",\n\"get_rule_type\": \"rule:regular_user\",\n</code></pre> <p>To enable bandwidth limit rule:</p> <pre><code>\"get_policy_bandwidth_limit_rule\": \"rule:regular_user\",\n\"create_policy_bandwidth_limit_rule\": \"rule:regular_user\",\n\"delete_policy_bandwidth_limit_rule\": \"rule:regular_user\",\n\"update_policy_bandwidth_limit_rule\": \"rule:regular_user\",\n</code></pre> <p>To enable DSCP marking rule:</p> <pre><code>\"get_policy_dscp_marking_rule\": \"rule:regular_user\",\n\"create_policy_dscp_marking_rule\": \"rule:regular_user\",\n\"delete_policy_dscp_marking_rule\": \"rule:regular_user\",\n\"update_policy_dscp_marking_rule\": \"rule:regular_user\",\n</code></pre> <p>To enable minimum bandwidth rule:</p> <pre><code>\"get_policy_minimum_bandwidth_rule\": \"rule:regular_user\",\n\"create_policy_minimum_bandwidth_rule\": \"rule:regular_user\",\n\"delete_policy_minimum_bandwidth_rule\": \"rule:regular_user\",\n\"update_policy_minimum_bandwidth_rule\": \"rule:regular_user\",\n</code></pre> <p>To enable minimum packet rate rule:</p> <pre><code>\"get_policy_minimum_packet_rate_rule\": \"rule:regular_user\",\n\"create_policy_minimum_packet_rate_rule\": \"rule:regular_user\",\n\"delete_policy_minimum_packet_rate_rule\": \"rule:regular_user\",\n\"update_policy_minimum_packet_rate_rule\": \"rule:regular_user\",\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Quality_of_Service/#user-workflow","title":"User workflow","text":"<p>QoS policies are only created by admins with the default\u00a0<code>policy.yaml</code>. Therefore, you should have the cloud operator set them up on behalf of the cloud projects.</p> <p>If projects are trusted to create their own policies, check the trusted projects\u00a0<code>policy.yaml</code>\u00a0configuration section.</p> <p>First, create a QoS policy and its bandwidth limit rule:</p> <pre><code>$ openstack network qos policy create bw-limiter\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| description       |                                      |\n| id                | 5df855e9-a833-49a3-9c82-c0839a5f103f |\n| is_default        | False                                |\n| name              | bw-limiter                           |\n| project_id        | 4db7c1ed114a4a7fb0f077148155c500     |\n| rules             | []                                   |\n| shared            | False                                |\n+-------------------+--------------------------------------+\n\n\n$ openstack network qos rule create --type bandwidth-limit --max-kbps 3000 \\\n    --max-burst-kbits 2400 --egress bw-limiter\n+----------------+--------------------------------------+\n| Field          | Value                                |\n+----------------+--------------------------------------+\n| direction      | egress                               |\n| id             | 92ceb52f-170f-49d0-9528-976e2fee2d6f |\n| max_burst_kbps | 2400                                 |\n| max_kbps       | 3000                                 |\n| name           | None                                 |\n| project_id     |                                      |\n+----------------+--------------------------------------+\n</code></pre> <p>Note The QoS implementation requires a burst value to ensure proper behavior of bandwidth limit rules in the Open vSwitch and Linux bridge agents. Configuring the proper burst value is very important. If the burst value is set too low, bandwidth usage will be throttled even with a proper bandwidth limit setting. This issue is discussed in various documentation sources, for example in\u00a0Juniper\u2019s documentation. For TCP traffic it is recommended to set burst value as 80% of desired bandwidth limit value. For example, if the bandwidth limit is set to 1000kbps then enough burst value will be 800kbit. If the configured burst value is too low, achieved bandwidth limit will be lower than expected. If the configured burst value is too high, too few packets could be limited and achieved bandwidth limit would be higher than expected. If you do not provide a value, it defaults to 80% of the bandwidth limit which works for typical TCP traffic.</p> <p>Second, associate the created policy with an existing neutron port. In order to do this, user extracts the port id to be associated to the already created policy. In the next example, we will assign the\u00a0<code>bw-limiter</code>\u00a0policy to the VM with IP address\u00a0<code>192.0.2.1</code>.</p> <pre><code>$ openstack port list\n+--------------------------------------+-----------------------------------+\n| ID                                   | Fixed IP Addresses                |\n+--------------------------------------+-----------------------------------+\n| 0271d1d9-1b16-4410-bd74-82cdf6dcb5b3 | { ... , \"ip_address\": \"192.0.2.1\"}|\n| 88101e57-76fa-4d12-b0e0-4fc7634b874a | { ... , \"ip_address\": \"192.0.2.3\"}|\n| e04aab6a-5c6c-4bd9-a600-33333551a668 | { ... , \"ip_address\": \"192.0.2.2\"}|\n+--------------------------------------+-----------------------------------+\n\n$ openstack port set --qos-policy bw-limiter \\\n    88101e57-76fa-4d12-b0e0-4fc7634b874a\n</code></pre> <pre><code>$ openstack port set --qos-policy bw-limiter \\\n    88101e57-76fa-4d12-b0e0-4fc7634b874a\n</code></pre> <p>In order to detach a port from the QoS policy, simply update again the port configuration.</p> <pre><code>$ openstack port unset --qos-policy 88101e57-76fa-4d12-b0e0-4fc7634b874a\n</code></pre> <p>Ports can be created with a policy attached to them too.</p> <pre><code>$ openstack port create --qos-policy bw-limiter --network private port1\n+-----------------------+--------------------------------------------------+\n| Field                 | Value                                            |\n+-----------------------+--------------------------------------------------+\n| admin_state_up        | UP                                               |\n| allowed_address_pairs |                                                  |\n| binding_host_id       |                                                  |\n| binding_profile       |                                                  |\n| binding_vif_details   |                                                  |\n| binding_vif_type      | unbound                                          |\n| binding_vnic_type     | normal                                           |\n| created_at            | 2017-05-15T08:43:00Z                             |\n| data_plane_status     | None                                             |\n| description           |                                                  |\n| device_id             |                                                  |\n| device_owner          |                                                  |\n| dns_assignment        | None                                             |\n| dns_name              | None                                             |\n| extra_dhcp_opts       |                                                  |\n| fixed_ips             | ip_address='10.0.10.4', subnet_id='292f8c1e-...' |\n| id                    | f51562ee-da8d-42de-9578-f6f5cb248226             |\n| ip_address            | None                                             |\n| mac_address           | fa:16:3e:d9:f2:ba                                |\n| name                  | port1                                            |\n| network_id            | 55dc2f70-0f92-4002-b343-ca34277b0234             |\n| option_name           | None                                             |\n| option_value          | None                                             |\n| port_security_enabled | False                                            |\n| project_id            | 4db7c1ed114a4a7fb0f077148155c500                 |\n| qos_policy_id         | 5df855e9-a833-49a3-9c82-c0839a5f103f             |\n| revision_number       | 6                                                |\n| security_group_ids    | 0531cc1a-19d1-4cc7-ada5-49f8b08245be             |\n| status                | DOWN                                             |\n| subnet_id             | None                                             |\n| tags                  | []                                               |\n| trunk_details         | None                                             |\n| updated_at            | 2017-05-15T08:43:00Z                             |\n+-----------------------+--------------------------------------------------+\n</code></pre> <p>You can attach networks to a QoS policy. The meaning of this is that any compute port connected to the network will use the network policy by default unless the port has a specific policy attached to it. Internal network owned ports like DHCP and internal router ports are excluded from network policy application.</p> <p>In order to attach a QoS policy to a network, update an existing network, or initially create the network attached to the policy.</p> <pre><code>$ openstack network set --qos-policy bw-limiter private\n</code></pre> <p>The created policy can be associated with an existing floating IP. In order to do this, user extracts the floating IP id to be associated to the already created policy. In the next example, we will assign the\u00a0<code>bw-limiter</code>\u00a0policy to the floating IP address\u00a0<code>172.16.100.18</code>.</p> <pre><code>$ openstack floating ip list\n+--------------------------------------+---------------------+------------------+------+-----+\n| ID                                   | Floating IP Address | Fixed IP Address | Port | ... |\n+--------------------------------------+---------------------+------------------+------+-----+\n| 1163d127-6df3-44bb-b69c-c0e916303eb3 | 172.16.100.9        | None             | None | ... |\n| d0ed7491-3eb7-4c4f-a0f0-df04f10a067c | 172.16.100.18       | None             | None | ... |\n| f5a9ed48-2e9f-411c-8787-2b6ecd640090 | 172.16.100.2        | None             | None | ... |\n+--------------------------------------+---------------------+------------------+------+-----+\n</code></pre> <pre><code>$ openstack floating ip set --qos-policy bw-limiter d0ed7491-3eb7-4c4f-a0f0-df04f10a067c\n</code></pre> <p>In order to detach a floating IP from the QoS policy, simply update the floating IP configuration.</p> <pre><code>$ openstack floating ip set --no-qos-policy d0ed7491-3eb7-4c4f-a0f0-df04f10a067c\n</code></pre> <p>Or use the\u00a0<code>unset</code>\u00a0action.</p> <pre><code>$ openstack floating ip unset --qos-policy d0ed7491-3eb7-4c4f-a0f0-df04f10a067c\n</code></pre> <p>Floating IPs can be created with a policy attached to them too.</p> <pre><code>$ openstack floating ip create --qos-policy bw-limiter public\n+---------------------+--------------------------------------+\n| Field               | Value                                |\n+---------------------+--------------------------------------+\n| created_at          | 2017-12-06T02:12:09Z                 |\n| description         |                                      |\n| fixed_ip_address    | None                                 |\n| floating_ip_address | 172.16.100.12                        |\n| floating_network_id | 4065eb05-cccb-4048-988c-e8c5480a746f |\n| id                  | 6a0efeef-462b-4312-b4ad-627cde8a20e6 |\n| name                | 172.16.100.12                        |\n| port_id             | None                                 |\n| project_id          | 916e39e8be52433ba040da3a3a6d0847     |\n| qos_policy_id       | 5df855e9-a833-49a3-9c82-c0839a5f103f |\n| revision_number     | 1                                    |\n| router_id           | None                                 |\n| status              | DOWN                                 |\n| updated_at          | 2017-12-06T02:12:09Z                 |\n+---------------------+--------------------------------------+\n</code></pre> <p>The QoS bandwidth limit rules attached to a floating IP will become active when you associate the latter with a port. For example, to associate the previously created floating IP\u00a0<code>172.16.100.12</code>\u00a0to the instance port with uuid\u00a0<code>a7f25e73-4288-4a16-93b9-b71e6fd00862</code>\u00a0and fixed IP\u00a0<code>192.168.222.5</code>:</p> <pre><code>$ openstack floating ip set --port a7f25e73-4288-4a16-93b9-b71e6fd00862 \\\n0eeb1f8a-de96-4cd9-a0f6-3f535c409558\n</code></pre> <p>Note The QoS policy attached to a floating IP is not applied to a port, it is applied to an associated floating IP only. Thus the ID of QoS policy attached to a floating IP will not be visible in a port\u2019s\u00a0<code>qos_policy_id</code>\u00a0field after asscoating a floating IP to the port. It is only visible in the floating IP attributes.</p> <p>Note For now, the L3 agent floating IP QoS extension only supports\u00a0<code>bandwidth_limit</code>\u00a0rules. Other rule types (like DSCP marking) will be silently ignored for floating IPs. A QoS policy that does not contain any\u00a0<code>bandwidth_limit</code>\u00a0rules will have no effect when attached to a floating IP.</p> <p>If floating IP is bound to a port, and both have binding QoS bandwidth rules, the L3 agent floating IP QoS extension ignores the behavior of the port QoS, and installs the rules from the QoS policy associated to the floating IP on the appropriate device in the router namespace.</p> <p>Each project can have at most one default QoS policy, although it is not mandatory. If a default QoS policy is defined, all new networks created within this project will have this policy assigned, as long as no other QoS policy is explicitly attached during the creation process. If the default QoS policy is unset, no change to existing networks will be made.</p> <p>In order to set a QoS policy as default, the parameter\u00a0<code>--default</code>\u00a0must be used. To unset this QoS policy as default, the parameter\u00a0<code>--no-default</code>\u00a0must be used.</p> <pre><code>$ openstack network qos policy create --default bw-limiter\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| description       |                                      |\n| id                | 5df855e9-a833-49a3-9c82-c0839a5f103f |\n| is_default        | True                                 |\n| name              | bw-limiter                           |\n| project_id        | 4db7c1ed114a4a7fb0f077148155c500     |\n| rules             | []                                   |\n| shared            | False                                |\n+-------------------+--------------------------------------+\n\n$ openstack network qos policy set --no-default bw-limiter\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| description       |                                      |\n| id                | 5df855e9-a833-49a3-9c82-c0839a5f103f |\n| is_default        | False                                |\n| name              | bw-limiter                           |\n| project_id        | 4db7c1ed114a4a7fb0f077148155c500     |\n| rules             | []                                   |\n| shared            | False                                |\n+-------------------+--------------------------------------+\n</code></pre> <p>Create qos policy with packet rate limit rules:</p> <pre><code>$ openstack network qos policy create pps-limiter\n+-------------+--------------------------------------+\n| Field       | Value                                |\n+-------------+--------------------------------------+\n| description |                                      |\n| id          | 97f0ac37-7dd6-4579-8359-3bef0751a505 |\n| is_default  | False                                |\n| name        | pps-limiter                          |\n| project_id  | 1d70739f831b421fb38a27adb368fc17     |\n| rules       | []                                   |\n| shared      | False                                |\n| tags        | []                                   |\n+-------------+--------------------------------------+\n\n$ openstack network qos rule create --max-kpps 1000 --max-burst-kpps 100 --ingress --type packet-rate-limit pps-limiter\n+----------------+--------------------------------------+\n| Field          | Value                                |\n+----------------+--------------------------------------+\n| direction      | ingress                              |\n| id             | 4a1cb166-9661-48d7-bddb-00b7d75846cd |\n| max_burst_kpps | 100                                  |\n| max_kpps       | 1000                                 |\n| name           | None                                 |\n| project_id     |                                      |\n+----------------+--------------------------------------+\n\n$ openstack network qos rule create --max-kpps 1000 --max-burst-kpps 100 --egress --type packet-rate-limit pps-limiter\n+----------------+--------------------------------------+\n| Field          | Value                                |\n+----------------+--------------------------------------+\n| direction      | egress                               |\n| id             | 6abd67f7-0bde-4ad3-ac54-b0a6103b0449 |\n| max_burst_kpps | 100                                  |\n| max_kpps       | 1000                                 |\n| name           | None                                 |\n| project_id     |                                      |\n+----------------+--------------------------------------+\n</code></pre> <p>Note The unit for the rate and burst is kilo (1000) packets per second.</p> <p>Now apply the packet rate limit QoS policy to a Port:</p> <pre><code>$ openstack port set --qos-policy pps-limiter 251948bd-08e4-4569-a47f-ecbc1fd4af4d\n</code></pre> <p>Note Packet rate limit is only supported by the ml2 ovs driver. And it leverages the meter actions of the ovs kernel datapath or the userspace ovs dpdk datapath. The meter action is only supported when the datapath is in user mode or ovs kernel datapath with kernerl version &gt;= 4.15.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Quality_of_Service/#administrator-enforcement","title":"Administrator enforcement","text":"<p>Administrators are able to enforce policies on project ports or networks. As long as the policy is not shared, the project is not be able to detach any policy attached to a network or port.</p> <p>If the policy is shared, the project is able to attach or detach such policy from its own ports and networks.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Quality_of_Service/#rule-modification","title":"Rule modification","text":"<p>You can modify rules at runtime. Rule modifications will be propagated to any attached port.</p> <pre><code>$ openstack network qos rule set --max-kbps 2000 --max-burst-kbits 1600 \\\n    --ingress bw-limiter 92ceb52f-170f-49d0-9528-976e2fee2d6f\n\n$ openstack network qos rule show \\\n    bw-limiter 92ceb52f-170f-49d0-9528-976e2fee2d6f\n+----------------+--------------------------------------+\n| Field          | Value                                |\n+----------------+--------------------------------------+\n| direction      | ingress                              |\n| id             | 92ceb52f-170f-49d0-9528-976e2fee2d6f |\n| max_burst_kbps | 1600                                 |\n| max_kbps       | 2000                                 |\n| name           | None                                 |\n| project_id     |                                      |\n+----------------+--------------------------------------+\n</code></pre> <p>Just like with bandwidth limiting, create a policy for DSCP marking rule:</p> <pre><code>$ openstack network qos policy create dscp-marking\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| description       |                                      |\n| id                | d1f90c76-fbe8-4d6f-bb87-a9aea997ed1e |\n| is_default        | False                                |\n| name              | dscp-marking                         |\n| project_id        | 4db7c1ed114a4a7fb0f077148155c500     |\n| rules             | []                                   |\n| shared            | False                                |\n+-------------------+--------------------------------------+\n</code></pre> <p>You can create, update, list, delete, and show DSCP markings with the neutron client:</p> <pre><code>$ openstack network qos rule create --type dscp-marking --dscp-mark 26 \\\n    dscp-marking\n+----------------+--------------------------------------+\n| Field          | Value                                |\n+----------------+--------------------------------------+\n| dscp_mark      | 26                                   |\n| id             | 115e4f70-8034-4176-8fe9-2c47f8878a7d |\n| name           | None                                 |\n| project_id     |                                      |\n+----------------+--------------------------------------+\n</code></pre> <pre><code>$ openstack network qos rule set --dscp-mark 22 \\\n    dscp-marking 115e4f70-8034-4176-8fe9-2c47f8878a7d\n\n$ openstack network qos rule list dscp-marking\n+--------------------------------------+----------------------------------+\n| ID                                   | DSCP Mark                        |\n+--------------------------------------+----------------------------------+\n| 115e4f70-8034-4176-8fe9-2c47f8878a7d | 22                               |\n+--------------------------------------+----------------------------------+\n\n$ openstack network qos rule show \\\n    dscp-marking 115e4f70-8034-4176-8fe9-2c47f8878a7d\n+----------------+--------------------------------------+\n| Field          | Value                                |\n+----------------+--------------------------------------+\n| dscp_mark      | 22                                   |\n| id             | 115e4f70-8034-4176-8fe9-2c47f8878a7d |\n| name           | None                                 |\n| project_id     |                                      |\n+----------------+--------------------------------------+\n\n$ openstack network qos rule delete \\\n    dscp-marking 115e4f70-8034-4176-8fe9-2c47f8878a7d\n</code></pre> <p>You can also include minimum bandwidth rules in your policy:</p> <pre><code>$ openstack network qos policy create bandwidth-control\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| description       |                                      |\n| id                | 8491547e-add1-4c6c-a50e-42121237256c |\n| is_default        | False                                |\n| name              | bandwidth-control                    |\n| project_id        | 7cc5a84e415d48e69d2b06aa67b317d8     |\n| revision_number   | 1                                    |\n| rules             | []                                   |\n| shared            | False                                |\n+-------------------+--------------------------------------+\n\n$ openstack network qos rule create \\\n  --type minimum-bandwidth --min-kbps 1000 --egress bandwidth-control\n+------------+--------------------------------------+\n| Field      | Value                                |\n+------------+--------------------------------------+\n| direction  | egress                               |\n| id         | da858b32-44bc-43c9-b92b-cf6e2fa836ab |\n| min_kbps   | 1000                                 |\n| name       | None                                 |\n| project_id |                                      |\n+------------+--------------------------------------+\n</code></pre> <p>A policy with a minimum bandwidth ensures best efforts are made to provide no less than the specified bandwidth to each port on which the rule is applied. However, as this feature is not yet integrated with the Compute scheduler, minimum bandwidth cannot be guaranteed.</p> <p>It is also possible to combine several rules in one policy, as long as the type or direction of each rule is different. For example, You can specify two\u00a0<code>bandwidth-limit</code>\u00a0rules, one with\u00a0<code>egress</code>\u00a0and one with\u00a0<code>ingress</code>\u00a0direction.</p> <pre><code>$ openstack network qos rule create --type bandwidth-limit \\\n    --max-kbps 50000 --max-burst-kbits 50000 --egress bandwidth-control\n+----------------+--------------------------------------+\n| Field          | Value                                |\n+----------------+--------------------------------------+\n| direction      | egress                               |\n| id             | 0db48906-a762-4d32-8694-3f65214c34a6 |\n| max_burst_kbps | 50000                                |\n| max_kbps       | 50000                                |\n| name           | None                                 |\n| project_id     |                                      |\n+----------------+--------------------------------------+\n\n$ openstack network qos rule create --type bandwidth-limit \\\n    --max-kbps 10000 --max-burst-kbits 10000 --ingress bandwidth-control\n+----------------+--------------------------------------+\n| Field          | Value                                |\n+----------------+--------------------------------------+\n| direction      | ingress                              |\n| id             | faabef24-e23a-4fdf-8e92-f8cb66998834 |\n| max_burst_kbps | 10000                                |\n| max_kbps       | 10000                                |\n| name           | None                                 |\n| project_id     |                                      |\n+----------------+--------------------------------------+\n\n$ openstack network qos rule create --type minimum-bandwidth \\\n    --min-kbps 1000 --egress bandwidth-control\n+------------+--------------------------------------+\n| Field      | Value                                |\n+------------+--------------------------------------+\n| direction  | egress                               |\n| id         | da858b32-44bc-43c9-b92b-cf6e2fa836ab |\n| min_kbps   | 1000                                 |\n| name       | None                                 |\n| project_id |                                      |\n+------------+--------------------------------------+\n\n$ openstack network qos policy show bandwidth-control\n+-------------------+-------------------------------------------------------------------+\n| Field             | Value                                                             |\n+-------------------+-------------------------------------------------------------------+\n| description       |                                                                   |\n| id                | 8491547e-add1-4c6c-a50e-42121237256c                              |\n| is_default        | False                                                             |\n| name              | bandwidth-control                                                 |\n| project_id        | 7cc5a84e415d48e69d2b06aa67b317d8                                  |\n| revision_number   | 4                                                                 |\n| rules             | [{u'max_kbps': 50000, u'direction': u'egress',                    |\n|                   |   u'type': u'bandwidth_limit',                                    |\n|                   |   u'id': u'0db48906-a762-4d32-8694-3f65214c34a6',                 |\n|                   |   u'max_burst_kbps': 50000,                                       |\n|                   |   u'qos_policy_id': u'8491547e-add1-4c6c-a50e-42121237256c'},     |\n|                   | [{u'max_kbps': 10000, u'direction': u'ingress',                   |\n|                   |   u'type': u'bandwidth_limit',                                    |\n|                   |   u'id': u'faabef24-e23a-4fdf-8e92-f8cb66998834',                 |\n|                   |   u'max_burst_kbps': 10000,                                       |\n|                   |   u'qos_policy_id': u'8491547e-add1-4c6c-a50e-42121237256c'},     |\n|                   |  {u'direction':                                                   |\n|                   |   u'egress', u'min_kbps': 1000, u'type': u'minimum_bandwidth',    |\n|                   |   u'id': u'da858b32-44bc-43c9-b92b-cf6e2fa836ab',                 |\n|                   |   u'qos_policy_id': u'8491547e-add1-4c6c-a50e-42121237256c'}]     |\n| shared            | False                                                             |\n+-------------------+-------------------------------------------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/","title":"Role-Based Access Control (RBAC)","text":"<p>The Role-Based Access Control (RBAC) policy framework enables both operators and users to grant access to resources for specific projects.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#supported-objects-for-sharing-with-specific-projects","title":"Supported objects for sharing with specific projects","text":"<p>Currently, the access that can be granted using this feature is supported by:</p> <ul> <li> <p>Regular port creation permissions on networks (since Liberty).</p> </li> <li> <p>Binding QoS policies permissions to networks or ports (since Mitaka).</p> </li> <li> <p>Attaching router gateways to networks (since Mitaka).</p> </li> <li> <p>Binding security groups to ports (since Stein).</p> </li> <li> <p>Assigning address scopes to subnet pools (since Ussuri).</p> </li> <li> <p>Assigning subnet pools to subnets (since Ussuri).</p> </li> <li> <p>Assigning address groups to security group rules (since Wallaby).</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#sharing-an-object-with-specific-projects","title":"Sharing an object with specific projects","text":"<p>Sharing an object with a specific project is accomplished by creating a policy entry that permits the target project the\u00a0<code>access_as_shared</code>\u00a0action on that object.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#sharing-a-network-with-specific-projects","title":"Sharing a network with specific projects","text":"<p>Create a network to share:</p> <pre><code>$ openstack network create secret_network\n+---------------------------+--------------------------------------+\n| Field                     | Value                                |\n+---------------------------+--------------------------------------+\n| admin_state_up            | UP                                   |\n| availability_zone_hints   |                                      |\n| availability_zones        |                                      |\n| created_at                | 2017-01-25T20:16:40Z                 |\n| description               |                                      |\n| dns_domain                | None                                 |\n| id                        | f55961b9-3eb8-42eb-ac96-b97038b568de |\n| ipv4_address_scope        | None                                 |\n| ipv6_address_scope        | None                                 |\n| is_default                | None                                 |\n| mtu                       | 1450                                 |\n| name                      | secret_network                       |\n| port_security_enabled     | True                                 |\n| project_id                | 61b7eba037fd41f29cfba757c010faff     |\n| provider:network_type     | vxlan                                |\n| provider:physical_network | None                                 |\n| provider:segmentation_id  | 9                                    |\n| qos_policy_id             | None                                 |\n| revision_number           | 3                                    |\n| router:external           | Internal                             |\n| segments                  | None                                 |\n| shared                    | False                                |\n| status                    | ACTIVE                               |\n| subnets                   |                                      |\n| tags                      | []                                   |\n| updated_at                | 2017-01-25T20:16:40Z                 |\n+---------------------------+--------------------------------------+\n</code></pre> <p>Create the policy entry using the\u00a0openstack network rbac create\u00a0command (in this example, the ID of the project we want to share with is\u00a0<code>b87b2fc13e0248a4a031d38e06dc191d</code>):</p> <pre><code>$ openstack network rbac create --target-project \\\nb87b2fc13e0248a4a031d38e06dc191d --action access_as_shared \\\n--type network f55961b9-3eb8-42eb-ac96-b97038b568de\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| action            | access_as_shared                     |\n| id                | f93efdbf-f1e0-41d2-b093-8328959d469e |\n| name              | None                                 |\n| object_id         | f55961b9-3eb8-42eb-ac96-b97038b568de |\n| object_type       | network                              |\n| project_id        | 61b7eba037fd41f29cfba757c010faff     |\n| target_project_id | b87b2fc13e0248a4a031d38e06dc191d     |\n+-------------------+--------------------------------------+\n</code></pre> <p>The\u00a0<code>target-project</code>\u00a0parameter specifies the project that requires access to the network. The\u00a0<code>action</code>\u00a0parameter specifies what the project is allowed to do. The\u00a0<code>type</code>\u00a0parameter says that the target object is a network. The final parameter is the ID of the network we are granting access to.</p> <p>Project\u00a0<code>b87b2fc13e0248a4a031d38e06dc191d</code>\u00a0will now be able to see the network when running\u00a0openstack network list\u00a0and\u00a0openstack network show\u00a0and will also be able to create ports on that network. No other users (other than admins and the owner) will be able to see the network.</p> <p>Note Subnets inherit the RBAC policy entries of their network.</p> <p>To remove access for that project, delete the policy that allows it using the\u00a0openstack network rbac delete\u00a0command:</p> <pre><code>$ openstack network rbac delete f93efdbf-f1e0-41d2-b093-8328959d469e\n</code></pre> <p>If that project has ports on the network, the server will prevent the policy from being deleted until the ports have been deleted:</p> <pre><code>$ openstack network rbac delete f93efdbf-f1e0-41d2-b093-8328959d469e\nRBAC policy on object f93efdbf-f1e0-41d2-b093-8328959d469e\ncannot be removed because other objects depend on it.\n</code></pre> <p>This process can be repeated any number of times to share a network with an arbitrary number of projects.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#sharing-a-qos-policy-with-specific-projects","title":"Sharing a QoS policy with specific projects","text":"<p>Create a QoS policy to share:</p> <pre><code>$ openstack network qos policy create secret_policy\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| description       |                                      |\n| id                | 1f730d69-1c45-4ade-a8f2-89070ac4f046 |\n| name              | secret_policy                        |\n| project_id        | 61b7eba037fd41f29cfba757c010faff     |\n| revision_number   | 1                                    |\n| rules             | []                                   |\n| shared            | False                                |\n| tags              | []                                   |\n+-------------------+--------------------------------------+\n</code></pre> <p>Create the RBAC policy entry using the\u00a0openstack network rbac create\u00a0command (in this example, the ID of the project we want to share with is\u00a0<code>be98b82f8fdf46b696e9e01cebc33fd9</code>):</p> <pre><code>$ openstack network rbac create --target-project \\\nbe98b82f8fdf46b696e9e01cebc33fd9 --action access_as_shared \\\n--type qos_policy 1f730d69-1c45-4ade-a8f2-89070ac4f046\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| action            | access_as_shared                     |\n| id                | 8828e38d-a0df-4c78-963b-e5f215d3d550 |\n| name              | None                                 |\n| object_id         | 1f730d69-1c45-4ade-a8f2-89070ac4f046 |\n| object_type       | qos_policy                           |\n| project_id        | 61b7eba037fd41f29cfba757c010faff     |\n| target_project_id | be98b82f8fdf46b696e9e01cebc33fd9     |\n+-------------------+--------------------------------------+\n</code></pre> <p>The\u00a0<code>target-project</code>\u00a0parameter specifies the project that requires access to the QoS policy. The\u00a0<code>action</code>\u00a0parameter specifies what the project is allowed to do. The\u00a0<code>type</code>\u00a0parameter says that the target object is a QoS policy. The final parameter is the ID of the QoS policy we are granting access to.</p> <p>Project\u00a0<code>be98b82f8fdf46b696e9e01cebc33fd9</code>\u00a0will now be able to see the QoS policy when running\u00a0openstack network qos policy list\u00a0and\u00a0openstack network qos policy show\u00a0and will also be able to bind it to its ports or networks. No other users (other than admins and the owner) will be able to see the QoS policy.</p> <p>To remove access for that project, delete the RBAC policy that allows it using the\u00a0openstack network rbac delete\u00a0command:</p> <pre><code>$ openstack network rbac delete 8828e38d-a0df-4c78-963b-e5f215d3d550\n</code></pre> <p>If that project has ports or networks with the QoS policy applied to them, the server will not delete the RBAC policy until the QoS policy is no longer in use:</p> <pre><code>$ openstack network rbac delete 8828e38d-a0df-4c78-963b-e5f215d3d550\nRBAC policy on object 8828e38d-a0df-4c78-963b-e5f215d3d550\ncannot be removed because other objects depend on it.\n</code></pre> <p>This process can be repeated any number of times to share a qos-policy with an arbitrary number of projects.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#sharing-a-security-group-with-specific-projects","title":"Sharing a security group with specific projects","text":"<p>Create a security group to share:</p> <pre><code>$ openstack security group create my_security_group\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| created_at        | 2019-02-07T06:09:59Z                 |\n| description       | my_security_group                    |\n| id                | 5ba835b7-22b0-4be6-bdbe-e0722d1b5f24 |\n| location          | None                                 |\n| name              | my_security_group                    |\n| project_id        | 077e8f39d3db4c9e998d842b0503283a     |\n| revision_number   | 1                                    |\n| rules             | ...                                  |\n| tags              | []                                   |\n| updated_at        | 2019-02-07T06:09:59Z                 |\n+-------------------+--------------------------------------+\n</code></pre> <p>Create the RBAC policy entry using the\u00a0openstack network rbac create\u00a0command (in this example, the ID of the project we want to share with is\u00a0<code>32016615de5d43bb88de99e7f2e26a1e</code>):</p> <pre><code>$ openstack network rbac create --target-project \\\n32016615de5d43bb88de99e7f2e26a1e --action access_as_shared \\\n--type security_group 5ba835b7-22b0-4be6-bdbe-e0722d1b5f24\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| action            | access_as_shared                     |\n| id                | 8828e38d-a0df-4c78-963b-e5f215d3d550 |\n| name              | None                                 |\n| object_id         | 5ba835b7-22b0-4be6-bdbe-e0722d1b5f24 |\n| object_type       | security_group                       |\n| project_id        | 077e8f39d3db4c9e998d842b0503283a     |\n| target_project_id | 32016615de5d43bb88de99e7f2e26a1e     |\n+-------------------+--------------------------------------+\n</code></pre> <p>The\u00a0<code>target-project</code>\u00a0parameter specifies the project that requires access to the security group. The\u00a0<code>action</code>\u00a0parameter specifies what the project is allowed to do. The\u00a0<code>type</code>\u00a0parameter says that the target object is a security group. The final parameter is the ID of the security group we are granting access to.</p> <p>Project\u00a0<code>32016615de5d43bb88de99e7f2e26a1e</code>\u00a0will now be able to see the security group when running\u00a0openstack security group list\u00a0and\u00a0openstack security group show\u00a0and will also be able to bind it to its ports. No other users (other than admins and the owner) will be able to see the security group.</p> <p>To remove access for that project, delete the RBAC policy that allows it using the\u00a0openstack network rbac delete\u00a0command:</p> <pre><code>$ openstack network rbac delete 8828e38d-a0df-4c78-963b-e5f215d3d550\n</code></pre> <p>If that project has ports with the security group applied to them, the server will not delete the RBAC policy until the security group is no longer in use:</p> <pre><code>$ openstack network rbac delete 8828e38d-a0df-4c78-963b-e5f215d3d550\nRBAC policy on object 8828e38d-a0df-4c78-963b-e5f215d3d550\ncannot be removed because other objects depend on it.\n</code></pre> <p>This process can be repeated any number of times to share a security-group with an arbitrary number of projects.</p> <p>Creating an instance which uses a security group shared through RBAC, but only specifying the network ID when calling Nova will not work currently. In such cases Nova will check if the given security group exists in Neutron before it creates a port in the given network. The problem with that is that Nova asks only for the security groups filtered by the project_id thus it will not get the shared security group back from the Neutron API. See\u00a0bug 1942615\u00a0for details. To workaround the issue, the user needs to create a port in Neutron first, and then pass that port to Nova:</p> <pre><code>$ openstack port create --network net1 --security-group\n5ba835b7-22b0-4be6-bdbe-e0722d1b5f24 shared-sg-port\n\n$ openstack server create --image cirros-0.5.1-x86_64-disk --flavor m1.tiny\n--port shared-sg-port vm-with-shared-sg\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#sharing-an-address-scope-with-specific-projects","title":"Sharing an address scope with specific projects","text":"<p>Create an address scope to share:</p> <pre><code>$ openstack address scope create my_address_scope\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| id                | c19cb654-3489-4160-9c82-8a3015483643 |\n| ip_version        | 4                                    |\n| location          | ...                                  |\n| name              | my_address_scope                     |\n| project_id        | 34304bc4f233470fa4a2448d153b6324     |\n| shared            | False                                |\n+-------------------+--------------------------------------+\n</code></pre> <p>Create the RBAC policy entry using the\u00a0openstack network rbac create\u00a0command (in this example, the ID of the project we want to share with is\u00a0<code>32016615de5d43bb88de99e7f2e26a1e</code>):</p> <pre><code>$ openstack network rbac create --target-project \\\n32016615de5d43bb88de99e7f2e26a1e --action access_as_shared \\\n--type address_scope c19cb654-3489-4160-9c82-8a3015483643\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| action            | access_as_shared                     |\n| id                | d54b1482-98c4-44aa-9115-ede80387ffe0 |\n| location          | ...                                  |\n| name              | None                                 |\n| object_id         | c19cb654-3489-4160-9c82-8a3015483643 |\n| object_type       | address_scope                        |\n| project_id        | 34304bc4f233470fa4a2448d153b6324     |\n| target_project_id | 32016615de5d43bb88de99e7f2e26a1e     |\n+-------------------+--------------------------------------+\n</code></pre> <p>The\u00a0<code>target-project</code>\u00a0parameter specifies the project that requires access to the address scope. The\u00a0<code>action</code>\u00a0parameter specifies what the project is allowed to do. The\u00a0<code>type</code>\u00a0parameter says that the target object is an address scope. The final parameter is the ID of the address scope we are granting access to.</p> <p>Project\u00a0<code>32016615de5d43bb88de99e7f2e26a1e</code>\u00a0will now be able to see the address scope when running\u00a0openstack address scope list\u00a0and\u00a0openstack address scope show\u00a0and will also be able to assign it to its subnet pools. No other users (other than admins and the owner) will be able to see the address scope.</p> <p>To remove access for that project, delete the RBAC policy that allows it using the\u00a0openstack network rbac delete\u00a0command:</p> <pre><code>$ openstack network rbac delete d54b1482-98c4-44aa-9115-ede80387ffe0\n</code></pre> <p>If that project has subnet pools with the address scope applied to them, the server will not delete the RBAC policy until the address scope is no longer in use:</p> <pre><code>$ openstack network rbac delete d54b1482-98c4-44aa-9115-ede80387ffe0\nRBAC policy on object c19cb654-3489-4160-9c82-8a3015483643\ncannot be removed because other objects depend on it.\n</code></pre> <p>This process can be repeated any number of times to share an address scope with an arbitrary number of projects.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#sharing-a-subnet-pool-with-specific-projects","title":"Sharing a subnet pool with specific projects","text":"<p>Create a subnet pool to share:</p> <pre><code>$ openstack subnet pool create my_subnetpool --pool-prefix 203.0.113.0/24\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| address_scope_id  | None                                 |\n| created_at        | 2020-03-16T14:23:01Z                 |\n| default_prefixlen | 8                                    |\n| default_quota     | None                                 |\n| description       |                                      |\n| id                | 11f79287-bc17-46b2-bfd0-2562471eb631 |\n| ip_version        | 4                                    |\n| is_default        | False                                |\n| location          | ...                                  |\n| max_prefixlen     | 32                                   |\n| min_prefixlen     | 8                                    |\n| name              | my_subnetpool                        |\n| project_id        | 290ccedbcf594ecc8e76eff06f964f7e     |\n| revision_number   | 0                                    |\n| shared            | False                                |\n| tags              |                                      |\n| updated_at        | 2020-03-16T14:23:01Z                 |\n+-------------------+--------------------------------------+\n</code></pre> <p>Create the RBAC policy entry using the\u00a0openstack network rbac create\u00a0command (in this example, the ID of the project we want to share with is\u00a0<code>32016615de5d43bb88de99e7f2e26a1e</code>):</p> <pre><code>$ openstack network rbac create --target-project \\\n32016615de5d43bb88de99e7f2e26a1e --action access_as_shared \\\n--type subnetpool 11f79287-bc17-46b2-bfd0-2562471eb631\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| action            | access_as_shared                     |\n| id                | d54b1482-98c4-44aa-9115-ede80387ffe0 |\n| location          | ...                                  |\n| name              | None                                 |\n| object_id         | 11f79287-bc17-46b2-bfd0-2562471eb631 |\n| object_type       | subnetpool                           |\n| project_id        | 290ccedbcf594ecc8e76eff06f964f7e     |\n| target_project_id | 32016615de5d43bb88de99e7f2e26a1e     |\n+-------------------+--------------------------------------+\n</code></pre> <p>The\u00a0<code>target-project</code>\u00a0parameter specifies the project that requires access to the subnet pool. The\u00a0<code>action</code>\u00a0parameter specifies what the project is allowed to do. The\u00a0<code>type</code>\u00a0parameter says that the target object is a subnet pool. The final parameter is the ID of the subnet pool we are granting access to.</p> <p>Project\u00a0<code>32016615de5d43bb88de99e7f2e26a1e</code>\u00a0will now be able to see the subnet pool when running\u00a0openstack subnet pool list\u00a0and\u00a0openstack subnet pool show\u00a0and will also be able to assign it to its subnets. No other users (other than admins and the owner) will be able to see the subnet pool.</p> <p>To remove access for that project, delete the RBAC policy that allows it using the\u00a0openstack network rbac delete\u00a0command:</p> <pre><code>$ openstack network rbac delete d54b1482-98c4-44aa-9115-ede80387ffe0\n</code></pre> <p>If that project has subnets with the subnet pool applied to them, the server will not delete the RBAC policy until the subnet pool is no longer in use:</p> <pre><code>$ openstack network rbac delete d54b1482-98c4-44aa-9115-ede80387ffe0\nRBAC policy on object 11f79287-bc17-46b2-bfd0-2562471eb631\ncannot be removed because other objects depend on it.\n</code></pre> <p>This process can be repeated any number of times to share a subnet pool with an arbitrary number of projects.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#sharing-an-address-group-with-specific-projects","title":"Sharing an address group with specific projects","text":"<p>Create an address group to share:</p> <pre><code>$ openstack address group create test-ag --address 10.1.1.1\n+-------------+--------------------------------------+\n| Field       | Value                                |\n+-------------+--------------------------------------+\n| addresses   | ['10.1.1.1/32']                      |\n| description |                                      |\n| id          | cdb6eb3e-f9a0-4d52-8478-358eaa2c4737 |\n| name        | test-ag                              |\n| project_id  | 66c77cf262454777a8f455cce48c12c0     |\n+-------------+--------------------------------------+\n</code></pre> <p>Create the RBAC policy entry using the\u00a0openstack network rbac create\u00a0command (in this example, the ID of the project we want to share with is\u00a0<code>bbd82892525d4372911390b984ed3265</code>):</p> <pre><code>$ openstack network rbac create --target-project \\\nbbd82892525d4372911390b984ed3265 --action access_as_shared \\\n--type address_group cdb6eb3e-f9a0-4d52-8478-358eaa2c4737\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| action            | access_as_shared                     |\n| id                | c7414ac2-9a6b-420b-84c5-4158a6cca4f9 |\n| name              | None                                 |\n| object_id         | cdb6eb3e-f9a0-4d52-8478-358eaa2c4737 |\n| object_type       | address_group                        |\n| project_id        | 66c77cf262454777a8f455cce48c12c0     |\n| target_project_id | bbd82892525d4372911390b984ed3265     |\n+-------------------+--------------------------------------+\n</code></pre> <p>The\u00a0<code>target-project</code>\u00a0parameter specifies the project that requires access to the address group. The\u00a0<code>action</code>\u00a0parameter specifies what the project is allowed to do. The\u00a0<code>type</code>\u00a0parameter says that the target object is an address group. The final parameter is the ID of the address group we are granting access to.</p> <p>Project\u00a0<code>bbd82892525d4372911390b984ed3265</code>\u00a0will now be able to see the address group when running\u00a0openstack address group list\u00a0and\u00a0openstack address group show\u00a0and will also be able to assign it to its security group rules. No other users (other than admins and the owner) will be able to see the address group.</p> <p>To remove access for that project, delete the RBAC policy that allows it using the\u00a0openstack network rbac delete\u00a0command:</p> <pre><code>$ openstack network rbac delete c7414ac2-9a6b-420b-84c5-4158a6cca4f9\n</code></pre> <p>If that project has security group rules with the address group applied to them, the server will not delete the RBAC policy until the address group is no longer in use:</p> <pre><code>$ openstack network rbac delete c7414ac2-9a6b-420b-84c5-4158a6cca4f9\nRBAC policy on object cdb6eb3e-f9a0-4d52-8478-358eaa2c4737\ncannot be removed because other objects depend on it\n</code></pre> <p>This process can be repeated any number of times to share an address group with an arbitrary number of projects.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#how-the-shared-flag-relates-to-these-entries","title":"How the \u2018shared\u2019 flag relates to these entries","text":"<p>As introduced in other guide entries, neutron provides a means of making an object (<code>address-scope</code>,\u00a0<code>network</code>,\u00a0<code>qos-policy</code>,\u00a0<code>security-group</code>,\u00a0<code>subnetpool</code>) available to every project. This is accomplished using the\u00a0<code>shared</code>\u00a0flag on the supported object:</p> <pre><code>$ openstack network create global_network --share\n+---------------------------+--------------------------------------+\n| Field                     | Value                                |\n+---------------------------+--------------------------------------+\n| admin_state_up            | UP                                   |\n| availability_zone_hints   |                                      |\n| availability_zones        |                                      |\n| created_at                | 2017-01-25T20:32:06Z                 |\n| description               |                                      |\n| dns_domain                | None                                 |\n| id                        | 84a7e627-573b-49da-af66-c9a65244f3ce |\n| ipv4_address_scope        | None                                 |\n| ipv6_address_scope        | None                                 |\n| is_default                | None                                 |\n| mtu                       | 1450                                 |\n| name                      | global_network                       |\n| port_security_enabled     | True                                 |\n| project_id                | 61b7eba037fd41f29cfba757c010faff     |\n| provider:network_type     | vxlan                                |\n| provider:physical_network | None                                 |\n| provider:segmentation_id  | 7                                    |\n| qos_policy_id             | None                                 |\n| revision_number           | 3                                    |\n| router:external           | Internal                             |\n| segments                  | None                                 |\n| shared                    | True                                 |\n| status                    | ACTIVE                               |\n| subnets                   |                                      |\n| tags                      | []                                   |\n| updated_at                | 2017-01-25T20:32:07Z                 |\n+---------------------------+--------------------------------------+\n</code></pre> <p>This is the equivalent of creating a policy on the network that permits every project to perform the action\u00a0<code>access_as_shared</code>\u00a0on that network. Neutron treats them as the same thing, so the policy entry for that network should be visible using the\u00a0openstack network rbac list\u00a0command:</p> <pre><code>$ openstack network rbac list\n+-------------------------------+-------------+--------------------------------+\n| ID                            | Object Type | Object ID                      |\n+-------------------------------+-------------+--------------------------------+\n| 58a5ee31-2ad6-467d-           | qos_policy  | 1f730d69-1c45-4ade-            |\n| 8bb8-8c2ae3dd1382             |             | a8f2-89070ac4f046              |\n| 27efbd79-f384-4d89-9dfc-      | network     | 84a7e627-573b-49da-            |\n| 6c4a606ceec6                  |             | af66-c9a65244f3ce              |\n+-------------------------------+-------------+--------------------------------+\n</code></pre> <p>Use the\u00a0openstack network rbac show\u00a0command to see the details:</p> <pre><code>$ openstack network rbac show 27efbd79-f384-4d89-9dfc-6c4a606ceec6\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| action            | access_as_shared                     |\n| id                | 27efbd79-f384-4d89-9dfc-6c4a606ceec6 |\n| name              | None                                 |\n| object_id         | 84a7e627-573b-49da-af66-c9a65244f3ce |\n| object_type       | network                              |\n| project_id        | 61b7eba037fd41f29cfba757c010faff     |\n| target_project_id | *                                    |\n+-------------------+--------------------------------------+\n</code></pre> <p>The output shows that the entry allows the action\u00a0<code>access_as_shared</code>\u00a0on object\u00a0<code>84a7e627-573b-49da-af66-c9a65244f3ce</code>\u00a0of type\u00a0<code>network</code>\u00a0to target_project\u00a0<code>*</code>, which is a wildcard that represents all projects.</p> <p>Currently, the\u00a0<code>shared</code>\u00a0flag is just a mapping to the underlying RBAC policies for a network. Setting the flag to\u00a0<code>True</code>\u00a0on a network creates a wildcard RBAC entry. Setting it to\u00a0<code>False</code>\u00a0removes the wildcard entry.</p> <p>When you run\u00a0openstack network list\u00a0or\u00a0openstack network show, the\u00a0<code>shared</code>\u00a0flag is calculated by the server based on the calling project and the RBAC entries for each network. For QoS objects use\u00a0openstack network qos policy list\u00a0or\u00a0openstack network qos policy show\u00a0respectively. If there is a wildcard entry, the\u00a0<code>shared</code>\u00a0flag is always set to\u00a0<code>True</code>. If there are only entries that share with specific projects, only the projects the object is shared to will see the flag as\u00a0<code>True</code>\u00a0and the rest will see the flag as\u00a0<code>False</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#allowing-a-network-to-be-used-as-an-external-network","title":"Allowing a network to be used as an external network","text":"<p>To make a network available as an external network for specific projects rather than all projects, use the\u00a0<code>access_as_external</code>\u00a0action.</p> <ol> <li>Create a network that you want to be available as an external network:</li> </ol> <pre><code>$ openstack network create secret_external_network\n+---------------------------+--------------------------------------+\n| Field                     | Value                                |\n+---------------------------+--------------------------------------+\n| admin_state_up            | UP                                   |\n| availability_zone_hints   |                                      |\n| availability_zones        |                                      |\n| created_at                | 2017-01-25T20:36:59Z                 |\n| description               |                                      |\n| dns_domain                | None                                 |\n| id                        | 802d4e9e-4649-43e6-9ee2-8d052a880cfb |\n| ipv4_address_scope        | None                                 |\n| ipv6_address_scope        | None                                 |\n| is_default                | None                                 |\n| mtu                       | 1450                                 |\n| name                      | secret_external_network              |\n| port_security_enabled     | True                                 |\n| project_id                | 61b7eba037fd41f29cfba757c010faff     |\n| proider:network_type      | vxlan                                |\n| provider:physical_network | None                                 |\n| provider:segmentation_id  | 21                                   |\n| qos_policy_id             | None                                 |\n| revision_number           | 3                                    |\n| router:external           | Internal                             |\n| segments                  | None                                 |\n| shared                    | False                                |\n| status                    | ACTIVE                               |\n| subnets                   |                                      |\n| tags                      | []                                   |\n| updated_at                | 2017-01-25T20:36:59Z                 |\n+---------------------------+--------------------------------------+\n</code></pre> <p>2. Create a policy entry using the\u00a0openstack network rbac create\u00a0command (in this example, the ID of the project we want to share with is\u00a0<code>838030a7bf3c4d04b4b054c0f0b2b17c</code>):</p> <pre><code>$ openstack network rbac create --target-project \\\n838030a7bf3c4d04b4b054c0f0b2b17c --action access_as_external \\\n--type network 802d4e9e-4649-43e6-9ee2-8d052a880cfb\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| action            | access_as_external                   |\n| id                | afdd5b8d-b6f5-4a15-9817-5231434057be |\n| name              | None                                 |\n| object_id         | 802d4e9e-4649-43e6-9ee2-8d052a880cfb |\n| object_type       | network                              |\n| project_id        | 61b7eba037fd41f29cfba757c010faff     |\n| target_project_id | 838030a7bf3c4d04b4b054c0f0b2b17c     |\n+-------------------+--------------------------------------+\n</code></pre> <p>The\u00a0<code>target-project</code>\u00a0parameter specifies the project that requires access to the network. The\u00a0<code>action</code>\u00a0parameter specifies what the project is allowed to do. The\u00a0<code>type</code>\u00a0parameter indicates that the target object is a network. The final parameter is the ID of the network we are granting external access to.</p> <p>Now project\u00a0<code>838030a7bf3c4d04b4b054c0f0b2b17c</code>\u00a0is able to see the network when running\u00a0openstack network list\u00a0and\u00a0openstack network show\u00a0and can attach router gateway ports to that network. No other users (other than admins and the owner) are able to see the network.</p> <p>To remove access for that project, delete the policy that allows it using the\u00a0openstack network rbac delete\u00a0command:</p> <pre><code>$ openstack network rbac delete afdd5b8d-b6f5-4a15-9817-5231434057be\n</code></pre> <p>If that project has router gateway ports attached to that network, the server prevents the policy from being deleted until the ports have been deleted:</p> <pre><code>$ openstack network rbac delete afdd5b8d-b6f5-4a15-9817-5231434057be\nRBAC policy on object afdd5b8d-b6f5-4a15-9817-5231434057be\ncannot be removed because other objects depend on it.\n</code></pre> <p>This process can be repeated any number of times to make a network available as external to an arbitrary number of projects.</p> <p>If a network is marked as external during creation, it now implicitly creates a wildcard RBAC policy granting everyone access to preserve previous behavior before this feature was added.</p> <pre><code>$ openstack network create global_external_network --external\n+---------------------------+--------------------------------------+\n| Field                     | Value                                |\n+---------------------------+--------------------------------------+\n| admin_state_up            | UP                                   |\n| availability_zone_hints   |                                      |\n| availability_zones        |                                      |\n| created_at                | 2017-01-25T20:41:44Z                 |\n| description               |                                      |\n| dns_domain                | None                                 |\n| id                        | 72a257a2-a56e-4ac7-880f-94a4233abec6 |\n| ipv4_address_scope        | None                                 |\n| ipv6_address_scope        | None                                 |\n| is_default                | None                                 |\n| mtu                       | 1450                                 |\n| name                      | global_external_network              |\n| port_security_enabled     | True                                 |\n| project_id                | 61b7eba037fd41f29cfba757c010faff     |\n| provider:network_type     | vxlan                                |\n| provider:physical_network | None                                 |\n| provider:segmentation_id  | 69                                   |\n| qos_policy_id             | None                                 |\n| revision_number           | 4                                    |\n| router:external           | External                             |\n| segments                  | None                                 |\n| shared                    | False                                |\n| status                    | ACTIVE                               |\n| subnets                   |                                      |\n| tags                      | []                                   |\n| updated_at                | 2017-01-25T20:41:44Z                 |\n+---------------------------+--------------------------------------+\n</code></pre> <p>In the output above the standard\u00a0<code>router:external</code>\u00a0attribute is\u00a0<code>External</code>\u00a0as expected. Now a wildcard policy is visible in the RBAC policy listings:</p> <pre><code>$ openstack network rbac list --long -c ID -c Action\n+--------------------------------------+--------------------+\n| ID                                   | Action             |\n+--------------------------------------+--------------------+\n| b694e541-bdca-480d-94ec-eda59ab7d71a | access_as_external |\n+--------------------------------------+--------------------+\n</code></pre> <p>You can modify or delete this policy with the same constraints as any other RBAC\u00a0<code>access_as_external</code>\u00a0policy.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#preventing-regular-users-from-sharing-objects-with-each-other","title":"Preventing regular users from sharing objects with each other","text":"<p>The default\u00a0<code>policy.yaml</code>\u00a0file will not allow regular users to share objects with every other project using a wildcard; however, it will allow them to share objects with specific project IDs.</p> <p>If an operator wants to prevent normal users from doing this, the\u00a0<code>\"create_rbac_policy\":</code>\u00a0entry in\u00a0<code>policy.yaml</code>\u00a0can be adjusted from\u00a0<code>\"\"</code>\u00a0to\u00a0<code>\"rule:admin_only\"</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Role-Based_Access_Control/#improve-database-rbac-query-operations","title":"Improve database RBAC query operations","text":"<p>Since\u00a01, present in Yoga version, Neutron has indexes for \u201ctarget_tenant\u201d (now \u201ctarget_project\u201d) and \u201caction\u201d columns in all RBAC related tables. That improves the SQL queries involving the RBAC tables\u00a02. Any system before Yoga won\u2019t have these indexes but the system administrator can manually add them to the Neutron database following the next steps:</p> <ul> <li>Find the RBAC tables:</li> </ul> <pre><code>$ tables=`mysql -e \"use ovs_neutron; show tables;\" | grep rbac\n</code></pre> <ul> <li>Insert the indexes for the \u201ctarget_tenant\u201d and \u201caction\u201d columns:$ for table in $tables do; mysql -e\u201calter table $table add key (action); alter table $table add key (target_tenant);\u201d; done</li> </ul> <p>In order to prevent errors during a system upgrade,\u00a03\u00a0was implemented and backported up to Yoga. This patch checks if any index is already present in the Neutron tables and avoids executing the index creation command again.1</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Subnet_Pools/","title":"Subnet Pools","text":"<p>Subnet pools have been made available since the Kilo release. It is a simple feature that has the potential to improve your workflow considerably. It also provides a building block from which other new features will be built in to OpenStack Networking.</p> <p>To see if your cloud has this feature available, you can check that it is listed in the supported aliases. You can do this with the OpenStack client.</p> <pre><code>$ openstack extension list | grep subnet_allocation\n| Subnet Allocation | subnet_allocation | Enables allocation of subnets\nfrom a subnet pool\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Subnet_Pools/#why-you-need-them","title":"Why you need them","text":"<p>Before Kilo, Networking had no automation around the addresses used to create a subnet. To create one, you had to come up with the addresses on your own without any help from the system. There are valid use cases for this but if you are interested in the following capabilities, then subnet pools might be for you.</p> <p>First, would not it be nice if you could turn your pool of addresses over to Neutron to take care of? When you need to create a subnet, you just ask for addresses to be allocated from the pool. You do not have to worry about what you have already used and what addresses are in your pool. Subnet pools can do this.</p> <p>Second, subnet pools can manage addresses across projects. The addresses are guaranteed not to overlap. If the addresses come from an externally routable pool then you know that all of the projects have addresses which are\u00a0routable\u00a0and unique. This can be useful in the following scenarios.</p> <ol> <li> <p>IPv6 since OpenStack Networking has no IPv6 floating IPs.</p> </li> <li> <p>Routing directly to a project network from an external network.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Subnet_Pools/#how-they-work","title":"How they work","text":"<p>A subnet pool manages a pool of addresses from which subnets can be allocated. It ensures that there is no overlap between any two subnets allocated from the same pool.</p> <p>As a regular project in an OpenStack cloud, you can create a subnet pool of your own and use it to manage your own pool of addresses. This does not require any admin privileges. Your pool will not be visible to any other project.</p> <p>If you are an admin, you can create a pool which can be accessed by any regular project. Being a shared resource, there is a quota mechanism to arbitrate access.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Subnet_Pools/#quotas","title":"Quotas","text":"<p>Subnet pools have a quota system which is a little bit different than other quotas in Neutron. Other quotas in Neutron count discrete instances of an object against a quota. Each time you create something like a router, network, or a port, it uses one from your total quota.</p> <p>With subnets, the resource is the IP address space. Some subnets take more of it than others. For example, 203.0.113.0/24 uses 256 addresses in one subnet but 198.51.100.224/28 uses only 16. If address space is limited, the quota system can encourage efficient use of the space.</p> <p>With IPv4, the default_quota can be set to the number of absolute addresses any given project is allowed to consume from the pool. For example, with a quota of 128, I might get 203.0.113.128/26, 203.0.113.224/28, and still have room to allocate 48 more addresses in the future.</p> <p>With IPv6 it is a little different. It is not practical to count individual addresses. To avoid ridiculously large numbers, the quota is expressed in the number of /64 subnets which can be allocated. For example, with a default_quota of 3, I might get 2001:db8:c18e:c05a::/64, 2001:db8:221c:8ef3::/64, and still have room to allocate one more prefix in the future.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Subnet_Pools/#default-subnet-pools","title":"Default subnet pools","text":"<p>Beginning with Mitaka, a subnet pool can be marked as the default. This is handled with a new extension.</p> <pre><code>$ openstack extension list | grep default-subnetpools\n| Default Subnetpools | default-subnetpools | Provides ability to mark\nand use a subnetpool as the default\n</code></pre> <p>An administrator can mark a pool as default. Only one pool from each address family can be marked default.</p> <pre><code>$ openstack subnet pool set --default 74348864-f8bf-4fc0-ab03-81229d189467\n</code></pre> <p>If there is a default, it can be requested by passing\u00a0<code>--use-default-subnetpool</code>\u00a0instead of\u00a0<code>--subnet-pool\u00a0SUBNETPOOL</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Subnet_Pools/#demo","title":"Demo","text":"<p>If you have access to an OpenStack Kilo or later based neutron, you can play with this feature now. Give it a try. All of the following commands work equally as well with IPv6 addresses.</p> <p>First, as admin, create a shared subnet pool:</p> <pre><code>$ openstack subnet pool create --share --pool-prefix 203.0.113.0/24 \\\n--default-prefix-length 26 demo-subnetpool4\n+-------------------+--------------------------------+\n| Field             | Value                          |\n+-------------------+--------------------------------+\n| address_scope_id  | None                           |\n| created_at        | 2016-12-14T07:21:26Z           |\n| default_prefixlen | 26                             |\n| default_quota     | None                           |\n| description       |                                |\n| headers           |                                |\n| id                | d3aefb76-2527-43d4-bc21-0ec253 |\n|                   | 908545                         |\n| ip_version        | 4                              |\n| is_default        | False                          |\n| max_prefixlen     | 32                             |\n| min_prefixlen     | 8                              |\n| name              | demo-subnetpool4               |\n| prefixes          | 203.0.113.0/24                 |\n| project_id        | cfd1889ac7d64ad891d4f20aef9f8d |\n|                   | 7c                             |\n| revision_number   | 1                              |\n| shared            | True                           |\n| tags              | []                             |\n| updated_at        | 2016-12-14T07:21:26Z           |\n+-------------------+--------------------------------+\n</code></pre> <p>The\u00a0<code>default_prefix_length</code>\u00a0defines the subnet size you will get if you do not specify\u00a0<code>--prefix-length</code>\u00a0when creating a subnet.</p> <p>Do essentially the same thing for IPv6 and there are now two subnet pools. Regular projects can see them. (the output is trimmed a bit for display)</p> <pre><code>$ openstack subnet pool list\n+------------------+------------------+--------------------+\n| ID               | Name             | Prefixes           |\n+------------------+------------------+--------------------+\n| 2b7cc19f-0114-4e | demo-subnetpool  | 2001:db8:a583::/48 |\n| f4-ad86-c1bb91fc |                  |                    |\n| d1f9             |                  |                    |\n| d3aefb76-2527-43 | demo-subnetpool4 | 203.0.113.0/24     |\n| d4-bc21-0ec25390 |                  |                    |\n| 8545             |                  |                    |\n+------------------+------------------+--------------------+\n</code></pre> <p>Now, use them. It is easy to create a subnet from a pool:</p> <pre><code>$ openstack subnet create --ip-version 4 --subnet-pool \\\ndemo-subnetpool4 --network demo-network1 demo-subnet1\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| allocation_pools  | 203.0.113.194-203.0.113.254          |\n| cidr              | 203.0.113.192/26                     |\n| created_at        | 2016-12-14T07:33:13Z                 |\n| description       |                                      |\n| dns_nameservers   |                                      |\n| enable_dhcp       | True                                 |\n| gateway_ip        | 203.0.113.193                        |\n| headers           |                                      |\n| host_routes       |                                      |\n| id                | 8d4fbae3-076c-4c08-b2dd-2d6175115a5e |\n| ip_version        | 4                                    |\n| ipv6_address_mode | None                                 |\n| ipv6_ra_mode      | None                                 |\n| name              | demo-subnet1                         |\n| network_id        | 6b377f77-ce00-4ff6-8676-82343817470d |\n| project_id        | cfd1889ac7d64ad891d4f20aef9f8d7c     |\n| revision_number   | 2                                    |\n| service_types     |                                      |\n| subnetpool_id     | d3aefb76-2527-43d4-bc21-0ec253908545 |\n| tags              | []                                   |\n| updated_at        | 2016-12-14T07:33:13Z                 |\n+-------------------+--------------------------------------+\n</code></pre> <p>You can request a specific subnet from the pool. You need to specify a subnet that falls within the pool\u2019s prefixes. If the subnet is not already allocated, the request succeeds. You can leave off the IP version because it is deduced from the subnet pool.</p> <pre><code>$ openstack subnet create --subnet-pool demo-subnetpool4 \\\n--network demo-network1 --subnet-range 203.0.113.128/26 subnet2\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| allocation_pools  | 203.0.113.130-203.0.113.190          |\n| cidr              | 203.0.113.128/26                     |\n| created_at        | 2016-12-14T07:27:40Z                 |\n| description       |                                      |\n| dns_nameservers   |                                      |\n| enable_dhcp       | True                                 |\n| gateway_ip        | 203.0.113.129                        |\n| headers           |                                      |\n| host_routes       |                                      |\n| id                | d32814e3-cf46-4371-80dd-498a80badfba |\n| ip_version        | 4                                    |\n| ipv6_address_mode | None                                 |\n| ipv6_ra_mode      | None                                 |\n| name              | subnet2                              |\n| network_id        | 6b377f77-ce00-4ff6-8676-82343817470d |\n| project_id        | cfd1889ac7d64ad891d4f20aef9f8d7c     |\n| revision_number   | 2                                    |\n| service_types     |                                      |\n| subnetpool_id     | d3aefb76-2527-43d4-bc21-0ec253908545 |\n| tags              | []                                   |\n| updated_at        | 2016-12-14T07:27:40Z                 |\n+-------------------+--------------------------------------+\n</code></pre> <p>If the pool becomes exhausted, load some more prefixes:</p> <pre><code>$ openstack subnet pool set --pool-prefix \\\n198.51.100.0/24 demo-subnetpool4\n$ openstack subnet pool show demo-subnetpool4\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| address_scope_id  | None                                 |\n| created_at        | 2016-12-14T07:21:26Z                 |\n| default_prefixlen | 26                                   |\n| default_quota     | None                                 |\n| description       |                                      |\n| id                | d3aefb76-2527-43d4-bc21-0ec253908545 |\n| ip_version        | 4                                    |\n| is_default        | False                                |\n| max_prefixlen     | 32                                   |\n| min_prefixlen     | 8                                    |\n| name              | demo-subnetpool4                     |\n| prefixes          | 198.51.100.0/24, 203.0.113.0/24      |\n| project_id        | cfd1889ac7d64ad891d4f20aef9f8d7c     |\n| revision_number   | 2                                    |\n| shared            | True                                 |\n| tags              | []                                   |\n| updated_at        | 2016-12-14T07:30:32Z                 |\n+-------------------+--------------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/","title":"Taikun OCP Networking Overview","text":"<p>OpenStack Networking allows you to create and manage network objects, such as networks, subnets, and ports, which other OpenStack services can use. Plug-ins can be implemented to accommodate different networking equipment and software, providing flexibility to OpenStack architecture and deployment.</p> <p>The Networking service, code-named neutron, provides an API that lets you define network connectivity and addressing in the cloud. The Networking service enables operators to leverage different networking technologies to power their cloud networking. The Networking service also provides an API to configure and manage a variety of network services ranging from L3 forwarding and Network Address Translation (NAT) to perimeter firewalls, and virtual private networks.</p> <p>It includes the following components:API server</p> <p>The OpenStack Networking API includes support for Layer 2 networking and IP Address Management (IPAM), as well as an extension for a Layer 3 router construct that enables routing between Layer 2 networks and gateways to external networks. OpenStack Networking includes a growing list of plug-ins that enable interoperability with various commercial and open source network technologies, including routers, switches, virtual switches and software-defined networking (SDN) controllers.OpenStack Networking plug-in and agents</p> <p>Plugs and unplugs ports, creates networks or subnets, and provides IP addressing. The chosen plug-in and agents differ depending on the vendor and technologies used in the particular cloud. It is important to mention that only one plug-in can be used at a time.Messaging queue</p> <p>Accepts and routes RPC requests between agents to complete API operations. Message queue is used in the ML2 plug-in for RPC between the neutron server and neutron agents that run on each hypervisor, in the ML2 mechanism drivers for Open vSwitch and Linux bridge.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#concepts","title":"Concepts","text":"<p>To configure rich network topologies, you can create and configure networks and subnets and instruct other OpenStack services like Compute to attach virtual devices to ports on these networks. OpenStack Compute is a prominent consumer of OpenStack Networking to provide connectivity for its instances. In particular, OpenStack Networking supports each project having multiple private networks and enables projects to choose their own IP addressing scheme, even if those IP addresses overlap with those that other projects use. There are two types of network, project and provider networks. It is possible to share any of these types of networks among projects as part of the network creation process.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#provider-networks","title":"Provider networks","text":"<p>Provider networks offer layer-2 connectivity to instances with optional support for DHCP and metadata services. These networks connect, or map, to existing layer-2 networks in the data center, typically using VLAN (802.1q) tagging to identify and separate them.</p> <p>Provider networks generally offer simplicity, performance, and reliability at the cost of flexibility. By default only administrators can create or update provider networks because they require configuration of physical network infrastructure. It is possible to change the user who is allowed to create or update provider networks with the following parameters of\u00a0<code>policy.yaml</code>:</p> <ul> <li> <p><code>create_network:provider:physical_network</code></p> </li> <li> <p><code>update_network:provider:physical_network</code></p> </li> </ul> <p>Warning  The creation and modification of provider networks enables use of physical network resources, such as VLAN-s. Enable these changes only for trusted projects.</p> <p>Also, provider networks only handle layer-2 connectivity for instances, thus lacking support for features such as routers and floating IP addresses.</p> <p>In many cases, operators who are already familiar with virtual networking architectures that rely on physical network infrastructure for layer-2, layer-3, or other services can seamlessly deploy the OpenStack Networking service. In particular, provider networks appeal to operators looking to migrate from the Compute networking service (nova-network) to the OpenStack Networking service. Over time, operators can build on this minimal architecture to enable more cloud networking features.</p> <p>In general, the OpenStack Networking software components that handle layer-3 operations impact performance and reliability the most. To improve performance and reliability, provider networks move layer-3 operations to the physical network infrastructure.</p> <p>In one particular use case, the OpenStack deployment resides in a mixed environment with conventional virtualization and bare-metal hosts that use a sizable physical network infrastructure. Applications that run inside the OpenStack deployment might require direct layer-2 access, typically using VLANs, to applications outside of the deployment.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#routed-provider-networks","title":"Routed provider networks","text":"<p>Routed provider networks offer layer-3 connectivity to instances. These networks map to existing layer-3 networks in the data center. More specifically, the network maps to multiple layer-2 segments, each of which is essentially a provider network. Each has a router gateway attached to it which routes traffic between them and externally. The Networking service does not provide the routing.</p> <p>Routed provider networks offer performance at scale that is difficult to achieve with a plain provider network at the expense of guaranteed layer-2 connectivity.</p> <p>Neutron port could be associated with only one network segment, but there is an exception for OVN distributed services like OVN Metadata.</p> <p>See\u00a0Routed provider networks\u00a0for more information.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#self-service-networks","title":"Self-service networks","text":"<p>Self-service networks primarily enable general (non-privileged) projects to manage networks without involving administrators. These networks are entirely virtual and require virtual routers to interact with provider and external networks such as the Internet. Self-service networks also usually provide DHCP and metadata services to instances.</p> <p>In most cases, self-service networks use overlay protocols such as VXLAN or GRE because they can support many more networks than layer-2 segmentation using VLAN tagging (802.1q). Furthermore, VLANs typically require additional configuration of physical network infrastructure.</p> <p>IPv4 self-service networks typically use private IP address ranges (RFC1918) and interact with provider networks via source NAT on virtual routers. Floating IP addresses enable access to instances from provider networks via destination NAT on virtual routers. IPv6 self-service networks always use public IP address ranges and interact with provider networks via virtual routers with static routes.</p> <p>The Networking service implements routers using a layer-3 agent that typically resides at least one network node. Contrary to provider networks that connect instances to the physical network infrastructure at layer-2, self-service networks must traverse a layer-3 agent. Thus, oversubscription or failure of a layer-3 agent or network node can impact a significant quantity of self-service networks and instances using them. Consider implementing one or more high-availability features to increase redundancy and performance of self-service networks.</p> <p>Users create project networks for connectivity within projects. By default, they are fully isolated and are not shared with other projects. OpenStack Networking supports the following types of network isolation and overlay technologies.</p> <p>Flat</p> <p>All instances reside on the same network, which can also be shared with the hosts. No VLAN tagging or other network segregation takes place.</p> <p>VLAN</p> <p>Networking allows users to create multiple provider or project networks using VLAN IDs (802.1Q tagged) that correspond to VLANs present in the physical network. This allows instances to communicate with each other across the environment. They can also communicate with dedicated servers, firewalls, and other networking infrastructure on the same layer 2 VLAN.</p> <p>GRE and VXLAN</p> <p>VXLAN and GRE are encapsulation protocols that create overlay networks to activate and control communication between compute instances. A Networking router is required to allow traffic to flow outside of the GRE or VXLAN project network. A router is also required to connect directly-connected project networks with external networks, including the Internet. The router provides the ability to connect to instances directly from an external network using floating IP addresses.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#subnets","title":"Subnets","text":"<p>A block of IP addresses and associated configuration state. This is also known as the native IPAM (IP Address Management) provided by the networking service for both project and provider networks. Subnets are used to allocate IP addresses when new ports are created on a network.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#subnet-pools","title":"Subnet pools","text":"<p>End users normally can create subnets with any valid IP addresses without other restrictions. However, in some cases, it is nice for the admin or the project to pre-define a pool of addresses from which to create subnets with automatic allocation.</p> <p>Using subnet pools constrains what addresses can be used by requiring that every subnet be within the defined pool. It also prevents address reuse or overlap by two subnets from the same pool.</p> <p>See\u00a0Subnet pools\u00a0for more information.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#ports","title":"Ports","text":"<p>A port is a connection point for attaching a single device, such as the NIC of a virtual server, to a virtual network. The port also describes the associated network configuration, such as the MAC and IP addresses to be used on that port.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#routers","title":"Routers","text":"<p>Routers provide virtual layer-3 services such as routing and NAT between self-service and provider networks or among self-service networks belonging to a project. The Networking service uses a layer-3 agent to manage routers via namespaces.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#security-groups","title":"Security groups","text":"<p>Security groups provide a container for virtual firewall rules that control ingress (inbound to instances) and egress (outbound from instances) network traffic at the port level. Security groups use a default deny policy and only contain rules that allow specific traffic. Each port can reference one or more security groups in an additive fashion. The firewall driver translates security group rules to a configuration for the underlying packet filtering technology such as\u00a0<code>iptables</code>.</p> <p>Each project contains a\u00a0<code>default</code>\u00a0security group that allows all egress traffic and denies all ingress traffic. You can change the rules in the\u00a0<code>default</code>\u00a0security group. If you launch an instance without specifying a security group, the\u00a0<code>default</code>\u00a0security group automatically applies to it. Similarly, if you create a port without specifying a security group, the\u00a0<code>default</code>\u00a0security group automatically applies to it.</p> <p>Note If you use the metadata service, removing the default egress rules denies access to TCP port 80 on 169.254.169.254, thus preventing instances from retrieving metadata.</p> <p>Security group rules are stateful. Thus, allowing ingress TCP port 22 for secure shell automatically creates rules that allow return egress traffic and ICMP error messages involving those TCP connections.</p> <p>By default, all security groups contain a series of basic (sanity) and anti-spoofing rules that perform the following actions:</p> <ul> <li> <p>Allow egress traffic only if it uses the source MAC and IP addresses of the port for the instance, source MAC and IP combination in\u00a0<code>allowed-address-pairs</code>, or valid MAC address (port or\u00a0<code>allowed-address-pairs</code>) and associated EUI64 link-local IPv6 address.</p> </li> <li> <p>Allow egress DHCP discovery and request messages that use the source MAC address of the port for the instance and the unspecified IPv4 address (0.0.0.0).</p> </li> <li> <p>Allow ingress DHCP and DHCPv6 responses from the DHCP server on the subnet so instances can acquire IP addresses.</p> </li> <li> <p>Deny egress DHCP and DHCPv6 responses to prevent instances from acting as DHCP(v6) servers.</p> </li> <li> <p>Allow ingress/egress ICMPv6 MLD, neighbor solicitation, and neighbor discovery messages so instances can discover neighbors and join multicast groups.</p> </li> <li> <p>Deny egress ICMPv6 router advertisements to prevent instances from acting as IPv6 routers and forwarding IPv6 traffic for other instances.</p> </li> <li> <p>Allow egress ICMPv6 MLD reports (v1 and v2) and neighbor solicitation messages that use the source MAC address of a particular instance and the unspecified IPv6 address (::). Duplicate address detection (DAD) relies on these messages.</p> </li> <li> <p>Allow egress non-IP traffic from the MAC address of the port for the instance and any additional MAC addresses in\u00a0<code>allowed-address-pairs</code>\u00a0on the port for the instance.</p> </li> </ul> <p>Although non-IP traffic, security groups do not implicitly allow all ARP traffic. Separate ARP filtering rules prevent instances from using ARP to intercept traffic for another instance. You cannot disable or remove these rules.</p> <p>You can disable security groups including basic and anti-spoofing rules by setting the port attribute\u00a0<code>port_security_enabled</code>\u00a0to\u00a0<code>False</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#extensions","title":"Extensions","text":"<p>The OpenStack Networking service is extensible. Extensions serve two purposes: they allow the introduction of new features in the API without requiring a version change and they allow the introduction of vendor specific niche functionality. Applications can programmatically list available extensions by performing a GET on the\u00a0<code>/extensions</code>\u00a0URI. Note that this is a versioned request; that is, an extension available in one API version might not be available in another.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#dhcp","title":"DHCP","text":"<p>The optional DHCP service manages IP addresses for instances on provider and self-service networks. The Networking service implements the DHCP service using an agent that manages\u00a0<code>qdhcp</code>\u00a0namespaces and the\u00a0<code>dnsmasq</code>\u00a0service.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#metadata","title":"Metadata","text":"<p>The optional metadata service provides an API for instances to obtain metadata such as SSH keys.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#service-and-component-hierarchy","title":"Service and component hierarchy","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#server","title":"Server","text":"<ul> <li>Provides API, manages database, etc.</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#plug-ins","title":"Plug-ins","text":"<ul> <li>Manages agents</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#agents","title":"Agents","text":"<ul> <li> <p>Provides layer 2/3 connectivity to instances</p> </li> <li> <p>Handles physical-virtual network transition</p> </li> <li> <p>Handles metadata, etc.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#layer-2-ethernet-and-switching","title":"Layer 2 (Ethernet and Switching)","text":"<ul> <li> <p>Linux Bridge</p> </li> <li> <p>OVS</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#layer-3-ip-and-routing","title":"Layer 3 (IP and Routing)","text":"<ul> <li> <p>L3</p> </li> <li> <p>DHCP</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Metadata</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#services","title":"Services","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#vpnaas","title":"VPNaaS","text":"<p>The Virtual Private Network-as-a-Service (VPNaaS) is a neutron extension that introduces the VPN feature set.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#lbaas","title":"LBaaS","text":"<p>The Load-Balancer-as-a-Service (LBaaS) API provisions and configures load balancers. The reference implementation is based on the HAProxy software load balancer. See the\u00a0Octavia project\u00a0for more information.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Neutron/Taikun_OCP_Networking_Overview/#fwaas","title":"FWaaS","text":"<p>The Firewall-as-a-Service (FWaaS) API allows to apply firewalls to OpenStack objects such as projects, routers, and router ports.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Attaching_virtual_GPU_devices_to_guests/","title":"Attaching virtual GPU devices to guests","text":"<p>The virtual GPU feature in Nova allows a deployment to provide specific GPU types for instances using physical GPUs that can provide virtual devices.</p> <p>For example, a single\u00a0Intel GVT-g\u00a0or a\u00a0NVIDIA GRID vGPU\u00a0physical Graphics Processing Unit (pGPU) can be virtualized as multiple virtual Graphics Processing Units (vGPUs) if the hypervisor supports the hardware driver and has the capability to create guests using those virtual devices.</p> <p>This feature is highly dependent on the version of libvirt and the physical devices present on the host. In addition, the vendor\u2019s vGPU driver software must be installed and configured on the host at the same time.</p> <p>Caveats are mentioned in the\u00a0Caveats\u00a0section.</p> <p>To enable virtual GPUs, follow the steps below:</p> <ol> <li> <p>Enable GPU types (Compute)</p> </li> <li> <p>Configure a flavor (Controller)</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Attaching_virtual_GPU_devices_to_guests/#enable-gpu-types-compute","title":"Enable GPU types (Compute)","text":"<p>1. Specify which specific GPU type(s) the instances would get.</p> <p>Edit\u00a0<code>devices.enabled_mdev_types</code>:</p> <pre><code>[devices]\nenabled_mdev_types = nvidia-35\n</code></pre> <p>If you want to support more than a single GPU type, you need to provide a separate configuration section for each device. For example:</p> <pre><code>[devices]\nenabled_mdev_types = nvidia-35, nvidia-36\n\n[mdev_nvidia-35]\ndevice_addresses = 0000:84:00.0,0000:85:00.0\n\n[mdev_nvidia-36]\ndevice_addresses = 0000:86:00.0\n</code></pre> <p>where you have to define which physical GPUs are supported per GPU type.</p> <p>If the same PCI address is provided for two different types, nova-compute will refuse to start and issue a specific error in the logs.</p> <p>To know which specific type(s) to mention, please refer to\u00a0How to discover a GPU type.</p> <p>2. Restart the\u00a0<code>nova-compute</code>\u00a0service.</p> <p>Warning Changing the type is possible but since existing physical GPUs can\u2019t address multiple guests having different types, that will make Nova return you a NoValidHost if existing instances with the original type still exist. Accordingly, it\u2019s highly recommended to instead deploy the new type to new compute nodes that don\u2019t already have workloads and rebuild instances on the nodes that need to change types.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Attaching_virtual_GPU_devices_to_guests/#configure-a-flavor-controller","title":"Configure a flavor (Controller)","text":"<p>Configure a flavor to request one virtual GPU:</p> <pre><code>$ openstack flavor set vgpu_1 --property \"resources:VGPU=1\"\n</code></pre> <p>The enabled vGPU types on the compute hosts are not exposed to API users. Flavors configured for vGPU support can be tied to host aggregates as a means to properly schedule those flavors onto the compute hosts that support them. See\u00a0Host aggregates\u00a0for more information.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Attaching_virtual_GPU_devices_to_guests/#create-instances-with-virtual-gpu-devices","title":"Create instances with virtual GPU devices","text":"<p>The\u00a0<code>nova-scheduler</code>\u00a0selects a destination host that has vGPU devices available by calling the Placement API for a specific VGPU resource class provided by compute nodes.</p> <pre><code>$ openstack server create --flavor vgpu_1 --image cirros-0.3.5-x86_64-uec --wait test-vgpu\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Attaching_virtual_GPU_devices_to_guests/#how-to-discover-a-gpu-type","title":"How to discover a GPU type","text":"<p>Virtual GPUs are seen as mediated devices. Physical PCI devices (the graphic card here) supporting virtual GPUs propose mediated device (mdev) types. Since mediated devices are supported by the Linux kernel through sysfs files after installing the vendor\u2019s virtual GPUs driver software, you can see the required properties as follows:</p> <pre><code>$ ls /sys/class/mdev_bus/*/mdev_supported_types\n/sys/class/mdev_bus/0000:84:00.0/mdev_supported_types:\nnvidia-35  nvidia-36  nvidia-37  nvidia-38  nvidia-39  nvidia-40  nvidia-41  nvidia-42  nvidia-43  nvidia-44  nvidia-45\n\n/sys/class/mdev_bus/0000:85:00.0/mdev_supported_types:\nnvidia-35  nvidia-36  nvidia-37  nvidia-38  nvidia-39  nvidia-40  nvidia-41  nvidia-42  nvidia-43  nvidia-44  nvidia-45\n\n/sys/class/mdev_bus/0000:86:00.0/mdev_supported_types:\nnvidia-35  nvidia-36  nvidia-37  nvidia-38  nvidia-39  nvidia-40  nvidia-41  nvidia-42  nvidia-43  nvidia-44  nvidia-45\n\n/sys/class/mdev_bus/0000:87:00.0/mdev_supported_types:\nnvidia-35  nvidia-36  nvidia-37  nvidia-38  nvidia-39  nvidia-40  nvidia-41  nvidia-42  nvidia-43  nvidia-44  nvidia-45\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Attaching_virtual_GPU_devices_to_guests/#checking-allocations-and-inventories-for-virtual-gpus","title":"Checking allocations and inventories for virtual GPUs","text":"<p>The examples you will see are using the\u00a0osc-placement plugin\u00a0for OpenStackClient. For details on specific commands, see its documentation.</p> <p>1. Get the list of resource providers$ openstack resource provider list</p> <pre><code>$ openstack resource provider list\n+--------------------------------------+---------------------------------------------------------+------------+\n| uuid                                 | name                                                    | generation |\n+--------------------------------------+---------------------------------------------------------+------------+\n| 5958a366-3cad-416a-a2c9-cfbb5a472287 | virtlab606.xxxxxxxxxxxxxxxxxxxxxxxxxxx                  |          7 |\n| fc9b9287-ef5e-4408-aced-d5577560160c | virtlab606.xxxxxxxxxxxxxxxxxxxxxxxxxxx_pci_0000_86_00_0 |          2 |\n| e2f8607b-0683-4141-a8af-f5e20682e28c | virtlab606.xxxxxxxxxxxxxxxxxxxxxxxxxxx_pci_0000_85_00_0 |          3 |\n| 85dd4837-76f9-41f2-9f19-df386017d8a0 | virtlab606.xxxxxxxxxxxxxxxxxxxxxxxxxxx_pci_0000_87_00_0 |          2 |\n| 7033d860-8d8a-4963-8555-0aa902a08653 | virtlab606.xxxxxxxxxxxxxxxxxxxxxxxxxxx_pci_0000_84_00_0 |          2 |\n+--------------------------------------+---------------------------------------------------------+------------+\n</code></pre> <p>In this example, we see the root resource provider\u00a0<code>5958a366-3cad-416a-a2c9-cfbb5a472287</code>\u00a0with four other resource providers that are its children and where each of them corresponds to a single physical GPU.</p> <p>2. Check the inventory of each resource provider to see resource classes</p> <pre><code>$ openstack resource provider inventory list 5958a366-3cad-416a-a2c9-cfbb5a472287\n+----------------+------------------+----------+----------+-----------+----------+-------+\n| resource_class | allocation_ratio | max_unit | reserved | step_size | min_unit | total |\n+----------------+------------------+----------+----------+-----------+----------+-------+\n| VCPU           |             16.0 |       48 |        0 |         1 |        1 |    48 |\n| MEMORY_MB      |              1.5 |    65442 |      512 |         1 |        1 | 65442 |\n| DISK_GB        |              1.0 |       49 |        0 |         1 |        1 |    49 |\n+----------------+------------------+----------+----------+-----------+----------+-------+\n$ openstack resource provider inventory list e2f8607b-0683-4141-a8af-f5e20682e28c\n+----------------+------------------+----------+----------+-----------+----------+-------+\n| resource_class | allocation_ratio | max_unit | reserved | step_size | min_unit | total |\n+----------------+------------------+----------+----------+-----------+----------+-------+\n| VGPU           |              1.0 |       16 |        0 |         1 |        1 |    16 |\n+----------------+------------------+----------+----------+-----------+----------+-------+\n</code></pre> <p>Here you can see a\u00a0<code>VGPU</code>\u00a0inventory on the child resource provider while other resource class inventories are still located on the root resource provider.</p> <p>3. Check allocations for each server that is using virtual GPUs</p> <pre><code>$ openstack server list\n+--------------------------------------+-------+--------+---------------------------------------------------------+--------------------------+--------+\n| ID                                   | Name  | Status | Networks                                                | Image                    | Flavor |\n+--------------------------------------+-------+--------+---------------------------------------------------------+--------------------------+--------+\n| 5294f726-33d5-472a-bef1-9e19bb41626d | vgpu2 | ACTIVE | private=10.0.0.14, fd45:cdad:c431:0:f816:3eff:fe78:a748 | cirros-0.4.0-x86_64-disk | vgpu   |\n| a6811fc2-cec8-4f1d-baea-e2c6339a9697 | vgpu1 | ACTIVE | private=10.0.0.34, fd45:cdad:c431:0:f816:3eff:fe54:cc8f | cirros-0.4.0-x86_64-disk | vgpu   |\n+--------------------------------------+-------+--------+---------------------------------------------------------+--------------------------+--------+\n\n$ openstack resource provider allocation show 5294f726-33d5-472a-bef1-9e19bb41626d\n+--------------------------------------+------------+------------------------------------------------+\n| resource_provider                    | generation | resources                                      |\n+--------------------------------------+------------+------------------------------------------------+\n| 5958a366-3cad-416a-a2c9-cfbb5a472287 |          8 | {u'VCPU': 1, u'MEMORY_MB': 512, u'DISK_GB': 1} |\n| 7033d860-8d8a-4963-8555-0aa902a08653 |          3 | {u'VGPU': 1}                                   |\n+--------------------------------------+------------+------------------------------------------------+\n\n$ openstack resource provider allocation show a6811fc2-cec8-4f1d-baea-e2c6339a9697\n+--------------------------------------+------------+------------------------------------------------+\n| resource_provider                    | generation | resources                                      |\n+--------------------------------------+------------+------------------------------------------------+\n| e2f8607b-0683-4141-a8af-f5e20682e28c |          3 | {u'VGPU': 1}                                   |\n| 5958a366-3cad-416a-a2c9-cfbb5a472287 |          8 | {u'VCPU': 1, u'MEMORY_MB': 512, u'DISK_GB': 1} |\n+--------------------------------------+------------+------------------------------------------------+\n</code></pre> <p>In this example, two servers were created using a flavor asking for 1\u00a0<code>VGPU</code>, so when looking at the allocations for each consumer UUID (which is the server UUID), you can see that VGPU allocation is against the child resource provider while other allocations are for the root resource provider. Here, that means that the virtual GPU used by\u00a0<code>a6811fc2-cec8-4f1d-baea-e2c6339a9697</code>\u00a0is actually provided by the physical GPU having the PCI ID\u00a0<code>0000:85:00.0</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Attaching_virtual_GPU_devices_to_guests/#optional-provide-custom-traits-for-multiple-gpu-types","title":"(Optional) Provide custom traits for multiple GPU types","text":"<p>Since operators want to support different GPU types per compute, it would be nice to have flavors asking for a specific GPU type. This is now possible using custom traits by decorating child Resource Providers that correspond to physical GPUs.</p> <p>1. Get the list of resource providers</p> <p>See\u00a0Checking allocations and inventories for virtual GPUs\u00a0first for getting the list of Resource Providers that support a\u00a0<code>VGPU</code>\u00a0resource class.</p> <p>2. Define custom traits that will correspond for each to a GPU type</p> <pre><code>$ openstack --os-placement-api-version 1.6 trait create CUSTOM_NVIDIA_11\n</code></pre> <p>In this example, we ask to create a custom trait named\u00a0`CUSTOM_NVIDIA_11.</p> <p>3. Add the corresponding trait to the Resource Provider matching the GPU</p> <pre><code>$ openstack --os-placement-api-version 1.6 resource provider trait set \\\n    --trait CUSTOM_NVIDIA_11 e2f8607b-0683-4141-a8af-f5e20682e28c\n</code></pre> <p>In this case, the trait\u00a0<code>CUSTOM_NVIDIA_11</code>\u00a0will be added to the Resource Provider with the UUID\u00a0<code>e2f8607b-0683-4141-a8af-f5e20682e28c</code>\u00a0that corresponds to the PCI address\u00a0<code>0000:85:00:0</code>\u00a0as shown above.</p> <p>4. Amend the flavor to add a requested trait</p> <pre><code>$ openstack flavor set --property trait:CUSTOM_NVIDIA_11=required vgpu_1\n</code></pre> <p>In this example, we add the\u00a0<code>CUSTOM_NVIDIA_11</code>\u00a0trait as a required information for the\u00a0<code>vgpu_1</code>\u00a0flavor we created earlier.</p> <p>This will allow the Placement service to only return the Resource Providers matching this trait so only the GPUs that were decorated with will be checked for this flavor.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Availability_Zones/","title":"Availability Zones","text":"<p>Note This section provides deployment and admin-user usage information about the availability zone feature. For end-user information about availability zones, refer to the\u00a0user guide.</p> <p>Availability Zones are an end-user visible logical abstraction for partitioning a cloud without knowing the physical infrastructure. They can be used to partition a cloud on arbitrary factors, such as location (country, datacenter, rack), network layout and/or power source.</p> <p>Note Availability Zones should not be assumed to map to fault domains and provide no intrinsic HA benefit by themselves.</p> <p>Availability zones are not modeled in the database; rather, they are defined by attaching specific metadata information to an\u00a0aggregate\u00a0The addition of this specific metadata to an aggregate makes the aggregate visible from an end-user perspective and consequently allows users to schedule instances to a specific set of hosts, the ones belonging to the aggregate. There are a few additional differences to note when comparing availability zones and host aggregates:</p> <ul> <li> <p>A host can be part of multiple aggregates but it can only be in one availability zone.</p> </li> <li> <p>By default a host is part of a default availability zone even if it doesn\u2019t belong to an aggregate. The name of this default availability zone can be configured using\u00a0<code>default_availability_zone</code>\u00a0config option.</p> </li> </ul> <p>Warning The use of the default availability zone name in requests can be very error-prone. Since the user can see the list of availability zones, they have no way to know whether the default availability zone name (currently\u00a0<code>nova</code>) is provided because an host belongs to an aggregate whose AZ metadata key is set to\u00a0<code>nova</code>, or because there is at least one host not belonging to any aggregate. Consequently, it is highly recommended for users to never ever ask for booting an instance by specifying an explicit AZ named\u00a0<code>nova</code>\u00a0and for operators to never set the AZ metadata for an aggregate to\u00a0<code>nova</code>. This can result is some problems due to the fact that the instance AZ information is explicitly attached to\u00a0<code>nova</code>\u00a0which could break further move operations when either the host is moved to another aggregate or when the user would like to migrate the instance.</p> <p>Note Availability zone names must NOT contain\u00a0<code>:</code>\u00a0since it is used by admin users to specify hosts where instances are launched in server creation. See\u00a0Using availability zones to select hosts\u00a0for more information.</p> <p>In addition, other services, such as the\u00a0networking service\u00a0and the\u00a0block storage service, also provide an availability zone feature. However, the implementation of these features differs vastly between these different services. Consult the documentation for these other services for more information on their implementation of this feature.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Availability_Zones/#availability-zones-with-placement","title":"Availability Zones with Placement","text":"<p>In order to use placement to honor availability zone requests, there must be placement aggregates that match the membership and UUID of nova host aggregates that you assign as availability zones. The same key in aggregate metadata used by the\u00a0AvailabilityZoneFilter\u00a0filter controls this function, and is enabled by setting\u00a0<code>scheduler.query_placement_for_availability_zone</code>\u00a0to\u00a0<code>True</code>. As of 24.0.0 (Xena), this is the default.</p> <pre><code>$ openstack --os-compute-api-version=2.53 aggregate create myaz\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| availability_zone | None                                 |\n| created_at        | 2018-03-29T16:22:23.175884           |\n| deleted           | False                                |\n| deleted_at        | None                                 |\n| id                | 4                                    |\n| name              | myaz                                 |\n| updated_at        | None                                 |\n| uuid              | 019e2189-31b3-49e1-aff2-b220ebd91c24 |\n+-------------------+--------------------------------------+\n\n$ openstack --os-compute-api-version=2.53 aggregate add host myaz node1\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| availability_zone | None                                 |\n| created_at        | 2018-03-29T16:22:23.175884           |\n| deleted           | False                                |\n| deleted_at        | None                                 |\n| hosts             | [u'node1']                           |\n| id                | 4                                    |\n| name              | myagg                                |\n| updated_at        | None                                 |\n| uuid              | 019e2189-31b3-49e1-aff2-b220ebd91c24 |\n+-------------------+--------------------------------------+\n\n$ openstack aggregate set --property availability_zone=az002 myaz\n\n$ openstack --os-placement-api-version=1.2 resource provider aggregate set --aggregate 019e2189-31b3-49e1-aff2-b220ebd91c24 815a5634-86fb-4e1e-8824-8a631fee3e06\n</code></pre> <p>Without the above configuration, the\u00a0AvailabilityZoneFilter\u00a0filter must be enabled in\u00a0<code>filter_scheduler.enabled_filters</code>\u00a0to retain proper behavior.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Availability_Zones/#implications-for-moving-servers","title":"Implications for moving servers","text":"<p>There are several ways to move a server to another host: evacuate, resize, cold migrate, live migrate, and unshelve. Move operations typically go through the scheduler to pick the target host\u00a0unless\u00a0a target host is specified and the request forces the server to that host by bypassing the scheduler. Only evacuate and live migrate can forcefully bypass the scheduler and move a server to a specified host and even then it is highly recommended to\u00a0not\u00a0force and bypass the scheduler.</p> <p>With respect to availability zones, a server is restricted to a zone if:</p> <ol> <li> <p>The server was created in a specific zone with the\u00a0<code>POST\u00a0/servers</code>\u00a0request containing the\u00a0<code>availability_zone</code>\u00a0parameter.</p> </li> <li> <p>If the server create request did not contain the\u00a0<code>availability_zone</code>\u00a0parameter but the API service is configured for\u00a0<code>default_schedule_zone</code>\u00a0then by default the server will be scheduled to that zone.</p> </li> <li> <p>The shelved offloaded server was unshelved by specifying the\u00a0<code>availability_zone</code>\u00a0with the\u00a0<code>POST\u00a0/servers/{server_id}/action</code>\u00a0request using microversion 2.77 or greater.</p> </li> <li> <p><code>cinder.cross_az_attach</code>\u00a0is False,\u00a0<code>default_schedule_zone</code>\u00a0is None, the server is created without an explicit zone but with pre-existing volume block device mappings. In that case the server will be created in the same zone as the volume(s) if the volume zone is not the same as\u00a0<code>default_availability_zone</code>. See\u00a0Resource affinity\u00a0for details.</p> </li> </ol> <p>If the server was not created in a specific zone then it is free to be moved to other zones, i.e. the\u00a0AvailabilityZoneFilter\u00a0is a no-op.</p> <p>Knowing this, it is dangerous to force a server to another host with evacuate or live migrate if the server is restricted to a zone and is then forced to move to a host in another zone, because that will create an inconsistency in the internal tracking of where that server should live and may require manually updating the database for that server. For example, if a user creates a server in zone A and then the admin force live migrates the server to zone B, and then the user resizes the server, the scheduler will try to move it back to zone A which may or may not work, e.g. if the admin deleted or renamed zone A in the interim.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Availability_Zones/#resource-affinity","title":"Resource affinity","text":"<p>The\u00a0<code>cinder.cross_az_attach</code>\u00a0configuration option can be used to restrict servers and the volumes attached to servers to the same availability zone.</p> <p>A typical use case for setting\u00a0<code>cross_az_attach=False</code>\u00a0is to enforce compute and block storage affinity, for example in a High Performance Compute cluster.</p> <p>By default\u00a0<code>cross_az_attach</code>\u00a0is True meaning that the volumes attached to a server can be in a different availability zone than the server. If set to False, then when creating a server with pre-existing volumes or attaching a volume to a server, the server and volume zone must match otherwise the request will fail. In addition, if the nova-compute service creates the volumes to attach to the server during server create, it will request that those volumes are created in the same availability zone as the server, which must exist in the block storage (cinder) service.</p> <p>As noted in the\u00a0Implications for moving servers\u00a0section, forcefully moving a server to another zone could also break affinity with attached volumes.</p> <p>Note <code>cross_az_attach=False</code>\u00a0is not widely used nor tested extensively and thus suffers from some known issues: * Bug 1694844. This is fixed in the 21.0.0 (Ussuri) release by using the volume zone for the server being created if the server is created without an explicit zone,\u00a0<code>default_schedule_zone</code>\u00a0is None, and the volume zone does not match the value of\u00a0<code>default_availability_zone</code>\u00a0. * Bug 1781421</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Availability_Zones/#using-availability-zones-to-select-hosts","title":"Using availability zones to select hosts","text":"<p>We can combine availability zones with a specific host and/or node to select where an instance is launched. For example:</p> <pre><code>$ openstack server create --availability-zone ZONE:HOST:NODE ... SERVER\n</code></pre> <p>Note It is possible to use\u00a0<code>ZONE</code>,\u00a0<code>ZONE:HOST</code>, and\u00a0<code>ZONE::NODE</code>.</p> <p>Note This is an admin-only operation by default, though you can modify this behavior using the\u00a0<code>os_compute_api:servers:create:forced_host</code>\u00a0rule in\u00a0<code>policy.yaml</code>.</p> <p>However, as discussed\u00a0previously, when launching instances in this manner the scheduler filters are not run. For this reason, this behavior is considered legacy behavior and, starting with the 2.74 microversion, it is now possible to specify a host or node explicitly. For example:</p> <pre><code>$ openstack --os-compute-api-version 2.74 server create \\\n    --host HOST --hypervisor-hostname HYPERVISOR ... SERVER\n</code></pre> <p>Note This is an admin-only operation by default, though you can modify this behavior using the\u00a0<code>compute:servers:create:requested_destination</code>\u00a0rule in\u00a0<code>policy.yaml</code>.</p> <p>This avoids the need to explicitly select an availability zone and ensures the scheduler filters are not bypassed.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Availability_Zones/#usage","title":"Usage","text":"<p>Creating an availability zone (AZ) is done by associating metadata with a\u00a0host aggregate. For this reason, the\u00a0openstack\u00a0client provides the ability to create a host aggregate and associate it with an AZ in one command. For example, to create a new aggregate, associating it with an AZ in the process, and add host to it using the\u00a0openstack\u00a0client, run:</p> <pre><code>$ openstack aggregate create --zone my-availability-zone my-aggregate\n$ openstack aggregate add host my-aggregate my-host\n</code></pre> <p>Note While it is possible to add a host to multiple host aggregates, it is not possible to add them to multiple availability zones. Attempting to add a host to multiple host aggregates associated with differing availability zones will result in a failure.</p> <p>Alternatively, you can set this metadata manually for an existing host aggregate. For example:</p> <pre><code>$ openstack aggregate set \\\n    --property availability_zone=my-availability-zone my-aggregate\n</code></pre> <p>To list all host aggregates and show information about a specific aggregate, in order to determine which AZ the host aggregate(s) belong to, run:</p> <pre><code>$ openstack aggregate list --long\n$ openstack aggregate show my-aggregate\n</code></pre> <p>Finally, to disassociate a host aggregate from an availability zone, run:</p> <pre><code>$ openstack aggregate unset --property availability_zone my-aggregate\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Availability_Zones/#configuration","title":"Configuration","text":"<p>Refer to\u00a0Host aggregates\u00a0for information on configuring both host aggregates and availability zones.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/","title":"Compute (nova)","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#what-is-nova","title":"What is nova?","text":"<p>Nova is the OpenStack project that provides a way to provision compute instances (aka virtual servers). Nova supports creating virtual machines, baremetal servers (through the use of ironic), and has limited support for system containers. Nova runs as a set of daemons on top of existing Linux servers to provide that service.</p> <p>It requires the following additional OpenStack services for basic function:</p> <ul> <li> <p>Keystone: This provides identity and authentication for all OpenStack services.</p> </li> <li> <p>Glance: This provides the compute image repository. All compute instances launch from glance images.</p> </li> <li> <p>Neutron: This is responsible for provisioning the virtual or physical networks that compute instances connect to on boot.</p> </li> <li> <p>Placement: This is responsible for tracking inventory of resources available in a cloud and assisting in choosing which provider of those resources will be used when creating a virtual machine.</p> </li> </ul> <p>It can also integrate with other services to include: persistent block storage, encrypted disks, and baremetal compute instances.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#for-end-users","title":"For End Users","text":"<p>As an end user of nova, you\u2019ll use nova to create and manage servers with either tools or the API directly.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#tools-for-using-nova","title":"Tools for using Nova","text":"<ul> <li> <p>Horizon: The official web UI for the OpenStack Project.</p> </li> <li> <p>OpenStack Client: The official CLI for OpenStack Projects. You should use this as your CLI for most things, it includes not just nova commands but also commands for most of the projects in OpenStack.</p> </li> <li> <p>Nova Client: For some very advanced features (or administrative commands) of nova you may need to use nova client. It is still supported, but the\u00a0<code>openstack</code>\u00a0cli is recommended.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#writing-to-the-api","title":"Writing to the API","text":"<p>All end user (and some administrative) features of nova are exposed via a REST API, which can be used to build more complicated logic or automation with nova. This can be consumed directly, or via various SDKs. The following resources will help you get started with consuming the API directly.</p> <ul> <li> <p>Compute API Guide: The concept guide for the API. This helps lay out the concepts behind the API to make consuming the API reference easier.</p> </li> <li> <p>Compute API Reference: The complete reference for the compute API, including all methods and request / response parameters and their meaning.</p> </li> <li> <p>Compute API Microversion History: The compute API evolves over time through\u00a0Microversions. This provides the history of all those changes. Consider it a \u201cwhat\u2019s new\u201d in the compute API.</p> </li> <li> <p>Block Device Mapping: One of the trickier parts to understand is the Block Device Mapping parameters used to connect specific block devices to computes. This deserves its own deep dive.</p> </li> <li> <p>Metadata: Provide information to the guest instance when it is created.</p> </li> </ul> <p>Nova can be configured to emit notifications over RPC.</p> <ul> <li>Versioned Notifications: This provides information on the notifications emitted by nova.</li> </ul> <p>Other end-user guides can be found under\u00a0User Documentation.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#for-operators","title":"For Operators","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#architecture-overview","title":"Architecture Overview","text":"<ul> <li>Nova architecture: An overview of how all the parts in nova fit together.</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#installation","title":"Installation","text":"<p>The detailed install guide for nova. A functioning nova will also require having installed\u00a0keystone,\u00a0glance,\u00a0neutron, and\u00a0placement. Ensure that you follow their install guides first.</p> <ul> <li> <p>Compute service</p> </li> <li> <p>Overview</p> </li> <li> <p>Compute service overview</p> </li> <li> <p>Install and configure controller node</p> </li> <li> <p>Install and configure a compute node</p> </li> <li> <p>Verify operation</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#deployment-considerations","title":"Deployment Considerations","text":"<p>There is information you might want to consider before doing your deployment, especially if it is going to be a larger deployment. For smaller deployments the defaults from the\u00a0install guide\u00a0will be sufficient.</p> <ul> <li> <p>Compute Driver Features Supported: While the majority of nova deployments use libvirt/kvm, you can use nova with other compute drivers. Nova attempts to provide a unified feature set across these, however, not all features are implemented on all backends, and not all features are equally well tested.</p> </li> <li> <p>Feature Support by Use Case: A view of what features each driver supports based on what\u2019s important to some large use cases (General Purpose Cloud, NFV Cloud, HPC Cloud).</p> </li> <li> <p>Feature Support full list: A detailed dive through features in each compute driver backend.</p> </li> <li> <p>Cells v2 configuration: For large deployments, cells v2 cells allow sharding of your compute environment. Upfront planning is key to a successful cells v2 layout.</p> </li> <li> <p>Running nova-api on wsgi: Considerations for using a real WSGI container instead of the baked-in eventlet web server.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#maintenance","title":"Maintenance","text":"<p>Once you are running nova, the following information is extremely useful.</p> <ul> <li> <p>Admin Guide: A collection of guides for administrating nova.</p> </li> <li> <p>Flavors: What flavors are and why they are used.</p> </li> <li> <p>Upgrades: How nova is designed to be upgraded for minimal service impact, and the order you should do them in.</p> </li> <li> <p>Quotas: Managing project quotas in nova.</p> </li> <li> <p>Aggregates: Aggregates are a useful way of grouping hosts together for scheduling purposes.</p> </li> <li> <p>Scheduling: How the scheduler is configured, and how that will impact where compute instances land in your environment. If you are seeing unexpected distribution of compute instances in your hosts, you\u2019ll want to dive into this configuration.</p> </li> <li> <p>Exposing custom metadata to compute instances: How and when you might want to extend the basic metadata exposed to compute instances (either via metadata server or config drive) for your specific purposes.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#reference-material","title":"Reference Material","text":"<ul> <li> <p>Nova CLI Command References: the complete command reference for all the daemons and admin tools that come with nova.</p> </li> <li> <p>Configuration Guide: Information on configuring the system, including role-based access control policy rules.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#for-contributors","title":"For Contributors","text":"<ul> <li> <p>So You Want to Contribute\u2026: If you are a new contributor this should help you to start contributing to Nova.</p> </li> <li> <p>Contributor Documentation: If you are new to Nova, this should help you start to understand what Nova actually does, and why.</p> </li> <li> <p>Technical Reference Deep Dives: There are also a number of technical references on both current and future looking parts of our architecture. These are collected here.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Compute-nova/#search","title":"Search","text":"<ul> <li> <p>Nova document search: Search the contents of this document.</p> </li> <li> <p>OpenStack wide search: Search the wider set of OpenStack documentation, including forums.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Emulated_Trusted_Platform_Module/","title":"Emulated Trusted Platform Module (vTPM)","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Emulated_Trusted_Platform_Module/#enabling-vtpm","title":"Enabling vTPM","text":"<p>The following are required on each compute host wishing to support the vTPM feature:</p> <ul> <li> <p>Currently vTPM is only supported when using the libvirt compute driver with a\u00a0<code>libvirt.virt_type</code>\u00a0of\u00a0<code>kvm</code>\u00a0or\u00a0<code>qemu</code>.</p> </li> <li> <p>A\u00a0key manager service, such as\u00a0barbican, must be configured to store secrets used to encrypt the virtual device files at rest.</p> </li> <li> <p>The\u00a0swtpm\u00a0binary and associated\u00a0libraries.</p> </li> <li> <p>Set the\u00a0<code>libvirt.swtpm_enabled</code>\u00a0config option to\u00a0<code>True</code>. This will enable support for both TPM version 1.2 and 2.0.</p> </li> </ul> <p>With the above requirements satisfied, verify vTPM support by inspecting the traits on the compute node\u2019s resource provider:</p> <pre><code>$ COMPUTE_UUID=$(openstack resource provider list --name $HOST -f value -c uuid)\n$ openstack resource provider trait list $COMPUTE_UUID | grep SECURITY_TPM\n| COMPUTE_SECURITY_TPM_1_2 |\n| COMPUTE_SECURITY_TPM_2_0 |\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Emulated_Trusted_Platform_Module/#configuring-a-flavor-or-image","title":"Configuring a flavor or image","text":"<p>A vTPM can be requested on a server via flavor extra specs or image metadata properties. There are two versions supported \u2013 1.2 and 2.0 \u2013 and two models \u2013 TPM Interface Specification (TIS) and Command-Response Buffer (CRB). The CRB model is only supported with version 2.0.</p> Flavor extra_specs Image metadata Description hw:tpm_version hw_tpm_version Specify the TPM version, <code>1.2</code> or <code>2.0</code>. Required if requesting a vTPM. hw:tpm_model hw_tpm_model Specify the TPM model, <code>tpm-tis</code> (the default) or <code>tpm-crb</code> (only valid with version <code>2.0</code>). <p>For example, to configure a flavor to use the TPM 2.0 with the CRB model:</p> <pre><code>$ openstack flavor set $FLAVOR \\\n    --property hw:tpm_version=2.0 \\\n    --property hw:tpm_model=tpm-crb\n</code></pre> <p>$ openstack flavor set $FLAVOR \\ \u2013property hw:tpm_version=2.0 \\ \u2013property hw:tpm_model=tpm-crb</p> <p>Scheduling will fail if flavor and image supply conflicting values, or if model\u00a0<code>tpm-crb</code>\u00a0is requested with version\u00a0<code>1.2</code>.</p> <p>Upon successful boot, the server should see a TPM device such as\u00a0<code>/dev/tpm0</code>\u00a0which can be used in the same manner as a hardware TPM.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Emulated_Trusted_Platform_Module/#limitations","title":"Limitations","text":"<ul> <li> <p>Only server operations performed by the server owner are supported, as the user\u2019s credentials are required to unlock the virtual device files on the host. Thus the admin may need to decide whether to grant the user additional policy roles; if not, those operations are effectively disabled.</p> </li> <li> <p>Live migration, evacuation, shelving and rescuing of servers with vTPMs is not currently supported.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Emulated_Trusted_Platform_Module/#security","title":"Security","text":"<p>With a hardware TPM, the root of trust is a secret known only to the TPM user. In contrast, an emulated TPM comprises a file on disk which the libvirt daemon must be able to present to the guest. At rest, this file is encrypted using a passphrase stored in a key manager service. The passphrase in the key manager is associated with the credentials of the owner of the server (the user who initially created it). The passphrase is retrieved and used by libvirt to unlock the emulated TPM data any time the server is booted.</p> <p>Although the above mechanism uses a libvirt\u00a0secret\u00a0that is both\u00a0<code>private</code>\u00a0(can\u2019t be displayed via the libvirt API or\u00a0<code>virsh</code>) and\u00a0<code>ephemeral</code>\u00a0(exists only in memory, never on disk), it is theoretically possible for a sufficiently privileged user to retrieve the secret and/or vTPM data from memory.</p> <p>A full analysis and discussion of security issues related to emulated TPM is beyond the scope of this document.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Emulated_Trusted_Platform_Module/#references","title":"References","text":"<ul> <li> <p>TCG PC Client Specific TPM Interface Specification (TIS)</p> </li> <li> <p>TCG PC Client Platform TPM Profile (PTP) Specification</p> </li> <li> <p>QEMU docs on tpm</p> </li> <li> <p>Libvirt XML to request emulated TPM device</p> </li> <li> <p>Libvirt secret for usage type \u201cvtpm\u201c</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Flavors_Overview/","title":"Flavors Overview","text":"<p>In OpenStack, flavors define the compute, memory, and storage capacity of nova computing instances. To put it simply, a flavor is an available hardware configuration for a server. It defines the\u00a0size\u00a0of a virtual server that can be launched.</p> <p>Note Flavors can also determine on which compute host a flavor can be used to launch an instance. For information about customizing flavors, refer to\u00a0Manage Flavors.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Flavors_Overview/#overview","title":"Overview","text":"<p>A flavor consists of the following parameters:Flavor ID</p> <p>Unique ID (integer or UUID) for the new flavor. This property is required. If specifying \u2018auto\u2019, a UUID will be automatically generated.Name</p> <p>Name for the new flavor. This property is required.</p> <p>Historically, names were given a format\u00a0XX.SIZE_NAME. These are typically not required, though some third party tools may rely on it.VCPUs</p> <p>Number of virtual CPUs to use. This property is required.Memory MB</p> <p>Amount of RAM to use (in megabytes). This property is required.Root Disk GB</p> <p>Amount of disk space (in gigabytes) to use for the root (<code>/</code>) partition. This property is required.</p> <p>The root disk is an ephemeral disk that the base image is copied into. When booting from a persistent volume it is not used. The\u00a0<code>0</code>\u00a0size is a special case which uses the native base image size as the size of the ephemeral root volume. However, in this case the scheduler cannot select the compute host based on the virtual image size. As a result,\u00a0<code>0</code>\u00a0should only be used for volume booted instances or for testing purposes. Volume-backed instances can be enforced for flavors with zero root disk via the\u00a0<code>os_compute_api:servers:create:zero_disk_flavor</code>\u00a0policy rule.Ephemeral Disk GB</p> <p>Amount of disk space (in gigabytes) to use for the ephemeral partition. This property is optional. If unspecified, the value is\u00a0<code>0</code>\u00a0by default.</p> <p>Ephemeral disks offer machine local disk storage linked to the lifecycle of a VM instance. When a VM is terminated, all data on the ephemeral disk is lost. Ephemeral disks are not included in any snapshots.Swap</p> <p>Amount of swap space (in megabytes) to use. This property is optional. If unspecified, the value is\u00a0<code>0</code>\u00a0by default.RXTX Factor (DEPRECATED)</p> <p>This value was only applicable when using the\u00a0<code>xen</code>\u00a0compute driver with the\u00a0<code>nova-network</code>\u00a0network driver. Since\u00a0<code>nova-network</code>\u00a0has been removed, this no longer applies and should not be specified. It will likely be removed in a future release.\u00a0<code>neutron</code>\u00a0users should refer to the\u00a0neutron QoS documentationIs Public</p> <p>Boolean value that defines whether the flavor is available to all users or private to the project it was created in. This property is optional. In unspecified, the value is\u00a0<code>True</code>\u00a0by default.</p> <p>By default, a flavor is public and available to all projects. Private flavors are only accessible to those on the access list for a given project and are invisible to other projects.Extra Specs</p> <p>Key and value pairs that define on which compute nodes a flavor can run. These are optional.</p> <p>Extra specs are generally used as scheduler hints for more advanced instance configuration. The key-value pairs used must correspond to well-known options. For more information on the standardized extra specs available,\u00a0see belowDescription</p> <p>A free form description of the flavor. Limited to 65535 characters in length. Only printable characters are allowed. Available starting in microversion 2.55.</p> <p>Hardware video RAM</p> <p>Specify\u00a0<code>hw_video:ram_max_mb</code>\u00a0to control the maximum RAM for the video image. Used in conjunction with the\u00a0<code>hw_video_ram</code>\u00a0image property.\u00a0<code>hw_video_ram</code>\u00a0must be less than or equal to\u00a0<code>hw_video:ram_max_mb</code>.</p> <p>This is currently supported by the libvirt and the vmware drivers.</p> <p>See\u00a0https://libvirt.org/formatdomain.html#elementsVideo\u00a0for more information on how this is used to set the\u00a0<code>vram</code>\u00a0attribute with the libvirt driver.</p> <p>Secure Boot\u00a0can help ensure the bootloader used for your instances is trusted, preventing a possible attack vector.</p> <pre><code>$ openstack flavor set FLAVOR-NAME \\\n    --property os:secure_boot=SECURE_BOOT_OPTION\n</code></pre> <p>Valid\u00a0<code>SECURE_BOOT_OPTION</code>\u00a0values are:</p> <ul> <li> <p><code>required</code>: Enable Secure Boot for instances running with this flavor.</p> </li> <li> <p><code>disabled</code>\u00a0or\u00a0<code>optional</code>: (default) Disable Secure Boot for instances running with this flavor.</p> </li> </ul> <p>Note Supported by the Hyper-V and libvirt drivers.</p> <p>Changed in version 23.0.0:\u00a0(Wallaby)</p> <p>Added support for secure boot to the libvirt driver.Custom resource classes and standard resource classes to override</p> <p>Specify custom resource classes to require or override quantity values of standard resource classes.</p> <p>The syntax of the extra spec is\u00a0<code>resources:&lt;resource_class_name&gt;=VALUE</code>\u00a0(<code>VALUE</code>\u00a0is integer). The name of custom resource classes must start with\u00a0<code>CUSTOM_</code>. Standard resource classes to override are\u00a0<code>VCPU</code>,\u00a0<code>MEMORY_MB</code>\u00a0or\u00a0<code>DISK_GB</code>. In this case, you can disable scheduling based on standard resource classes by setting the value to\u00a0<code>0</code>.</p> <p>For example:</p> <ul> <li> <p><code>resources:CUSTOM_BAREMETAL_SMALL=1</code></p> </li> <li> <p><code>resources:VCPU=0</code></p> </li> </ul> <p>See\u00a0Create flavors for use with the Bare Metal service\u00a0for more examples.</p> <p>New in version 16.0.0:\u00a0(Pike)Required traits</p> <p>Required traits allow specifying a server to build on a compute node with the set of traits specified in the flavor. The traits are associated with the resource provider that represents the compute node in the Placement API. See the resource provider traits API reference for more details:\u00a0https://docs.openstack.org/api-ref/placement/#resource-provider-traits</p> <p>The syntax of the extra spec is\u00a0<code>trait:&lt;trait_name&gt;=required</code>, for example:</p> <ul> <li> <p><code>trait:HW_CPU_X86_AVX2=required</code></p> </li> <li> <p><code>trait:STORAGE_DISK_SSD=required</code></p> </li> </ul> <p>The scheduler will pass required traits to the\u00a0<code>GET\u00a0/allocation_candidates</code>\u00a0endpoint in the Placement API to include only resource providers that can satisfy the required traits. In 17.0.0 the only valid value is\u00a0<code>required</code>. In 18.0.0\u00a0<code>forbidden</code>\u00a0is added (see below). Any other value will be considered invalid.</p> <p>Traits can be managed using the\u00a0osc-placement plugin.</p> <p>New in version 17.0.0:\u00a0(Queens)Forbidden traits</p> <p>Forbidden traits are similar to required traits, described above, but instead of specifying the set of traits that must be satisfied by a compute node, forbidden traits must\u00a0not\u00a0be present.</p> <p>The syntax of the extra spec is\u00a0<code>trait:&lt;trait_name&gt;=forbidden</code>, for example:</p> <ul> <li> <p><code>trait:HW_CPU_X86_AVX2=forbidden</code></p> </li> <li> <p><code>trait:STORAGE_DISK_SSD=forbidden</code></p> </li> </ul> <p>Traits can be managed using the\u00a0osc-placement plugin.</p> <p>New in version 18.0.0:\u00a0(Rocky)Numbered groupings of resource classes and traits</p> <p>Specify numbered groupings of resource classes and traits.</p> <p>The syntax is as follows (<code>N</code>\u00a0and\u00a0<code>VALUE</code>\u00a0are integers):</p> <pre><code>resourcesN:&lt;resource_class_name&gt;=VALUE\ntraitN:&lt;trait_name&gt;=required\n</code></pre> <p>A given numbered\u00a0<code>resources</code>\u00a0or\u00a0<code>trait</code>\u00a0key may be repeated to specify multiple resources/traits in the same grouping, just as with the un-numbered syntax.</p> <p>Specify inter-group affinity policy via the\u00a0<code>group_policy</code>\u00a0key, which may have the following values:</p> <ul> <li> <p><code>isolate</code>: Different numbered request groups will be satisfied by\u00a0different\u00a0providers.</p> </li> <li> <p><code>none</code>: Different numbered request groups may be satisfied by different providers\u00a0or\u00a0common providers.</p> </li> </ul> <p>Note If more than one group is specified then the\u00a0<code>group_policy</code>\u00a0is mandatory in the request. However such groups might come from other sources than flavor extra_spec (e.g. from Neutron ports with QoS minimum bandwidth policy). If the flavor does not specify any groups and\u00a0<code>group_policy</code>\u00a0but more than one group is coming from other sources then nova will default the\u00a0<code>group_policy</code>\u00a0to\u00a0<code>none</code>\u00a0to avoid scheduler failure.</p> <p>For example, to create a server with the following VFs:</p> <ul> <li> <p>One SR-IOV virtual function (VF) on NET1 with bandwidth 10000 bytes/sec</p> </li> <li> <p>One SR-IOV virtual function (VF) on NET2 with bandwidth 20000 bytes/sec on a\u00a0different\u00a0NIC with SSL acceleration</p> </li> </ul> <p>It is specified in the extra specs as follows:</p> <pre><code>resources1:SRIOV_NET_VF=1\nresources1:NET_EGRESS_BYTES_SEC=10000\ntrait1:CUSTOM_PHYSNET_NET1=required\nresources2:SRIOV_NET_VF=1\nresources2:NET_EGRESS_BYTES_SEC:20000\ntrait2:CUSTOM_PHYSNET_NET2=required\ntrait2:HW_NIC_ACCEL_SSL=required\ngroup_policy=isolate\n</code></pre> <p>See\u00a0Granular Resource Request Syntax\u00a0for more details.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Host_Aggregates/","title":"Host Aggregates","text":"<p>Host aggregates are a mechanism for partitioning hosts in an OpenStack cloud, or a region of an OpenStack cloud, based on arbitrary characteristics. Examples where an administrator may want to do this include where a group of hosts have additional hardware or performance characteristics.</p> <p>Host aggregates started out as a way to use Xen hypervisor resource pools, but have been generalized to provide a mechanism to allow administrators to assign key-value pairs to groups of machines. Each node can have multiple aggregates, each aggregate can have multiple key-value pairs, and the same key-value pair can be assigned to multiple aggregates. This information can be used in the scheduler to enable advanced scheduling, to set up Xen hypervisor resource pools or to define logical groups for migration.</p> <p>Host aggregates are not explicitly exposed to users. Instead administrators map flavors to host aggregates. Administrators do this by setting metadata on a host aggregate, and matching flavor extra specifications. The scheduler then endeavors to match user requests for instances of the given flavor to a host aggregate with the same key-value pair in its metadata. Compute nodes can be in more than one host aggregate. Weight multipliers can be controlled on a per-aggregate basis by setting the desired\u00a0<code>xxx_weight_multiplier</code>\u00a0aggregate metadata.</p> <p>Administrators are able to optionally expose a host aggregate as an\u00a0Availability Zone. Availability zones are different from host aggregates in that they are explicitly exposed to the user, and hosts can only be in a single availability zone. Administrators can configure a default availability zone where instances will be scheduled when the user fails to specify one. For more information on how to do this, refer to\u00a0Availability Zones.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Host_Aggregates/#configure-scheduler-to-support-host-aggregates","title":"Configure scheduler to support host aggregates","text":"<p>One common use case for host aggregates is when you want to support scheduling instances to a subset of compute hosts because they have a specific capability. For example, you may want to allow users to request compute hosts that have SSD drives if they need access to faster disk I/O, or access to compute hosts that have GPU cards to take advantage of GPU-accelerated code.</p> <p>To configure the scheduler to support host aggregates, the\u00a0<code>filter_scheduler.enabled_filters</code>\u00a0configuration option must contain the\u00a0<code>AggregateInstanceExtraSpecsFilter</code>\u00a0in addition to the other filters used by the scheduler. Add the following line to\u00a0<code>nova.conf</code>\u00a0on the host that runs the\u00a0<code>nova-scheduler</code>\u00a0service to enable host aggregates filtering, as well as the other filters that are typically enabled:</p> <pre><code>[filter_scheduler]\nenabled_filters=...,AggregateInstanceExtraSpecsFilter\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Host_Aggregates/#example-specify-compute-hosts-with-ssds","title":"Example: Specify compute hosts with SSDs","text":"<p>This example configures the Compute service to enable users to request nodes that have solid-state drives (SSDs). You create a\u00a0<code>fast-io</code>\u00a0host aggregate in the\u00a0<code>nova</code>\u00a0availability zone and you add the\u00a0<code>ssd=true</code>\u00a0key-value pair to the aggregate. Then, you add the\u00a0<code>node1</code>, and\u00a0<code>node2</code>\u00a0compute nodes to it.</p> <pre><code>$ openstack aggregate create --zone nova fast-io\n+-------------------+----------------------------+\n| Field             | Value                      |\n+-------------------+----------------------------+\n| availability_zone | nova                       |\n| created_at        | 2016-12-22T07:31:13.013466 |\n| deleted           | False                      |\n| deleted_at        | None                       |\n| id                | 1                          |\n| name              | fast-io                    |\n| updated_at        | None                       |\n+-------------------+----------------------------+\n\n$ openstack aggregate set --property ssd=true 1\n+-------------------+----------------------------+\n| Field             | Value                      |\n+-------------------+----------------------------+\n| availability_zone | nova                       |\n| created_at        | 2016-12-22T07:31:13.000000 |\n| deleted           | False                      |\n| deleted_at        | None                       |\n| hosts             | []                         |\n| id                | 1                          |\n| name              | fast-io                    |\n| properties        | ssd='true'                 |\n| updated_at        | None                       |\n+-------------------+----------------------------+\n\n$ openstack aggregate add host 1 node1\n+-------------------+--------------------------------------------------+\n| Field             | Value                                            |\n+-------------------+--------------------------------------------------+\n| availability_zone | nova                                             |\n| created_at        | 2016-12-22T07:31:13.000000                       |\n| deleted           | False                                            |\n| deleted_at        | None                                             |\n| hosts             | [u'node1']                                       |\n| id                | 1                                                |\n| metadata          | {u'ssd': u'true', u'availability_zone': u'nova'} |\n| name              | fast-io                                          |\n| updated_at        | None                                             |\n+-------------------+--------------------------------------------------+\n\n$ openstack aggregate add host 1 node2\n+-------------------+--------------------------------------------------+\n| Field             | Value                                            |\n+-------------------+--------------------------------------------------+\n| availability_zone | nova                                             |\n| created_at        | 2016-12-22T07:31:13.000000                       |\n| deleted           | False                                            |\n| deleted_at        | None                                             |\n| hosts             | [u'node1', u'node2']                             |\n| id                | 1                                                |\n| metadata          | {u'ssd': u'true', u'availability_zone': u'nova'} |\n| name              | fast-io                                          |\n| updated_at        | None                                             |\n+-------------------+--------------------------------------------------+\n</code></pre> <p>Use the\u00a0openstack flavor create\u00a0command to create the\u00a0<code>ssd.large</code>\u00a0flavor called with an ID of 6, 8 GB of RAM, 80 GB root disk, and 4 vCPUs.</p> <pre><code>$ openstack flavor create --id 6 --ram 8192 --disk 80 --vcpus 4 ssd.large\n+----------------------------+-----------+\n| Field                      | Value     |\n+----------------------------+-----------+\n| OS-FLV-DISABLED:disabled   | False     |\n| OS-FLV-EXT-DATA:ephemeral  | 0         |\n| disk                       | 80        |\n| id                         | 6         |\n| name                       | ssd.large |\n| os-flavor-access:is_public | True      |\n| ram                        | 8192      |\n| rxtx_factor                | 1.0       |\n| swap                       |           |\n| vcpus                      | 4         |\n+----------------------------+-----------+\n</code></pre> <p>Once the flavor is created, specify one or more key-value pairs that match the key-value pairs on the host aggregates with scope\u00a0<code>aggregate_instance_extra_specs</code>. In this case, that is the\u00a0<code>aggregate_instance_extra_specs:ssd=true</code>\u00a0key-value pair. Setting a key-value pair on a flavor is done using the\u00a0openstack flavor set\u00a0command.</p> <pre><code>$ openstack flavor set \\\n    --property aggregate_instance_extra_specs:ssd=true ssd.large\n</code></pre> <p>Once it is set, you should see the\u00a0<code>extra_specs</code>\u00a0property of the\u00a0<code>ssd.large</code>\u00a0flavor populated with a key of\u00a0<code>ssd</code>\u00a0and a corresponding value of\u00a0<code>true</code>.</p> <pre><code>$ openstack flavor show ssd.large\n+----------------------------+-------------------------------------------+\n| Field                      | Value                                     |\n+----------------------------+-------------------------------------------+\n| OS-FLV-DISABLED:disabled   | False                                     |\n| OS-FLV-EXT-DATA:ephemeral  | 0                                         |\n| disk                       | 80                                        |\n| id                         | 6                                         |\n| name                       | ssd.large                                 |\n| os-flavor-access:is_public | True                                      |\n| properties                 | aggregate_instance_extra_specs:ssd='true' |\n| ram                        | 8192                                      |\n| rxtx_factor                | 1.0                                       |\n| swap                       |                                           |\n| vcpus                      | 4                                         |\n+----------------------------+-------------------------------------------+\n</code></pre> <p>Now, when a user requests an instance with the\u00a0<code>ssd.large</code>\u00a0flavor, the scheduler only considers hosts with the\u00a0<code>ssd=true</code>\u00a0key-value pair. In this example, these are\u00a0<code>node1</code>\u00a0and\u00a0<code>node2</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Host_Aggregates/#aggregates-in-placement","title":"Aggregates in Placement","text":"<p>Aggregates also exist in placement and are not the same thing as host aggregates in nova. These aggregates are defined (purely) as groupings of related resource providers. Since compute nodes in nova are represented in placement as resource providers, they can be added to a placement aggregate as well. For example, get the UUID of the compute node using\u00a0openstack hypervisor list\u00a0and add it to an aggregate in placement using\u00a0openstack resource provider aggregate set.</p> <pre><code>$ openstack --os-compute-api-version=2.53 hypervisor list\n+--------------------------------------+---------------------+-----------------+-----------------+-------+\n| ID                                   | Hypervisor Hostname | Hypervisor Type | Host IP         | State |\n+--------------------------------------+---------------------+-----------------+-----------------+-------+\n| 815a5634-86fb-4e1e-8824-8a631fee3e06 | node1               | QEMU            | 192.168.1.123   | up    |\n+--------------------------------------+---------------------+-----------------+-----------------+-------+\n\n$ openstack --os-placement-api-version=1.2 resource provider aggregate set \\\n    --aggregate df4c74f3-d2c4-4991-b461-f1a678e1d161 \\\n    815a5634-86fb-4e1e-8824-8a631fee3e06\n</code></pre> <p>Some scheduling filter operations can be performed by placement for increased speed and efficiency.</p> <p>Note The nova-api service attempts (as of nova 18.0.0) to automatically mirror the association of a compute host with an aggregate when an administrator adds or removes a host to/from a nova host aggregate. This should alleviate the need to manually create those association records in the placement API using the\u00a0<code>openstack\u00a0resource\u00a0provider\u00a0aggregate\u00a0set</code>\u00a0CLI invocation.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Host_Aggregates/#tenant-isolation-with-placement","title":"Tenant Isolation with Placement","text":"<p>In order to use placement to isolate tenants, there must be placement aggregates that match the membership and UUID of nova host aggregates that you want to use for isolation. The same key pattern in aggregate metadata used by the\u00a0AggregateMultiTenancyIsolation\u00a0filter controls this function, and is enabled by setting\u00a0<code>scheduler.limit_tenants_to_placement_aggregate</code>\u00a0to\u00a0<code>True</code>.</p> <pre><code>$ openstack --os-compute-api-version=2.53 aggregate create myagg\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| availability_zone | None                                 |\n| created_at        | 2018-03-29T16:22:23.175884           |\n| deleted           | False                                |\n| deleted_at        | None                                 |\n| id                | 4                                    |\n| name              | myagg                                |\n| updated_at        | None                                 |\n| uuid              | 019e2189-31b3-49e1-aff2-b220ebd91c24 |\n+-------------------+--------------------------------------+\n\n$ openstack --os-compute-api-version=2.53 aggregate add host myagg node1\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| availability_zone | None                                 |\n| created_at        | 2018-03-29T16:22:23.175884           |\n| deleted           | False                                |\n| deleted_at        | None                                 |\n| hosts             | [u'node1']                           |\n| id                | 4                                    |\n| name              | myagg                                |\n| updated_at        | None                                 |\n| uuid              | 019e2189-31b3-49e1-aff2-b220ebd91c24 |\n+-------------------+--------------------------------------+\n\n$ openstack project list -f value | grep 'demo'\n9691591f913949818a514f95286a6b90 demo\n\n$ openstack aggregate set \\\n    --property filter_tenant_id=9691591f913949818a514f95286a6b90 myagg\n\n$ openstack --os-placement-api-version=1.2 resource provider aggregate set \\\n    --aggregate 019e2189-31b3-49e1-aff2-b220ebd91c24 \\\n    815a5634-86fb-4e1e-8824-8a631fee3e06\n</code></pre> <p>Note that the\u00a0<code>filter_tenant_id</code>\u00a0metadata key can be optionally suffixed with any string for multiple tenants, such as\u00a0<code>filter_tenant_id3=$tenantid</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Host_Aggregates/#usage","title":"Usage","text":"<p>Much of the configuration of host aggregates is driven from the API or command-line clients. For example, to create a new aggregate and add hosts to it using the\u00a0openstack\u00a0client, run:</p> <pre><code>$ openstack aggregate create my-aggregate\n$ openstack aggregate add host my-aggregate my-host\n</code></pre> <p>To list all aggregates and show information about a specific aggregate, run:</p> <pre><code>$ openstack aggregate list\n$ openstack aggregate show my-aggregate\n</code></pre> <p>To set and unset a property on the aggregate, run:</p> <pre><code>$ openstack aggregate set --property pinned=true my-aggregrate\n$ openstack aggregate unset --property pinned my-aggregate\n</code></pre> <p>To rename the aggregate, run:</p> <pre><code>$ openstack aggregate set --name my-awesome-aggregate my-aggregate\n</code></pre> <p>To remove a host from an aggregate and delete the aggregate, run:</p> <pre><code>$ openstack aggregate remove host my-aggregate my-host\n$ openstack aggregate delete my-aggregate\n</code></pre> <p>For more information, refer to the\u00a0OpenStack Client documentation.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Host_Aggregates/#configuration","title":"Configuration","text":"<p>In addition to CRUD operations enabled by the API and clients, the following configuration options can be used to configure how host aggregates and the related availability zones feature operate under the hood:</p> <ul> <li> <p><code>default_schedule_zone</code></p> </li> <li> <p><code>scheduler.limit_tenants_to_placement_aggregate</code></p> </li> <li> <p><code>cinder.cross_az_attach</code></p> </li> </ul> <p>Finally, as discussed previously, there are a number of host aggregate-specific scheduler filters. These are:</p> <ul> <li> <p>AggregateImagePropertiesIsolation</p> </li> <li> <p>AggregateInstanceExtraSpecsFilter</p> </li> <li> <p>AggregateIoOpsFilter</p> </li> <li> <p>AggregateMultiTenancyIsolation</p> </li> <li> <p>AggregateNumInstancesFilter</p> </li> <li> <p>AggregateTypeAffinityFilter</p> </li> </ul> <p>The following configuration options are applicable to the scheduler configuration:</p> <ul> <li> <p><code>cpu_allocation_ratio</code></p> </li> <li> <p><code>ram_allocation_ratio</code></p> </li> <li> <p><code>filter_scheduler.max_instances_per_host</code></p> </li> <li> <p><code>filter_scheduler.aggregate_image_properties_isolation_separator</code></p> </li> <li> <p><code>filter_scheduler.aggregate_image_properties_isolation_namespace</code></p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Host_Aggregates/#image-caching","title":"Image Caching","text":"<p>Aggregates can be used as a way to target multiple compute nodes for the purpose of requesting that images be pre-cached for performance reasons.</p> <p>Note Some of the virt drivers\u00a0provide image caching support, which improves performance of second-and-later boots of the same image by keeping the base image in an on-disk cache. This avoids the need to re-download the image from Glance, which reduces network utilization and time-to-boot latency. Image pre-caching is the act of priming that cache with images ahead of time to improve performance of the first boot.</p> <p>Assuming an aggregate called\u00a0<code>my-aggregate</code>\u00a0where two images should be pre-cached, running the following command will initiate the request:</p> <pre><code>$ nova aggregate-cache-images my-aggregate image1 image2\n</code></pre> <p>Note that image pre-caching happens asynchronously in a best-effort manner. The images and aggregate provided are checked by the server when the command is run, but the compute nodes are not checked to see if they support image caching until the process runs. Progress and results are logged by each compute, and the process sends\u00a0<code>aggregate.cache_images.start</code>,\u00a0<code>aggregate.cache_images.progress</code>, and\u00a0<code>aggregate.cache_images.end</code>\u00a0notifications, which may be useful for monitoring the operation externally.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Host_Aggregates/#references","title":"References","text":"<ul> <li>Curse your bones, Availability Zones! (Openstack Summit Vancouver 2018)</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/","title":"Hypervisors","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#kernel-virtual-machine","title":"Kernel Virtual Machine","text":"<p>KVM (for Kernel-based Virtual Machine) is a full virtualization solution for Linux on x86 hardware containing virtualization extensions (Intel VT or AMD-V). It consists of a loadable kernel module, kvm.ko, that provides the core virtualization infrastructure and a processor specific module, kvm-intel.ko or kvm-amd.ko.</p> <p>Using KVM, one can run multiple virtual machines running unmodified Linux or Windows images. Each virtual machine has private virtualized hardware: a network card, disk, graphics adapter, etc.</p> <p>KVM is open source software. The kernel component of KVM is included in mainline Linux, as of 2.6.20. The userspace component of KVM is included in mainline QEMU, as of 1.3.</p> <p>Blogs from people active in KVM-related virtualization development are syndicated at\u00a0http://planet.virt-tools.org/</p> <p>KVM is configured as the default hypervisor for Compute.</p> <p>The KVM hypervisor supports the following virtual machine image formats:</p> <ul> <li> <p>Raw</p> </li> <li> <p>QEMU Copy-on-write (QCOW2)</p> </li> <li> <p>QED Qemu Enhanced Disk</p> </li> <li> <p>VMware virtual machine disk format (vmdk)</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#configuration","title":"Configuration","text":"<p>To enable KVM explicitly, add the following configuration options to the\u00a0<code>/etc/nova/nova.conf</code>\u00a0file:</p> <pre><code>[DEFAULT]\ncompute_driver = libvirt.LibvirtDriver\n\n[libvirt] \nvirt_type = kvm\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#enable-kvm","title":"Enable KVM","text":"<p>The following sections outline how to enable KVM based hardware virtualization on different architectures and platforms. To perform these steps, you must be logged in as the\u00a0<code>root</code>\u00a0user.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#for-x86-based-systems","title":"For x86-based systems","text":"<p>1. To determine whether the\u00a0<code>svm</code>\u00a0or\u00a0<code>vmx</code>\u00a0CPU extensions are present, run this command:</p> <pre><code>grep -E 'svm|vmx' /proc/cpuinfo\n</code></pre> <p>This command generates output if the CPU is capable of hardware-virtualization. Even if output is shown, you might still need to enable virtualization in the system BIOS for full support.</p> <p>If no output appears, consult your system documentation to ensure that your CPU and motherboard support hardware virtualization. Verify that any relevant hardware virtualization options are enabled in the system BIOS.</p> <p>The BIOS for each manufacturer is different. If you must enable virtualization in the BIOS, look for an option containing the words\u00a0<code>virtualization</code>,\u00a0<code>VT</code>,\u00a0<code>VMX</code>, or\u00a0<code>SVM</code>.</p> <p>2. To list the loaded kernel modules and verify that the\u00a0<code>kvm</code>\u00a0modules are loaded, run this command:</p> <pre><code>lsmod | grep kvm\n</code></pre> <p>If the output includes\u00a0<code>kvm_intel</code>\u00a0or\u00a0<code>kvm_amd</code>, the\u00a0<code>kvm</code>\u00a0hardware virtualization modules are loaded and your kernel meets the module requirements for OpenStack Compute.</p> <p>If the output does not show that the\u00a0<code>kvm</code>\u00a0module is loaded, run this command to load it:</p> <pre><code>modprobe -a kvm\n</code></pre> <p>Run the command for your CPU. For Intel, run this command:</p> <pre><code>modprobe -a kvm-intel\n</code></pre> <p>For AMD, run this command:</p> <pre><code>modprobe -a kvm-amd\n</code></pre> <p>Because a KVM installation can change user group membership, you might need to log in again for changes to take effect.</p> <p>If the kernel modules do not load automatically, use the procedures listed in these subsections.</p> <p>If the checks indicate that required hardware virtualization support or kernel modules are disabled or unavailable, you must either enable this support on the system or find a system with this support.</p> <p>Note Some systems require that you enable VT support in the system BIOS. If you believe your processor supports hardware acceleration but the previous command did not produce output, reboot your machine, enter the system BIOS, and enable the VT option.</p> <p>If KVM acceleration is not supported, configure Compute to use a different hypervisor, such as\u00a0QEMU.</p> <p>These procedures help you load the kernel modules for Intel-based and AMD-based processors if they do not load automatically during KVM installation.</p> <p>Intel-based processors</p> <p>If your compute host is Intel-based, run these commands as root to load the kernel modules:</p> <pre><code>modprobe kvm\nmodprobe kvm-intel\n</code></pre> <p>Add these lines to the\u00a0<code>/etc/modules</code>\u00a0file so that these modules load on reboot:</p> <pre><code>kvm\nkvm-intel\n</code></pre> <p>AMD-based processors</p> <p>If your compute host is AMD-based, run these commands as root to load the kernel modules:</p> <pre><code>modprobe kvm\nmodprobe kvm-amd\n</code></pre> <p>Add these lines to\u00a0<code>/etc/modules</code>\u00a0file so that these modules load on reboot:</p> <pre><code>kvm\nkvm-amd\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#for-power-based-systems","title":"For POWER-based systems","text":"<p>KVM as a hypervisor is supported on POWER system\u2019s PowerNV platform.</p> <p>1. To determine if your POWER platform supports KVM based virtualization run the following command:</p> <pre><code>cat /proc/cpuinfo | grep PowerNV\n</code></pre> <p>If the previous command generates the following output, then CPU supports KVM based virtualization.</p> <pre><code>platform: PowerNV\n</code></pre> <p>If no output is displayed, then your POWER platform does not support KVM based hardware virtualization.</p> <p>2. To list the loaded kernel modules and verify that the\u00a0<code>kvm</code>\u00a0modules are loaded, run the following command:</p> <pre><code>lsmod | grep kvm\n</code></pre> <p>If the output includes\u00a0<code>kvm_hv</code>, the\u00a0<code>kvm</code>\u00a0hardware virtualization modules are loaded and your kernel meets the module requirements for OpenStack Compute.</p> <p>If the output does not show that the\u00a0<code>kvm</code>\u00a0module is loaded, run the following command to load it:</p> <pre><code>modprobe -a kvm\n</code></pre> <p>For PowerNV platform, run the following command:</p> <pre><code>modprobe -a kvm-hv\n</code></pre> <p>Because a KVM installation can change user group membership, you might need to log in again for changes to take effect.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#configure-compute-backing-storage","title":"Configure Compute backing storage","text":"<p>Backing Storage is the storage used to provide the expanded operating system image, and any ephemeral storage. Inside the virtual machine, this is normally presented as two virtual hard disks (for example,\u00a0<code>/dev/vda</code>\u00a0and\u00a0<code>/dev/vdb</code>\u00a0respectively). However, inside OpenStack, this can be derived from one of these methods:\u00a0<code>lvm</code>,\u00a0<code>qcow</code>,\u00a0<code>rbd</code>\u00a0or\u00a0<code>flat</code>, chosen using the\u00a0<code>libvirt.images_type</code>\u00a0option in\u00a0<code>nova.conf</code>\u00a0on the compute node.</p> <p>Note The option\u00a0<code>raw</code>\u00a0is acceptable but deprecated in favor of\u00a0<code>flat</code>. The Flat back end uses either raw or QCOW2 storage. It never uses a backing store, so when using QCOW2 it copies an image rather than creating an overlay. By default, it creates raw files but will use QCOW2 when creating a disk from a QCOW2 if\u00a0<code>force_raw_images</code>\u00a0is not set in configuration.</p> <p>QCOW is the default backing store. It uses a copy-on-write philosophy to delay allocation of storage until it is actually needed. This means that the space required for the backing of an image can be significantly less on the real disk than what seems available in the virtual machine operating system.</p> <p>Flat creates files without any sort of file formatting, effectively creating files with the plain binary one would normally see on a real disk. This can increase performance, but means that the entire size of the virtual disk is reserved on the physical disk.</p> <p>Local\u00a0LVM volumes\u00a0can also be used. Set the\u00a0<code>libvirt.images_volume_group</code>\u00a0configuration option to the name of the LVM group you have created.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#direct-download-of-images-from-ceph","title":"Direct download of images from Ceph","text":"<p>When the Glance image service is set up with the Ceph backend and Nova is using a local ephemeral store (<code>[libvirt]/images_type!=rbd</code>), it is possible to configure Nova to download images directly into the local compute image cache.</p> <p>With the following configuration, images are downloaded using the RBD export command instead of using the Glance HTTP API. In some situations, especially for very large images, this could be substantially faster and can improve the boot times of instances.</p> <p>On the Glance API node in\u00a0<code>glance-api.conf</code>:</p> <pre><code>[DEFAULT]\nshow_image_direct_url=true\n</code></pre> <p>On the Nova compute node in nova.conf:</p> <pre><code>[glance]\n\nenable_rbd_download=true\nrbd_user=glance\nrbd_pool=images\nrbd_ceph_conf=/etc/ceph/ceph.conf\nrbd_connect_timeout=5\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#nested-guest-support","title":"Nested guest support","text":"<p>You may choose to enable support for nested guests \u2014 that is, allow your Nova instances to themselves run hardware-accelerated virtual machines with KVM. Doing so requires a module parameter on your KVM kernel module, and corresponding\u00a0<code>nova.conf</code>\u00a0settings.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#host-configuration","title":"Host configuration","text":"<p>To enable nested KVM guests, your compute node must load the\u00a0<code>kvm_intel</code>\u00a0or\u00a0<code>kvm_amd</code>\u00a0module with\u00a0<code>nested=1</code>. You can enable the\u00a0<code>nested</code>\u00a0parameter permanently, by creating a file named\u00a0<code>/etc/modprobe.d/kvm.conf</code>\u00a0and populating it with the following content:</p> <pre><code>options kvm_intel nested=1\noptions kvm_amd nested=1\n</code></pre> <p>A reboot may be required for the change to become effective.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#nova-configuration","title":"Nova configuration","text":"<p>To support nested guests, you must set your\u00a0<code>libvirt.cpu_mode</code>\u00a0configuration to one of the following options:</p> <p>Host passthrough (<code>host-passthrough</code>)</p> <p>In this mode, nested virtualization is automatically enabled once the KVM kernel module is loaded with nesting support.</p> <pre><code>[libvirt]\n\ncpu_mode = host-passthrough\n</code></pre> <p>However, do consider the other implications that\u00a0host passthrough\u00a0mode has on compute functionality.</p> <p>Host model (<code>host-model</code>)</p> <p>In this mode, nested virtualization is automatically enabled once the KVM kernel module is loaded with nesting support,\u00a0if\u00a0the matching CPU model exposes the\u00a0<code>vmx</code>\u00a0feature flag to guests by default (you can verify this with\u00a0<code>virsh\u00a0capabilities</code>\u00a0on your compute node). If your CPU model does not pass in the\u00a0<code>vmx</code>\u00a0flag, you can force it with\u00a0<code>libvirt.cpu_model_extra_flags</code>:</p> <pre><code>[libvirt]\ncpu_mode = host-model cpu_model_extra_flags = vmx\n</code></pre> <p>Again, consider the other implications that apply to the\u00a0host model\u00a0mode.</p> <p>Custom (<code>custom</code>)</p> <p>In custom mode, the same considerations apply as in host-model mode, but you may\u00a0additionally\u00a0want to ensure that libvirt passes not only the\u00a0<code>vmx</code>, but also the\u00a0<code>pcid</code>\u00a0flag to its guests:</p> <pre><code>[libvirt]\ncpu_mode = custom\ncpu_models = IvyBridge\ncpu_model_extra_flags = vmx,pcid\n</code></pre> <p>More information on CPU models can be found in\u00a0CPU models.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#limitations","title":"Limitations","text":"<p>When enabling nested guests, you should be aware of (and inform your users about) certain limitations that are currently inherent to nested KVM virtualization. Most importantly, guests using nested virtualization will,\u00a0while nested guests are running,</p> <ul> <li> <p>fail to complete live migration;</p> </li> <li> <p>fail to resume from suspend.</p> </li> </ul> <p>See\u00a0the KVM documentation\u00a0for more information on these limitations.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#kvm-performance-tweaks","title":"KVM performance tweaks","text":"<p>The\u00a0VHostNet\u00a0kernel module improves network performance. To load the kernel module, run the following command as root:</p> <pre><code>modprobe vhost_net\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#troubleshooting","title":"Troubleshooting","text":"<p>Trying to launch a new virtual machine instance fails with the\u00a0<code>ERROR</code>\u00a0state, and the following error appears in the\u00a0<code>/var/log/nova/nova-compute.log</code>\u00a0file:</p> <pre><code>libvirtError: internal error no supported architecture for os type 'hvm'\n</code></pre> <p>This message indicates that the KVM kernel modules were not loaded.</p> <p>If you cannot start VMs after installation without rebooting, the permissions might not be set correctly. This can happen if you load the KVM module before you install\u00a0<code>nova-compute</code>. To check whether the group is set to\u00a0<code>kvm</code>, run:</p> <pre><code>ls -l /dev/kvm\n</code></pre> <p>If it is not set to\u00a0<code>kvm</code>, run:</p> <pre><code>udevadm trigger\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#hw-tpm2","title":"HW TPM2","text":"<p>In Taikun OCP we use TPM2 tools to strengthen the security infrastructure. The Trusted Platform Module 2.0 (TPM2) plays a central role in ensuring the integrity and confidentiality of sensitive data within our ecosystem.</p> <p>Description</p> <p>tpm2(1) \u2013 Simplifying the installation process of tpm2-tools in initrd or embedded systems where optimizing size and managing limited resources are crucial. This means that a unified executable is available that is capable of executing various TPM2 functions based on the specified argument corresponding to one of the available tool names. Subsequent options and arguments either pertain to common options or are specific to the chosen tool name. It\u2019s important that individual tools prefixed with \u2018tpm2_\u2019 remain callable, but they are now symbolically linked to the \u2018tpm2\u2019 executable. Therefore, unlike BusyBox, the executable offers the individual tools\u2019 complete functionality. For instance, \u2018tpm2_getrandom 8\u2019 can alternatively be invoked as \u2018tpm2.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Hypervisors/#enabling-tpm-functionality-in-taikun-ocp","title":"Enabling TPM functionality in Taikun OCP","text":"<p>Enabling Trusted Platform Module (TPM) functionality in Taikun OCP involves several steps, including hardware configuration and software setup:</p> <p>1. Ensure Hardware Support:\u00a0Verify that the server hardware used for Taikun OCP is equipped with a TPM chip that supports TPM2.0. Most modern server hardware should include TPM support, but it\u2019s essential to confirm compatibility.</p> <p>2. Enable TPM in BIOS/UEFI:\u00a0Access the BIOS or UEFI settings of the server and enable TPM. The specific steps to do this vary depending on the server manufacturer and model. Look for options related to security or TPM in the BIOS/UEFI settings and ensure TPM is enabled.</p> <p>3. Install Required Software Packages:\u00a0Install the necessary software packages for TPM support in Taikun OCP. This may include TPM2-tools, which provide utilities and libraries for interacting with TPM devices.</p> <p>4. Configure TPM in Taikun OCP:\u00a0Once TPM support is enabled in the hardware and the required software packages are installed, configure Taikun OCP to utilize TPM. This typically involves setting up TPM-related options in the hypervisor configuration or management interface.</p> <p>5. Verify TPM Functionality:\u00a0After configuring TPM in Taikun OCP, verify that it is functioning correctly. This may involve running tests or commands provided by TPM2-tools to ensure that the TPM chip is detected and operational.</p> <p>6. Integrate TPM Features:\u00a0Depending on the specific use cases and security requirements, integrate TPM features into your virtualized environments running on Taikun OCP. This may include tasks such as enabling secure boot, attestation, key management, and encryption using TPM functionality.</p> <p>By following these steps, you can enable TPM functionality in Taikun OCP, enhancing the security and integrity of your virtualized environments.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Project_Security/","title":"Manage Project Security","text":"<p>Security groups are sets of IP filter rules that are applied to all project instances, which define networking access to the instance. Group rules are project specific; project members can edit the default rules for their group and add new rule sets.</p> <p>All projects have a\u00a0<code>default</code>\u00a0security group which is applied to any instance that has no other defined security group. Unless you change the default, this security group denies all incoming traffic and allows only outgoing traffic to your instance.</p> <p>Security groups (and their quota) are managed by\u00a0Neutron, the networking service.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Project_Security/#working-with-security-groups","title":"Working with security groups","text":"<p>From the command-line you can get a list of security groups for the project, using the\u00a0openstack\u00a0commands.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Project_Security/#list-and-view-current-security-groups","title":"List and view current security groups","text":"<p>1. Ensure your system variables are set for the user and project for which you are checking security group rules. For example:</p> <pre><code>export OS_USERNAME=demo00\nexport OS_TENANT_NAME=tenant01\n</code></pre> <p>2. Output security groups, as follows:</p> <pre><code>$ openstack security group list\n+--------------------------------------+---------+-------------+\n| Id                                   | Name    | Description |\n+--------------------------------------+---------+-------------+\n| 73580272-d8fa-4927-bd55-c85e43bc4877 | default | default     |\n| 6777138a-deb7-4f10-8236-6400e7aff5b0 | open    | all ports   |\n+--------------------------------------+---------+-------------+\n</code></pre> <p>3. View the details of a group, as follows:</p> <pre><code>$ openstack security group rule list GROUPNAME\n</code></pre> <p>For example:</p> <pre><code>$ openstack security group rule list open\n+--------------------------------------+-------------+-----------+-----------------+-----------------------+\n| ID                                   | IP Protocol | IP Range  | Port Range      | Remote Security Group |\n+--------------------------------------+-------------+-----------+-----------------+-----------------------+\n| 353d0611-3f67-4848-8222-a92adbdb5d3a | udp         | 0.0.0.0/0 | 1:65535         | None                  |\n| 63536865-e5b6-4df1-bac5-ca6d97d8f54d | tcp         | 0.0.0.0/0 | 1:65535         | None                  |\n+--------------------------------------+-------------+-----------+-----------------+-----------------------+\n</code></pre> <p>These rules are allow type rules as the default is deny. The first column is the IP protocol (one of ICMP, TCP, or UDP). The second and third columns specify the affected port range. The third column specifies the IP range in CIDR format. This example shows the full port range for all protocols allowed from all IPs.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Project_Security/#create-a-security-group","title":"Create a security group","text":"<p>When adding a new security group, you should pick a descriptive but brief name. This name shows up in brief descriptions of the instances that use it where the longer description field often does not. For example, seeing that an instance is using security group \u201chttp\u201d is much easier to understand than \u201cbobs_group\u201d or \u201csecgrp1\u201d.</p> <p>1. Ensure your system variables are set for the user and project for which you are creating security group rules.</p> <p>2. Add the new security group, as follows:</p> <pre><code>$ openstack security group create GroupName --description Description\n</code></pre> <p>For example:</p> <pre><code>$ openstack security group create global_http --description \"Allows Web traffic anywhere on the Internet.\"\n+-----------------+--------------------------------------------------------------------------------------------------------------------------+\n| Field           | Value                                                                                                                    |\n+-----------------+--------------------------------------------------------------------------------------------------------------------------+\n| created_at      | 2016-11-03T13:50:53Z                                                                                                     |\n| description     | Allows Web traffic anywhere on the Internet.                                                                             |\n| headers         |                                                                                                                          |\n| id              | c0b92b20-4575-432a-b4a9-eaf2ad53f696                                                                                     |\n| name            | global_http                                                                                                              |\n| project_id      | 5669caad86a04256994cdf755df4d3c1                                                                                         |\n| project_id      | 5669caad86a04256994cdf755df4d3c1                                                                                         |\n| revision_number | 1                                                                                                                        |\n| rules           | created_at='2016-11-03T13:50:53Z', direction='egress', ethertype='IPv4', id='4d8cec94-e0ee-4c20-9f56-8fb67c21e4df',      |\n|                 | project_id='5669caad86a04256994cdf755df4d3c1', revision_number='1', updated_at='2016-11-03T13:50:53Z'                    |\n|                 | created_at='2016-11-03T13:50:53Z', direction='egress', ethertype='IPv6', id='31be2ad1-be14-4aef-9492-ecebede2cf12',      |\n|                 | project_id='5669caad86a04256994cdf755df4d3c1', revision_number='1', updated_at='2016-11-03T13:50:53Z'                    |\n| updated_at      | 2016-11-03T13:50:53Z                                                                                                     |\n+-----------------+--------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>3. Add a new group rule, as follows:</p> <pre><code>$ openstack security group rule create SEC_GROUP_NAME \\\n    --protocol PROTOCOL --dst-port FROM_PORT:TO_PORT --remote-ip CIDR\n</code></pre> <p>The arguments are positional, and the\u00a0<code>from-port</code>\u00a0and\u00a0<code>to-port</code>\u00a0arguments specify the local port range connections are allowed to access, not the source and destination ports of the connection. For example:</p> <pre><code>$ openstack security group rule create global_http \\\n    --protocol tcp --dst-port 80:80 --remote-ip 0.0.0.0/0\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| created_at        | 2016-11-06T14:02:00Z                 |\n| description       |                                      |\n| direction         | ingress                              |\n| ethertype         | IPv4                                 |\n| headers           |                                      |\n| id                | 2ba06233-d5c8-43eb-93a9-8eaa94bc9eb5 |\n| port_range_max    | 80                                   |\n| port_range_min    | 80                                   |\n| project_id        | 5669caad86a04256994cdf755df4d3c1     |\n| project_id        | 5669caad86a04256994cdf755df4d3c1     |\n| protocol          | tcp                                  |\n| remote_group_id   | None                                 |\n| remote_ip_prefix  | 0.0.0.0/0                            |\n| revision_number   | 1                                    |\n| security_group_id | c0b92b20-4575-432a-b4a9-eaf2ad53f696 |\n| updated_at        | 2016-11-06T14:02:00Z                 |\n+-------------------+--------------------------------------+\n</code></pre> <p>You can create complex rule sets by creating additional rules. For example, if you want to pass both HTTP and HTTPS traffic, run:</p> <pre><code>$ openstack security group rule create global_http \\\n    --protocol tcp --dst-port 443:443 --remote-ip 0.0.0.0/0\n+-------------------+--------------------------------------+\n| Field             | Value                                |\n+-------------------+--------------------------------------+\n| created_at        | 2016-11-06T14:09:20Z                 |\n| description       |                                      |\n| direction         | ingress                              |\n| ethertype         | IPv4                                 |\n| headers           |                                      |\n| id                | 821c3ef6-9b21-426b-be5b-c8a94c2a839c |\n| port_range_max    | 443                                  |\n| port_range_min    | 443                                  |\n| project_id        | 5669caad86a04256994cdf755df4d3c1     |\n| project_id        | 5669caad86a04256994cdf755df4d3c1     |\n| protocol          | tcp                                  |\n| remote_group_id   | None                                 |\n| remote_ip_prefix  | 0.0.0.0/0                            |\n| revision_number   | 1                                    |\n| security_group_id | c0b92b20-4575-432a-b4a9-eaf2ad53f696 |\n| updated_at        | 2016-11-06T14:09:20Z                 |\n+-------------------+--------------------------------------+\n</code></pre> <p>Despite only outputting the newly added rule, this operation is additive (both rules are created and enforced).</p> <p>4. View all rules for the new security group, as follows:</p> <pre><code>$ openstack security group rule list global_http\n+--------------------------------------+-------------+-----------+-----------------+-----------------------+\n| ID                                   | IP Protocol | IP Range  | Port Range      | Remote Security Group |\n+--------------------------------------+-------------+-----------+-----------------+-----------------------+\n| 353d0611-3f67-4848-8222-a92adbdb5d3a | tcp         | 0.0.0.0/0 | 80:80           | None                  |\n| 63536865-e5b6-4df1-bac5-ca6d97d8f54d | tcp         | 0.0.0.0/0 | 443:443         | None                  |\n+--------------------------------------+-------------+-----------+-----------------+-----------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Project_Security/#delete-a-security-group","title":"Delete a security group","text":"<p>1. Ensure your system variables are set for the user and project for which you are deleting a security group.</p> <p>2. Delete the new security group, as follows:</p> <pre><code>$ openstack security group delete GROUPNAME\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Project_Security/#create-security-group-rules-for-a-cluster-of-instances","title":"Create security group rules for a cluster of instances","text":"<p>Source Groups are a special, dynamic way of defining the CIDR of allowed sources. The user specifies a Source Group (Security Group name), and all the user\u2019s other Instances using the specified Source Group are selected dynamically. This alleviates the need for individual rules to allow each new member of the cluster.</p> <p>1. Make sure to set the system variables for the user and project for which you are creating a security group rule.</p> <p>2. Add a source group, as follows:</p> <pre><code>$ openstack security group rule create cluster \\\n    --remote-group global_http --protocol tcp --dst-port 22:22\n</code></pre> <p>The\u00a0<code>cluster</code>\u00a0rule allows SSH access from any other instance that uses the\u00a0<code>global_http</code>\u00a0group.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Quotas/","title":"Manage Quotas","text":"<p>Note This section provides deployment information about the quota feature. For end-user information about quotas, including information about the type of quotas available, refer to the\u00a0user guide.</p> <p>To prevent system capacities from being exhausted without notification, you can set up quotas. Quotas are operational limits. For example, the number of gigabytes allowed for each project can be controlled so that cloud resources are optimized. Quotas can be enforced at both the project and the project-user level.</p> <p>Starting in the 16.0.0 Pike release, the quota calculation system in nova was overhauled and the old reserve/commit/rollback flow was changed to\u00a0count resource usage\u00a0at the point of whatever operation is being performed, such as creating or resizing a server. A check will be performed by counting current usage for the relevant resource and then, if\u00a0<code>quota.recheck_quota</code>\u00a0is True, another check will be performed to ensure the initial check is still valid.</p> <p>By default resource usage is counted using the API and cell databases but nova can be configured to count some resource usage without using the cell databases. See\u00a0Quota usage from placement\u00a0for details.</p> <p>Using the command-line interface, you can manage quotas for nova, along with\u00a0cinder\u00a0and\u00a0neutron. You would typically change default values because, for example, a project requires more than ten volumes or 1 TB on a compute node.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Quotas/#checking-quota","title":"Checking quota","text":"<p>When calculating limits for a given resource and project, the following checks are made in order:</p> <p>1. Project-specific limitsDepending on the resource, is there a project-specific limit on the resource in either the\u00a0<code>quotas</code>\u00a0or\u00a0<code>project_user_quotas</code>\u00a0tables in the database? If so, use that as the limit. You can create these resources using:</p> <pre><code>$ openstack quota set --instances 5 &lt;project&gt;\n</code></pre> <p>2. Default limits</p> <p>Check to see if there is a hard limit for the given resource in the\u00a0<code>quota_classes</code>\u00a0table in the database for the\u00a0<code>default</code>\u00a0quota class. If so, use that as the limit. You can modify the default quota limit for a resource using:</p> <pre><code>$ openstack quota set --instances 5 --class default\n</code></pre> <p>Note Only the\u00a0<code>default</code>\u00a0class is supported by nova.</p> <p>3. Config-driven limits</p> <p>If the above does not provide a resource limit, then rely on the configuration options in the\u00a0<code>quota</code>\u00a0config group for the default limits.</p> <p>Note The API sets the limit in the\u00a0<code>quota_classes</code>\u00a0table. Once a default limit is set via the\u00a0default\u00a0quota class, that takes precedence over any changes to that resource limit in the configuration options. In other words, once you\u2019ve changed things via the API, you either have to keep those synchronized with the configuration values or remove the default limit from the database manually as there is no REST API for removing quota class values from the database.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Quotas/#quota-usage-from-placement","title":"Quota usage from placement","text":"<p>Starting in the Train (20.0.0) release, it is possible to configure quota usage counting of cores and RAM from the placement service and instances from instance mappings in the API database instead of counting resources from cell databases. This makes quota usage counting resilient in the presence of\u00a0down or poor-performing cells.</p> <p>Quota usage counting from placement is opt-in via the :<code>quota.count_usage_from_placement</code>\u00a0config option:</p> <pre><code>[quota]\ncount_usage_from_placement = True\n</code></pre> <p>There are some things to note when opting in to counting quota usage from placement:</p> <ul> <li> <p>Counted usage will not be accurate in an environment where multiple Nova deployments are sharing a placement deployment because currently placement has no way of partitioning resource providers between different Nova deployments. Operators who are running multiple Nova deployments that share a placement deployment should not set the\u00a0<code>quota.count_usage_from_placement</code>\u00a0configuration option to\u00a0<code>True</code>.</p> </li> <li> <p>Behavior will be different for resizes. During a resize, resource allocations are held on both the source and destination (even on the same host, see\u00a0https://bugs.launchpad.net/nova/+bug/1790204) until the resize is confirmed or reverted. Quota usage will be inflated for servers in this state and operators should weigh the advantages and disadvantages before enabling\u00a0<code>quota.count_usage_from_placement</code>.</p> </li> <li> <p>The\u00a0<code>populate_queued_for_delete</code>\u00a0and\u00a0<code>populate_user_id</code>\u00a0online data migrations must be completed before usage can be counted from placement. Until the data migration is complete, the system will fall back to legacy quota usage counting from cell databases depending on the result of an EXISTS database query during each quota check, if\u00a0<code>quota.count_usage_from_placement</code>\u00a0is set to\u00a0<code>True</code>. Operators who want to avoid the performance hit from the EXISTS queries should wait to set the\u00a0<code>quota.count_usage_from_placement</code>\u00a0configuration option to\u00a0<code>True</code>\u00a0until after they have completed their online data migrations via\u00a0<code>nova-manage\u00a0db\u00a0online_data_migrations</code>.</p> </li> <li> <p>Behavior will be different for unscheduled servers in\u00a0<code>ERROR</code>\u00a0state. A server in\u00a0<code>ERROR</code>\u00a0state that has never been scheduled to a compute host will not have placement allocations, so it will not consume quota usage for cores and ram.</p> </li> <li> <p>Behavior will be different for servers in\u00a0<code>SHELVED_OFFLOADED</code>\u00a0state. A server in\u00a0<code>SHELVED_OFFLOADED</code>\u00a0state will not have placement allocations, so it will not consume quota usage for cores and ram. Note that because of this, it will be possible for a request to unshelve a server to be rejected if the user does not have enough quota available to support the cores and ram needed by the server to be unshelved.</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Quotas/#known-issues","title":"Known issues","text":"<p>If not\u00a0counting quota usage from placement\u00a0it is possible for down or poor-performing cells to impact quota calculations. See the\u00a0cells documentation\u00a0for details.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Quotas/#future-plans","title":"Future plans","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Quotas/#hierarchical-quotas","title":"Hierarchical quotas","text":"<p>There has long been a desire to support hierarchical or nested quotas leveraging support in the identity service for hierarchical projects. See the\u00a0unified limits\u00a0spec for details.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Quotas/#configuration","title":"Configuration","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Quotas/#view-and-update-default-quota-values","title":"View and update default quota values","text":"<p>To list all default quotas for a project, run:</p> <pre><code>$ openstack quota show --default\n</code></pre> <p>Note This lists default quotas for all services and not just nova.</p> <p>To update a default value for a new project, run:</p> <pre><code>$ openstack quota set --class --instances 15 default\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Manage_Quotas/#view-and-update-quota-values-for-a-project-or-class","title":"View and update quota values for a project or class","text":"<p>To list quotas for a project, run:</p> <pre><code>$ openstack quota show PROJECT\n</code></pre> <p>Note</p> <p>This lists project quotas for all services and not just nova.</p> <p>To update quotas for a project, run:</p> <pre><code>$ openstack quota set --QUOTA QUOTA_VALUE PROJECT\n</code></pre> <p>To update quotas for a class, run:</p> <pre><code>$ openstack quota set --class --QUOTA QUOTA_VALUE CLASS\n</code></pre> <p>Note Only the\u00a0<code>default</code>\u00a0class is supported by nova.</p> <p>For example:</p> <pre><code>$ openstack quota set --instances 12 my-project\n$ openstack quota show my-project\n+----------------------+----------------------------------+\n| Field                | Value                            |\n+----------------------+----------------------------------+\n| backup-gigabytes     | 1000                             |\n| backups              | 10                               |\n| cores                | 32                               |\n| fixed-ips            | -1                               |\n| floating-ips         | 10                               |\n| gigabytes            | 1000                             |\n| health_monitors      | None                             |\n| injected-file-size   | 10240                            |\n| injected-files       | 5                                |\n| injected-path-size   | 255                              |\n| instances            | 12                               |\n| key-pairs            | 100                              |\n| l7_policies          | None                             |\n| listeners            | None                             |\n| load_balancers       | None                             |\n| location             | None                             |\n| name                 | None                             |\n| networks             | 20                               |\n| per-volume-gigabytes | -1                               |\n| pools                | None                             |\n| ports                | 60                               |\n| project              | c8156b55ec3b486193e73d2974196993 |\n| project_name         | project                          |\n| properties           | 128                              |\n| ram                  | 65536                            |\n| rbac_policies        | 10                               |\n| routers              | 10                               |\n| secgroup-rules       | 50                               |\n| secgroups            | 50                               |\n| server-group-members | 10                               |\n| server-groups        | 10                               |\n| snapshots            | 10                               |\n| subnet_pools         | -1                               |\n| subnets              | 20                               |\n| volumes              | 10                               |\n+----------------------+----------------------------------+\n</code></pre> <p>To view a list of options for the\u00a0openstack quota show\u00a0and\u00a0openstack quota set\u00a0commands, run:</p> <pre><code>$ openstack quota show --help\n$ openstack quota set --help\n</code></pre> <p>View and update quota values for a project user</p> <p>Note User-specific quotas are legacy and will be removed when migration to\u00a0unified limits\u00a0is complete. User-specific quotas were added as a way to provide two-level hierarchical quotas and this feature is already being offered in unified limits. For this reason, the below commands have not and will not be ported to openstackclient.</p> <p>To show quotas for a specific project user, run:</p> <pre><code>$ nova quota-show --user USER PROJECT\n</code></pre> <p>To update quotas for a specific project user, run:</p> <pre><code>$ nova quota-update --user USER --QUOTA QUOTA_VALUE PROJECT\n</code></pre> <p>For example:</p> <pre><code>$ projectUser=$(openstack user show -f value -c id USER)\n$ project=$(openstack project show -f value -c id PROJECT)\n\n$ nova quota-update --user $projectUser --instance 12 $project\n$ nova quota-show --user $projectUser --tenant $project\n+-----------------------------+-------+\n| Quota                       | Limit |\n+-----------------------------+-------+\n| instances                   | 12    |\n| cores                       | 20    |\n| ram                         | 51200 |\n| floating_ips                | 10    |\n| fixed_ips                   | -1    |\n| metadata_items              | 128   |\n| injected_files              | 5     |\n| injected_file_content_bytes | 10240 |\n| injected_file_path_bytes    | 255   |\n| key_pairs                   | 100   |\n| security_groups             | 10    |\n| security_group_rules        | 20    |\n| server_groups               | 10    |\n| server_group_members        | 10    |\n+-----------------------------+-------+\n</code></pre> <p>To view the quota usage for the current user, run:</p> <pre><code>$ nova limits --tenant PROJECT\n</code></pre> <p>For example:</p> <pre><code>$ nova limits --tenant my-project\n+------+-----+-------+--------+------+----------------+\n| Verb | URI | Value | Remain | Unit | Next_Available |\n+------+-----+-------+--------+------+----------------+\n+------+-----+-------+--------+------+----------------+\n\n+--------------------+------+-------+\n| Name               | Used | Max   |\n+--------------------+------+-------+\n| Cores              | 0    | 20    |\n| Instances          | 0    | 10    |\n| Keypairs           | -    | 100   |\n| Personality        | -    | 5     |\n| Personality Size   | -    | 10240 |\n| RAM                | 0    | 51200 |\n| Server Meta        | -    | 128   |\n| ServerGroupMembers | -    | 10    |\n| ServerGroups       | 0    | 10    |\n+--------------------+------+-------+\n</code></pre> <p>Note The\u00a0nova limits\u00a0command generates an empty table as a result of the Compute API, which prints an empty list for backward compatibility purposes.</p> <p>To view a list of options for the\u00a0nova quota-show\u00a0and\u00a0nova quota-update\u00a0commands, run:</p> <pre><code>$ nova help quota-show\n$ nova help quota-update\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Show_Usage_Statistics_for_Hosts_and_Instances/","title":"Show Usage Statistics for Hosts and Instances","text":"<p>You can show basic statistics on resource usage for hosts and instances.</p> <p>Note For more sophisticated monitoring, see the\u00a0ceilometer\u00a0project. You can also use tools, such as\u00a0Ganglia\u00a0or\u00a0Graphite, to gather more detailed data.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Show_Usage_Statistics_for_Hosts_and_Instances/#show-host-usage-statistics","title":"Show host usage statistics","text":"<p>The following examples show the host usage statistics for a host called\u00a0<code>devstack</code>.</p> <ul> <li>List the hosts and the nova-related services that run on them:</li> </ul> <pre><code>$ openstack host list\n+-----------+-------------+----------+\n| Host Name | Service     | Zone     |\n+-----------+-------------+----------+\n| devstack  | conductor   | internal |\n| devstack  | compute     | nova     |\n| devstack  | cert        | internal |\n| devstack  | network     | internal |\n| devstack  | scheduler   | internal |\n| devstack  | consoleauth | internal |\n+-----------+-------------+----------+\n</code></pre> <ul> <li>Get a summary of resource usage of all of the instances running on the host:</li> </ul> <pre><code>$ openstack host show devstack\n+----------+----------------------------------+-----+-----------+---------+\n| Host     | Project                          | CPU | MEMORY MB | DISK GB |\n+----------+----------------------------------+-----+-----------+---------+\n| devstack | (total)                          | 2   | 4003      | 157     |\n| devstack | (used_now)                       | 3   | 5120      | 40      |\n| devstack | (used_max)                       | 3   | 4608      | 40      |\n| devstack | b70d90d65e464582b6b2161cf3603ced | 1   | 512       | 0       |\n| devstack | 66265572db174a7aa66eba661f58eb9e | 2   | 4096      | 40      |\n+----------+----------------------------------+-----+-----------+---------+\n</code></pre> <p>The\u00a0<code>CPU</code>\u00a0column shows the sum of the virtual CPUs for instances running on the host.</p> <p>The\u00a0<code>MEMORY\u00a0MB</code>\u00a0column shows the sum of the memory (in MB) allocated to the instances that run on the host.</p> <p>The\u00a0<code>DISK\u00a0GB</code>\u00a0column shows the sum of the root and ephemeral disk sizes (in GB) of the instances that run on the host.</p> <p>The row that has the value\u00a0<code>used_now</code>\u00a0in the\u00a0<code>PROJECT</code>\u00a0column shows the sum of the resources allocated to the instances that run on the host, plus the resources allocated to the virtual machine of the host itself.</p> <p>The row that has the value\u00a0<code>used_max</code>\u00a0in the\u00a0<code>PROJECT</code>\u00a0column shows the sum of the resources allocated to the instances that run on the host.</p> <p>Note These values are computed by using information about the flavors of the instances that run on the hosts. This command does not query the CPU usage, memory usage, or hard disk usage of the physical host.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Nova/Show_Usage_Statistics_for_Hosts_and_Instances/#show-instance-usage-statistics","title":"Show instance usage statistics","text":"<ul> <li> <p>Get CPU, memory, I/O, and network statistics for an instance.</p> </li> <li> <p>List instances:</p> </li> </ul> <pre><code>$ openstack server list\n+----------+----------------------+--------+------------+-------------+------------------+------------+\n| ID       | Name                 | Status | Task State | Power State | Networks         | Image Name |\n+----------+----------------------+--------+------------+-------------+------------------+------------+\n| 84c6e... | myCirrosServer       | ACTIVE | None       | Running     | private=10.0.0.3 | cirros     |\n| 8a995... | myInstanceFromVolume | ACTIVE | None       | Running     | private=10.0.0.4 | ubuntu     |\n+----------+----------------------+--------+------------+-------------+------------------+------------+\n</code></pre> <p>Get diagnostic statistics:</p> <pre><code>$ nova diagnostics myCirrosServer\n+---------------------------+--------+\n| Property                  | Value  |\n+---------------------------+--------+\n| memory                    | 524288 |\n| memory-actual             | 524288 |\n| memory-rss                | 6444   |\n| tap1fec8fb8-7a_rx         | 22137  |\n| tap1fec8fb8-7a_rx_drop    | 0      |\n| tap1fec8fb8-7a_rx_errors  | 0      |\n| tap1fec8fb8-7a_rx_packets | 166    |\n| tap1fec8fb8-7a_tx         | 18032  |\n| tap1fec8fb8-7a_tx_drop    | 0      |\n| tap1fec8fb8-7a_tx_errors  | 0      |\n| tap1fec8fb8-7a_tx_packets | 130    |\n| vda_errors                | -1     |\n| vda_read                  | 2048   |\n| vda_read_req              | 2      |\n| vda_write                 | 182272 |\n| vda_write_req             | 74     |\n+---------------------------+--------+\n</code></pre> <ul> <li>Get summary statistics for each project:</li> </ul> <pre><code>$ openstack usage list\nUsage from 2013-06-25 to 2013-07-24:\n+---------+---------+--------------+-----------+---------------+\n| Project | Servers | RAM MB-Hours | CPU Hours | Disk GB-Hours |\n+---------+---------+--------------+-----------+---------------+\n| demo    | 1       | 344064.44    | 672.00    | 0.00          |\n| stack   | 3       | 671626.76    | 327.94    | 6558.86       |\n+---------+---------+--------------+-----------+---------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Octavia/Devstack_with_Octavia_Load_Balancing/","title":"Devstack with Octavia Load Balancing","text":"<p>This guide will show you how to create a devstack with\u00a0Octavia API\u00a0enabled.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Octavia/Devstack_with_Octavia_Load_Balancing/#phase-1-create-devstack-2-nova-instances","title":"Phase 1: Create DevStack + 2 nova instances","text":"<p>First, set up a vm of your choice with at least 8 GB RAM and 16 GB disk space, make sure it is updated. Install git and any other developer tools you find useful.</p> <p>Install devstack</p> <pre><code>git clone https://opendev.org/openstack/devstack\ncd devstack/tools\nsudo ./create-stack-user.sh\ncd ../..\nsudo mv devstack /opt/stack\nsudo chown -R stack.stack /opt/stack/devstack\n</code></pre> <p>This will clone the current devstack code locally, then setup the \u201cstack\u201d account that devstack services will run under. Finally, it will move devstack into its default location in /opt/stack/devstack.</p> <p>Edit your\u00a0<code>/opt/stack/devstack/local.conf</code>\u00a0to look like:</p> <pre><code>[[local|localrc]]\nenable_plugin octavia https://opendev.org/openstack/octavia\n# If you are enabling horizon, include the octavia dashboard\n# enable_plugin octavia-dashboard https://opendev.org/openstack/octavia-dashboard.git\n# If you are enabling barbican for TLS offload in Octavia, include it here.\n# enable_plugin barbican https://opendev.org/openstack/barbican\n\n# ===== BEGIN localrc =====\nDATABASE_PASSWORD=password\nADMIN_PASSWORD=password\nSERVICE_PASSWORD=password\nSERVICE_TOKEN=password\nRABBIT_PASSWORD=password\n# Enable Logging\nLOGFILE=$DEST/logs/stack.sh.log\nVERBOSE=True\nLOG_COLOR=True\n# Pre-requisite\nENABLED_SERVICES=rabbit,mysql,key\n# Horizon - enable for the OpenStack web GUI\n# ENABLED_SERVICES+=,horizon\n# Nova\nENABLED_SERVICES+=,n-api,n-crt,n-cpu,n-cond,n-sch,n-api-meta,n-sproxy\nENABLED_SERVICES+=,placement-api,placement-client\n# Glance\nENABLED_SERVICES+=,g-api\n# Neutron\nENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron\nENABLED_SERVICES+=,octavia,o-cw,o-hk,o-hm,o-api\n# Cinder\nENABLED_SERVICES+=,c-api,c-vol,c-sch\n# Tempest\nENABLED_SERVICES+=,tempest\n# Barbican - Optionally used for TLS offload in Octavia\n# ENABLED_SERVICES+=,barbican\n# ===== END localrc =====\n</code></pre> <p>Run stack.sh and do some sanity checks</p> <pre><code>sudo su - stack\ncd /opt/stack/devstack\n./stack.sh\n. ./openrc\n\nopenstack network list  # should show public and private networks\n</code></pre> <p>Create two nova instances that we can use as test http servers:</p> <pre><code>#create nova instances on private network\nopenstack server create --image $(openstack image list | awk '/ cirros-.*-x86_64-.* / {print $2}') --flavor 1 --nic net-id=$(openstack network list | awk '/ private / {print $2}') node1\nopenstack server create --image $(openstack image list | awk '/ cirros-.*-x86_64-.* / {print $2}') --flavor 1 --nic net-id=$(openstack network list | awk '/ private / {print $2}') node2\nopenstack server list # should show the nova instances just created\n\n#add secgroup rules to allow ssh etc..\nopenstack security group rule create default --protocol icmp\nopenstack security group rule create default --protocol tcp --dst-port 22:22\nopenstack security group rule create default --protocol tcp --dst-port 80:80\n</code></pre> <p>Set up a simple web server on each of these instances. Ssh into each instance (username \u2018cirros\u2019, password \u2018cubswin:)\u2019 or \u2018gocubsgo\u2019) and run:</p> <pre><code>MYIP=$(ifconfig eth0|grep 'inet addr'|awk -F: '{print $2}'| awk '{print $1}')\nwhile true; do echo -e \"HTTP/1.0 200 OK\\r\\n\\r\\nWelcome to $MYIP\" | sudo nc -l -p 80 ; done&amp;\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Octavia/Devstack_with_Octavia_Load_Balancing/#phase-2-create-your-load-balancer","title":"Phase 2: Create your load balancer","text":"<p>Make sure you have the \u2018openstack loadbalancer\u2019 commands:</p> <pre><code>pip install python-octaviaclient\n</code></pre> <p>Create your load balancer:</p> <pre><code>openstack loadbalancer create --name lb1 --vip-subnet-id private-subnet\nopenstack loadbalancer show lb1  # Wait for the provisioning_status to be ACTIVE.\nopenstack loadbalancer listener create --protocol HTTP --protocol-port 80 --name listener1 lb1\nopenstack loadbalancer show lb1  # Wait for the provisioning_status to be ACTIVE.\nopenstack loadbalancer pool create --lb-algorithm ROUND_ROBIN --listener listener1 --protocol HTTP --name pool1\nopenstack loadbalancer show lb1  # Wait for the provisioning_status to be ACTIVE.\nopenstack loadbalancer healthmonitor create --delay 5 --timeout 2 --max-retries 1 --type HTTP pool1\nopenstack loadbalancer show lb1  # Wait for the provisioning_status to be ACTIVE.\nopenstack loadbalancer member create --subnet-id private-subnet --address &lt;web server 1 address&gt; --protocol-port 80 pool1\nopenstack loadbalancer show lb1  # Wait for the provisioning_status to be ACTIVE.\nopenstack loadbalancer member create --subnet-id private-subnet --address &lt;web server 2 address&gt; --protocol-port 80 pool1\n</code></pre> <p>Please note: The \\ fields are the IP addresses of the nova servers created in Phase 1. Also note, using the API directly you can do all of the above commands in one API call."},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Octavia/Devstack_with_Octavia_Load_Balancing/#phase-3-test-your-load-balancer","title":"Phase 3: Test your load balancer","text":"<pre><code>openstack loadbalancer show lb1 # Note the vip_address\ncurl http://&lt;vip_address&gt;\ncurl http://&lt;vip_address&gt;\n</code></pre> <p>This should show the \u201cWelcome to \\\u201d message from each member server."},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Octavia/Load_Balancing_Overview/","title":"Load Balancing Overview (octavia)","text":"<p>Welcome to Octavia!</p> <p>Octavia is an open source, operator-scale load balancing solution designed to work with OpenStack.</p> <p>Octavia was born out of the Neutron LBaaS project. Its conception influenced the transformation of the Neutron LBaaS project, as Neutron LBaaS moved from version 1 to version 2. Starting with the Liberty release of OpenStack, Octavia has become the reference implementation for Neutron LBaaS version 2.</p> <p>Octavia accomplishes its delivery of load balancing services by managing a fleet of virtual machines, containers, or bare metal servers\u2014collectively known as\u00a0amphorae\u2014 which it spins up on demand. This on-demand, horizontal scaling feature differentiates Octavia from other load balancing solutions, thereby making Octavia truly suited \u201cfor the cloud\u201d.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Octavia/Load_Balancing_Overview/#where-octavia-fits-into-the-openstack-ecosystem","title":"Where Octavia fits into the OpenStack ecosystem","text":"<p>Load balancing is essential for enabling simple or automatic delivery scaling and availability. In turn, application delivery scaling and availability must be considered vital features of any cloud. Together, these facts imply that load balancing is a vital feature of any cloud.</p> <p>Therefore, we consider Octavia to be as essential as Nova, Neutron, Glance or any other \u201ccore\u201d project that enables the essential features of a modern OpenStack cloud.</p> <p>In accomplishing its role, Octavia makes use of other OpenStack projects:</p> <ul> <li> <p>Nova\u00a0\u2013 For managing amphora lifecycle and spinning up compute resources on demand.</p> </li> <li> <p>Neutron\u00a0\u2013 For network connectivity between amphorae, tenant environments, and external networks.</p> </li> <li> <p>Barbican\u00a0\u2013 For managing TLS certificates and credentials, when TLS session termination is configured on the amphorae.</p> </li> <li> <p>Keystone\u00a0\u2013 For authentication against the Octavia API, and for Octavia to authenticate with other OpenStack projects.</p> </li> <li> <p>Glance\u00a0\u2013 For storing the amphora virtual machine image.</p> </li> <li> <p>Oslo\u00a0\u2013 For communication between Octavia controller components, making Octavia work within the standard OpenStack framework and review system, and project code structure.</p> </li> <li> <p>Taskflow\u00a0\u2013 Is technically part of Oslo; however, Octavia makes extensive use of this job flow system when orchestrating back-end service configuration and management.</p> </li> </ul> <p>Octavia is designed to interact with the components listed previously. In each case, we\u2019ve taken care to define this interaction through a driver interface. That way, external components can be swapped out with functionally-equivalent replacements\u2014 without having to restructure major components of Octavia. For example, if you use an SDN solution other than Neutron in your environment, it should be possible for you to write an Octavia networking driver for your SDN environment, which can be a drop-in replacement for the standard Neutron networking driver in Octavia.</p> <p>As of Pike, it is recommended to run Octavia as a standalone load balancing solution. Neutron LBaaS is deprecated in the Queens release, and Octavia is its replacement. Whenever possible, operators are\u00a0strongly\u00a0advised to migrate to Octavia. For end-users, this transition should be relatively seamless, because Octavia supports the Neutron LBaaS v2 API and it has a similar CLI interface. Alternatively, if end-users cannot migrate on their side in the forseable future, operators could enable the experimental Octavia proxy plugin in Neutron LBaaS.</p> <p>It is also possible to use Octavia as a Neutron LBaaS plugin, in the same way as any other vendor. You can think of Octavia as an \u201copen source vendor\u201d for Neutron LBaaS.</p> <p>Octavia supports third-party vendor drivers just like Neutron LBaaS, and fully replaces Neutron LBaaS as the load balancing solution for OpenStack.</p> <p>For further information on OpenStack Neutron LBaaS deprecation, please refer to\u00a0https://wiki.openstack.org/wiki/Neutron/LBaaS/Deprecation.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Octavia/Load_Balancing_Overview/#octavia-terminology","title":"Octavia terminology","text":"<p>Before you proceed further in this introduction, please note:</p> <p>Experience shows that\u2014within the subsegment of the IT industry that creates, deploys, and uses load balancing devices or services\u2014 terminology is often used inconsistently. To reduce confusion, the Octavia team has created a glossary of terms as they are defined and used within the context of the Octavia project and Neutron LBaaS version 2. This glossary is available here:\u00a0Octavia Glossary</p> <p>If you are familiar with Neutron LBaaS version 1 terms and usage, it is especially important for you to understand how the meanings of the terms \u201cVIP,\u201d \u201cload balancer,\u201d and \u201cload balancing,\u201d have changed in Neutron LBaaS version 2.</p> <p>Our use of these terms should remain consistent with the\u00a0Octavia Glossary\u00a0throughout Octavia\u2019s documentation, in discussions held by Octavia team members on public mailing lists, in IRC channels, and at conferences. To avoid misunderstandings, it\u2019s a good idea to familiarize yourself with these glossary definitions.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Octavia/Load_Balancing_Overview/#a-10000-foot-overview-of-octavia-components","title":"A 10,000-foot overview of Octavia components","text":"<p>Octavia version 4.0 consists of the following major components:</p> <ul> <li> <p>amphorae\u00a0\u2013 Amphorae are the individual virtual machines, containers, or bare metal servers that accomplish the delivery of load balancing services to tenant application environments. In Octavia version 0.8, the reference implementation of the amphorae image is an Ubuntu virtual machine running HAProxy.</p> </li> <li> <p>controller\u00a0\u2013 The Controller is the \u201cbrains\u201d of Octavia. It consists of five sub-components, which are individual daemons. They can be run on separate back-end infrastructure if desired:</p> </li> <li> <p>API Controller\u00a0\u2013 As the name implies, this subcomponent runs Octavia\u2019s API. It takes API requests, performs simple sanitizing on them, and ships them off to the controller worker over the Oslo messaging bus.</p> </li> <li> <p>Controller Worker\u00a0\u2013 This subcomponent takes sanitized API commands from the API controller and performs the actions necessary to fulfill the API request.</p> </li> <li> <p>Health Manager\u00a0\u2013 This subcomponent monitors individual amphorae to ensure they are up and running, and otherwise healthy. It also handles failover events if amphorae fail unexpectedly.</p> </li> <li> <p>Housekeeping Manager\u00a0\u2013 This subcomponent cleans up stale (deleted) database records and manages amphora certificate rotation.</p> </li> <li> <p>Driver Agent\u00a0\u2013 The driver agent receives status and statistics updates from provider drivers.</p> </li> <li> <p>network\u00a0\u2013 Octavia cannot accomplish what it does without manipulating the network environment. Amphorae are spun up with a network interface on the \u201cload balancer network,\u201d and they may also plug directly into tenant networks to reach back-end pool members, depending on how any given load balancing service is deployed by the tenant.</p> </li> </ul> <p>For a more complete description of Octavia\u2019s components, please see the\u00a0Octavia v0.5 Component Design\u00a0document within this documentation repository.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Automatic_Migration_of_VMs_Upon_Failure_of_Host/","title":"Automatic Migration of VMs Upon Failure of Host","text":"<p>Taikun Open Cloud Platform includes an Automatic Migration feature, designed to safeguard your Virtual Machines (VMs) in the event of a host failure. This article provides insights into how this feature, seamlessly integrated into our customized Taikun OCP deployment, guarantees business continuity and minimizes downtime.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Automatic_Migration_of_VMs_Upon_Failure_of_Host/#key-features","title":"Key Features:","text":"<ol> <li> <p>Customized Deployment:\u00a0This customization extends to our Automatic Migration feature, enhancing the resilience of your cloud infrastructure.</p> </li> <li> <p>Host Health Monitoring:\u00a0Our platform incorporates a sophisticated host health monitoring solution. This custom-built feature continuously assesses the status of each host within the Taikun OCP environment, proactively identifying potential failures before they impact your VMs.</p> </li> <li> <p>Automatic VM Migration:\u00a0In the unfortunate event of a host failure, Taikun OpenCloud Platform\u2019s Automatic Migration feature swings into action. All VMs hosted on the affected host are seamlessly and automatically migrated to a healthy host, ensuring minimal disruption to your services.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Automatic_Migration_of_VMs_Upon_Failure_of_Host/#how-it-works","title":"How It Works:","text":"<ol> <li> <p>Continuous Monitoring:\u00a0The host health monitoring solution runs in the background, constantly evaluating the status of each host in the Taikun OCP deployment.</p> </li> <li> <p>Failure Detection:\u00a0Upon detecting a potential failure, the system triggers an alert and initiates the Automatic Migration process.</p> </li> <li> <p>Efficient Migration:\u00a0Taikun Open Cloud Platform orchestrates the migration of all VMs from the affected host to a healthy one, optimizing the allocation of resources to maintain performance levels.</p> </li> </ol>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/CLI_Client/","title":"CLI Client","text":"<p>OpenStackClient (aka OSC) is a command-line client for OpenStack that brings the command set for Compute, Identity, Image, Object Storage and Block Storage APIs together in a single shell with a uniform command structure.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/CLI_Client/#using-openstackclient","title":"Using OpenStackClient","text":"<ul> <li> <p>User Documentation</p> </li> <li> <p>Manual Page</p> </li> <li> <p>Command List</p> </li> <li> <p>Command Structure</p> </li> <li> <p>Plugin Commands</p> </li> <li> <p>Authentication</p> </li> <li> <p>Interactive Mode</p> </li> <li> <p>Mapping Guide</p> </li> <li> <p>Backwards Incompatible Changes</p> </li> <li> <p>Configuration</p> </li> <li> <p>Global Options</p> </li> <li> <p>Environment Variables</p> </li> <li> <p>Configuration Files</p> </li> <li> <p>Logging Settings</p> </li> <li> <p>Locale and Language Support</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/CLI_Client/#getting-started","title":"Getting Started","text":"<ul> <li> <p>Try\u00a0some commands</p> </li> <li> <p>Read the source\u00a0on OpenStack\u2019s Git server</p> </li> <li> <p>Install OpenStackClient from\u00a0PyPi\u00a0or a\u00a0tarball</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/CLI_Client/#release-notes","title":"Release Notes","text":"<ul> <li>Release Notes</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/CLI_Client/#contributor-documentation","title":"Contributor Documentation","text":"<ul> <li> <p>Contributor Documentation</p> </li> <li> <p>Developing with OpenStackClient</p> </li> <li> <p>Command Beta</p> </li> <li> <p>Command Options</p> </li> <li> <p>Command Class Wrappers</p> </li> <li> <p>Command Errors</p> </li> <li> <p>Command Logs</p> </li> <li> <p>Command Specs</p> </li> <li> <p>Plugins</p> </li> <li> <p>Human Interface Guide</p> </li> <li> <p>openstackclient</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/CLI_Client/#project-goals","title":"Project Goals","text":"<ul> <li> <p>Use the OpenStack Python API libraries, extending or replacing them as required</p> </li> <li> <p>Use a consistent naming and structure for commands and arguments</p> </li> <li> <p>Provide consistent output formats with optional machine parseable formats</p> </li> <li> <p>Use a single-binary approach that also contains an embedded shell that can execute multiple commands on a single authentication (see libvirt\u2019s virsh for an example)</p> </li> <li> <p>Independence from the OpenStack project names; only API names are referenced (to the extent possible)</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/CLI_Client/#contributing","title":"Contributing","text":"<p>OpenStackClient utilizes all of the usual OpenStack processes and requirements for contributions. The code is hosted\u00a0on OpenStack\u2019s Git server.\u00a0Bug reports\u00a0may be submitted to the\u00a0<code>python-openstackclient</code> Storyboard project. Code may be submitted to the\u00a0<code>openstack/python-openstackclient</code>\u00a0project using\u00a0Gerrit. Developers may also be found in the\u00a0IRC channel <code>#openstack-sdks</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/CLI_Client/#indices-and-tables","title":"Indices and Tables","text":"<ul> <li> <p>Index</p> </li> <li> <p>Search Page</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/CLI_Command_List/","title":"CLI Command List","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/","title":"Configure Postfix Client","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#install-postfix","title":"Install Postfix","text":"<p>To install\u00a0Postfix\u00a0run the following command:</p> <p>sudo apt install postfix</p> <p>It is OK to accept defaults initially by pressing return for each question. Some of the configuration options will be investigated in greater detail in the configuration stage.</p> <p>Deprecation warning: The\u00a0<code>mail-stack-delivery</code>\u00a0metapackage has been deprecated in Focal. The package still exists for compatibility reasons, but won\u2019t setup a working email system.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#configure-postfix","title":"Configure Postfix","text":"<p>There are four things you should decide before configuring:</p> <ul> <li> <p>The \\ for which you\u2019ll accept email (we\u2019ll use\u00a0<code>mail.example.com</code>\u00a0in our example) <li> <p>The network and class range of your mail server (we\u2019ll use\u00a0<code>192.168.0.0/24</code>)</p> </li> <li> <p>The username (we\u2019re using\u00a0<code>steve</code>)</p> </li> <li> <p>Type of mailbox format (<code>mbox</code>\u00a0is the default, but we\u2019ll use the alternative,\u00a0<code>Maildir</code>)</p> </li> <p>To configure postfix, run the following command:</p> <p>sudo dpkg-reconfigure postfix</p> <p>The user interface will be displayed. On each screen, select the following values:</p> <ul> <li> <p>Internet Site</p> </li> <li> <p><code>mail.example.com</code></p> </li> <li> <p><code>steve</code></p> </li> <li> <p><code>mail.example.com</code>,\u00a0<code>localhost.localdomain</code>,\u00a0<code>localhost</code></p> </li> <li> <p>No</p> </li> <li> <p><code>127.0.0.0/8 \\[::ffff:127.0.0.0\\]/104 \\[::1\\]/128</code> <code>192.168.0.0/24</code></p> </li> <li> <p>0</p> </li> <li> <p>+</p> </li> <li> <p>all</p> </li> </ul> <p>To set the mailbox format, you can either edit the configuration file directly, or use the\u00a0<code>postconf</code>\u00a0command. In either case, the configuration parameters will be stored in\u00a0<code>/etc/postfix/main.cf</code>\u00a0file. Later if you wish to re-configure a particular parameter, you can either run the command or change it manually in the file.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#configure-mailbox-format","title":"Configure mailbox format","text":"<p>To configure the mailbox format for\u00a0<code>Maildir</code>:</p> <p>sudo postconf -e 'home_mailbox = Maildir/'</p> <p>This will place new mail in\u00a0<code>/home/&lt;username&gt;/Maildir</code>\u00a0so you will need to configure your Mail Delivery Agent (MDA) to use the same path.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#smtp-authentication","title":"SMTP authentication","text":"<p>SMTP-AUTH allows a client to identify itself through the Simple Authentication and Security Layer (SASL) authentication mechanism, using Transport Layer Security (TLS) to encrypt the authentication process. Once it has been authenticated, the SMTP server will allow the client to relay mail.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#configure-smtp-authentication","title":"Configure SMTP authentication","text":"<p>To configure Postfix for SMTP-AUTH using SASL (Dovecot SASL), run these commands at a terminal prompt:</p> <p>sudo postconf -e 'smtpd_sasl_type = dovecot' sudo postconf -e 'smtpd_sasl_path = private/auth' sudo postconf -e 'smtpd_sasl_local_domain =' sudo postconf -e 'smtpd_sasl_security_options = noanonymous,noplaintext' sudo postconf -e 'smtpd_sasl_tls_security_options = noanonymous' sudo postconf -e 'broken_sasl_auth_clients = yes' sudo postconf -e 'smtpd_sasl_auth_enable = yes' sudo postconf -e 'smtpd_recipient_restrictions = \\ permit_sasl_authenticated,permit_mynetworks,reject_unauth_destination'</p> <p>Note: The\u00a0<code>smtpd_sasl_path</code>\u00a0config parameter is a path relative to the Postfix queue directory.</p> <p>There are several SASL mechanism properties worth evaluating to improve the security of your deployment. The options \u201cnoanonymous,noplaintext\u201d prevent the use of mechanisms that permit anonymous authentication or that transmit credentials unencrypted.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#configure-tls","title":"Configure TLS","text":"<p>Next, generate or obtain a digital certificate for TLS. MUAs connecting to your mail server via TLS will need to recognise the certificate used for TLS. This can either be done using a certificate from Let\u2019s Encrypt, from a commercial CA or with a self-signed certificate that users manually install/accept.</p> <p>For MTA-to-MTA, TLS certificates are never validated without prior agreement from the affected organisations. For MTA-to-MTA TLS, there is no reason not to use a self-signed certificate unless local policy requires it. See our\u00a0guide on security certificates\u00a0for details about generating digital certificates and setting up your own Certificate Authority (CA).</p> <p>Once you have a certificate, configure Postfix to provide TLS encryption for both incoming and outgoing mail:</p> <p>sudo postconf -e 'smtp_tls_security_level = may' sudo postconf -e 'smtpd_tls_security_level = may' sudo postconf -e 'smtp_tls_note_starttls_offer = yes' sudo postconf -e 'smtpd_tls_key_file = /etc/ssl/private/server.key' sudo postconf -e 'smtpd_tls_cert_file = /etc/ssl/certs/server.crt' sudo postconf -e 'smtpd_tls_loglevel = 1' sudo postconf -e 'smtpd_tls_received_header = yes' sudo postconf -e 'myhostname = mail.example.com'</p> <p>If you are using your own Certificate Authority to sign the certificate, enter:</p> <p>sudo postconf -e 'smtpd_tls_CAfile = /etc/ssl/certs/cacert.pem'</p> <p>Again, for more details about certificates see our\u00a0security certificates guide.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#outcome-of-initial-configuration","title":"Outcome of initial configuration","text":"<p>After running all the above commands, Postfix will be configured for SMTP-AUTH with a self-signed certificate for TLS encryption.</p> <p>Now, the file\u00a0<code>/etc/postfix/main.cf</code>\u00a0should look like this:</p> <p># See /usr/share/postfix/main.cf.dist for a commented, more complete # version      smtpd_banner = $myhostname ESMTP $mail_name (Ubuntu) biff = no      # appending .domain is the MUA's job. append_dot_mydomain = no      # Uncomment the next line to generate \"delayed mail\" warnings #delay_warning_time = 4h      myhostname = server1.example.com alias_maps = hash:/etc/aliases alias_database = hash:/etc/aliases myorigin = /etc/mailname mydestination = server1.example.com, localhost.example.com, localhost relayhost = mynetworks = 127.0.0.0/8 mailbox_command = procmail -a \"$EXTENSION\" mailbox_size_limit = 0 recipient_delimiter = + inet_interfaces = all smtpd_sasl_local_domain = smtpd_sasl_auth_enable = yes smtpd_sasl_security_options = noanonymous broken_sasl_auth_clients = yes smtpd_recipient_restrictions = permit_sasl_authenticated,permit_mynetworks,reject _unauth_destination smtpd_tls_auth_only = no smtp_tls_security_level = may smtpd_tls_security_level = may smtp_tls_note_starttls_offer = yes smtpd_tls_key_file = /etc/ssl/private/smtpd.key smtpd_tls_cert_file = /etc/ssl/certs/smtpd.crt smtpd_tls_CAfile = /etc/ssl/certs/cacert.pem smtpd_tls_loglevel = 1 smtpd_tls_received_header = yes smtpd_tls_session_cache_timeout = 3600s tls_random_source = dev:/dev/urandom</p> <p>The Postfix initial configuration is now complete. Run the following command to restart the Postfix daemon:</p> <p>sudo systemctl restart postfix.service</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#sasl","title":"SASL","text":"<p>Postfix supports SMTP-AUTH as defined in\u00a0RFC2554. It is based on\u00a0SASL. However it is still necessary to set up SASL authentication before you can use SMTP-AUTH.</p> <p>When using IPv6, the\u00a0<code>mynetworks</code>\u00a0parameter may need to be modified to allow IPv6 addresses, for example:</p> <p>mynetworks = 127.0.0.0/8, [::1]/128</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#configure-sasl","title":"Configure SASL","text":"<p>Postfix supports two SASL implementations:\u00a0Cyrus SASL\u00a0and\u00a0Dovecot SASL.</p> <p>To enable Dovecot SASL the\u00a0<code>dovecot-core</code>\u00a0package will need to be installed:</p> <p>sudo apt install dovecot-core</p> <p>Next, edit\u00a0<code>/etc/dovecot/conf.d/10-master.conf</code>\u00a0and change the following:</p> <p>service auth {   # auth_socket_path points to this userdb socket by default. It's typically   # used by dovecot-lda, doveadm, possibly imap process, etc. Its default   # permissions make it readable only by root, but you may need to relax these   # permissions. Users that have access to this socket are able to get a list   # of all usernames and get results of everyone's userdb lookups.   unix_listener auth-userdb {     #mode = 0600     #user =      #group =    }        # Postfix smtp-auth   unix_listener /var/spool/postfix/private/auth {     mode = 0660     user = postfix     group = postfix   }  }</p> <p>To permit use of SMTP-AUTH by Outlook clients, change the following line in the\u00a0authentication mechanisms\u00a0section of\u00a0<code>/etc/dovecot/conf.d/10-auth.conf</code>\u00a0from:</p> <p>auth_mechanisms = plain</p> <p>to this:</p> <p>auth_mechanisms = plain login</p> <p>Once you have configured Dovecot, restart it with:</p> <p>sudo systemctl restart dovecot.service</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#test-your-setup","title":"Test your setup","text":"<p>SMTP-AUTH configuration is complete \u2013 now it is time to test the setup. To see if SMTP-AUTH and TLS work properly, run the following command:</p> <p>telnet mail.example.com 25</p> <p>After you have established the connection to the Postfix mail server, type:</p> <p>ehlo mail.example.com</p> <p>If you see the following in the output, then everything is working perfectly. Type\u00a0<code>quit</code>\u00a0to exit.</p> <p>250-STARTTLS 250-AUTH LOGIN PLAIN 250-AUTH=LOGIN PLAIN 250 8BITMIME</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#troubleshooting","title":"Troubleshooting","text":"<p>When problems arise, there are a few common ways to diagnose the cause.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#escaping-chroot","title":"Escaping\u00a0<code>chroot</code>","text":"<p>The Ubuntu Postfix package will, by default, install into a\u00a0<code>chroot</code>\u00a0environment for security reasons. This can add greater complexity when troubleshooting problems.</p> <p>To turn off the\u00a0<code>chroot</code>\u00a0usage, locate the following line in the\u00a0<code>/etc/postfix/master.cf</code>\u00a0configuration file:</p> <p>smtp      inet  n       -       -       -       -       smtpd</p> <p>Modify it as follows:</p> <p>smtp      inet  n       -       n       -       -       smtpd</p> <p>You will then need to restart Postfix to use the new configuration. From a terminal prompt enter:</p> <p>sudo service postfix restart</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#smtps","title":"SMTPS","text":"<p>If you need secure SMTP, edit\u00a0<code>/etc/postfix/master.cf</code>\u00a0and uncomment the following line:</p> <p>smtps     inet  n       -       -       -       -       smtpd   -o smtpd_tls_wrappermode=yes   -o smtpd_sasl_auth_enable=yes   -o smtpd_client_restrictions=permit_sasl_authenticated,reject   -o milter_macro_daemon_name=ORIGINATING</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#log-viewing","title":"Log viewing","text":"<p>Postfix sends all log messages to\u00a0<code>/var/log/mail.log</code>. However, error and warning messages can sometimes get lost in the normal log output so they are also logged to\u00a0<code>/var/log/mail.err</code>\u00a0and\u00a0<code>/var/log/mail.warn</code>\u00a0respectively.</p> <p>To see messages entered into the logs in real time you can use the\u00a0<code>tail -f</code>\u00a0command:</p> <p>tail -f /var/log/mail.err</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#increase-logging-detail","title":"Increase logging detail","text":"<p>The amount of detail recorded in the logs can be increased via the configuration options. For example, to increase TLS activity logging set the\u00a0<code>smtpd_tls_loglevel</code>\u00a0option to a value from 1 to 4.</p> <p>sudo postconf -e 'smtpd_tls_loglevel = 4'</p> <p>Reload the service after any configuration change, to activate the new config:</p> <p>sudo systemctl reload postfix.service</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#logging-mail-delivery","title":"Logging mail delivery","text":"<p>If you are having trouble sending or receiving mail from a specific domain you can add the domain to the\u00a0<code>debug_peer_list</code>\u00a0parameter.</p> <p>sudo postconf -e 'debug_peer_list = problem.domain' sudo systemctl reload postfix.service</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#increase-daemon-verbosity","title":"Increase daemon verbosity","text":"<p>You can increase the verbosity of any Postfix daemon process by editing the\u00a0<code>/etc/postfix/master.cf</code>\u00a0and adding a\u00a0<code>-v</code>\u00a0after the entry. For example, edit the\u00a0<code>smtp</code>\u00a0entry:</p> <p>smtp      unix  -       -       -       -       -       smtp -v</p> <p>Then, reload the service as usual:</p> <p>sudo systemctl reload postfix.service</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Configure_Postfix_CLient/#log-sasl-debug-info","title":"Log SASL debug info","text":"<p>To increase the amount of information logged when troubleshooting SASL issues you can set the following options in\u00a0<code>/etc/dovecot/conf.d/10-logging.conf</code></p> <p>auth_debug=yes auth_debug_passwords=yes</p> <p>As with Postfix, if you change a Dovecot configuration the process will need to be reloaded:</p> <p><code>sudo systemctl reload dovecot.service</code></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Get_Images/","title":"Get Images","text":"<p>The simplest way to obtain a virtual machine image that works with OpenStack is to download one that someone else has already created. Most of the images contain the\u00a0<code>cloud-init</code>\u00a0package to support the SSH key pair and user data injection. Because many of the images disable SSH password authentication by default, boot the image with an injected key pair. You can\u00a0<code>SSH</code>\u00a0into the instance with the private key and default login account. See\u00a0Configure access and security for instances\u00a0for more information on how to create and inject key pairs with OpenStack.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Get_Images/#centos","title":"CentOS","text":"<p>The CentOS project maintains official images for direct download.</p> <ul> <li> <p>CentOS 6 images</p> </li> <li> <p>CentOS 7 images</p> </li> <li> <p>CentOS 8 images</p> </li> <li> <p>CentOS 8 stream images</p> </li> <li> <p>CentOS 9 stream images</p> </li> </ul> <p>Note In a CentOS cloud image, the login account is\u00a0<code>centos</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Get_Images/#cirros-test","title":"CirrOS (test)","text":"<p>CirrOS is a minimal Linux distribution that was designed for use as a test image on clouds such as OpenStack Compute. You can download a CirrOS image in various formats from the\u00a0CirrOS download page.</p> <p>If your deployment uses QEMU or KVM, we recommend using the images in qcow2 format. The most recent 64-bit qcow2 image as of this writing is\u00a0cirros-0.5.1-x86_64-disk.img.</p> <p>Note In a CirrOS image, the login account is\u00a0<code>cirros</code>. The password is\u00a0<code>gocubsgo</code>. Since the fixed PW allows anyone to login, you should not run this image with a public IP attached.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Get_Images/#debian","title":"Debian","text":"<p>Debian provides images for direct download. They are made at the same time as the CD and DVD images of Debian. Therefore, images are available on each point release of Debian. Also, weekly images of the testing distribution are available.</p> <p>Note In a Debian image, the login account is\u00a0<code>debian</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Get_Images/#fedora","title":"Fedora","text":"<p>The Fedora project maintains a list of official cloud images at\u00a0Fedora download page.</p> <p>Note In a Fedora cloud image, the login account is\u00a0<code>fedora</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Get_Images/#microsoft-windows","title":"Microsoft Windows","text":"<p>Cloudbase Solutions provides the last available trial version of\u00a0Windows Server 2012 R2. This image includes cloudbase-init plus VirtIO drivers on KVM. You can build your own image based on Windows Server 2016, 2019, Windows 10 etc) with\u00a0Cloudbase Imaging Tools.</p> <p>ISO files for Windows 10 are available on\u00a0Microsoft Windows 10 Downloadpage\u00a0and\u00a0Microsoft Evaluation Center.</p> <p>Fedora Virtio\u00a0provides also Windows images.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Get_Images/#ubuntu","title":"Ubuntu","text":"<p>Canonical maintains an official set of\u00a0Ubuntu-based images.</p> <p>Images are arranged by Ubuntu release, and by image release date, with\u00a0<code>current</code>\u00a0being the most recent. For example, the page that contains the most recently built image for Ubuntu 18.04 Bionic Beaver is\u00a0Ubuntu 18.04 LTS (Bionic Beaver) Daily Build. Scroll to the bottom of the page for links to the images that can be downloaded directly.</p> <p>If your deployment uses QEMU or KVM, we recommend using the images in qcow2 format, with name ending in\u00a0<code>.img</code>. The most recent version of the 64-bit amd64-arch QCOW2 image for Ubuntu 18.04 is\u00a0bionic-server-cloudimg-amd64-disk.img.</p> <p>Note In an Ubuntu cloud image, the login account is\u00a0<code>ubuntu</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Get_Images/#opensuse-and-suse-linux-enterprise-server","title":"openSUSE and SUSE Linux Enterprise Server","text":"<p>The openSUSE community provides images for\u00a0openSUSE.</p> <p>SUSE maintains official SUSE Linux Enterprise Server cloud images. Go to the\u00a0SUSE Linux Enterprise Server download page, select the\u00a0<code>AMD64\u00a0/\u00a0Intel\u00a064</code>\u00a0architecture and search for\u00a0<code>OpenStack-Cloud</code>.</p> <p>Note In an openSUSE cloud image, the login account is\u00a0<code>opensuse</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Get_Images/#red-hat-enterprise-linux","title":"Red Hat Enterprise Linux","text":"<p>Red Hat maintains official Red Hat Enterprise Linux cloud images. A valid Red Hat Enterprise Linux subscription is required to download these images.</p> <ul> <li> <p>Red Hat Enterprise Linux 8 KVM Guest Image</p> </li> <li> <p>Red Hat Enterprise Linux 7 KVM Guest Image</p> </li> <li> <p>Red Hat Enterprise Linux 6 KVM Guest Image</p> </li> </ul> <p>Note In a RHEL cloud image, the login account is\u00a0<code>cloud-user</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Get_Images/#freebsd-openbsd-and-netbsd","title":"FreeBSD, OpenBSD, and NetBSD","text":"<p>Unofficial images for BSD are available on\u00a0BSD-Cloud-Image.org.</p> <p>Note The login accounts are\u00a0<code>freebsd</code>\u00a0for FreeBSD,\u00a0<code>openbsd</code>\u00a0for OpenBSD, and\u00a0<code>netbsd</code>\u00a0for NetBSD.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Get_Images/#arch-linux","title":"Arch Linux","text":"<p>Arch Linux provides a cloud image for download. More details can be found on the\u00a0arch-boxes project page.</p> <p>Note In a Arch Linux image, the login account is\u00a0<code>arch</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Getting_Started_with_Ceilometer/","title":"Getting Started with Ceilometer","text":"<p>The Telemetry Data Collection services provide the following functions:</p> <ul> <li> <p>Efficiently polls metering data related to OpenStack services.</p> </li> <li> <p>Collects event and metering data by monitoring notifications sent from services.</p> </li> <li> <p>Publishes collected data to various targets including data stores and message queues.</p> </li> </ul> <p>The Telemetry service consists of the following components:A compute agent (<code>ceilometer-agent-compute</code>)</p> <p>Runs on each compute node and polls for resource utilization statistics. This is actually the polling agent\u00a0<code>ceilometer-polling</code>\u00a0running with parameter\u00a0<code>--polling-namespace\u00a0compute</code>.A central agent (<code>ceilometer-agent-central</code>)</p> <p>Runs on a central management server to poll for resource utilization statistics for resources not tied to instances or compute nodes. Multiple agents can be started to scale service horizontally. This is actually the polling agent\u00a0<code>ceilometer-polling</code>\u00a0running with parameter\u00a0<code>--polling-namespace\u00a0central</code>.A notification agent (<code>ceilometer-agent-notification</code>)</p> <p>Runs on a central management server(s) and consumes messages from the message queue(s) to build event and metering data. Data is then published to defined targets. By default, data is pushed to\u00a0Gnocchi.</p> <p>These services communicate by using the OpenStack messaging bus. Ceilometer data is designed to be published to various endpoints for storage and analysis.</p> <p>Note Ceilometer previously provided a storage and API solution. As of Newton, this functionality is officially deprecated and discouraged. For efficient storage and statistical analysis of Ceilometer data,\u00a0Gnocchi\u00a0is recommended.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Install_CLI/","title":"Install CLI","text":"<p>OpenStackClient (aka OSC) is a command-line client for OpenStack that brings the command set for Compute, Identity, Image, Object Storage and Block Storage APIs together in a single shell with a uniform command structure.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Install_CLI/#using-openstackclient","title":"Using OpenStackClient","text":"<ul> <li> <p>User Documentation</p> </li> <li> <p>Manual Page</p> </li> <li> <p>Command List</p> </li> <li> <p>Command Structure</p> </li> <li> <p>Plugin Commands</p> </li> <li> <p>Authentication</p> </li> <li> <p>Interactive Mode</p> </li> <li> <p>Mapping Guide</p> </li> <li> <p>Backwards Incompatible Changes</p> </li> <li> <p>Configuration</p> </li> <li> <p>Global Options</p> </li> <li> <p>Environment Variables</p> </li> <li> <p>Configuration Files</p> </li> <li> <p>Logging Settings</p> </li> <li> <p>Locale and Language Support</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Install_CLI/#getting-started","title":"Getting Started","text":"<ul> <li> <p>Try\u00a0some commands</p> </li> <li> <p>Read the source\u00a0on OpenStack\u2019s Git server</p> </li> <li> <p>Install OpenStackClient from\u00a0PyPi\u00a0or a\u00a0tarball</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Install_CLI/#release-notes","title":"Release Notes","text":"<ul> <li>Release Notes</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Install_CLI/#contributor-documentation","title":"Contributor Documentation","text":"<ul> <li> <p>Contributor Documentation</p> </li> <li> <p>Developing with OpenStackClient</p> </li> <li> <p>Command Beta</p> </li> <li> <p>Command Options</p> </li> <li> <p>Command Class Wrappers</p> </li> <li> <p>Command Errors</p> </li> <li> <p>Command Logs</p> </li> <li> <p>Command Specs</p> </li> <li> <p>Plugins</p> </li> <li> <p>Human Interface Guide</p> </li> <li> <p>openstackclient</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Install_CLI/#project-goals","title":"Project Goals","text":"<ul> <li> <p>Use the OpenStack Python API libraries, extending or replacing them as required</p> </li> <li> <p>Use a consistent naming and structure for commands and arguments</p> </li> <li> <p>Provide consistent output formats with optional machine parseable formats</p> </li> <li> <p>Use a single-binary approach that also contains an embedded shell that can execute multiple commands on a single authentication (see libvirt\u2019s virsh for an example)</p> </li> <li> <p>Independence from the OpenStack project names; only API names are referenced (to the extent possible)</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Install_CLI/#contributing","title":"Contributing","text":"<p>OpenStackClient utilizes all of the usual OpenStack processes and requirements for contributions. The code is hosted\u00a0on OpenStack\u2019s Git server.\u00a0Bug reports\u00a0may be submitted to the\u00a0<code>python-openstackclient</code> Storyboard project. Code may be submitted to the\u00a0<code>openstack/python-openstackclient</code>\u00a0project using\u00a0Gerrit. Developers may also be found in the\u00a0IRC channel <code>#openstack-sdks</code>.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Install_CLI/#indices-and-tables","title":"Indices and Tables","text":"<ul> <li> <p>Index</p> </li> <li> <p>Search Page</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Maintanance_Mode/","title":"Maintanance Mode","text":"<p>Once all the services are running and configured properly, and a node has been enrolled with the Bare Metal service and is in the\u00a0<code>available</code>\u00a0provision state, the Compute service should detect the node as an available resource and expose it to the scheduler.</p> <p>In addition to watching\u00a0<code>nova-compute</code>\u00a0log files, you can see the available resources by looking at the list of Compute hypervisors. The resources reported therein should match the bare metal node properties, and the Compute service flavor.</p> <p>Here is an example set of commands to compare the resources in Compute service and Bare Metal service:</p> <pre><code>$ ironic node-list\n+--------------------------------------+---------------+-------------+--------------------+-------------+\n| UUID                                 | Instance UUID | Power State | Provisioning State | Maintenance |\n+--------------------------------------+---------------+-------------+--------------------+-------------+\n| 86a2b1bb-8b29-4964-a817-f90031debddb | None          | power off   | available          | False       |\n+--------------------------------------+---------------+-------------+--------------------+-------------+\n\n$ ironic node-show 86a2b1bb-8b29-4964-a817-f90031debddb\n+------------------------+----------------------------------------------------------------------+\n| Property               | Value                                                                |\n+------------------------+----------------------------------------------------------------------+\n| instance_uuid          | None                                                                 |\n| properties             | {u'memory_mb': u'1024', u'cpu_arch': u'x86_64', u'local_gb': u'10',  |\n|                        | u'cpus': u'1'}                                                       |\n| maintenance            | False                                                                |\n| driver_info            | { [SNIP] }                                                           |\n| extra                  | {}                                                                   |\n| last_error             | None                                                                 |\n| created_at             | 2014-11-20T23:57:03+00:00                                            |\n| target_provision_state | None                                                                 |\n| driver                 | pxe_ipmitool                                                         |\n| updated_at             | 2014-11-21T00:47:34+00:00                                            |\n| instance_info          | {}                                                                   |\n| chassis_uuid           | 7b49bbc5-2eb7-4269-b6ea-3f1a51448a59                                 |\n| provision_state        | available                                                            |\n| reservation            | None                                                                 |\n| power_state            | power off                                                            |\n| console_enabled        | False                                                                |\n| uuid                   | 86a2b1bb-8b29-4964-a817-f90031debddb                                 |\n+------------------------+----------------------------------------------------------------------+\n\n$ nova hypervisor-show 1\n+-------------------------+--------------------------------------+\n| Property                | Value                                |\n+-------------------------+--------------------------------------+\n| cpu_info                | baremetal cpu                        |\n| current_workload        | 0                                    |\n| disk_available_least    | -                                    |\n| free_disk_gb            | 10                                   |\n| free_ram_mb             | 1024                                 |\n| host_ip                 | [ SNIP ]                             |\n| hypervisor_hostname     | 86a2b1bb-8b29-4964-a817-f90031debddb |\n| hypervisor_type         | ironic                               |\n| hypervisor_version      | 1                                    |\n| id                      | 1                                    |\n| local_gb                | 10                                   |\n| local_gb_used           | 0                                    |\n| memory_mb               | 1024                                 |\n| memory_mb_used          | 0                                    |\n| running_vms             | 0                                    |\n| service_disabled_reason | -                                    |\n| service_host            | my-test-host                         |\n| service_id              | 6                                    |\n| state                   | up                                   |\n| status                  | enabled                              |\n| vcpus                   | 1                                    |\n| vcpus_used              | 0                                    |\n+-------------------------+--------------------------------------+\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Maintanance_Mode/#maintenance-mode","title":"Maintenance mode","text":"<p>Maintenance mode may be used if you need to take a node out of the resource pool. Putting a node in maintenance mode will prevent Bare Metal service from executing periodic tasks associated with the node. This will also prevent Compute service from placing a tenant instance on the node by not exposing the node to the nova scheduler. Nodes can be placed into maintenance mode with the following command.</p> <pre><code>$ ironic node-set-maintenance $NODE_UUID on\n</code></pre> <p>As of the Kilo release, a maintenance reason may be included with the optional\u00a0<code>--reason</code>\u00a0command line option. This is a free form text field that will be displayed in the\u00a0<code>maintenance_reason</code>\u00a0section of the\u00a0<code>node-show</code>\u00a0command.</p> <pre><code>$ ironic node-set-maintenance $UUID on --reason \"Need to add ram.\"\n\n$ ironic node-show $UUID\n\n+------------------------+--------------------------------------+\n| Property               | Value                                |\n+------------------------+--------------------------------------+\n| target_power_state     | None                                 |\n| extra                  | {}                                   |\n| last_error             | None                                 |\n| updated_at             | 2015-04-27T15:43:58+00:00            |\n| maintenance_reason     | Need to add ram.                     |\n| ...                    | ...                                  |\n| maintenance            | True                                 |\n| ...                    | ...                                  |\n+------------------------+--------------------------------------+\n</code></pre> <p>To remove maintenance mode and clear any\u00a0<code>maintenance_reason</code>\u00a0use the following command.</p> <pre><code>$ ironic node-set-maintenance $NODE_UUID off\n</code></pre>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Overcommitting_CPU_and_RAM/","title":"Overcommitting CPU and RAM","text":"<p>OpenStack allows you to overcommit CPU and RAM on compute nodes. This allows you to increase the number of instances running on your cloud at the cost of reducing the performance of the instances. The Compute service uses the following ratios by default:</p> <ul> <li> <p>CPU allocation ratio: 16:1</p> </li> <li> <p>RAM allocation ratio: 1.5:1</p> </li> </ul> <p>Caution Using a RAM allocation ratio above 1:1 can impact running VMs if all available memory on the hypervisor is used. If sufficient swap space is not available the system will have to rely on OOM killer to free space which can have a detrimental effect on running VMs.</p> <p>The default CPU allocation ratio of 16:1 means that the scheduler allocates up to 16 virtual cores per physical core. For example, if a physical node has 12 cores, the scheduler sees 192 available virtual cores. With typical flavor definitions of 4 virtual cores per instance, this ratio would provide 48 instances on a physical node.</p> <p>The formula for the number of virtual instances on a compute node is\u00a0<code>(OR*PC)/VC</code>, where:</p> <p>OR</p> <p>CPU overcommit ratio (virtual cores per physical core)</p> <p>PC</p> <p>Number of physical cores</p> <p>VC</p> <p>Number of virtual cores per instance</p> <p>Similarly, the default RAM allocation ratio of 1.5:1 means that the scheduler allocates instances to a physical node as long as the total amount of RAM associated with the instances is less than 1.5 times the amount of RAM available on the physical node.</p> <p>For example, if a physical node has 48 GB of RAM, the scheduler allocates instances to that node until the sum of the RAM associated with the instances reaches 72 GB (such as nine instances, in the case where each instance has 8 GB of RAM).</p> <p>Note Regardless of the overcommit ratio, an instance can not be placed on any physical node with fewer raw (pre-overcommit) resources than the instance flavor requires. You must select the appropriate CPU and RAM allocation ratio for your particular use case.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/","title":"Routers","text":"<p>A\u00a0router\u00a0is a logical component that forwards data packets between networks. It also provides Layer 3 and NAT forwarding to provide external network access for servers on project networks.</p> <p>Network v2</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-add-port","title":"router add port","text":"<p>Add a port to a router</p> <p>openstack router add port \\ \\ <p>router</p> <p>Router to which port will be added (name or ID)</p> <p>port</p> <p>Port to be added (name or ID)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-add-route","title":"router add route","text":"<p>Add extra static routes to a router\u2019s routing table.</p> <p>openstack router add route     [--route destination=\\,gateway=\\]     \\ <p>\u2013route\u00a0destination=\\,gateway=\\ <p>Add extra static route to the router. destination: destination subnet (in CIDR notation), gateway: nexthop IP address. Repeat option to add multiple routes. Trying to add a route that\u2019s already present (exactly, including destination and nexthop) in the routing table is allowed and is considered a successful operation.</p> <p>router</p> <p>Router to which extra static routes will be added (name or ID).</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-add-subnet","title":"router add subnet","text":"<p>Add a subnet to a router</p> <p>openstack router add subnet \\ \\ <p>router</p> <p>Router to which subnet will be added (name or ID)</p> <p>subnet</p> <p>Subnet to be added (name or ID)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-create","title":"router create","text":"<p>Create a new router</p> <p>openstack router create     [--extra-property type=\\,name=\\,value=\\]     [--enable | --disable]     [--distributed | --centralized]     [--ha | --no-ha]     [--description \\]     [--project \\]     [--project-domain \\]     [--availability-zone-hint \\]     [--tag \\ | --no-tag]     [--external-gateway \\]     [--fixed-ip subnet=\\,ip-address=\\]     [--enable-snat | --disable-snat]     \\ <p>\u2013extra-property\u00a0type=\\,name=\\,value=\\ <p>Additional parameters can be passed using this property. Default type of the extra property is string (\u2018str\u2019), but other types can be used as well. Available types are: \u2018dict\u2019, \u2018list\u2019, \u2018str\u2019, \u2018bool\u2019, \u2018int\u2019. In case of \u2018list\u2019 type, \u2018value\u2019 can be semicolon-separated list of values. For \u2018dict\u2019 value is semicolon-separated list of the key:value pairs.</p> <p>\u2013enable</p> <p>Enable router (default)</p> <p>\u2013disable</p> <p>Disable router</p> <p>\u2013distributed</p> <p>Create a distributed router</p> <p>\u2013centralized</p> <p>Create a centralized router</p> <p>\u2013ha</p> <p>Create a highly available router</p> <p>\u2013no-ha</p> <p>Create a legacy router</p> <p>\u2013description\u00a0\\ <p>Set router description</p> <p>\u2013project\u00a0\\ <p>Owner\u2019s project (name or ID)</p> <p>\u2013project-domain\u00a0\\ <p>Domain the project belongs to (name or ID). This can be used in case collisions between project names exist.</p> <p>\u2013availability-zone-hint\u00a0\\ <p>Availability Zone in which to create this router (Router Availability Zone extension required, repeat option to set multiple availability zones)</p> <p>\u2013tag\u00a0\\ <p>Tag to be added to the router (repeat option to set multiple tags)</p> <p>\u2013no-tag</p> <p>No tags associated with the router</p> <p>\u2013external-gateway\u00a0\\ <p>External Network used as router\u2019s gateway (name or ID)</p> <p>\u2013fixed-ip\u00a0subnet=\\,ip-address=\\ <p>Desired IP and/or subnet (name or ID) on external gateway: subnet=\\,ip-address=\\ (repeat option to set multiple fixed IP addresses) <p>\u2013enable-snat</p> <p>Enable Source NAT on external gateway</p> <p>\u2013disable-snat</p> <p>Disable Source NAT on external gateway</p> <p>name</p> <p>New router name</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-delete","title":"router delete","text":"<p>Delete router(s)</p> <p>openstack router delete \\ [\\ ...] <p>router</p> <p>Router(s) to delete (name or ID)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-list","title":"router list","text":"<p>List routers</p> <p>openstack router list     [--sort-column SORT_COLUMN]     [--sort-ascending | --sort-descending]     [--name \\]     [--enable | --disable]     [--long]     [--project \\]     [--project-domain \\]     [--agent \\]     [--tags \\[,\\,...]]     [--any-tags \\[,\\,...]]     [--not-tags \\[,\\,...]]     [--not-any-tags \\[,\\,...]] <p>\u2013sort-column\u00a0SORT_COLUMN</p> <p>specify the column(s) to sort the data (columns specified first have a priority, non-existing columns are ignored), can be repeated</p> <p>\u2013sort-ascending</p> <p>sort the column(s) in ascending order</p> <p>\u2013sort-descending</p> <p>sort the column(s) in descending order</p> <p>\u2013name\u00a0\\ <p>List routers according to their name</p> <p>\u2013enable</p> <p>List enabled routers</p> <p>\u2013disable</p> <p>List disabled routers</p> <p>\u2013long</p> <p>List additional fields in output</p> <p>\u2013project\u00a0\\ <p>List routers according to their project (name or ID)</p> <p>\u2013project-domain\u00a0\\ <p>Domain the project belongs to (name or ID). This can be used in case collisions between project names exist.</p> <p>\u2013agent\u00a0\\ <p>List routers hosted by an agent (ID only)</p> <p>\u2013tags\u00a0\\[,\\,\u2026] <p>List routers which have all given tag(s) (Comma-separated list of tags)</p> <p>\u2013any-tags\u00a0\\[,\\,\u2026] <p>List routers which have any given tag(s) (Comma-separated list of tags)</p> <p>\u2013not-tags\u00a0\\[,\\,\u2026] <p>Exclude routers which have all given tag(s) (Comma-separated list of tags)</p> <p>\u2013not-any-tags\u00a0\\[,\\,\u2026] <p>Exclude routers which have any given tag(s) (Comma-separated list of tags)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-remove-port","title":"router remove port","text":"<p>Remove a port from a router</p> <p>openstack router remove port \\ \\ <p>router</p> <p>Router from which port will be removed (name or ID)</p> <p>port</p> <p>Port to be removed and deleted (name or ID)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-remove-route","title":"router remove route","text":"<p>Remove extra static routes from a router\u2019s routing table.</p> <p>openstack router remove route     [--route destination=\\,gateway=\\]     \\ <p>\u2013route\u00a0destination=\\,gateway=\\ <p>Remove extra static route from the router. destination: destination subnet (in CIDR notation), gateway: nexthop IP address. Repeat option to remove multiple routes. Trying to remove a route that\u2019s already missing (fully, including destination and nexthop) from the routing table is allowed and is considered a successful operation.</p> <p>router</p> <p>Router from which extra static routes will be removed (name or ID).</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-remove-subnet","title":"router remove subnet","text":"<p>Remove a subnet from a router</p> <p>openstack router remove subnet \\ \\ <p>router</p> <p>Router from which the subnet will be removed (name or ID)</p> <p>subnet</p> <p>Subnet to be removed (name or ID)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-set","title":"router set","text":"<p>Set router properties</p> <p>openstack router set     [--extra-property type=\\,name=\\,value=\\]     [--name \\]     [--description \\]     [--enable | --disable]     [--distributed | --centralized]     [--route destination=\\,gateway=\\]     [--no-route]     [--ha | --no-ha]     [--external-gateway \\]     [--fixed-ip subnet=\\,ip-address=\\]     [--enable-snat | --disable-snat]     [--qos-policy \\ | --no-qos-policy]     [--tag \\]     [--no-tag]     \\ <p>\u2013extra-property\u00a0type=\\,name=\\,value=\\ <p>Additional parameters can be passed using this property. Default type of the extra property is string (\u2018str\u2019), but other types can be used as well. Available types are: \u2018dict\u2019, \u2018list\u2019, \u2018str\u2019, \u2018bool\u2019, \u2018int\u2019. In case of \u2018list\u2019 type, \u2018value\u2019 can be semicolon-separated list of values. For \u2018dict\u2019 value is semicolon-separated list of the key:value pairs.</p> <p>\u2013name\u00a0\\ <p>Set router name</p> <p>\u2013description\u00a0\\ <p>Set router description</p> <p>\u2013enabl</p> <p>Enable router</p> <p>\u2013disable</p> <p>Disable router</p> <p>\u2013distributed</p> <p>Set router to distributed mode (disabled router only)</p> <p>\u2013centralized</p> <p>Set router to centralized mode (disabled router only)</p> <p>\u2013route\u00a0destination=\\,gateway=\\ <p>Add routes to the router destination: destination subnet (in CIDR notation) gateway: nexthop IP address (repeat option to add multiple routes). This is deprecated in favor of \u2018router add/remove route\u2019 since it is prone to race conditions between concurrent clients when not used together with \u2013no-route to overwrite the current value of \u2018routes\u2019.</p> <p>\u2013no-route</p> <p>Clear routes associated with the router. Specify both \u2013route and \u2013no-route to overwrite current value of routes.</p> <p>\u2013ha</p> <p>Set the router as highly available (disabled router only)</p> <p>\u2013no-ha</p> <p>Clear high availability attribute of the router (disabled router only)</p> <p>\u2013external-gateway\u00a0\\ <p>External Network used as router\u2019s gateway (name or ID)</p> <p>\u2013fixed-ip\u00a0subnet=\\,ip-address=\\ <p>Desired IP and/or subnet (name or ID) on external gateway: subnet=\\,ip-address=\\ (repeat option to set multiple fixed IP addresses) <p>\u2013enable-snat</p> <p>Enable Source NAT on external gateway</p> <p>\u2013disable-snat</p> <p>Disable Source NAT on external gateway</p> <p>\u2013qos-policy\u00a0\\ <p>Attach QoS policy to router gateway IPs</p> <p>\u2013no-qos-policy</p> <p>Remove QoS policy from router gateway IPs</p> <p>\u2013tag\u00a0\\ <p>Tag to be added to the router (repeat option to set multiple tags)</p> <p>\u2013no-tag</p> <p>Clear tags associated with the router. Specify both \u2013tag and \u2013no-tag to overwrite current tags</p> <p>router</p> <p>Router to modify (name or ID)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-show","title":"router show","text":"<p>Display router details</p> <p>openstack router show \\ <p>router</p> <p>Router to display (name or ID)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Routers/#router-unset","title":"router unset","text":"<p>Unset router properties</p> <p>openstack router unset     [--extra-property type=\\,name=\\,value=\\]     [--route destination=\\,gateway=\\]     [--external-gateway]     [--qos-policy]     [--tag \\ | --all-tag]     \\ <p>\u2013extra-property\u00a0type=\\,name=\\,value=\\ <p>Additional parameters can be passed using this property. Default type of the extra property is string (\u2018str\u2019), but other types can be used as well. Available types are: \u2018dict\u2019, \u2018list\u2019, \u2018str\u2019, \u2018bool\u2019, \u2018int\u2019. In case of \u2018list\u2019 type, \u2018value\u2019 can be semicolon-separated list of values. For \u2018dict\u2019 value is semicolon-separated list of the key:value pairs.</p> <p>\u2013route\u00a0destination=\\,gateway=\\ <p>Routes to be removed from the router destination: destination subnet (in CIDR notation) gateway: nexthop IP address (repeat option to unset multiple routes)</p> <p>\u2013external-gateway</p> <p>Remove external gateway information from the router</p> <p>\u2013qos-policy</p> <p>Remove QoS policy from router gateway IPs</p> <p>\u2013tag\u00a0\\ <p>Tag to be removed from the router (repeat option to remove multiple tags)</p> <p>\u2013all-tag</p> <p>Clear all tags associated with the router</p> <p>router</p> <p>Router to modify (name or ID)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/SDK_Reference/","title":"SDK Reference","text":"<p>This documentation is split into three sections:</p> <ul> <li> <p>An\u00a0installation\u00a0guide</p> </li> <li> <p>A section for\u00a0users\u00a0looking to build applications which make use of OpenStack</p> </li> <li> <p>A section for those looking to\u00a0contribute\u00a0to this project</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/SDK_Reference/#installation","title":"Installation","text":"<ul> <li>Installation guide</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/SDK_Reference/#for-users","title":"For Users","text":"<ul> <li> <p>Using the OpenStack SDK</p> </li> <li> <p>User Guides</p> </li> <li> <p>API Documentation</p> </li> <li> <p>Presentations</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/SDK_Reference/#for-contributors","title":"For Contributors","text":"<ul> <li> <p>Contributing to the OpenStack SDK</p> </li> <li> <p>About the Project</p> </li> <li> <p>Contribution Mechanics</p> </li> <li> <p>Contacting the Developers</p> </li> <li> <p>Coding Standards</p> </li> <li> <p>Development Environment</p> </li> <li> <p>Testing</p> </li> <li> <p>Project Layout</p> </li> <li> <p>Adding Features</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/SDK_Reference/#general-information","title":"General Information","text":"<p>General information about the SDK including a glossary and release history.</p> <ul> <li> <p>Glossary of Terms</p> </li> <li> <p>Release Notes</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Security_Group_Rules_in_CLI/","title":"Security Group Rules in CLI","text":"<p>A\u00a0security group rule\u00a0specifies the network access rules for servers and other resources on the network.</p> <p>Compute v2, Network v2</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Security_Group_Rules_in_CLI/#security-group-rule-create","title":"security group rule create","text":"<p>Create a new security group rule</p> <p>openstack security group rule create     [--extra-property type=\\,name=\\,value=\\] <p>    [--remote-ip \\ | --remote-group \\] <p>    [--dst-port \\] <p>    [--protocol \\]     [--description \\]     [--icmp-type \\]     [--icmp-code \\]     [--ingress | --egress]     [--ethertype \\]     [--project \\]     [--project-domain \\]     \\ <p>\u2013extra-property\u00a0type=\\,name=\\,value=\\ <p>Additional parameters can be passed using this property. Default type of the extra property is string (\u2018str\u2019), but other types can be used as well. Available types are: \u2018dict\u2019, \u2018list\u2019, \u2018str\u2019, \u2018bool\u2019, \u2018int\u2019. In case of \u2018list\u2019 type, \u2018value\u2019 can be semicolon-separated list of values. For \u2018dict\u2019 value is semicolon-separated list of the key:value pairs.</p> <p>\u2013remote-ip\u00a0\\ <p>Remote IP address block (may use CIDR notation; default for IPv4 rule: 0.0.0.0/0, default for IPv6 rule: ::/0)</p> <p>\u2013remote-group\u00a0\\ <p>Remote security group (name or ID)</p> <p>\u2013dst-port\u00a0\\ <p>Destination port, may be a single port or a starting and ending port range: 137:139. Required for IP protocols TCP and UDP. Ignored for ICMP IP protocols.</p> <p>\u2013protocol\u00a0\\ <p>Network version 2:</p> <p>IP protocol (ah, dccp, egp, esp, gre, icmp, igmp, ipv6-encap, ipv6-frag, ipv6-icmp, ipv6-nonxt, ipv6-opts, ipv6-route, ospf, pgm, rsvp, sctp, tcp, udp, udplite, vrrp and integer representations [0-255] or any; default: any (all protocols))</p> <p>Compute version 2:</p> <p>IP protocol (icmp, tcp, udp; default: tcp)</p> <p>\u2013description\u00a0\\ <p>Set security group rule description</p> <p>Network version 2 only</p> <p>\u2013icmp-type\u00a0\\ <p>ICMP type for ICMP IP protocols</p> <p>Network version 2 only</p> <p>\u2013icmp-code\u00a0\\ <p>ICMP code for ICMP IP protocols</p> <p>Network version 2 only</p> <p>\u2013ingress</p> <p>Rule applies to incoming network traffic (default)</p> <p>Network version 2 only</p> <p>\u2013egress</p> <p>Rule applies to outgoing network traffic</p> <p>Network version 2 only</p> <p>\u2013ethertype\u00a0\\ <p>Ethertype of network traffic (IPv4, IPv6; default: based on IP protocol)</p> <p>Network version 2 only</p> <p>\u2013project\u00a0\\ <p>Owner\u2019s project (name or ID)</p> <p>Network version 2 only</p> <p>\u2013project-domain\u00a0\\ <p>Domain the project belongs to (name or ID). This can be used in case collisions between project names exist.</p> <p>Network version 2 only</p> <p>group</p> <p>Create rule in this security group (name or ID)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Security_Group_Rules_in_CLI/#security-group-rule-delete","title":"security group rule delete","text":"<p>Delete security group rule(s)</p> <p>openstack security group rule delete \\ [\\ ...] <p>rule</p> <p>Security group rule(s) to delete (ID only)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Security_Group_Rules_in_CLI/#security-group-rule-list","title":"security group rule list","text":"<p>List security group rules</p> <p>openstack security group rule list     [--sort-column SORT_COLUMN]     [--sort-ascending | --sort-descending]     [--protocol \\]     [--ethertype \\]     [--ingress | --egress]     [--long]     [--all-projects]     [\\] <p>\u2013sort-column\u00a0SORT_COLUMN</p> <p>specify the column(s) to sort the data (columns specified first have a priority, non-existing columns are ignored), can be repeated</p> <p>\u2013sort-ascending</p> <p>sort the column(s) in ascending order</p> <p>\u2013sort-descending</p> <p>sort the column(s) in descending order</p> <p>\u2013protocol\u00a0\\ <p>List rules by the IP protocol (ah, dhcp, egp, esp, gre, icmp, igmp, ipv6-encap, ipv6-frag, ipv6-icmp, ipv6-nonxt, ipv6-opts, ipv6-route, ospf, pgm, rsvp, sctp, tcp, udp, udplite, vrrp and integer representations [0-255] or any; default: any (all protocols))</p> <p>Network version 2 only</p> <p>\u2013ethertype\u00a0\\ <p>List rules by the Ethertype (IPv4 or IPv6)</p> <p>Network version 2 only</p> <p>\u2013ingress</p> <p>List rules applied to incoming network traffic</p> <p>Network version 2 only</p> <p>\u2013egress</p> <p>List rules applied to outgoing network traffic</p> <p>Network version 2 only</p> <p>\u2013long</p> <p>Deprecated\u00a0This argument is no longer needed</p> <p>Network version 2 only</p> <p>\u2013all-projects</p> <p>Display information from all projects (admin only)</p> <p>Compute version 2 only</p> <p>group</p> <p>List all rules in this security group (name or ID)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Security_Group_Rules_in_CLI/#security-group-rule-show","title":"security group rule show","text":"<p>Display security group rule details</p> <p>openstack security group rule show \\ <p>rule</p> <p>Security group rule to display (ID only)</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/","title":"Supported OS Images","text":"Guest Guest Bitness Host Version Host CPU Host Bitness Status Ubuntu (LTS) 64-bit Ubuntu 22.04 Intel/AMD 64-bit Supported Ubuntu (LTS) 64-bit Ubuntu 20.04 Intel/AMD 64-bit Supported Ubuntu (LTS) 64-bit Ubuntu 18.04 Intel/AMD 64-bit Supported Debian 64-bit Debian 11 Intel/AMD 64-bit Supported Debian 64-bit Debian 10 Intel/AMD 64-bit Supported Debian 64-bit Debian 9 Intel/AMD 64-bit Supported Oracle Linux Enterprise 64-bit OEL 8 Intel/AMD 64-bit Supported Oracle Linux Enterprise 64-bit OEL 7 Intel/AMD 64-bit Supported Oracle Linux Enterprise 64-bit OEL 6 Intel/AMD 64-bit Supported Windows Server 64-bit Windows Server 2022 Intel/AMD 64-bit Supported Windows Server 64-bit Windows Server 2019 Intel/AMD 64-bit Supported Windows Server 64-bit Windows Server 2016 Intel/AMD 64-bit Supported RedHat Enterprise Linux 64-bit RHEL 8 Intel/AMD 64-bit Supported RedHat Enterprise Linux 64-bit RHEL 7 Intel/AMD 64-bit Supported RedHat Enterprise Linux 64-bit Intel/AMD 64-bit Supported <p>Note All OS versions newer than the ones listed in the tables below are supported by the KVM.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#windows-family","title":"Windows Family","text":"Guest Guest bitness Host version Host CPU Host bitness Status Windows Small Business Server 2011 64 qemu-kvm-0.12.1.2-2.355.0.1.el6.centos.2 Intel 64 Integrated Windows 2012 R2 Standard 64 qemu-kvm.x86_64 10:1.5.3-60.el7_0.5 Intel 64 Integrated Windows 2012 Standard 180-days Evaluation 64 qemu-kvm-0.12.1.2-2.295.el6 Intel 64 Integrated Windows 2008 Essential Business Server 64 qemu-kvm-0.11.0 AMD 64 Integrated Windows 2008 Small Business Server 64 qemu-kvm-0.11.0 AMD 64 Integrated Windows 2008 Standard 64 qemu-kvm-0.11.0 AMD 64 Integrated Windows 2008 Standard 32 qemu-kvm-0.11.0 Intel 32 Integrated Windows 2008 R2 RTM 64 kvm-88 Intel 64 Integrated Windows 2008 R2 Standard 64 qemu-kvm-0.12.5 Intel 64 Integrated Windows 2008 R2 Datacenter 64 qemu-kvm-0.12.5 Intel 64 Integrated Windows 10 Technical Preview for Enterprise 64 qemu-kvm-1.1.2 AMD 64 Integrated Windows 10 Professional build 10240 (release) 64 qemu-kvm-2.3.0 Intel 64 Integrated Windows 8 Enterprise 64 qemu-kvm-devel-1.1.92 3.7.0-rc2 AMD 64 Integrated Windows 7 Professional (Final) 64 qemu-kvm-0.12.1.2-1 ARCH-2.6.32 AMD 64 Integrated Windows 7 RTM 32, 64 kvm-88 Intel 64 Integrated Windows 7 RC 64 kvm-72+dfsg-5 Intel 64 Integrated Windows 7 Beta 64 kvm-84 AMD 64 Integrated Windows 7 Beta 32 kvm-83 Intel 32 Integrated Windows 7 Beta 32 kvm-62 AMD 64 Integrated Windows Server 2008 (Datacenter) 64 kvm-72 Intel, AMD 64 Integrated Windows Server 2008 (Datacenter) 32 kvm-72 Intel, AMD 64, 32 Integrated Windows Vista Ultimate 64 kvm-84 Intel, AMD 64 Integrated Windows Vista Ultimate 32 kvm-84 Intel, AMD 64, 32 Integrated Windows Server 2003 R2 (Std) 32 qemu-kvm-0.11.1 Intel 64 Integrated Windows Server 2003 R2 (Std) 32 qemu-kvm-0.13 Intel 64 Integrated Windows Server 2003 x64 64 qemu-kvm-0.13 Intel, AMD 64 Integrated Windows Server 2003 x64 64 kvm-72 Intel, AMD 64 Integrated Windows Server 2003 (Enterprise) 32 kvm-72 Intel 32, 64 Integrated Windows Server 2003 (Enterprise) 32 kvm-72 AMD 64 Integrated Windows XP Pro x64 64 kvm-72 Intel, AMD 64 Integrated Windows XP Pro 32 kvm-72 Intel 32, 64 Integrated Windows XP Pro 32 qemu-kvm-0.11.0 AMD 64 Integrated Windows XP Pro 32 qemu-kvm-0.12.1.2-2.415; kernel 2.6.32-358.23.2, CentOS 6.5 Intel 64 Integrated Windows XP Pro 32 kvm-72 AMD 64 Integrated Windows 2000 Pro (SP4) 32 kvm-64 Intel, AMD 64 Integrated Windows 2000 Pro (SP4) 32 kvm-64 Intel 32 Integrated Windows 2000 Advanced Server 32 kvm-44 Intel 64 Integrated Windows NT Server 4.0 sp 1 32 kvm-72, 2.6.30 Intel 64 Integrated Windows NT Workstation 4.0 (no SP) 32 qemu-kvm 0.12.3, 2.6.33 AMD 64 Integrated Windows NT 4.0 SP6 32 kvm 1.0, 2.6.32 Intel 64 Integrated Windows NT 4.0 SP6 32 qemu-kvm 1.4.1, 2.6.32 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#linux-family-fedoraredhat-derivatives","title":"Linux Family: Fedora/RedHat Derivatives","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#centos","title":"CentOS","text":"Guest Guest bitness Host version Host CPU Host bitness Status CentOS 6.5 64 CentOS 6.5, qemu-kvm-0.12.1.2-2.415, kernel 2.6.32-358.23.2 Intel 64 Integrated CentOS 6.2 32, 64 0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated CentOS 6.1 32, 64 0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated CentOS 6.0 64 kvm-83 Intel 64 Integrated CentOS 5.8 32 qemu-kvm-0.12.1.2-2.295.el6 Intel 64 Integrated CentOS 5.6 64 kvm-83 Intel 64 Integrated CentOS 5.5 64 kvm-83-164.el5.x86_64.rpm AMD (Sempron 140) 64 Integrated CentOS 5.5 32 qemu-kvm-0.12.1.2-2.295.el6 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#enterprise-linux","title":"Enterprise Linux","text":"Guest Guest bitness Host version Host CPU Host bitness Status Enterprise Linux 5.4 / Unbreakable Linux 64 kvm-83 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#fedora","title":"Fedora","text":"Guest Guest bitness Host version Host CPU Host bitness Status Fedora 18 32, 64 qemu-kvm 0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated Fedora 17 32, 64 qemu-kvm 0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated Fedora 16 32, 64 qemu-kvm 0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated Fedora 15 64 kvm-83 Intel 64 Integrated Fedora 13 64 kvm-83 Intel 64 Integrated Fedora 11 32 kvm-87 Intel 64 Integrated Fedora 10 32, 64 kvm-85 Intel, AMD 64 Integrated Fedora 9 32, 64 kvm-85 Intel, AMD 64 Integrated Fedora 8 32, 64 kvm-85 Intel, AMD 64 Integrated Fedora 7 32, 64 kvm-75 Intel, AMD 64 Integrated Fedora 6 32 kvm-26 Intel, AMD 32, 64 Integrated Fedora 5 32, 64 kvm-12 Intel, AMD 32, 64 Integrated Fedora 3 64 kvm-78 AMD 64 Integrated Fedora 1 32 qemu-kvm-0.11.1 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#redhat-enterprise-linux","title":"RedHat Enterprise Linux","text":"Guest Guest bitness Host version Host CPU Host bitness Status RHEL6 beta 64 kvm-83 Intel 64 Integrated RHEL5 32, 64 kvm-85 Intel, AMD 64 Integrated RHEL4 32, 64 kvm-85 Intel, AMD 64 Integrated RHEL3 32, 64 kvm-85 Intel, AMD 64 Integrated Red Hat Linux 9 32 kvm-51 Intel 64 Integrated Red Hat Linux 7.3 32 kvm-78 AMD 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#scientific-linux","title":"Scientific Linux","text":"Guest Guest bitness Host version Host CPU Host bitness Status Scientific Linux 5.4.1 64 kvm-83 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#trixbox","title":"Trixbox","text":"Guest Guest bitness Host version Host CPU Host bitness Status Trixbox (CentOS) 32 kvm-12 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#linux-family-ubuntudebian-derivatives","title":"Linux Family: Ubuntu/Debian Derivatives","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#debian-gnulinux","title":"Debian GNU/Linux","text":"Guest Guest bitness Host version Host CPU Host bitness Status Debian GNU/Linux 7.0 beta (Kernel 3.4.4) 32, 64 qemu-kvm 0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated Debian GNU/Linux 6.0 64 kvm-83 Intel 64 Integrated Debian GNU/Linux 5.0 64 kvm-88 Intel, AMD 64 Integrated Debian GNU/Linux 5.0 64 kvm-83 Intel 64 Integrated Debian Lenny 5.0 64 qemu-kvm-0.12.3 Intel 64 Integrated Debian Lenny 5.0 64 qemu-kvm-0.11.1 Intel 64 Integrated Debian Lenny 5.0 32, 64 kvm-72/77 Intel 64 Integrated Debian Etch 4.0 64 kvm-72 Intel, AMD 64 Integrated Debian Etch 4.0 32 kvm-64 Intel 64, 32 Integrated Debian Sarge 3.1 32 kvm-12 Intel 32 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#ubuntu","title":"Ubuntu","text":"Guest Guest bitness Host version Host CPU Host bitness Status Ubuntu 12.10 32, 64 qemu-kvm 0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated Ubuntu 12.04 32, 64 qemu-kvm 0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated Ubuntu 11.10 32, 64 qemu-kvm 0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated Ubuntu 11.04 32, 64 qemu-kvm 0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated Ubuntu 10.10 64 kvm-83 Intel, AMD 64 Integrated Ubuntu 10.04 LTS 32, 64 kvm-84 Intel, AMD 64 Integrated Ubuntu 9.04 64 kvm-84 Intel 64 Integrated Ubuntu 8.10 32 kvm-85 Intel, AMD 32, 64 Integrated Ubuntu 8.04 LTS 32, 64 kvm-64 Intel, AMD 32, 64 Integrated Ubuntu 7.10 Server 64 kvm-60 AMD 64 Integrated Ubuntu 6.06 LTS Server 32 kvm-20 Intel 32 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#xandros","title":"Xandros","text":"Guest Guest bitness Host version Host CPU Host bitness Status Xandros 3 OCE 32 kvm-29 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#knoppix","title":"Knoppix","text":"Guest Guest bitness Host version Host CPU Host bitness Status KNOPPIX 5.1.1 32 kvm-18 Intel 64 Integrated Knoppix 5.2 32 kvm-18 Intel 32 Integrated Knoppix 6.2 32 kvm-1.12.5 AMD 32, 64 Integrated Knoppix 6.7.1 32 kvm-1.12.5 AMD 32, 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#other-linux-distros","title":"Other Linux Distros","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#android","title":"Android","text":"Guest Guest bitness Host version Host CPU Host bitness Status Android 2.2 32 kvm-83 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#suse-linux-enterprise-server","title":"SUSE Linux Enterprise Server","text":"Guest Guest bitness Host version Host CPU Host bitness Status SUSE Linux Enterprise Server 11 32, 64 kvm-88 Intel, AMD 64 Integrated SUSE Linux Enterprise Server 10 32 kvm-88 Intel, AMD 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#opensuse","title":"openSUSE","text":"Guest Guest bitness Host version Host CPU Host bitness Status openSUSE 11.3 64 kvm-83 Intel 64 Integrated openSUSE 11.2 64 kvm-83 Intel 64 Integrated openSUSE 11.1 32, 64 kvm-88 Intel, AMD 64 Integrated openSUSE 11.1 32, 64 kvm-85 Intel, AMD 64 Integrated openSUSE 11.0 32, 64 kvm-85 Intel, AMD 64 Integrated openSUSE 10.3 32 kvm-57 AMD 64 Integrated SUSE Linux 9.1 32 kvm-72 Intel/AMD 64, 32 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#slackware","title":"Slackware","text":"Guest Guest bitness Host version Host CPU Host bitness Status Slackware 12.2 32 kvm-36 Intel 32 Integrated Slackware 12 32 kvm-36 Intel 32 Integrated Slackware 11 32 kvm-15 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#qemu-puppy","title":"Qemu-Puppy","text":"Guest Guest bitness Host version Host CPU Host bitness Status Qemu-Puppy 2.01-3 32 kvm-17/18 Intel 32 Integrated Qemu-Puppy 2.13-1 32 kvm-17/18 Intel 32 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#systemrescuecd","title":"SystemRescueCD","text":"Guest Guest bitness Host version Host CPU Host bitness Status SystemRescueCD 0.3.4 32 kvm-18 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#gentoo","title":"Gentoo","text":"Guest Guest bitness Host version Host cpu Host bitness Status Gentoo 2006.1 32 kvm-24 Intel 64 Integrated Gentoo 2007.0 32 kvm-24 Intel 64 Integrated Gentoo 20100311 64 qemu-kvm-0.11.1 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#arch-linux","title":"Arch Linux","text":"Guest Guest bitness Host version Host cpu Host bitness Status Arch Linux (Duke) 32 kvm-17 AMD 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#mandrake-linux","title":"Mandrake Linux","text":"Guest Guest bitness Host version Host cpu Host bitness Status Mandrake Linux 9.2 32 kvm-51 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#crux-linux","title":"Crux Linux","text":"Guest Guest bitness Host version Host cpu Host bitness Status Crux Linux 2.5 32 kvm-82 Intel 32 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#mikrotik","title":"Mikrotik","text":"Guest Guest bitness Host version Host cpu Host bitness Status Mikrotik 5.0rc3 32 qemu-kvm-0.13.0 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#unix-family-bsd","title":"UNIX Family: BSD","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#openbsd","title":"OpenBSD","text":"Guest Guest bitness Host version Host cpu Host bitness Status OpenBSD 5.5/5.6 32 qemu 1.7 Intel with flexpriority 32/64 Integrated OpenBSD 5.5/5.6 64 qemu 1.7 Intel+AMD 64 Integrated OpenBSD 5.2 (prerelease) 32 qemu-kvm-1.0+noroms-0ubuntu14.1 Intel Core2 6400 64 Integrated OpenBSD 5.2 (prerelease) 64 qemu-kvm-1.0+noroms-0ubuntu14.1 Intel Core2 6400 64 Integrated OpenBSD 5.0 64 qemu-kvm-0.15.0, Linux 2.6.37.6 (Slackware 13.37) Intel Core2Duo E8400 64 Integrated OpenBSD 4.9 64 qemu-kvm-0.14, linux 2.6.38.4 (Fedora 15 Beta) AMD Phenom(tm) 9650 Quad-Core 64 Integrated OpenBSD 4.8 32 kvm-83, linux 2.6.18 (CentOS 5.5) Intel E5700 64 Integrated OpenBSD 4.7 64 qemu-kvm-0.12.5 Intel 64 Integrated OpenBSD 4.6 32, 64 qemu-kvm-0.11.0-0ubuntu6.3 Intel 64 Integrated OpenBSD 4.6 32 kvm-84-7.6 AMD 32 Integrated OpenBSD 4.5 64 kvm-84 Intel 64 Integrated OpenBSD 4.4 64 kvm-78 AMD 64 Integrated OpenBSD 4.2 32 kvm-58 Intel 32 Integrated OpenBSD 4.1 32 kvm-72 Intel, AMD 64 Integrated OpenBSD 4.0 64 kvm-12 Intel 64 Integrated OpenBSD 4.0 32 kvm-16 + CVS Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#netbsd","title":"NetBSD","text":"Guest Guest bitness Host version Host cpu Host bitness Status NetBSD 3.1 32 kvm-17/18 Intel 32 Integrated NetBSD 3.1 32 kvm-21 AMD 64 Integrated NetBSD 4.0 32 kvm-60 Intel 64 Integrated NetBSD 5.0.2 32 kvm from 2.6.32-2-amd64 debian kernel on debian sid both Intel and Amd 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#pc-bsd","title":"PC-BSD","text":"Guest Guest bitness Host version Host cpu Host bitness Status PC-BSD 1.4 32 kvm-56 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#darwin","title":"Darwin","text":"Guest Guest bitness Host version Host cpu Host bitness Status Darwin 8.0.1 32 kvm-29 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#dragonfly-bsd","title":"Dragonfly BSD","text":"Guest Guest bitness Host version Host cpu Host bitness Status DragonflyBSD 1.10.1 32 kvm-58 Intel 64 Integrated DragonflyBSD 2.2.1 32 kvm-85 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#unix-family-solarisopensolaris","title":"UNIX Family: Solaris/OpenSolaris","text":"Guest Guest bitness Host version Host cpu Host bitness Status Solaris 10 U1 32 kvm-12 Intel 64 Integrated Solaris 10 U3 64 kvm-58 + patch Intel 64 Integrated Oracle Solaris 10 1/13 64 qemu-kvm 2.3.1 Intel 64 Integrated Nexenta Core 1.0 64 kvm-61 Intel 64 Integrated Nexenta Core 2.0 b104 rc3 32 / 64 qemu-kvm-0.11.0 Intel 64 Integrated OpenSolaris 2008.05 64 kvm-69 Intel 64 Integrated Milax 0.3.2 32 kvm-62 Intel 64 Integrated Belenix 0.7.1 64 kvm-62 Intel 64 Integrated OpenSolaris 2008.11 64 kvm-62 Intel 64 Integrated OpenSolaris 2009.06 32, 64 qemu-kvm-0.11.0-0ubuntu6.3 Intel 64 Integrated OpenIndiana Build 151a (Desktop) 64 qemu-kvm-0.12.5+dfsg-5+squeeze Intel Core i7 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#other-unix-system","title":"Other UNIX System","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#minix","title":"Minix","text":"Guest Guest bitness Host version Host cpu Host bitness Status MINIX 3.1.2a 32 kvm-71, qemu-kvm-0.14.1+noroms-0ubuntu6.2 Intel 32, 64 Works"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#debian-gnuhurd","title":"Debian GNU/Hurd","text":"Guest Guest bitness Host version Host cpu Host bitness Status GNU hurd live CD (20051117) 32 kvm-28, qemu-kvm-0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated GNU/Hurd (Debian K16) 32 kvm-69, qemu-kvm-0.14.1+noroms-0ubuntu6.2 Intel, AMD 32, 64 Integrated GNU/Hurd (Debian K14) 32 kvm-33 Intel, AMD 32, 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#qnx","title":"QNX","text":"Guest Guest bitness Host version Host cpu Host bitness Status QNX 6.4.1 32 qemu-kvm-0.11.0, 2.6.31.5; qemu-kvm-0.14.1+noroms-0ubuntu6.2 Intel 32, 64 Integrated QNX 6.4.0 32 kvm-83, qemu-kvm-0.14.1+noroms-0ubuntu6.2 Intel 32, 64 Integrated QNX 6.3.2 32 qemu-kvm 88, host kernel: 2.6.30; qemu-kvm-0.14.1+noroms-0ubuntu6.2 Intel 32, 64 Integrated QNX 4.25 32 qemu-kvm-0.12.3, kernel 2.6.32-24 Intel 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Supported_OS_Images/#others","title":"Others","text":"Guest Guest bitness Host version Host cpu Host bitness Status Haiku OS 32 kvm-12 AMD 64 Integrated Amiga Research OS (AROS) 32 kvm-12 AMD 64 Integrated Amiga Research OS (AROS) 32 kvm-58 Intel 32 Integrated ReactOS 0.3.0 32 kvm-14 AMD 64 Integrated FreeDOS 1.0 32 kvm-71 Intel 64 Integrated MS DOS 6.22 32 kvm-88 Intel 64 Integrated MS DOS 5.0 32 kvm-62 Intel 64 Integrated Plan 9 32 kvm-68 Intel 64 Integrated FreeDOS 7 + Novell Netware 4.1 32 kvm-83 AMD 64 Integrated clonezilla-live-1.2.* \u2013 iso 32 kvm-1.12.5 AMD 32, 64 Integrated"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Architecture/","title":"Taikun OCP Architecture","text":"<p>The Taikun OCP platform built on open-source principles, is an Infrastructure as a Service (IaaS) solution catering to the management and orchestration of computing, storage, and networking resources. Its architecture comprises diverse components, each fulfilling distinct functions and offering the flexibility of independent or combined usage to deliver a comprehensive cloud experience. Operated on an Infrastructure as Code (IaC) foundation, Taikun OCP ensures consistent and automated deployment processes, streamlining cloud infrastructure management.</p> <p>Engineered for scalability and fault tolerance, Taikun OCP excels in managing large-scale cloud environments with high availability. Its distributed architecture facilitates horizontal scaling by seamlessly adding nodes to the infrastructure. Moreover, Taikun OCP incorporates various high availability features, including automatic failover and load balancing.</p> <p>Deployed as a containerized platform atop the Kubernetes orchestration platform, Taikun OCP leverages containerization and Kubernetes orchestration to optimize resource utilization, scalability, resilience, deployment simplicity, and ecosystem integration.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Architecture/#key-features-of-taikun-ocp","title":"Key features of Taikun OCP","text":"<p>\u2013 Creation and management of virtual machines (VMs) and virtualized compute resources</p> <p>\u2013 Network resource management, encompassing virtual switches, routers, and firewalls</p> <p>\u2013 Integration with diverse storage technologies, such as block storage, object storage, and file storage</p> <p>\u2013 Identity and access management facilitated by the Keystone service</p> <p>\u2013 Orchestration and automation of cloud resources via the Heat service</p> <p>\u2013 Monitoring and logging of cloud resources through the Ceilometer and Gnocchi services</p> <p>\u2013 Image management via the Glance service</p> <p>\u2013 User interface management via the Horizon dashboard</p> <p>Taikun OCP provides a comprehensive set of APIs, empowering users to programmatically manage and interact with cloud resources. By abstracting the underlying infrastructure, these APIs enable users to define and request resources tailored to their needs, thereby optimizing cloud maintenance costs and fostering competitiveness with major public cloud providers.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Architecture/#used-projects","title":"Used Projects","text":"<p>Keystone:\u00a0Serving as the identity service within Taikun OCP, Keystone offers authentication and authorization functionalities for all other services within the platform. It supports a variety of authentication methods, including username/password, token-based, and federated identity.</p> <p>Barbican:\u00a0Functioning as the key management service in Taikun OCP, Barbican facilitates secure storage, provisioning, and management of sensitive data such as cryptographic keys, passwords, and certificates. By providing a centralized and secure repository for key management, Barbican ensures the confidentiality and integrity of stored information.</p> <p>Glance:\u00a0Operating as the image service in Taikun OCP, Glance provides a catalog of virtual machine images for launching instances. Users can upload, share, and manage images through Glance, which supports various image formats like qcow2, vmdk, and raw.</p> <p>Nova:\u00a0Nova, the compute service within Taikun OCP, offers a scalable and elastic computing environment for cloud instances. Supporting various hypervisors such as KVM, Xen, and VMware, Nova provides features like live migration and auto-scaling.</p> <p>Neutron:\u00a0As the networking service in Taikun OCP, Neutron delivers network connectivity for cloud instances. Supporting different network types including flat, VLAN, and VXLAN, Neutron offers features like security groups and floating IPs.</p> <p>Cinder:\u00a0Cinder serves as the block storage service in Taikun OCP, allowing users to create and manage block storage volumes attachable to cloud instances. Cinder offers features such as snapshotting, cloning, and volume encryption.</p> <p>Manila:\u00a0Manila functions as the shared file system service in Taikun OCP, enabling users to create and manage shared file systems accessible by multiple instances simultaneously. Supporting various backends like NFS and CIFS, Manila provides features such as snapshotting and access controls.</p> <p>Designate:\u00a0Operating as the DNS service within Taikun OCP, Designate offers domain registration, management, and resolution services for cloud instances and applications. It automates the process of associating domain names with IP addresses and supports features like DNSSEC for enhanced security.</p> <p>Ironic:\u00a0Ironic serves as the bare-metal provisioning service in Taikun OCP, allowing users to provision and manage physical bare-metal servers as instances in the cloud. Particularly useful for workloads requiring direct hardware access, Ironic can be combined with other Taikun OCP services to create a hybrid cloud environment.</p> <p>Octavia:\u00a0As the load balancing service in Taikun OCP, Octavia provides scalable and reliable load-balancing solutions for distributing network traffic across multiple instances. Supporting various load-balancing algorithms and protocols, Octavia helps optimize application performance and availability by efficiently distributing traffic across the cloud infrastructure.</p> <p>Ceilometer:\u00a0Ceilometer serves as Taikun OCP\u2019s telemetry and metering service, collecting and managing cloud infrastructure metrics. It aids in monitoring, tracking resource utilization, and generating usage reports, contributing to efficient resource management within the platform.</p> <p>Horizon:\u00a0Horizon functions as the web-based dashboard for Taikun OCP, offering a graphical interface for users to manage their cloud resources including instances, volumes, and networks.</p> <p>Taikun OCP Baremetal is a platform to manage servers, storage, network devices, and rack elements via the DMTF Redfish standard 2. Taikun OCP Baremetal\u2019s focus is to remove the need to use multiple proprietary tools and platforms to manage multi-OEM infrastructure</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Architecture/#taikun-ocp-multi-site","title":"Taikun OCP Multi-site","text":"<p>OpenStack multi-site deployment involves setting up OpenStack cloud infrastructure across multiple geographical locations or data centers. This setup enables organizations to distribute resources and workloads for improved performance, resilience, and scalability. Key components include dividing the infrastructure into regions, organizing resources into availability zones within each region, deploying global services for centralized management, establishing inter-region networking for seamless communication, implementing data replication and synchronization for resilience, utilizing load balancing and traffic management for optimization, ensuring disaster recovery and high availability, and addressing security and compliance requirements. Overall, OpenStack multi-site deployments offer scalability, resilience, and flexibility to support diverse workloads across different locations.</p> <p></p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Limits/","title":"Taikun OCP Limits","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Limits/#servers-in-a-cluster","title":"Servers in a Cluster","text":"<p>The cluster boundaries define the constraints and capabilities of the OpenStack deployment within the Kubernetes environment. This encompasses the minimum and maximum limits for various components such as processing nodes, hypervisor resources, software-defined networking (SDN), and software-defined storage (SDS). These boundaries ensure efficient resource allocation and scalability within the Taikun OCP infrastructure.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Limits/#range-of-computing-servers","title":"Range of Computing servers","text":"<ul> <li>2 \u2013 300 compute servers</li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Limits/#minimum-ha-recommendation","title":"Minimum HA recommendation:","text":"<ul> <li> <p>3 control servers\u00a0</p> </li> <li> <p>3 storage servers</p> </li> <li> <p>10 compute servers\u00a0</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Limits/#os-hypervisor","title":"OS Hypervisor","text":"<p>The OS hypervisor component ensures optimal utilization of physical resources by hosting virtual machines (VMs) within the cluster. It supports a high density of VMs and provides extensive support for physical configurations at the host system level. This includes minimum specifications for RAM, virtual instances, and logical CPUs per physical host, ensuring robust performance and scalability for virtualized workloads.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Limits/#range","title":"Range","text":"<ul> <li> <p>Minimum 128 GB to 24TB RAM per OS Hypervisor</p> </li> <li> <p>Minimum 1 to 1100 virtual instances per OS Hypervisor</p> </li> <li> <p>Minimum 24 to 1024 logical CPU per OS Hypervisor</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Limits/#sdn-software-defined-networking","title":"SDN (Software-Defined Networking)","text":"<p>The SDN component enables flexible networking configurations within the OpenStack environment. It facilitates the creation of various network types, including private and public networks directly associated with physical network infrastructure. The SDN solution supports a minimum number of private and public networks, along with capabilities for VLANs, VXLANs, and multiple internal private networks, ensuring comprehensive networking functionality for diverse workload requirements.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Limits/#range_1","title":"Range","text":"<ul> <li> <p>Minimum 3 to 4070 VLANs for provider, public and private networks</p> </li> <li> <p>Minimum 1 to 16 000 000 VXLAN for multiple private network</p> </li> </ul>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Limits/#sds-software-defined-storage","title":"SDS (Software-Defined Storage)","text":"<p>The SDS solution provides scalable and resilient storage capabilities within the OpenStack deployment. It supports a minimum number of nodes in a cluster and ensures reliable storage provisioning and management for virtualized workloads. By leveraging SDS, Taikun OCP enables efficient storage utilization and dynamic scaling to meet evolving storage demands within the Kubernetes-based environment.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Taikun_OCP_Limits/#range_2","title":"Range","text":"<ul> <li> <p>Minimum 3 to 100 storage servers</p> </li> <li> <p>Minimum 3 TB to 70 PB of RAW storage</p> </li> </ul> <p>These descriptions outline the key components and capabilities within the cluster boundaries of the Taikun OCP OpenStack deployment, setting the foundation for efficient resource management and scalability in virtualized environments.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Using_Mojo_with_Taikun_OCP/","title":"Using Mojo with Taikun OCP","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Using_Mojo_with_Taikun_OCP/#available-bundles","title":"Available Bundles","text":"<p>The following OS Bundles are available from Metify. Input the values from this table into the\u00a0<code>./mojo-manage --os-image</code>\u00a0command. OS Bundles are organized by\u00a0<code>name</code>\u00a0and\u00a0<code>version</code>. The combination of these two fields is unique within the Mojo Platform.</p> Name Version Architecture Ubuntu 22.04.03 x64 RHEL 8.8-net x64 RHEL 9.2-net x64"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Using_Mojo_with_Taikun_OCP/#redfish-api-overview","title":"Redfish API Overview","text":""},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Using_Mojo_with_Taikun_OCP/#overview","title":"Overview","text":"<p>RackHD is designed to provide a REST (Representational state transfer) architecture to provide a RESTful API. RackHD currently has two RESTful interfaces: a Redfish API and native REST API 2.0.</p> <p>The Redfish API is compliant with the Redfish specification as an additional REST API. It provides a common data model for representing bare metal hardware, as an aggregate for multiple backend servers and systems.</p> <p>The REST API 2.0 provides unique features that are not provided in the Redfish API.</p>"},{"location":"Taikun_Open_Cloud_Platform/Taikun_OCP-Overview/Using_Mojo_with_Taikun_OCP/#redfish-api-example","title":"Redfish API Example","text":"<p>Redfish API \u2013 Chassis</p> <p>List the Chassis that is managed by RackHD (equivalent to the enclosure node in REST API 2.0), by running the following command.curl 127.0.0.1:9090/redfish/v1/Chassis| jq \u2018.\u2019</p> <pre><code>{\n  \"@odata.context\": \"/redfish/v1/$metadata#Systems\",\n  \"@odata.id\": \"/redfish/v1/Chassis\",\n  \"@odata.type\": \"#ChassisCollection.ChassisCollection\",\n  \"Oem\": {},\n  \"Name\": \"Chassis Collection\",\n  \"Members@odata.count\": 1,\n  \"Members\": [\n    {\n      \"@odata.id\": \"/redfish/v1/Chassis/58df2a1b83d44813084b45f9\"\n    }\n  ]\n}\n</code></pre> <p>Redfish API \u2013 System</p> <ol> <li>In the rackhd-server, list the System that is managed by RackHD (equivalent to compute node in API 2.0), by running the following command</li> </ol> <p>curl 127.0.0.1:9090/redfish/v1/Systems| jq \u2018.\u2019</p> <ol> <li>Use the mouse to select the\u00a0System-ID\u00a0as below example, then the ID will be in your clipboard. This ID will be used in the following steps.</li> </ol> <pre><code>{\n  \"@odata.context\": \"/redfish/v1/$metadata#Systems\",\n  \"@odata.id\": \"/redfish/v1/Systems\",\n  \"@odata.type\": \"#ComputerSystemCollection.ComputerSystemCollection\",\n  \"Oem\": {},\n  \"Name\": \"Computer System Collection\",\n  \"Members@odata.count\": 1,\n  \"Members\": [\n    {\n      \"@odata.id\": \"/redfish/v1/Systems/58df293a65b2761908e6b78c\"\n    }\n  ]\n}\n</code></pre> <p>Redfish API \u2013 SEL Logcurl 127.0.0.1:9090/redfish/v1/systems/\\/LogServices/Sel| jq \u2018.\u2019 <pre><code>{\n  \"@odata.context\": \"/redfish/v1/$metadata#Systems/Links/Members/58df293a65b2761908e6b78d/LogServices/Members/@entity\",\n  \"@odata.id\": \"/redfish/v1/systems/58df293a65b2761908e6b78d/LogServices/Sel\",\n  \"@odata.type\": \"#LogService.1.0.0.LogService\",\n  \"Oem\": {},\n  \"Id\": \"SEL\",\n  \"Description\": \"IPMI System Event Log\",\n  \"Name\": \"ipmi-sel-information\",\n  \"ServiceEnabled\": true,\n  \"MaxNumberOfRecords\": 0,\n  \"OverWritePolicy\": \"WrapsWhenFull\",\n  \"DateTimeLocalOffset\": \"+00:00\",\n  \"Actions\": {\n    \"Oem\": {},\n    \"#LogService.ClearLog\": {\n      \"target\": \"/api/current/node/58df293a65b2761908e6b78d/workflows?name=Graph.ClearSEL.Node\"\n    }\n  },\n  \"Status\": {},\n  \"Entries\": {\n    \"@odata.id\": \"/redfish/v1/systems/58df293a65b2761908e6b78d/LogServices/Sel/Entries\"\n  }\n}\n</code></pre> <p>Redfish API \u2013 CPU infocurl 127.0.0.1:9090/redfish/v1/Systems/\\/Processors/0| jq \u2018.\u2019 <pre><code>{\n  \"@odata.context\": \"/redfish/v1/$metadata#Systems/Links/Members/58df293a65b2761908e6b78d/Processors/Members/@entity\",\n  \"@odata.id\": \"/redfish/v1/systems/58df293a65b2761908e6b78d/Processors/0\",\n  \"@odata.type\": \"#Processor.1.0.0.Processor\",\n  \"Oem\": {},\n  \"Id\": \"0\",\n  \"Name\": \"\",\n  \"Socket\": \"SOCKET 0\",\n  \"ProcessorType\": \"CPU\",\n  \"ProcessorArchitecture\": \"x86\",\n  \"InstructionSet\": \"x86-64\",\n  \"Manufacturer\": \"Intel\",\n  \"Model\": \"Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz\",\n  \"MaxSpeedMHz\": 2300,\n  \"TotalCores\": 10,\n  \"TotalThreads\": 20,\n  \"Status\": {},\n  \"ProcessorId\": {\n    \"VendorId\": \"GenuineIntel\",\n    \"IdentificationRegisters\": \"F2 06 03 00 FF FB EB BF\",\n    \"EffectiveFamily\": \"Xeon\",\n    \"EffectiveModel\": \"Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz\",\n    \"Step\": \"\",\n    \"MicrocodeInfo\": \"\"\n  }\n}\n</code></pre> <p>Redfish API \u2013 Helper</p> <p>Show the list of RackHD Redfish APIs\u2019 by running the below command: curl 127.0.0.1:9090/redfish/v1| jq \u2018.\u2019</p> <pre><code>{\n  \"@odata.context\": \"/redfish/v1/$metadata#ServiceRoot\",\n  \"@odata.id\": \"/redfish/v1/\",\n  \"@odata.type\": \"#ServiceRoot.1.0.0.ServiceRoot\",\n  \"Oem\": {},\n  \"Id\": \"\",\n  \"Description\": \"\",\n  \"Name\": \"Root Service\",\n  \"RedfishVersion\": \"1.0.0\",\n  \"UUID\": \"423c839f-1e57-4081-b0bb-ac59ed46267f\",\n  \"Links\": {\n    \"Oem\": {},\n    \"Sessions\": {}\n  },\n  \"Systems\": {\n    \"@odata.id\": \"/redfish/v1/Systems\"\n  },\n  \"Chassis\": {\n    \"@odata.id\": \"/redfish/v1/Chassis\"\n  },\n  \"Managers\": {\n    \"@odata.id\": \"/redfish/v1/Managers\"\n  },\n  \"Tasks\": {\n    \"@odata.id\": \"/redfish/v1/TaskService\"\n  },\n  \"SessionService\": {}\n}\n</code></pre> <p>Full RedFish API documentation is available\u00a0here.</p>"}]}